package com.avcdata.spark.job.test

import com.avcdata.spark.job.until.TimeUtils
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 李晋豫广告收视率分析
  * FAQ:时间要统一 数据不能为空
  */
object AdTimeRangeShoushiJob {
  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    System.setProperty("hadoop.home.dir", "D:\\04coding\\Hadoop\\hadoop-2.5.0-cdh5.3.6");
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SearchIndexDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-01")
    sc.stop()
  }

  def run(sc: SparkContext, analysisDate: String) = {

    //////////////test/////////////////////////////////
    //    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\爬虫\\11月\\"+ analysisDate + "_index_360.csv"
    //////////////test/////////////////////////////////
    //TODO 原始广告日志格式
    //    日期,直播频道,频道修正名称,广告前节目,广告后节目,广告主品牌,广告类型,广告开始时间,广告结束时间,大行业,中行业,小行业
    val hdfsPathAd = "S:\\aowei\\data-report\\ad-shoushi\\ad.csv"

    //TODO 原始收视日志格式
    //    日期,频道,频道修正名称,0:00:00,0:01:00,0:02:00,0:03:00,0:04:00,0:05:00,0:06:00,0:07:00,0:08:00,0:09:00,0:10:00,0:11:00,0:12:00,0:13:00,0:14:00,0:15:00,0:16:00,0:17:00,0:18:00,0:19:00,0:20:00,0:21:00,0:22:00,0:23:00,0:24:00,0:01:00,0:26:00,0:27:00,0:28:00,0:29:00,0:30:00,0:31:00,0:32:00,0:33:00,0:34:00,0:35:00,0:36:00,0:37:00,0:38:00,0:39:00,0:40:00,0:41:00,0:42:00,0:43:00,0:44:00,0:45:00,0:46:00,0:47:00,0:48:00,0:49:00,0:50:00,0:51:00,0:52:00,0:53:00,0:54:00,0:55:00,0:56:00,0:57:00,0:58:00,0:59:00,1:00:00,1:01:00,1:02:00,1:03:00,1:04:00,1:05:00,1:06:00,1:07:00,1:08:00,1:09:00,1:10:00,1:11:00,1:12:00,1:13:00,1:14:00,1:15:00,1:16:00,1:17:00,1:18:00,1:19:00,1:20:00,1:21:00,1:22:00,1:23:00,1:24:00,1:25:00,1:26:00,1:27:00,1:28:00,1:29:00,1:30:00,1:31:00,1:32:00,1:33:00,1:34:00,1:35:00,1:36:00,1:37:00,1:38:00,1:39:00,1:40:00,1:41:00,1:42:00,1:43:00,1:44:00,1:45:00,1:46:00,1:47:00,1:48:00,1:49:00,1:50:00,1:51:00,1:52:00,1:53:00,1:54:00,1:55:00,1:56:00,1:57:00,1:58:00,1:59:00,02:00,02:01,02:02,02:03,02:04,02:05,02:06,02:07,02:08,02:09,02:10,02:11,02:12,02:13,02:14,02:15,02:16,02:17,02:18,02:19,02:20,02:21,02:22,02:23,02:24,02:25,02:26,02:27,02:28,02:29,02:30,02:31,02:32,02:33,02:34,02:35,02:36,02:37,02:38,02:39,02:40,02:41,02:42,02:43,02:44,02:45,02:46,02:47,02:48,02:49,02:50,02:51,02:52,02:53,02:54,02:55,02:56,02:57,02:58,02:59,03:00,03:01,03:02,03:03,03:04,03:05,03:06,03:07,03:08,03:09,03:10,03:11,03:12,03:13,03:14,03:15,03:16,03:17,03:18,03:19,03:20,03:21,03:22,03:23,03:24,03:25,03:26,03:27,03:28,03:29,03:30,03:31,03:32,03:33,03:34,03:35,03:36,03:37,03:38,03:39,03:40,03:41,03:42,03:43,03:44,03:45,03:46,03:47,03:48,03:49,03:50,03:51,03:52,03:53,03:54,03:55,03:56,03:57,03:58,03:59,04:00,04:01,04:02,04:03,04:04,04:05,04:06,04:07,04:08,04:09,04:10,04:11,04:12,04:13,04:14,04:15,04:16,04:17,04:18,04:19,04:20,04:21,04:22,04:23,04:24,04:25,04:26,04:27,04:28,04:29,04:30,04:31,04:32,04:33,04:34,04:35,04:36,04:37,04:38,04:39,04:40,04:41,04:42,04:43,04:44,04:45,04:46,04:47,04:48,04:49,04:50,04:51,04:52,04:53,04:54,04:55,04:56,04:57,04:58,04:59,05:00,05:01,05:02,05:03,05:04,05:05,05:06,05:07,05:08,05:09,05:10,05:11,05:12,05:13,05:14,05:15,05:16,05:17,05:18,05:19,05:20,05:21,05:22,05:23,05:24,05:25,05:26,05:27,05:28,05:29,05:30,05:31,05:32,05:33,05:34,05:35,05:36,05:37,05:38,05:39,05:40,05:41,05:42,05:43,05:44,05:45,05:46,05:47,05:48,05:49,05:50,05:51,05:52,05:53,05:54,05:55,05:56,05:57,05:58,05:59,06:00,06:01,06:02,06:03,06:04,06:05,06:06,06:07,06:08,06:09,06:10,06:11,06:12,06:13,06:14,06:15,06:16,06:17,06:18,06:19,06:20,06:21,06:22,06:23,06:24,06:25,06:26,06:27,06:28,06:29,06:30,06:31,06:32,06:33,06:34,06:35,06:36,06:37,06:38,06:39,06:40,06:41,06:42,06:43,06:44,06:45,06:46,06:47,06:48,06:49,06:50,06:51,06:52,06:53,06:54,06:55,06:56,06:57,06:58,06:59,07:00,07:01,07:02,07:03,07:04,07:05,07:06,07:07,07:08,07:09,07:10,07:11,07:12,07:13,07:14,07:15,07:16,07:17,07:18,07:19,07:20,07:21,07:22,07:23,07:24,07:25,07:26,07:27,07:28,07:29,07:30,07:31,07:32,07:33,07:34,07:35,07:36,07:37,07:38,07:39,07:40,07:41,07:42,07:43,07:44,07:45,07:46,07:47,07:48,07:49,07:50,07:51,07:52,07:53,07:54,07:55,07:56,07:57,07:58,07:59,08:00,08:01,08:02,08:03,08:04,08:05,08:06,08:07,08:08,08:09,08:10,08:11,08:12,08:13,08:14,08:15,08:16,08:17,08:18,08:19,08:20,08:21,08:22,08:23,08:24,08:25,08:26,08:27,08:28,08:29,08:30,08:31,08:32,08:33,08:34,08:35,08:36,08:37,08:38,08:39,08:40,08:41,08:42,08:43,08:44,08:45,08:46,08:47,08:48,08:49,08:50,08:51,08:52,08:53,08:54,08:55,08:56,08:57,08:58,08:59,09:00,09:01,09:02,09:03,09:04,09:05,09:06,09:07,09:08,09:09,09:10,09:11,09:12,09:13,09:14,09:15,09:16,09:17,09:18,09:19,09:20,09:21,09:22,09:23,09:24,09:25,09:26,09:27,09:28,09:29,09:30,09:31,09:32,09:33,09:34,09:35,09:36,09:37,09:38,09:39,09:40,09:41,09:42,09:43,09:44,09:45,09:46,09:47,09:48,09:49,09:50,09:51,09:52,09:53,09:54,09:55,09:56,09:57,09:58,09:59,10:00,10:01,10:02,10:03,10:04,10:05,10:06,10:07,10:08,10:09,10:10,10:11,10:12,10:13,10:14,10:15,10:16,10:17,10:18,10:19,10:20,10:21,10:22,10:23,10:24,10:25,10:26,10:27,10:28,10:29,10:30,10:31,10:32,10:33,10:34,10:35,10:36,10:37,10:38,10:39,10:40,10:41,10:42,10:43,10:44,10:45,10:46,10:47,10:48,10:49,10:50,10:51,10:52,10:53,10:54,10:55,10:56,10:57,10:58,10:59,11:00,11:01,11:02,11:03,11:04,11:05,11:06,11:07,11:08,11:09,11:10,11:11,11:12,11:13,11:14,11:15,11:16,11:17,11:18,11:19,11:20,11:21,11:22,11:23,11:24,11:25,11:26,11:27,11:28,11:29,11:30,11:31,11:32,11:33,11:34,11:35,11:36,11:37,11:38,11:39,11:40,11:41,11:42,11:43,11:44,11:45,11:46,11:47,11:48,11:49,11:50,11:51,11:52,11:53,11:54,11:55,11:56,11:57,11:58,11:59,12:00,12:01,12:02,12:03,12:04,12:05,12:06,12:07,12:08,12:09,12:10,12:11,12:12,12:13,12:14,12:15,12:16,12:17,12:18,12:19,12:20,12:21,12:22,12:23,12:24,12:25,12:26,12:27,12:28,12:29,12:30,12:31,12:32,12:33,12:34,12:35,12:36,12:37,12:38,12:39,12:40,12:41,12:42,12:43,12:44,12:45,12:46,12:47,12:48,12:49,12:50,12:51,12:52,12:53,12:54,12:55,12:56,12:57,12:58,12:59,13:00,13:01,13:02,13:03,13:04,13:05,13:06,13:07,13:08,13:09,13:10,13:11,13:12,13:13,13:14,13:15,13:16,13:17,13:18,13:19,13:20,13:21,13:22,13:23,13:24,13:25,13:26,13:27,13:28,13:29,13:30,13:31,13:32,13:33,13:34,13:35,13:36,13:37,13:38,13:39,13:40,13:41,13:42,13:43,13:44,13:45,13:46,13:47,13:48,13:49,13:50,13:51,13:52,13:53,13:54,13:55,13:56,13:57,13:58,13:59,14:00,14:01,14:02,14:03,14:04,14:05,14:06,14:07,14:08,14:09,14:10,14:11,14:12,14:13,14:14,14:15,14:16,14:17,14:18,14:19,14:20,14:21,14:22,14:23,14:24,14:25,14:26,14:27,14:28,14:29,14:30,14:31,14:32,14:33,14:34,14:35,14:36,14:37,14:38,14:39,14:40,14:41,14:42,14:43,14:44,14:45,14:46,14:47,14:48,14:49,14:50,14:51,14:52,14:53,14:54,14:55,14:56,14:57,14:58,14:59,15:00,15:01,15:02,15:03,15:04,15:05,15:06,15:07,15:08,15:09,15:10,15:11,15:12,15:13,15:14,15:15,15:16,15:17,15:18,15:19,15:20,15:21,15:22,15:23,15:24,15:25,15:26,15:27,15:28,15:29,15:30,15:31,15:32,15:33,15:34,15:35,15:36,15:37,15:38,15:39,15:40,15:41,15:42,15:43,15:44,15:45,15:46,15:47,15:48,15:49,15:50,15:51,15:52,15:53,15:54,15:55,15:56,15:57,15:58,15:59,16:00,16:01,16:02,16:03,16:04,16:05,16:06,16:07,16:08,16:09,16:10,16:11,16:12,16:13,16:14,16:15,16:16,16:17,16:18,16:19,16:20,16:21,16:22,16:23,16:24,16:25,16:26,16:27,16:28,16:29,16:30,16:31,16:32,16:33,16:34,16:35,16:36,16:37,16:38,16:39,16:40,16:41,16:42,16:43,16:44,16:45,16:46,16:47,16:48,16:49,16:50,16:51,16:52,16:53,16:54,16:55,16:56,16:57,16:58,16:59,17:00,17:01,17:02,17:03,17:04,17:05,17:06,17:07,17:08,17:09,17:10,17:11,17:12,17:13,17:14,17:15,17:16,17:17,17:18,17:19,17:20,17:21,17:22,17:23,17:24,17:25,17:26,17:27,17:28,17:29,17:30,17:31,17:32,17:33,17:34,17:35,17:36,17:37,17:38,17:39,17:40,17:41,17:42,17:43,17:44,17:45,17:46,17:47,17:48,17:49,17:50,17:51,17:52,17:53,17:54,17:55,17:56,17:57,17:58,17:59,18:00,18:01,18:02,18:03,18:04,18:05,18:06,18:07,18:08,18:09,18:10,18:11,18:12,18:13,18:14,18:15,18:16,18:17,18:18,18:19,18:20,18:21,18:22,18:23,18:24,18:25,18:26,18:27,18:28,18:29,18:30,18:31,18:32,18:33,18:34,18:35,18:36,18:37,18:38,18:39,18:40,18:41,18:42,18:43,18:44,18:45,18:46,18:47,18:48,18:49,18:50,18:51,18:52,18:53,18:54,18:55,18:56,18:57,18:58,18:59,19:00,19:01,19:02,19:03,19:04,19:05,19:06,19:07,19:08,19:09,19:10,19:11,19:12,19:13,19:14,19:15,19:16,19:17,19:18,19:19,19:20,19:21,19:22,19:23,19:24,19:25,19:26,19:27,19:28,19:29,19:30,19:31,19:32,19:33,19:34,19:35,19:36,19:37,19:38,19:39,19:40,19:41,19:42,19:43,19:44,19:45,19:46,19:47,19:48,19:49,19:50,19:51,19:52,19:53,19:54,19:55,19:56,19:57,19:58,19:59,20:00,20:01,20:02,20:03,20:04,20:05,20:06,20:07,20:08,20:09,20:10,20:11,20:12,20:13,20:14,20:15,20:16,20:17,20:18,20:19,20:20,20:21,20:22,20:23,20:24,20:25,20:26,20:27,20:28,20:29,20:30,20:31,20:32,20:33,20:34,20:35,20:36,20:37,20:38,20:39,20:40,20:41,20:42,20:43,20:44,20:45,20:46,20:47,20:48,20:49,20:50,20:51,20:52,20:53,20:54,20:55,20:56,20:57,20:58,20:59,21:00,21:01,21:02,21:03,21:04,21:05,21:06,21:07,21:08,21:09,21:10,21:11,21:12,21:13,21:14,21:15,21:16,21:17,21:18,21:19,21:20,21:21,21:22,21:23,21:24,21:25,21:26,21:27,21:28,21:29,21:30,21:31,21:32,21:33,21:34,21:35,21:36,21:37,21:38,21:39,21:40,21:41,21:42,21:43,21:44,21:45,21:46,21:47,21:48,21:49,21:50,21:51,21:52,21:53,21:54,21:55,21:56,21:57,21:58,21:59,22:00,22:01,22:02,22:03,22:04,22:05,22:06,22:07,22:08,22:09,22:10,22:11,22:12,22:13,22:14,22:15,22:16,22:17,22:18,22:19,22:20,22:21,22:22,22:23,22:24,22:25,22:26,22:27,22:28,22:29,22:30,22:31,22:32,22:33,22:34,22:35,22:36,22:37,22:38,22:39,22:40,22:41,22:42,22:43,22:44,22:45,22:46,22:47,22:48,22:49,22:50,22:51,22:52,22:53,22:54,22:55,22:56,22:57,22:58,22:59,23:00,23:01,23:02,23:03,23:04,23:05,23:06,23:07,23:08,23:09,23:10,23:11,23:12,23:13,23:14,23:15,23:16,23:17,23:18,23:19,23:20,23:21,23:22,23:23,23:24,23:25,23:26,23:27,23:28,23:29,23:30,23:31,23:32,23:33,23:34,23:35,23:36,23:37,23:38,23:39,23:40,23:41,23:42,23:43,23:44,23:45,23:46,23:47,23:48,23:49,23:50,23:51,23:52,23:53,23:54,23:55,23:56,23:57,23:58,23:59

    val adRDD = sc.textFile(hdfsPathAd).distinct()

      //        .filter(line=>{
      //          val cols = line.split(",")
      //         cols.length<11
      //        })

      .map(line => {
      val cols = line.split(",")
      val date = cols(0).trim
      val channel = cols(2).trim
      val prePg = cols(3)
      val proPg = cols(4)
      val brand = cols(5)
      val adType = cols(6)
      //      val startTime = cols(7)
      //      val endTime = cols(8)

      val startTime = TimeUtils.addZero(cols(7).split(":")(0)) + ":" + cols(7).split(":")(1)
      val endTime = TimeUtils.addZero(cols(8).split(":")(0)) + ":" + cols(8).split(":")(1)
      val bigHY = cols(9)
      var middleHY = "\\N"
      var minHY = "\\N"
      if (cols.length == 12) {
        middleHY = cols(10)
        minHY = cols(11)
      }
      (date + "," + channel, startTime + "," + endTime + "," + prePg + "," + proPg + "," + brand + "," + adType + "," + bigHY + "," + middleHY + "," + minHY)
    })

    //      .foreach(println(_))

    //TODO 收视数据
    val hdfsPathShouShi = "S:\\aowei\\data-report\\ad-shoushi\\shoushi.csv"
    val timeArr = Array(
      "00:00", "00:01", "00:02", "00:03", "00:04", "00:05", "00:06", "00:07", "00:08", "00:09", "00:10", "00:11", "00:12", "00:13", "00:14", "00:15", "00:16", "00:17", "00:18", "00:19", "00:20", "00:21", "00:22", "00:23", "00:24", "00:25", "00:26", "00:27", "00:28", "00:29", "00:30", "00:31", "00:32", "00:33", "00:34", "00:35", "00:36", "00:37", "00:38", "00:39", "00:40", "00:41", "00:42", "00:43", "00:44", "00:45", "00:46", "00:47", "00:48", "00:49", "00:50", "00:51", "00:52", "00:53", "00:54", "00:55", "00:56", "00:57", "00:58", "00:59", "01:00", "01:01", "01:02", "01:03", "01:04", "01:05", "01:06", "01:07", "01:08", "01:09", "01:10", "01:11", "01:12", "01:13", "01:14", "01:15", "01:16", "01:17", "01:18", "01:19", "01:20", "01:21", "01:22", "01:23", "01:24", "01:25", "01:26", "01:27", "01:28", "01:29", "01:30", "01:31", "01:32", "01:33", "01:34", "01:35", "01:36", "01:37", "01:38", "01:39", "01:40", "01:41", "01:42", "01:43", "01:44", "01:45", "01:46", "01:47", "01:48", "01:49", "01:50", "01:51", "01:52", "01:53", "01:54", "01:55", "01:56", "01:57", "01:58", "01:59", "02:00", "02:01", "02:02", "02:03", "02:04", "02:05", "02:06", "02:07", "02:08", "02:09", "02:10", "02:11", "02:12", "02:13", "02:14", "02:15", "02:16", "02:17", "02:18", "02:19", "02:20", "02:21", "02:22", "02:23", "02:24", "02:25", "02:26", "02:27", "02:28", "02:29", "02:30", "02:31", "02:32", "02:33", "02:34", "02:35", "02:36", "02:37", "02:38", "02:39", "02:40", "02:41", "02:42", "02:43", "02:44", "02:45", "02:46", "02:47", "02:48", "02:49", "02:50", "02:51", "02:52", "02:53", "02:54", "02:55", "02:56", "02:57", "02:58", "02:59", "03:00", "03:01", "03:02", "03:03", "03:04", "03:05", "03:06", "03:07", "03:08", "03:09", "03:10", "03:11", "03:12", "03:13", "03:14", "03:15", "03:16", "03:17", "03:18", "03:19", "03:20", "03:21", "03:22", "03:23", "03:24", "03:25", "03:26", "03:27", "03:28", "03:29", "03:30", "03:31", "03:32", "03:33", "03:34", "03:35", "03:36", "03:37", "03:38", "03:39", "03:40", "03:41", "03:42", "03:43", "03:44", "03:45", "03:46", "03:47", "03:48", "03:49", "03:50", "03:51", "03:52", "03:53", "03:54", "03:55", "03:56", "03:57", "03:58", "03:59", "04:00", "04:01", "04:02", "04:03", "04:04", "04:05", "04:06", "04:07", "04:08", "04:09", "04:10", "04:11", "04:12", "04:13", "04:14", "04:15", "04:16", "04:17", "04:18", "04:19", "04:20", "04:21", "04:22", "04:23", "04:24", "04:25", "04:26", "04:27", "04:28", "04:29", "04:30", "04:31", "04:32", "04:33", "04:34", "04:35", "04:36", "04:37", "04:38", "04:39", "04:40", "04:41", "04:42", "04:43", "04:44", "04:45", "04:46", "04:47", "04:48", "04:49", "04:50", "04:51", "04:52", "04:53", "04:54", "04:55", "04:56", "04:57", "04:58", "04:59", "05:00", "05:01", "05:02", "05:03", "05:04", "05:05", "05:06", "05:07", "05:08", "05:09", "05:10", "05:11", "05:12", "05:13", "05:14", "05:15", "05:16", "05:17", "05:18", "05:19", "05:20", "05:21", "05:22", "05:23", "05:24", "05:25", "05:26", "05:27", "05:28", "05:29", "05:30", "05:31", "05:32", "05:33", "05:34", "05:35", "05:36", "05:37", "05:38", "05:39", "05:40", "05:41", "05:42", "05:43", "05:44", "05:45", "05:46", "05:47", "05:48", "05:49", "05:50", "05:51", "05:52", "05:53", "05:54", "05:55", "05:56", "05:57", "05:58", "05:59", "06:00", "06:01", "06:02", "06:03", "06:04", "06:05", "06:06", "06:07", "06:08", "06:09", "06:10", "06:11", "06:12", "06:13", "06:14", "06:15", "06:16", "06:17", "06:18", "06:19", "06:20", "06:21", "06:22", "06:23", "06:24", "06:25", "06:26", "06:27", "06:28", "06:29", "06:30", "06:31", "06:32", "06:33", "06:34", "06:35", "06:36", "06:37", "06:38", "06:39", "06:40", "06:41", "06:42", "06:43", "06:44", "06:45", "06:46", "06:47", "06:48", "06:49", "06:50", "06:51", "06:52", "06:53", "06:54", "06:55", "06:56", "06:57", "06:58", "06:59", "07:00", "07:01", "07:02", "07:03", "07:04", "07:05", "07:06", "07:07", "07:08", "07:09", "07:10", "07:11", "07:12", "07:13", "07:14", "07:15", "07:16", "07:17", "07:18", "07:19", "07:20", "07:21", "07:22", "07:23", "07:24", "07:25", "07:26", "07:27", "07:28", "07:29", "07:30", "07:31", "07:32", "07:33", "07:34", "07:35", "07:36", "07:37", "07:38", "07:39", "07:40", "07:41", "07:42", "07:43", "07:44", "07:45", "07:46", "07:47", "07:48", "07:49", "07:50", "07:51", "07:52", "07:53", "07:54", "07:55", "07:56", "07:57", "07:58", "07:59", "08:00", "08:01", "08:02", "08:03", "08:04", "08:05", "08:06", "08:07", "08:08", "08:09", "08:10", "08:11", "08:12", "08:13", "08:14", "08:15", "08:16", "08:17", "08:18", "08:19", "08:20", "08:21", "08:22", "08:23", "08:24", "08:25", "08:26", "08:27", "08:28", "08:29", "08:30", "08:31", "08:32", "08:33", "08:34", "08:35", "08:36", "08:37", "08:38", "08:39", "08:40", "08:41", "08:42", "08:43", "08:44", "08:45", "08:46", "08:47", "08:48", "08:49", "08:50", "08:51", "08:52", "08:53", "08:54", "08:55", "08:56", "08:57", "08:58", "08:59", "09:00", "09:01", "09:02", "09:03", "09:04", "09:05", "09:06", "09:07", "09:08", "09:09", "09:10", "09:11", "09:12", "09:13", "09:14", "09:15", "09:16", "09:17", "09:18", "09:19", "09:20", "09:21", "09:22", "09:23", "09:24", "09:25", "09:26", "09:27", "09:28", "09:29", "09:30", "09:31", "09:32", "09:33", "09:34", "09:35", "09:36", "09:37", "09:38", "09:39", "09:40", "09:41", "09:42", "09:43", "09:44", "09:45", "09:46", "09:47", "09:48", "09:49", "09:50", "09:51", "09:52", "09:53", "09:54", "09:55", "09:56", "09:57", "09:58", "09:59", "10:00", "10:01", "10:02", "10:03", "10:04", "10:05", "10:06", "10:07", "10:08", "10:09", "10:10", "10:11", "10:12", "10:13", "10:14", "10:15", "10:16", "10:17", "10:18", "10:19", "10:20", "10:21", "10:22", "10:23", "10:24", "10:25", "10:26", "10:27", "10:28", "10:29", "10:30", "10:31", "10:32", "10:33", "10:34", "10:35", "10:36", "10:37", "10:38", "10:39", "10:40", "10:41", "10:42", "10:43", "10:44", "10:45", "10:46", "10:47", "10:48", "10:49", "10:50", "10:51", "10:52", "10:53", "10:54", "10:55", "10:56", "10:57", "10:58", "10:59", "11:00", "11:01", "11:02", "11:03", "11:04", "11:05", "11:06", "11:07", "11:08", "11:09", "11:10", "11:11", "11:12", "11:13", "11:14", "11:15", "11:16", "11:17", "11:18", "11:19", "11:20", "11:21", "11:22", "11:23", "11:24", "11:25", "11:26", "11:27", "11:28", "11:29", "11:30", "11:31", "11:32", "11:33", "11:34", "11:35", "11:36", "11:37", "11:38", "11:39", "11:40", "11:41", "11:42", "11:43", "11:44", "11:45", "11:46", "11:47", "11:48", "11:49", "11:50", "11:51", "11:52", "11:53", "11:54", "11:55", "11:56", "11:57", "11:58", "11:59", "12:00", "12:01", "12:02", "12:03", "12:04", "12:05", "12:06", "12:07", "12:08", "12:09", "12:10", "12:11", "12:12", "12:13", "12:14", "12:15", "12:16", "12:17", "12:18", "12:19", "12:20", "12:21", "12:22", "12:23", "12:24", "12:25", "12:26", "12:27", "12:28", "12:29", "12:30", "12:31", "12:32", "12:33", "12:34", "12:35", "12:36", "12:37", "12:38", "12:39", "12:40", "12:41", "12:42", "12:43", "12:44", "12:45", "12:46", "12:47", "12:48", "12:49", "12:50", "12:51", "12:52", "12:53", "12:54", "12:55", "12:56", "12:57", "12:58", "12:59", "13:00", "13:01", "13:02", "13:03", "13:04", "13:05", "13:06", "13:07", "13:08", "13:09", "13:10", "13:11", "13:12", "13:13", "13:14", "13:15", "13:16", "13:17", "13:18", "13:19", "13:20", "13:21", "13:22", "13:23", "13:24", "13:25", "13:26", "13:27", "13:28", "13:29", "13:30", "13:31", "13:32", "13:33", "13:34", "13:35", "13:36", "13:37", "13:38", "13:39", "13:40", "13:41", "13:42", "13:43", "13:44", "13:45", "13:46", "13:47", "13:48", "13:49", "13:50", "13:51", "13:52", "13:53", "13:54", "13:55", "13:56", "13:57", "13:58", "13:59", "14:00", "14:01", "14:02", "14:03", "14:04", "14:05", "14:06", "14:07", "14:08", "14:09", "14:10", "14:11", "14:12", "14:13", "14:14", "14:15", "14:16", "14:17", "14:18", "14:19", "14:20", "14:21", "14:22", "14:23", "14:24", "14:25", "14:26", "14:27", "14:28", "14:29", "14:30", "14:31", "14:32", "14:33", "14:34", "14:35", "14:36", "14:37", "14:38", "14:39", "14:40", "14:41", "14:42", "14:43", "14:44", "14:45", "14:46", "14:47", "14:48", "14:49", "14:50", "14:51", "14:52", "14:53", "14:54", "14:55", "14:56", "14:57", "14:58", "14:59", "15:00", "15:01", "15:02", "15:03", "15:04", "15:05", "15:06", "15:07", "15:08", "15:09", "15:10", "15:11", "15:12", "15:13", "15:14", "15:15", "15:16", "15:17", "15:18", "15:19", "15:20", "15:21", "15:22", "15:23", "15:24", "15:25", "15:26", "15:27", "15:28", "15:29", "15:30", "15:31", "15:32", "15:33", "15:34", "15:35", "15:36", "15:37", "15:38", "15:39", "15:40", "15:41", "15:42", "15:43", "15:44", "15:45", "15:46", "15:47", "15:48", "15:49", "15:50", "15:51", "15:52", "15:53", "15:54", "15:55", "15:56", "15:57", "15:58", "15:59", "16:00", "16:01", "16:02", "16:03", "16:04", "16:05", "16:06", "16:07", "16:08", "16:09", "16:10", "16:11", "16:12", "16:13", "16:14", "16:15", "16:16", "16:17", "16:18", "16:19", "16:20", "16:21", "16:22", "16:23", "16:24", "16:25", "16:26", "16:27", "16:28", "16:29", "16:30", "16:31", "16:32", "16:33", "16:34", "16:35", "16:36", "16:37", "16:38", "16:39", "16:40", "16:41", "16:42", "16:43", "16:44", "16:45", "16:46", "16:47", "16:48", "16:49", "16:50", "16:51", "16:52", "16:53", "16:54", "16:55", "16:56", "16:57", "16:58", "16:59", "17:00", "17:01", "17:02", "17:03", "17:04", "17:05", "17:06", "17:07", "17:08", "17:09", "17:10", "17:11", "17:12", "17:13", "17:14", "17:15", "17:16", "17:17", "17:18", "17:19", "17:20", "17:21", "17:22", "17:23", "17:24", "17:25", "17:26", "17:27", "17:28", "17:29", "17:30", "17:31", "17:32", "17:33", "17:34", "17:35", "17:36", "17:37", "17:38", "17:39", "17:40", "17:41", "17:42", "17:43", "17:44", "17:45", "17:46", "17:47", "17:48", "17:49", "17:50", "17:51", "17:52", "17:53", "17:54", "17:55", "17:56", "17:57", "17:58", "17:59", "18:00", "18:01", "18:02", "18:03", "18:04", "18:05", "18:06", "18:07", "18:08", "18:09", "18:10", "18:11", "18:12", "18:13", "18:14", "18:15", "18:16", "18:17", "18:18", "18:19", "18:20", "18:21", "18:22", "18:23", "18:24", "18:25", "18:26", "18:27", "18:28", "18:29", "18:30", "18:31", "18:32", "18:33", "18:34", "18:35", "18:36", "18:37", "18:38", "18:39", "18:40", "18:41", "18:42", "18:43", "18:44", "18:45", "18:46", "18:47", "18:48", "18:49", "18:50", "18:51", "18:52", "18:53", "18:54", "18:55", "18:56", "18:57", "18:58", "18:59", "19:00", "19:01", "19:02", "19:03", "19:04", "19:05", "19:06", "19:07", "19:08", "19:09", "19:10", "19:11", "19:12", "19:13", "19:14", "19:15", "19:16", "19:17", "19:18", "19:19", "19:20", "19:21", "19:22", "19:23", "19:24", "19:25", "19:26", "19:27", "19:28", "19:29", "19:30", "19:31", "19:32", "19:33", "19:34", "19:35", "19:36", "19:37", "19:38", "19:39", "19:40", "19:41", "19:42", "19:43", "19:44", "19:45", "19:46", "19:47", "19:48", "19:49", "19:50", "19:51", "19:52", "19:53", "19:54", "19:55", "19:56", "19:57", "19:58", "19:59", "20:00", "20:01", "20:02", "20:03", "20:04", "20:05", "20:06", "20:07", "20:08", "20:09", "20:10", "20:11", "20:12", "20:13", "20:14", "20:15", "20:16", "20:17", "20:18", "20:19", "20:20", "20:21", "20:22", "20:23", "20:24", "20:25", "20:26", "20:27", "20:28", "20:29", "20:30", "20:31", "20:32", "20:33", "20:34", "20:35", "20:36", "20:37", "20:38", "20:39", "20:40", "20:41", "20:42", "20:43", "20:44", "20:45", "20:46", "20:47", "20:48", "20:49", "20:50", "20:51", "20:52", "20:53", "20:54", "20:55", "20:56", "20:57", "20:58", "20:59", "21:00", "21:01", "21:02", "21:03", "21:04", "21:05", "21:06", "21:07", "21:08", "21:09", "21:10", "21:11", "21:12", "21:13", "21:14", "21:15", "21:16", "21:17", "21:18", "21:19", "21:20", "21:21", "21:22", "21:23", "21:24", "21:25", "21:26", "21:27", "21:28", "21:29", "21:30", "21:31", "21:32", "21:33", "21:34", "21:35", "21:36", "21:37", "21:38", "21:39", "21:40", "21:41", "21:42", "21:43", "21:44", "21:45", "21:46", "21:47", "21:48", "21:49", "21:50", "21:51", "21:52", "21:53", "21:54", "21:55", "21:56", "21:57", "21:58", "21:59", "22:00", "22:01", "22:02", "22:03", "22:04", "22:05", "22:06", "22:07", "22:08", "22:09", "22:10", "22:11", "22:12", "22:13", "22:14", "22:15", "22:16", "22:17", "22:18", "22:19", "22:20", "22:21", "22:22", "22:23", "22:24", "22:25", "22:26", "22:27", "22:28", "22:29", "22:30", "22:31", "22:32", "22:33", "22:34", "22:35", "22:36", "22:37", "22:38", "22:39", "22:40", "22:41", "22:42", "22:43", "22:44", "22:45", "22:46", "22:47", "22:48", "22:49", "22:50", "22:51", "22:52", "22:53", "22:54", "22:55", "22:56", "22:57", "22:58", "22:59", "23:00", "23:01", "23:02", "23:03", "23:04", "23:05", "23:06", "23:07", "23:08", "23:09", "23:10", "23:11", "23:12", "23:13", "23:14", "23:15", "23:16", "23:17", "23:18", "23:19", "23:20", "23:21", "23:22", "23:23", "23:24", "23:25", "23:26", "23:27", "23:28", "23:29", "23:30", "23:31", "23:32", "23:33", "23:34", "23:35", "23:36", "23:37", "23:38", "23:39", "23:40", "23:41", "23:42", "23:43", "23:44", "23:45", "23:46", "23:47", "23:48", "23:49", "23:50", "23:51", "23:52", "23:53", "23:54", "23:55", "23:56", "23:57", "23:58", "23:59"
    )

    val ssRDD = sc.textFile(hdfsPathShouShi).distinct()
      //日期,直播频道,频道修正名称,广告前节目,广告后节目,广告主品牌,广告类型,广告开始时间,广告结束时间,大行业,中行业,小行业
      .map(line => {
      val cols = line.split(",")
      val date = cols(0).trim
      val channel = cols(2).trim
      var time2SSMap: Map[String, String] = Map()
      for (i <- 0 until timeArr.length) {
        //            println(date + "," + channel + "#############"+timeArr(i)+"-->"+cols(3+i))
        if (cols(3 + i).isEmpty) {
          time2SSMap += (timeArr(i) -> "0")
        } else {
          time2SSMap += (timeArr(i) -> cols(3 + i))
        }
      }

      (date + "," + channel, time2SSMap)
    })


    //TODO  关联
    adRDD.join(ssRDD)
      .map(line => {

        val dateChannel = line._1
        val otherInfo = line._2._1
        val cols = otherInfo.split(",")
        val startTime = cols(0)
        val endTime = cols(1)
        val time2SSMap = line._2._2

        val startHourStr = startTime.split(":")(0)
        val startHourNum = TimeUtils.delZero(startTime.split(":")(0)).toInt
        val startMinNum = TimeUtils.delZero(startTime.split(":")(1)).toInt

        val endHourStr = endTime.split(":")(0)
        val endHourNum = TimeUtils.delZero(endTime.split(":")(0)).toInt
        val endMinNum = TimeUtils.delZero(endTime.split(":")(1)).toInt

        var timeArrOFRange = ArrayBuffer[String]()

        if (startHourNum == endHourNum) {
          for (i <- startMinNum to endMinNum) {
            timeArrOFRange += (startHourStr + ":" + TimeUtils.addZero(i.toString))
          }
        }

        if ((startHourNum + 1) == endHourNum) {
          for (i <- startMinNum to 59) {
            timeArrOFRange += (startHourStr + ":" + TimeUtils.addZero(i.toString))
          }

          for (i <- 0 to endHourNum) {
            timeArrOFRange += (endHourStr + ":" + TimeUtils.addZero(i.toString))
          }

        }

        //      var adTime2ShoushiMap = Map[String, String]()
        //      for (ele <- timeArrOFRange) {
        //        val shoushi = time2SSMap.get(ele)
        //        adTime2ShoushiMap += (ele -> shoushi)
        //      }

        var shoushiArr = ArrayBuffer[Double]()
        for (ele <- timeArrOFRange) {
          val shoushi = time2SSMap.get(ele.trim)
          if (!shoushi.isEmpty) {
            shoushiArr += shoushi.get.toDouble
          }
          //          else{
          //            shoushiArr += 0
          //          }
        }

        (dateChannel, otherInfo, shoushiArr)
      })


      //TODO 过滤
      .filter(line => {
      //      val cols = line.split(",")
      val shoushiArr = line._3

      shoushiArr.length != 0 && (shoushiArr != null)
    })
      .map(line => {
        val shoushiArr = line._3

        //        var max = 0
        //        var sum = 0
        //        for (ele <- shoushiArr) {
        //          if (max < ele) max = ele
        //          sum += ele
        //        }


        line._1 + "," + line._2 + "," + shoushiArr.max + "," + shoushiArr.sum / shoushiArr.size
      }).repartition(1).saveAsTextFile("out.txt")


    //TODO   打印出结果

  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.common.Codearea
import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define terminal COOCAA样本库导入 一个月
  */
object Apk2SampleTerminalLoadJob {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {

    val sqlContext = new HiveContext(sc)
    sqlContext.sql("use hr")
    sqlContext.sql("select * from hr.terminal where brand = 'CC'")
      //    val terminalDF = sqlContext.sql("select * from hr.terminal where brand = 'CC'")
      //    terminalDF.write.mode(SaveMode.Append).insertInto("hr.sample_terminal")

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("sample_terminal")

      //res0: org.apache.spark.sql.DataFrame = [key: string, sn: string, brand: string, last_poweron: string, area: string, province: string, city: string, citylevel: string, size: string, model: string, license: string]

      try {

        items.foreach(line => {

          //牌照
          val license = line(10)

          var province = line(5).toString

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""

          //型号
          val model = line(9)
          val size = line(8)
          val city = line(6).toString

          val sn = line(1)

          //大区
          val area = line(4)

          //城市级别
          val citylevel = line(7)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SampleTerminal("CC", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将apk的数据导入到apk分区表
  */
object ApkData2Partition {
  def main(args: Array[String]) {
    val arr = "#,#".split(",")
    println(arr.length)
  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    //加载当天的数据
//    val sql = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + currentDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + currentDate + "'"


    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate,TimeUtils.DAY_DATE_FORMAT_ONE,-2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime,TimeUtils.DAY_DATE_FORMAT_ONE)

    val sql = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + currentDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + currentDate + "'"
//
    sqlContext.sql(sql)

    println(sql)

//    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")



  }

}
package com.avcdata.vbox.clean.apk

import com.avcdata.vbox.util.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将apk的数据导入到apk分区表
  */
object ApkData2Partition {
  def main(args: Array[String]) {
    val arr = "#,#".split(",")
    println(arr.length)
  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    //加载当天的数据
//    val sql = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + currentDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + currentDate + "'"


    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate,TimeUtils.DAY_DATE_FORMAT_ONE,-2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime,TimeUtils.DAY_DATE_FORMAT_ONE)

    val sql = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + twoDaysAgoDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + twoDaysAgoDate +
      "'"+ " and key not like '%TCL%'"

    sqlContext.sql(sql)

    println(sql)



//    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")



  }

}
package com.avcdata.vbox.clean.apk

import com.avcdata.vbox.util.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将apk的数据导入到apk分区表
  */
object ApkData2PartitionTCL {
  def main(args: Array[String]) {
    val arr = "#,#".split(",")
    println(arr.length)
  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    //加载当天的数据
    //    val sql = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + currentDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + currentDate + "'"


    //加载四天前的数据
    val daysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -3)

    val daysAgoDate = TimeUtils.convertTimeStamp2DateStr(daysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)

    val sql = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition_tcl PARTITION (date='" + daysAgoDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + daysAgoDate +
      "'" + " and key like '%TCL%'"

    sqlContext.sql(sql)

    println(sql)



    //加载三天前的数据
    val daysAgoTime01 = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val daysAgoDate01 = TimeUtils.convertTimeStamp2DateStr(daysAgoTime01, TimeUtils.DAY_DATE_FORMAT_ONE)

    val sql01 = "INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition_tcl2 PARTITION (date='" + daysAgoDate01 + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact where dim_date='" + daysAgoDate01 +
      "'" + " and key like '%TCL%'"

    sqlContext.sql(sql01)


    println(sql01)

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")


  }

}
package com.avcdata.spark.job.konka

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by wxy on 8/29/16.
  *Konka apk的清洗
  */
object ApkDataLoadJob {

  def run(sc: SparkContext, analysisDate: String) = {
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    //val dimAreaCol = Bytes.toBytes("dim_area")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //val rdd = sc.textFile("F:/avc/docs/konka/activity.log.2016-08-16")
    val rdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/activity.log." + analysisDate)
      //val rdd = sc.textFile("/user/hdfs/rsync/KONKA/history/activity.log.since_10-01")
      .filter(x => {
      val date = x.split('|')(8)
      date == analysisDate || date == preDate || date == yesBeforeDate
    })

    rdd.foreachPartition(items => {

      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact")) //tracker_apk_active_fact
      //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_apk_active_fact"))

      try {

        items.foreach(line => {
          val cols = line.split('|')
          val sn = cols(0)
          val area = cols(5).substring(0,3)
          val apk = cols(6)
          val date = cols(8)
          val hour = cols(9)
          val cnt = cols(10)
          val duration = cols(11)

          val put = new Put(Bytes.toBytes(apk + date + hour + sn + "KO"))
          put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
          put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apk))
          //put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
          //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
          put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))
          put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(hour))
          put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(cnt))
          put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(duration))

          mutator.mutate(put)
        })

        mutator.flush()
      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })

    //val frdd = rdd.map(x=> (x.split('|')(6), 1)).reduceByKey(_+_)
    //file.writer.close()
    //frdd.saveAsTextFile("hdfs:///user/hdfs/apk/KO-" + analysisDate + ".txt")
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2016-11-20")
    //
    //     sc.stop()

  }

//49.82.43.21	COOCAAOS_TV	fca38688dcd6	appStatus	1492910088401						江苏省	淮安市	E3500	8S62
  def run(sc: SparkContext, currentDate: String) = {

    //TODO 原始日志格式
    //    113.249.247.212	COOCAAOS_TV	fca386edf206	appStatus	1483372857784	com.tianci.appstore	1483372726640	1483372736868	10228	2017-01-02 23:58:56	重庆市	重庆市	E5	8S82

    //TODO 转换当前日期为yyyyMMdd
    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //TODO HDFS文件->RDD
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)

    //TODO 提取
    val filteredRDD = initRDD
      .map(line => {
        val cols = line.split('\t')
        //终端唯一码mac
        val sn = cols(2).trim
        //apk package
        val apkPackage = cols(5).trim
        //启动日期
        val launchTime = cols(6).trim
        //退出时间
        val exitTime = cols(7).trim
        val duration = cols(8).trim
        //省 可以用ip库解析
        val province = cols(10).trim
        //市
        val city = cols(11).trim

        sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city
      }).distinct()
      //TODO 过滤
      .filter(line => {
      val cols = line.split('\t')

      //终端唯一码mac
      val sn = cols(0)
      val snIsRight = !sn.isEmpty()

      //apk package
      val apkPackage = cols(1)
      val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


      //启动日期
      var launchTimeIsRight = ValidateUtils.isNumber(cols(2))
      if (launchTimeIsRight) {
        //        dateIsRight = cols(6).toLong > 0
        launchTimeIsRight = cols(2).toLong > 0
      }

      //退出时间
      var exitTimeIsRight = ValidateUtils.isNumber(cols(3))
      if (exitTimeIsRight) {
        exitTimeIsRight = cols(3).toLong > 0
      }

      var durationIsRight = ValidateUtils.isNumber(cols(4))
      if (durationIsRight) {
        durationIsRight = cols(4).toLong > 0
      }

      //省 可以用ip库解析
      val province = cols(5)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(6)
      //      val cityIsRight = !city.isEmpty() && !city.equals("未匹配")
      val cityIsRight = true

      provinceIsRight && cityIsRight && launchTimeIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

    })

    //TODO 理清启动时间 退出时间和时长的顺序
    val sortedRDD = filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(2).toLong, cols(3).toLong, cols(4).toLong).sorted
      //终端唯一码mac
      val sn = cols(0)
      //apk package
      val apkPackage = cols(1)
      val launchTime = sortedTimeList.apply(1)
      val exitTime = sortedTimeList.apply(2)
      val duration = sortedTimeList.apply(0)
      //省 可以用ip库解析
      val province = cols(5)
      //市
      val city = cols(6)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })

    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)

    //TODO 根据牌照修改com.tianci.movieplatform包名
    val sn2LicenseRDD = sqlContext.sql("select distinct sn,license from hr.terminal where brand = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(1).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(0)
        val launchTime = cols(2)
        val exitTime = cols(3)
        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        //不包含包名
        val other = sn + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      val sn = cols(0)
      val launchTime = cols(1)
      val exitTime = cols(2)
      val duration = cols(3)
      val province = cols(4)
      val city = cols(5)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(1).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD).distinct()

    //TODO 分近三天


    val currentDayStartTime = TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE)


    val ffDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -3)

    val firstDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val secondDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -1)

    val currentDateStartTime = TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    val currentDateEndTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, 1)

    //    val currentDateEndTime =  new DateTime(currentDayStartTime).plusDays(1).getMillis

    //    val firstDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 2)
    //    val secondDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 1)

    //TODO 第一天
    val firstDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime >= ffDayStartTime && launchTime < secondDayStartTime && launchTime < exitTime && exitTime > firstDayStartTime && exitTime < currentDateStartTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      var exitTime = cols(3).toLong

      if (launchTime < firstDayStartTime) {
        launchTime = firstDayStartTime
      }

      if (exitTime > secondDayStartTime) {
        exitTime = secondDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(firstDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

  println("firstDayRDD count:"+firstDayRDD.count())

    //TODO 第二天
    val secondDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime >= firstDayStartTime && launchTime < currentDateStartTime && launchTime < exitTime && exitTime > secondDayStartTime && exitTime < currentDateEndTime

      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < secondDayStartTime) {
        launchTime = secondDayStartTime
      }

      var exitTime = cols(3).toLong
      if (exitTime > currentDayStartTime) {
        exitTime = currentDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(secondDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

  println("secondDayRDD count:"+secondDayRDD.count())

    //TODO 当天
    val currentDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime >= secondDayStartTime && launchTime < currentDateEndTime && launchTime <
        exitTime && exitTime > currentDateStartTime && exitTime <= currentDateEndTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < currentDayStartTime) {
        launchTime = currentDayStartTime
      }

      val exitTime = cols(3).toLong

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = currentDate

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

  println("currentDayRDD count:"+currentDayRDD.count())

    val allRDD = firstDayRDD.union(secondDayRDD).union(currentDayRDD)


    //TODO 分时
    val splitTimeRdd = allRDD

      .flatMap(line => {

        val cols = line.split("\t")

        //终端唯一码
        val sn = cols(0)
        //apk package
        val apkPackage = cols(1)

        val launchTime = cols(2).toLong

        val exitTime = cols(3).toLong

        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        val launchDate = cols(7)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", launchTime, exitTime)

        tmpArrayBuffer

      })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val province = cols(i)
        i = i + 1

        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        val sn = cols(i)
        i = i + 1

        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)
      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")
    //TODO 关联apkinfo 过滤
    //统计分析
    val resultDF = sqlContext.sql(
      """
        |SELECT
        |  ap.sn,
        |  ap.apkPackage,
        |  ap.date,
        |  ap.hour,
        |  SUM(ap.launchCnt) launchCnts,
        |  SUM(ap.duration) dura
        |FROM
        |  tb_apk_extract ap
        |GROUP BY
        |  ap.sn,
        |  ap.apkPackage,
        |  ap.date,
        |  ap.hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
    //inner join hr.apkinfo ai
    //on (ap.apkPackage=ai.packagename)


    //TODO 直接存入分区表
//    sqlContext.sql(
//      """
//        |insert into tracker_apk_partition partition(date='""" + currentDate +
//        """"')
//          |  SELECT
//          |    ap.sn,
//          |    ap.apkPackage,
//          |    ap.date,
//          |    ap.hour,
//          |    SUM (ap.launchCnt) launchCnts,
//          |    SUM (ap.duration) dura
//          |FROM
//          |    tb_apk_extract ap
//          |GROUP BY
//          |    ap.sn,
//          |    ap.apkPackage,
//          |    ap.date,
//          |    ap.hour
//        """.stripMargin)


    //TODO  写入到Hbase
    resultDF.foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
      //   val mutator = HBaseUtils.getMutator("test_apk_active_fact")
      //   val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine, brand))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })
    sqlContext.uncacheTable("tb_apk_extract")
    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}


//验证
//select dim_apk,count(distinct dim_sn) sn_num, sum(fact_cnt) as cnt ,sum(fact_duration)/3600 as dura,sum(fact_duration)/3600/count(distinct dim_sn) tcntavgclient  from tracker_apk_fact_test apt where apt.dim_date = '2016-12-21' and apt.key like '%CC'
//group by dim_apk
package com.avcdata.spark.job.coocaa

import java.text.SimpleDateFormat
import java.util.Date

import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.{SparkConf, SparkContext}
import org.joda.time.DateTime

/**
  * Created by wxy on 8/29/16.
  */
object ApkDataLoadJob01 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[2]")
      .setAppName("coocaa-ApkDataLoadJob")

    val sc = new SparkContext(conf)

    run(sc)

  }


  def run(sc: SparkContext) = {

    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimAreaCol = Bytes.toBytes("dim_area")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")


    val df: SimpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")

    val initRDD = sc.textFile("E:\\data\\COOCAA\\2016-10-18\\aowei_appStatus20161018_1.txt")



    initRDD.foreachPartition(lines => {
      lines.foreach(line => {
        val cols = line.split('\t')
        //终端唯一码
        val sn = cols(2)
        //日期
        val date = df.format(new Date(cols(6).toLong))
        //小时
        val hour = new DateTime(cols(6).toLong).hourOfDay().getAsString()
        //apk package
        val apkPackage = cols(5)
        //启动次数
        //val cnt = cols(10)
        //时长
        val duration = cols(8)
        //??省 市 区 or  区域码
        val area = cols(9)


        printApkData(area, apkPackage, hour, date, sn, null, duration)
        println("--------------------------------------------")

      })
    })

    //(终端SN,启动次数)
    val cntRDD = initRDD.map(line => {
      val cols = line.split('\t')
      (cols(2), 1)
    }).reduceByKey(_ + _)

    cntRDD.foreach(println(_))

  }

  def printApkData(area: String, apkPackage: String, hour: String, date: String, sn: String, cnt: String, duration: String): Unit = {
    println("区域码：" + area)
    println("apk包名：" + apkPackage)
    println("小时：" + hour)
    println("日期：" + date)
    println("终端唯一码：" + sn)
    println("启动次数：" + cnt)
    println("时长：" + duration)
  }
}
package com.avcdata.spark.job.coocaa

import java.text.SimpleDateFormat
import java.util.Date

import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.{SparkConf, SparkContext}
import org.joda.time.DateTime
import org.joda.time.format.DateTimeFormat

/**
  * Created by wxy on 8/29/16.
  */
object ApkDataLoadJob02 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[2]")
      .setAppName("coocaa-ApkDataLoadJob")

    val sc = new SparkContext(conf)

    run(sc)

  }


  def run(sc: SparkContext) = {

    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimAreaCol = Bytes.toBytes("dim_area")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")


    val df: SimpleDateFormat = new SimpleDateFormat("yyyy-MM-dd")

    val initRDD = sc.textFile("E:\\data\\COOCAA\\2016-10-18\\aowei_appStatus20161018_1.txt")


    val cleanRDD = initRDD.map(line => {
      val cols = line.split('\t')
      //终端唯一码
      val sn = cols(2)
      //日期
      val date = df.format(new Date(cols(4).toLong))
      //小时
      val hour = new DateTime(cols(4).toLong).hourOfDay().getAsString()
      //apk package
      val apkPackage = cols(5)
      //启动次数
      //val cnt = cols(10)
      //时长
      val duration = cols(8)
      //??省 市 区 or  区域码
      val area = cols(9)

      (("sn", sn), ("cnt", null), ("date", date), ("hour", hour), ("apkPackage", apkPackage), ("duration", duration), ("area", area))

    })

    //(终端SN,启动次数)
    val cntRDD = initRDD.map(line => {
      val cols = line.split('\t')
      (cols(2), 1)
    }).reduceByKey(_ + _).map(line => {

      (("sn", line._1), ("cnt", line._2), ("date", null), ("hour", null), ("apkPackage", null), ("duration", null), ("area", null))
    })

//    cntRDD.join(cleanRDD);

  }

  def printApkData(area: String, apkPackage: String, hour: String, date: String, sn: String, cnt: String, duration: String): Unit = {
    println("区域码：" + area)
    println("apk包名：" + apkPackage)
    println("小时：" + hour)
    println("日期：" + date)
    println("终端唯一码：" + sn)
    println("启动次数：" + cnt)
    println("时长：" + duration)
  }


}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.hadoop.hbase.{TableName, HBaseConfiguration}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.Logger
import org.apache.spark.sql.{SQLContext, Row}
import org.apache.spark.sql.types._
import org.apache.spark.{SparkContext, SparkConf}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob03 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "20161026")

    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()
    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)


    //合并文件 略
    //   val initRDD = sc.textFile("S:\\奥维云网\\code\\doc\\data\\COOCAA\\" + analysisDate + "\\aowei_appStatus" + analysisDate  + ".txt").distinct()

    //testing
    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\04test\\coocaa\\apk-data-sample-02.txt").distinct()

    //    val initRDD = sc.textFile("hdfs://user/hdfs/rsync/COOCAA").distinct()

    //    val initRDD2 = initRDD.map(line=>(line.trim,"")).groupByKey().keys //去重

    val testInitRDD = initRDD.map(line => {
      val cols = line.split('\t')

      val launchTimeStamp = TimeUtils.convertDateStr2TimeStamp(cols(6), TimeUtils.SECOND_DATE_FORMAT)
      val exitTimeStamp = TimeUtils.convertDateStr2TimeStamp(cols(7), TimeUtils.SECOND_DATE_FORMAT)


      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStamp + "\t" + exitTimeStamp + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    })

    //提取部分数据
    val filteredRDD = testInitRDD
      //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      //apk package
      val apkPackage = cols(5)
      val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


      //启动日期
      var dateIsRight = ValidateUtils.isNumber(cols(6))
      if (dateIsRight) {
        //        dateIsRight = cols(6).toLong > 0
        dateIsRight = cols(6).toLong > 0
      }

      //退出时间
      var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
      if (exitTimeIsRight) {
        exitTimeIsRight = cols(7).toLong >= checkTime
      }

      val durationIsRight = ValidateUtils.isNumber(cols(8))

      provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

    })
      /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
      .map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted


      //只截取当天的时间
      var launchDate = sortedTimeList.apply(1)
      if (launchDate < checkTime) {
        launchDate = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchDate + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
      //////////////////////////////////过滤时长/////////////////////////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////


    val splitTimeRdd = filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })


    /////////////////////////////////映射成临时表中的行////////////////////////////////////////////

    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1


        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })

    //    cleanedRDDRows.foreach(println(_))
    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////

    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")


    val dimFamilyCol = Bytes.toBytes("dim")

    //    val dimProvinceCol = Bytes.toBytes("dim_province")
    //    val dimCityCol = Bytes.toBytes("dim_city")

    val dimAreaCol = Bytes.toBytes("dim_area")
    val dimSnCol = Bytes.toBytes("dim_sn")

    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")

    val factFamilyCol = Bytes.toBytes("fact")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  province,
        |  city,
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  province,
        |  city,
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(7).toString().toLong <= 3600
      //          line(8).toString().toLong <= (24 * 3600)
    })

      //    写入到Hbase
      .foreachPartition(lines => {

      //      val myConf = HBaseConfiguration.create()
      //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
      //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //      val hbaseConn = ConnectionFactory.createConnection(myConf);
      //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))

      try {

        lines.foreach(line => {
          var i = 0

          val province = line(i).toString
          i = i + 1

          val city = line(i).toString
          i = i + 1

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString.toInt
          i = i + 1

          val duration = line(i).toString.toLong
          i = i + 1

          val result = province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          println(result)

          val put = new Put(Bytes.toBytes(date + "-" + hour + "-" + sn + "-COOCAA"))
          //          put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
          //          put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))

          //将省市写入区域码字段
          put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(province + "_" + city))
          put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
          put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apkPackage))

          put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))
          put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(hour))
          put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(launchCnt))
          put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(duration))


          //          mutator.mutate(put)
        })
        //        mutator.flush()

      } finally {
        //        mutator.close()
        //        hbaseConn.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }


  //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


  def put2Hbase(): Unit = {
    val myConf = HBaseConfiguration.create()
    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    myConf.set("hbase.zookeeper.property.clientPort", "2181")
    val hbaseConn = ConnectionFactory.createConnection(myConf);
    val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.log4j.Logger
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob04 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "20161026")

    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()
    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)


    //合并文件 略
    //   val initRDD = sc.textFile("S:\\奥维云网\\code\\doc\\data\\COOCAA\\" + analysisDate + "\\aowei_appStatus" + analysisDate  + ".txt").distinct()

    //testing
    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\04test\\coocaa\\apk-data-sample-02.txt").distinct()

//    val initRDD = sc.textFile("hdfs://user/hdfs/rsync/COOCAA").distinct()

    //    val initRDD2 = initRDD.map(line=>(line.trim,"")).groupByKey().keys //去重

        val testInitRDD = initRDD.map(line => {
          val cols = line.split('\t')

          val launchTimeStamp = TimeUtils.convertDateStr2TimeStamp(cols(6), TimeUtils.SECOND_DATE_FORMAT)
          val exitTimeStamp = TimeUtils.convertDateStr2TimeStamp(cols(7), TimeUtils.SECOND_DATE_FORMAT)


          cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStamp + "\t" + exitTimeStamp + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
        })

    //提取部分数据
    val filteredRDD = testInitRDD
      //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      //apk package
      val apkPackage = cols(5)
      val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


      //启动日期
      var dateIsRight = ValidateUtils.isNumber(cols(6))
      if (dateIsRight) {
        //        dateIsRight = cols(6).toLong > 0
        dateIsRight = cols(6).toLong > 0
      }

      //退出时间
      var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
      if (exitTimeIsRight) {
        exitTimeIsRight = cols(7).toLong >= checkTime
      }

      val durationIsRight = ValidateUtils.isNumber(cols(8))

      provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

    })
      /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
      .map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted


      //只截取当天的时间
      var launchDate = sortedTimeList.apply(1)
      if (launchDate < checkTime) {
        launchDate = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchDate + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
      //////////////////////////////////过滤时长/////////////////////////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
    val rdd1 = filteredRDD.filter(line => {
      val cols = line.split("\t")

      val launchTimeStamp = cols(6).toLong
      val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)

      val exitTimeStamp = cols(7).toLong
      val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)

      exitHour - launchHour == 0

    })

      .map(line => {
        val cols = line.split("\t")
        //省
        val province = cols(10)
        //市
        val city = cols(11)
        //终端唯一码
        val sn = cols(2)
        //apk package
        val apkPackage = cols(5)

        val launchTimeStamp = cols(6).toLong
        val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)

        val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        val exitTimeStamp = cols(7).toLong

        val diffTimeStamp = exitTimeStamp - launchTimeStamp

        province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t" + launchHour + "\t" + 1 + "\t" + (diffTimeStamp / 1000)
      })


    val rdd2 = filteredRDD.filter(line => {
      val cols = line.split("\t")

      val launchTimeStamp = cols(6).toLong
      val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)

      val exitTimeStamp = cols(7).toLong
      val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)
      //      val exitMinute = TimeUtils.convertTimeStamp2Minute(exitTimeStamp)
      //      val exitSec = TimeUtils.convertTimeStamp2Sec(exitTimeStamp)

      exitHour - launchHour > 0

    }).flatMap(line => {

      val cols = line.split("\t")

      //省
      val province = cols(10)

      //市
      val city = cols(11)

      //终端唯一码
      val sn = cols(2)

      //apk package
      val apkPackage = cols(5)

      val launchTimeStamp = cols(6).toLong
      val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)
      val launchMinute = TimeUtils.convertTimeStamp2Minute(launchTimeStamp)
      val launchSec = TimeUtils.convertTimeStamp2Sec(launchTimeStamp)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val exitTimeStamp = cols(7).toLong
      val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)
      val exitMinute = TimeUtils.convertTimeStamp2Minute(exitTimeStamp)
      val exitSec = TimeUtils.convertTimeStamp2Sec(exitTimeStamp)


      val arr = ArrayBuffer[String]()

      arr.append(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t" + launchHour + "\t" + 1 + "\t" + (60 * 60 - (launchMinute * 60 + launchSec)))

      var h = launchHour
      //需要分时
      while (h < exitHour) {
        h = h + 1
        if (h != exitHour) {
          arr.append(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t" + h +
            "\t" + 0 + "\t" + 1 * 60 * 60)
        } else {
          arr.append(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t" + h + "\t" + 0 + "\t" + (exitMinute * 60 + exitSec))
        }
      }
      arr

    })


    ///////////////////////////////映射成临时表中的行////////////////////////////////////////////

    val cleanedRDDRows = rdd1.union(rdd2)
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1


        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })

    //    cleanedRDDRows.foreach(println(_))
    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////

    val sqlContext = new SQLContext(sc)

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")


    val dimFamilyCol = Bytes.toBytes("dim")

    //    val dimProvinceCol = Bytes.toBytes("dim_province")
    //    val dimCityCol = Bytes.toBytes("dim_city")

    val dimAreaCol = Bytes.toBytes("dim_area")
    val dimSnCol = Bytes.toBytes("dim_sn")

    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")

    val factFamilyCol = Bytes.toBytes("fact")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  province,
        |  city,
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  province,
        |  city,
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(7).toString().toLong <= 3600
      //          line(8).toString().toLong <= (24 * 3600)
    })

      //    写入到Hbase
      .foreachPartition(lines => {

      //      val myConf = HBaseConfiguration.create()
      //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
      //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //      val hbaseConn = ConnectionFactory.createConnection(myConf);
      //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))

      try {

        lines.foreach(line => {
          var i = 0

          val province = line(i).toString
          i = i + 1

          val city = line(i).toString
          i = i + 1

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString.toInt
          i = i + 1

          val duration = line(i).toString.toLong
          i = i + 1

          val result = province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          println(result)

          val put = new Put(Bytes.toBytes(date + "-" + hour + "-" + sn + "-COOCAA"))
          //          put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
          //          put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))

          //将省市写入区域码字段
          put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(province + "_" + city))
          put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
          put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apkPackage))

          put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))
          put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(hour))
          put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(launchCnt))
          put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(duration))


          //          mutator.mutate(put)
        })
        //        mutator.flush()

      } finally {
        //        mutator.close()
        //        hbaseConn.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }


  //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


  def put2Hbase(): Unit = {
    val myConf = HBaseConfiguration.create()
    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    myConf.set("hbase.zookeeper.property.clientPort", "2181")
    val hbaseConn = ConnectionFactory.createConnection(myConf);
    val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))
  }


}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.log4j.Logger
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob05 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "20161026")

    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()
    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)


    //合并文件 略
    //   val initRDD = sc.textFile("S:\\奥维云网\\code\\doc\\data\\COOCAA\\" + analysisDate + "\\aowei_appStatus" + analysisDate  + ".txt").distinct()

    //testing
    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\04test\\coocaa\\aowei_appStatus20161111_1.txt")
      .distinct()

    //    val initRDD = sc.textFile("hdfs://user/hdfs/rsync/COOCAA").distinct()

    //    val initRDD2 = initRDD.map(line=>(line.trim,"")).groupByKey().keys //去重

//    val testInitRDD = initRDD.map(line => {
//      val cols = line.split('\t')
//
//      val launchTimeStamp = TimeUtils.convertDateStr2TimeStamp(cols(6), TimeUtils.SECOND_DATE_FORMAT)
//      val exitTimeStamp = TimeUtils.convertDateStr2TimeStamp(cols(7), TimeUtils.SECOND_DATE_FORMAT)
//
//
//      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStamp + "\t" + exitTimeStamp + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
//    })

    //提取部分数据
    val filteredRDD = initRDD
      //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      //apk package
      val apkPackage = cols(5)
      val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


      //启动日期
      var dateIsRight = ValidateUtils.isNumber(cols(6))
      if (dateIsRight) {
        //        dateIsRight = cols(6).toLong > 0
        dateIsRight = cols(6).toLong > 0
      }

      //退出时间
      var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
      if (exitTimeIsRight) {
        exitTimeIsRight = cols(7).toLong >= checkTime
      }

      val durationIsRight = ValidateUtils.isNumber(cols(8))

      provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

    })
      /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
      .map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted


      //只截取当天的时间
      var launchDate = sortedTimeList.apply(1)
      if (launchDate < checkTime) {
        launchDate = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchDate + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
      //////////////////////////////////过滤时长/////////////////////////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////


    val splitTimeRdd = filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })


    ///////////////////////////////映射成临时表中的行////////////////////////////////////////////

    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1


        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })

    //    cleanedRDDRows.foreach(println(_))
    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////

    val sqlContext = new SQLContext(sc)

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")

    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    val accum = sc.accumulator(0)


    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimAreaCol = Bytes.toBytes("dim_area")

    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val dimDateCol = Bytes.toBytes("dim_date")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val factDurationCol = Bytes.toBytes("fact_duration")


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })

      //    写入到Hbase
      .foreachPartition(lines => {

      //      val myConf = HBaseConfiguration.create()
      //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
      //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //      val hbaseConn = ConnectionFactory.createConnection(myConf);
      //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))

      //      val myConf = HBaseConfiguration.create()
      //      myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //      val hbaseConn = ConnectionFactory.createConnection(myConf);
      //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))

      //      try {

      lines.foreach(line => {
        var i = 0

        val sn = line(i).toString
        i = i + 1

        val apkPackage = line(i).toString
        i = i + 1

        val date = line(i).toString
        i = i + 1


        val hour = line(i).toString
        i = i + 1

        val launchCnt = line(i).toString.toInt
        i = i + 1

        val duration = line(i).toString.toLong
        i = i + 1

        val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

        println(result)

        val put = new Put(Bytes.toBytes(date + "-" + hour + "-" + accum + "-CC"))

        //将省市写入区域码字段
        put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apkPackage))
        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(hour))
        put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(launchCnt))

        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))
        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))

        put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(duration))

        accum += 1

        //          mutator.mutate(put)
      })
      //        mutator.flush()
      //
      //      } finally {
      //        mutator.close()
      //        hbaseConn.close()
      //      }
    }

    )

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }


  //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.log4j.Logger
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob06 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-19")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String) = {

    //    log.debug("coocaa-ApkDataLoadJob start ...")

    println("coocaa-ApkDataLoadJob start ...")


    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //合并文件 略
//    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate).distinct()

        val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })

    //////////////////////////////////////////////////////////补充终端信息到hbase terminal表


    //提取部分数据
    val filteredRDD = initRDD
      //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      //apk package
      val apkPackage = cols(5)
      val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


      //启动日期
      var dateIsRight = ValidateUtils.isNumber(cols(6))
      if (dateIsRight) {
        //        dateIsRight = cols(6).toLong > 0
        dateIsRight = cols(6).toLong > 0
      }

      //退出时间
      var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
      if (exitTimeIsRight) {
        exitTimeIsRight = cols(7).toLong >= checkTime
      }

      val durationIsRight = ValidateUtils.isNumber(cols(8))

      provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

    })
      /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
      .map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted


      //只截取当天的时间
      var launchDate = sortedTimeList.apply(1)
      if (launchDate < checkTime) {
        launchDate = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchDate + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
      //////////////////////////////////过滤时长/////////////////////////////////////////////////////////////
      .filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////


    val splitTimeRdd = filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })


    ///////////////////////////////映射成临时表中的行////////////////////////////////////////////

    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1


        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })

    //    cleanedRDDRows.foreach(println(_))
    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////

    //    log.debug("sql on temptable ...")

    println("sql on temptable ...")

    val sqlContext = new SQLContext(sc)

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    val accum = sc.accumulator(0)


    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val dimDateCol = Bytes.toBytes("dim_date")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val factDurationCol = Bytes.toBytes("fact_duration")






    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //    写入到Hbase
      .foreachPartition(lines => {

      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf);
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))

      //            val myConf = HBaseConfiguration.create()
      //            myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //            myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //            val hbaseConn = ConnectionFactory.createConnection(myConf);
      //            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact_test"))


      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          println(result)

          val put = new Put(Bytes.toBytes(date + "-" + hour + "-" + accum + "-CC"))


          //        将省市写入区域码字段
          put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apkPackage))
          put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(hour))
          put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(launchCnt))

          put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))
          put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))

          put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(duration))

          accum += 1

          mutator.mutate(put)
        })
        mutator.flush()

      } finally {
        mutator.close()
        hbaseConn.close()
      }
    }

    )

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob07 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {


    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    println(hdfsPath)

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)


    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)

    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })



    //分时
    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)

    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")

    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      //            val myConf = HBaseConfiguration.create()
      //            myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //            myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //            val hbaseConn = ConnectionFactory.createConnection(myConf);
      //            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact_test"))


      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)

          val brand = "CC"
          mutator.mutate(HBaseUtils.getPut_apk(sortedLine,brand))

          //          accum += 1

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //////////////////////////////////////////////////////////补充终端信息到hbase terminal表
  def load2terminal(initRDD: RDD[String]) = {
    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      val cityIsRight = !city.isEmpty()
      //      && !city.equals("未匹配")
      //      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      provinceIsRight && cityIsRight && snIsRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = ""

          var province = cols(10)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""
          //型号
          val model = cols(12) + "_" + cols(13)

          val size = ""

          val city = cols(11)

          val sn = cols(2)

          //大区
          val area = MapingUtils.getArea(province)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", sortedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }

  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(7).toLong >= checkTime
        }

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted


      //只截取当天的时间
      var launchDate = sortedTimeList.apply(1)
      if (launchDate < checkTime) {
        launchDate = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchDate + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob09 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {

    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    println(hdfsPath)

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)


    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)


    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)


    ///////////////////////////////////////////////////根据牌照修改com.tianci.movieplatform包名///////////////////////////

    val sn2LicenseRDD = sqlContext.sql("select distinct sn,license from hr.terminal_partition where br = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(5).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(2)
        //不包含包名
        val other = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(6) + "\t" + cols(7) + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + apkPackage + "\t" + cols(5) + "\t" + cols(6) + "\t" + cols(7) + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10)

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(5).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD)


    /////////////////////////////////////////////////////分时////////////////////////////////////////////
    //    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)
    val splitTimeRdd = compactRDD

      .flatMap(line => {

        val cols = line.split("\t")
        //省
        val province = cols(10)
        //市
        val city = cols(11)
        //终端唯一码
        val sn = cols(2)
        //apk package
        val apkPackage = cols(5)

        val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

        val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

        tmpArrayBuffer

      })

    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")



    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)


    //    //统计分析
    //    sqlContext.sql(
    //      """
    //        |SELECT
    //        |  sn,
    //        |  apkPackage,
    //        |  date,
    //        |  hour,
    //        |  SUM(launchCnt) launchCnts,
    //        |  SUM(duration) dura
    //        |FROM
    //        |  tb_apk_extract
    //        |GROUP BY
    //        |  sn,
    //        |  apkPackage,
    //        |  date,
    //        |  hour
    //      """.stripMargin).rdd
    //      //再次过滤
    //      .filter(line => {
    //      line(5).toString().toLong <= 3600
    //      //          line(6).toString().toLong <= (24 * 3600)
    //    })
    //      .map(line => {
    //
    //      Apk(line(0).toString, line(1).toString, line(2).toString, line(3).toString, line(4).toString, line(5).toString)
    //    }).toDF().registerTempTable("tb_apk_tmp")
    //    sqlContext.cacheTable("tb_apk_tmp")


    //    写入到Hive
    sqlContext.sql("use hr")

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    sqlContext.sql("INSERT OVERWRITE TABLE  hr.tracker_apk_fact_partition partition(date='" + currentDate + "') " +
      "SELECT  apkPackage as dim_apk,date as dim_date, hour as dim_hour,sn as dim_sn, SUM(launchCnt) as fact_cnt, SUM(duration) as fact_duration FROM tb_apk_extract where dim_date='" + currentDate + "' " +
      "GROUP BY sn,apkPackage, date, hour")

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  case class Apk(dim_sn: String,
                 dim_apk: String,
                 dim_date: String,
                 dim_hour: String,
                 fact_cnt: String,
                 fact_duration: String)

  //////////////////////////////////////////////////////////补充终端信息到hbase terminal表
  def load2terminal(initRDD: RDD[String]) = {
    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      val cityIsRight = !city.isEmpty()
      //      && !city.equals("未匹配")
      //      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      provinceIsRight && cityIsRight && snIsRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = ""

          var province = cols(10)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""
          //型号
          val model = cols(12) + "_" + cols(13)

          val size = ""

          val city = cols(11)

          val sn = cols(2)

          //大区
          val area = MapingUtils.getArea(province)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", sortedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }

  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(7).toLong >= checkTime
        }

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

      //只截取当天的时间
      var launchTime = sortedTimeList.apply(1)
      if (launchTime < checkTime) {
        launchTime = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import java.util.Date

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob10 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {


    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    println(hdfsPath)

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)




    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)

    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)

    //    val snLicenseArr = sqlContext.sql("select distinct sn,license from hr.terminal where brand = 'COOCAA' and license" +
    //      " = 'yinhe' or license  = 'tencent'").collect()

    //    snLicenseArr.map { row => {
    //      println(row.toString())
    //    }
    //    }

    //    val snLicenseArrBV = sc.broadcast(snLicenseArr)

    //分时
    //    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)
    val splitTimeRdd = filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      var apkPackage = cols(5)
      //      if (apkPackage.equals("com.tianci.movieplatform")) {
      //        snLicenseArrBV.value.foreach(ele => {
      //          if (sn.equals(ele.get(0))) {
      //            if (ele.get(1).equals("tencent")) {
      //              apkPackage = "腾讯launcher"
      //            }
      //
      //            if (ele.get(1).equals("yinhe")) {
      //              apkPackage = "爱奇艺launcher"
      //            }
      //          }
      //
      //        })
      //      }

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })



    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")



    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)

    val now = new Date()
    val output_tmp_dir = "/tmp/apk" + now.getTime

    //如果output_tmp_dir 已经存在  就删除



    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    }).map { r => r.mkString("\t") }.saveAsTextFile(output_tmp_dir)


    //    写入到Hive
    sqlContext.sql("use hr")
    sqlContext.sql(s"""load data inpath '$output_tmp_dir' overwrite into table hr.tracker_apk_fact_partition_txt partition (date='$currentDate')""")

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  case class Apk(dim_sn: String,
                 dim_apk: String,
                 dim_date: String,
                 dim_hour: String,
                 fact_cnt: String,
                 fact_duration: String)

  //////////////////////////////////////////////////////////补充终端信息到hbase terminal表
  def load2terminal(initRDD: RDD[String]) = {
    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      val cityIsRight = !city.isEmpty()
      //      && !city.equals("未匹配")
      //      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      provinceIsRight && cityIsRight && snIsRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = ""

          var province = cols(10)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""
          //型号
          val model = cols(12) + "_" + cols(13)

          val size = ""

          val city = cols(11)

          val sn = cols(2)

          //大区
          val area = MapingUtils.getArea(province)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", sortedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }

  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(7).toLong >= checkTime
        }

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

      //只截取当天的时间
      var launchTime = sortedTimeList.apply(1)
      if (launchTime < checkTime) {
        launchTime = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗 增加包名判断后 一直停留 报错 洗不进Hbase
  */
object ApkDataLoadJob11 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {


    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    println(hdfsPath)

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)


    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)

    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)

    val snLicenseArr = sqlContext.sql("select distinct sn,license from hr.terminal_partition where br = 'CC' " +
      "and " +
      "license" +
      " = 'yinhe' or license  = 'tencent'").collect()

    //    snLicenseArr.map { row => {
    //      println(row.toString())
    //    }
    //    }
    val snLicenseArrBV = sc.broadcast(snLicenseArr)

    //分时
    //    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)
    val splitTimeRdd = filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      var apkPackage = cols(5)
      if (apkPackage.equals("com.tianci.movieplatform")) {
        snLicenseArrBV.value.foreach(ele => {
          if (sn.equals(ele.get(0))) {
            if (ele.get(1).equals("tencent")) {
              apkPackage = "腾讯launcher"
            }

            if (ele.get(1).equals("yinhe")) {
              apkPackage = "爱奇艺launcher"
            }
          }

        })
      }

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })



    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")



    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //    写入到Hbase
      .foreachPartition(lines => {

//      val mutator = HBaseUtils.getMutator("test_apk_active_fact")
            val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      //            val myConf = HBaseConfiguration.create()
      //            myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //            myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //            val hbaseConn = ConnectionFactory.createConnection(myConf);
      //            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact_test"))


      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine,brand))

          //          accum += 1

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

    sqlContext.uncacheTable("tb_apk_extract")

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + currentDate + "') select * from hr.tracker_apk_fact where dim_date='" + currentDate + "'")
    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //////////////////////////////////////////////////////////补充终端信息到hbase terminal表
  def load2terminal(initRDD: RDD[String]) = {
    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      val cityIsRight = !city.isEmpty()
      //      && !city.equals("未匹配")
      //      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      provinceIsRight && cityIsRight && snIsRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = ""

          var province = cols(10)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""
          //型号
          val model = cols(12) + "_" + cols(13)

          val size = ""

          val city = cols(11)

          val sn = cols(2)

          //大区
          val area = MapingUtils.getArea(province)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", sortedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }

  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(7).toLong >= checkTime
        }

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

      //只截取当天的时间
      var launchTime = sortedTimeList.apply(1)
      if (launchTime < checkTime) {
        launchTime = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob12 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {


    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    println(hdfsPath)

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)


    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)


    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)


    ///////////////////////////////////////////////////根据牌照修改com.tianci.movieplatform包名///////////////////////////

    val sn2LicenseRDD = sqlContext.sql("select distinct sn,license from hr.terminal_partition where br = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(5).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(2)
        //不包含包名
        val other = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(6) + "\t" + cols(7) + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + apkPackage + "\t" + cols(5) + "\t" + cols(6) + "\t" + cols(7) + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10)

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(5).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD)


    /////////////////////////////////////////////////////分时////////////////////////////////////////////
    //    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)
    val splitTimeRdd = compactRDD

      .flatMap(line => {

        val cols = line.split("\t")
        //省
        val province = cols(10)
        //市
        val city = cols(11)
        //终端唯一码
        val sn = cols(2)
        //apk package
        val apkPackage = cols(5)

        val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

        val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

        tmpArrayBuffer

      })

    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")



    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("test_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      //            val myConf = HBaseConfiguration.create()
      //            myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //            myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //            val hbaseConn = ConnectionFactory.createConnection(myConf);
      //            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact_test"))


      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine,brand))

          //          accum += 1

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

    sqlContext.uncacheTable("tb_apk_extract")

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date='" + currentDate + "') select * from hr.tracker_apk_fact where dim_date='" + currentDate + "'")
    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //////////////////////////////////////////////////////////补充终端信息到hbase terminal表
  def load2terminal(initRDD: RDD[String]) = {
    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      val cityIsRight = !city.isEmpty()
      //      && !city.equals("未匹配")
      //      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      provinceIsRight && cityIsRight && snIsRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = ""

          var province = cols(10)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""
          //型号
          val model = cols(12) + "_" + cols(13)

          val size = ""

          val city = cols(11)

          val sn = cols(2)

          //大区
          val area = MapingUtils.getArea(province)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", sortedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }

  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(7).toLong >= checkTime
        }

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

      //只截取当天的时间
      var launchTime = sortedTimeList.apply(1)
      if (launchTime < checkTime) {
        launchTime = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob13 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {

    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    println(hdfsPath)

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)


    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)


    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {
      val cols = line.split('\t')

      //时长
      //过滤掉时长超过合理范围的数据 24：00 - 应用启动时间
      val diffLaunchDate2CurrentDayEnd = (TimeUtils.dateStrAddDays2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO, 1) - cols(6).toLong)
      val durationIsRight = cols(8).toLong >= 1000 && cols(8).toLong <= diffLaunchDate2CurrentDayEnd
      durationIsRight

    })


    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._


    ///////////////////////////////////////////////////根据牌照修改com.tianci.movieplatform包名///////////////////////////

    val sn2LicenseRDD = sqlContext.sql("select distinct sn,license from hr.terminal_partition where br = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(5).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(2)
        //不包含包名
        val other = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(6) + "\t" + cols(7) + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + apkPackage + "\t" + cols(5) + "\t" + cols(6) + "\t" + cols(7) + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10)

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(5).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD)


    /////////////////////////////////////////////////////分时////////////////////////////////////////////
    //    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)
    val splitTimeRdd = compactRDD

      .flatMap(line => {

        val cols = line.split("\t")
        //省
        val province = cols(10)
        //市
        val city = cols(11)
        //终端唯一码
        val sn = cols(2)
        //apk package
        val apkPackage = cols(5)

        val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

        val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

        tmpArrayBuffer

      })

    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")



    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //写入HBase
    //    //统计分析
    val df = sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
    //      //    写入到Hbase
    df.foreachPartition(lines => {

      //      val mutator = HBaseUtils.getMutator("test_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")
      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")

      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)

          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine, brand))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )


    df.map(line => {
      val dim_sn = line(0).toString
      val dim_apk = line(1).toString
      val dim_date = line(2).toString
      val dim_hour = line(3).toString
      val fact_cnt = line(4).toString
      val fact_duration = line(5).toString
      val brand = "CC"
      val key = dim_sn + dim_apk + dim_date + dim_hour + brand

      Apk(key, dim_sn, dim_apk, dim_date, dim_hour, fact_cnt, fact_duration)
    }).toDF().registerTempTable("tb_apk_extract2")


    //    写入到Hive
    sqlContext.sql("use hr")

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    sqlContext.sql("INSERT INTO TABLE hr.tracker_apk_fact_partition partition (date='" + currentDate + "') " +
      "SELECT key,dim_sn,dim_apk,dim_date,dim_hour, fact_cnt, fact_duration FROM  tb_apk_extract2 "
    )

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  case class Apk(key: String,
                 dim_sn: String,
                 dim_apk: String,
                 dim_date: String,
                 dim_hour: String,
                 fact_cnt: String,
                 fact_duration: String)

  //////////////////////////////////////////////////////////补充终端信息到hbase terminal表
  def load2terminal(initRDD: RDD[String]) = {
    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      //省 可以用ip库解析
      val province = cols(10)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(11)
      val cityIsRight = !city.isEmpty()
      //      && !city.equals("未匹配")
      //      val cityIsRight = true

      //终端唯一码mac
      val sn = cols(2)
      val snIsRight = !sn.isEmpty()

      provinceIsRight && cityIsRight && snIsRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = ""

          var province = cols(10)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""
          //型号
          val model = cols(12) + "_" + cols(13)

          val size = ""

          val city = cols(11)

          val sn = cols(2)

          //大区
          val area = MapingUtils.getArea(province)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", sortedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }

  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(7).toLong >= checkTime
        }

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

      //只截取当天的时间
      var launchTime = sortedTimeList.apply(1)
      if (launchTime < checkTime) {
        launchTime = checkTime
      }

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}
import org.joda.time.DateTime

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob14 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {

    //TODO 原始日志格式
    //    113.249.247.212	COOCAAOS_TV	fca386edf206	appStatus	1483372857784	com.tianci.appstore	1483372726640	1483372736868	10228	2017-01-02 23:58:56	重庆市	重庆市	E5	8S82

    //TODO 转换当前日期为yyyyMMdd
    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //TODO HDFS文件->RDD
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)

    //TODO 提取
    val filteredRDD = initRDD
      .map(line => {
        val cols = line.split('\t')
        //终端唯一码mac
        val sn = cols(2)
        //apk package
        val apkPackage = cols(5)
        //启动日期
        val launchTime = cols(6)
        //退出时间
        val exitTime = cols(7)
        val duration = cols(8)
        //省 可以用ip库解析
        val province = cols(10)
        //市
        val city = cols(11)

        sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city
      })
      //TODO 过滤
      .filter(line => {
        val cols = line.split('\t')

        //终端唯一码mac
        val sn = cols(0)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(1)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var launchTimeIsRight = ValidateUtils.isNumber(cols(2))
        if (launchTimeIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          launchTimeIsRight = cols(2).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(3))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(3).toLong > 0
        }

        var durationIsRight = ValidateUtils.isNumber(cols(4))
        if (durationIsRight) {
          durationIsRight = cols(4).toLong > 0
        }

        //省 可以用ip库解析
        val province = cols(5)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(6)
        //      val cityIsRight = !city.isEmpty() && !city.equals("未匹配")
        val cityIsRight = true

        provinceIsRight && cityIsRight && launchTimeIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })

    //TODO 理清启动时间 退出时间和时长的顺序
    val sortedRDD = filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(2).toLong, cols(3).toLong, cols(4).toLong).sorted
      //终端唯一码mac
      val sn = cols(0)
      //apk package
      val apkPackage = cols(1)
      val launchTime = sortedTimeList.apply(1)
      val exitTime = sortedTimeList.apply(2)
      val duration = sortedTimeList.apply(0)
      //省 可以用ip库解析
      val province = cols(5)
      //市
      val city = cols(6)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })

    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)

    //TODO 根据牌照修改com.tianci.movieplatform包名
    val sn2LicenseRDD = sqlContext.sql("select distinct sn,license from hr.terminal_partition where br = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(1).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(0)
        val launchTime = cols(2)
        val exitTime = cols(3)
        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        //不包含包名
        val other = sn + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      val sn = cols(0)
      val launchTime = cols(1)
      val exitTime = cols(2)
      val duration = cols(3)
      val province = cols(4)
      val city = cols(5)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(1).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD)

    //TODO 分近三天

    //TODO 第一天

    val currentDayStartTime = TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE)


    val ffDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -3)

    val firstDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val secondDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -1)

    //    val firstDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 2)
    //    val secondDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 1)


    val firstDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime > ffDayStartTime && launchTime < secondDayStartTime && launchTime < exitTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < firstDayStartTime) {
        launchTime = firstDayStartTime
      }

      var exitTime = cols(3).toLong
      if (exitTime > secondDayStartTime) {
        exitTime = secondDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(firstDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    //TODO 第二天
    val secondDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime < currentDayStartTime && launchTime > firstDayStartTime && launchTime < exitTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < secondDayStartTime) {
        launchTime = secondDayStartTime
      }

      var exitTime = cols(3).toLong
      if (exitTime > currentDayStartTime) {
        exitTime = currentDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(secondDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })


    //TODO 当天
    val currentDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime < new DateTime(currentDayStartTime).plusDays(1).getMillis && launchTime >
        secondDayStartTime && launchTime < exitTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < currentDayStartTime) {
        launchTime = currentDayStartTime
      }

      val exitTime = cols(3).toLong

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = currentDate

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    val allRDD = firstDayRDD.union(secondDayRDD).union(currentDayRDD)


    //TODO 分时
    val splitTimeRdd = allRDD

      .flatMap(line => {

        val cols = line.split("\t")

        //终端唯一码
        val sn = cols(0)
        //apk package
        val apkPackage = cols(1)

        val launchTime = cols(2).toLong

        val exitTime = cols(3).toLong

        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        val launchDate = cols(7)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", launchTime, exitTime)

        tmpArrayBuffer

      })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val province = cols(i)
        i = i + 1

        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        val sn = cols(i)
        i = i + 1

        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)
      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")
    //TODO 关联apkinfo 过滤
   //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //TODO  写入到Hbase
      .foreachPartition(lines => {

            val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
//            val mutator = HBaseUtils.getMutator("test_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine, brand))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )
    sqlContext.uncacheTable("tb_apk_extract")
    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.joda.time.DateTime

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJob15 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
//    val conf = new SparkConf()
//      .setMaster("local[1]")
//      .setAppName("coocaa-ApkDataLoadJob")
//    val sc = new SparkContext(conf)
//    run(sc, "2016-11-20")
//
//     sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {

    //TODO 原始日志格式
    //    113.249.247.212	COOCAAOS_TV	fca386edf206	appStatus	1483372857784	com.tianci.appstore	1483372726640	1483372736868	10228	2017-01-02 23:58:56	重庆市	重庆市	E5	8S82

    //TODO 转换当前日期为yyyyMMdd
    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //TODO HDFS文件->RDD
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)

    //TODO 提取
    val filteredRDD = initRDD
      .map(line => {
        val cols = line.split('\t')
        //终端唯一码mac
        val sn = cols(2)
        //apk package
        val apkPackage = cols(5)
        //启动日期
        val launchTime = cols(6)
        //退出时间
        val exitTime = cols(7)
        val duration = cols(8)
        //省 可以用ip库解析
        val province = cols(10)
        //市
        val city = cols(11)

        sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city
      })
      //TODO 过滤
      .filter(line => {
        val cols = line.split('\t')

        //终端唯一码mac
        val sn = cols(0)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(1)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var launchTimeIsRight = ValidateUtils.isNumber(cols(2))
        if (launchTimeIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          launchTimeIsRight = cols(2).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(3))
        if (exitTimeIsRight) {
          exitTimeIsRight = cols(3).toLong > 0
        }

        var durationIsRight = ValidateUtils.isNumber(cols(4))
        if (durationIsRight) {
          durationIsRight = cols(4).toLong > 0
        }

        //省 可以用ip库解析
        val province = cols(5)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(6)
        //      val cityIsRight = !city.isEmpty() && !city.equals("未匹配")
        val cityIsRight = true

        provinceIsRight && cityIsRight && launchTimeIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })

    //TODO 理清启动时间 退出时间和时长的顺序
    val sortedRDD = filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(2).toLong, cols(3).toLong, cols(4).toLong).sorted
      //终端唯一码mac
      val sn = cols(0)
      //apk package
      val apkPackage = cols(1)
      val launchTime = sortedTimeList.apply(1)
      val exitTime = sortedTimeList.apply(2)
      val duration = sortedTimeList.apply(0)
      //省 可以用ip库解析
      val province = cols(5)
      //市
      val city = cols(6)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })

    //    val sqlContext = new SQLContext(sc)
    val sqlContext = new HiveContext(sc)

    //TODO 根据牌照修改com.tianci.movieplatform包名
    val sn2LicenseRDD = sqlContext.sql("select distinct sn,license from hr.terminal_partition where br = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(1).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(0)
        val launchTime = cols(2)
        val exitTime = cols(3)
        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        //不包含包名
        val other = sn + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      val sn = cols(0)
      val launchTime = cols(1)
      val exitTime = cols(2)
      val duration = cols(3)
      val province = cols(4)
      val city = cols(5)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(1).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD).distinct()

    //TODO 分近三天



    val currentDayStartTime = TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE)


    val ffDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -3)

    val firstDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val secondDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -1)

    //    val firstDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 2)
    //    val secondDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 1)

    //TODO 第一天
    val firstDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime > ffDayStartTime && launchTime < secondDayStartTime && launchTime < exitTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < firstDayStartTime) {
        launchTime = firstDayStartTime
      }

      var exitTime = cols(3).toLong
      if (exitTime > secondDayStartTime) {
        exitTime = secondDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(firstDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    //TODO 第二天
    val secondDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime < currentDayStartTime && launchTime > firstDayStartTime && launchTime < exitTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < secondDayStartTime) {
        launchTime = secondDayStartTime
      }

      var exitTime = cols(3).toLong
      if (exitTime > currentDayStartTime) {
        exitTime = currentDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(secondDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })


    //TODO 当天
    val currentDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime < new DateTime(currentDayStartTime).plusDays(1).getMillis && launchTime >
        secondDayStartTime && launchTime < exitTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < currentDayStartTime) {
        launchTime = currentDayStartTime
      }

      val exitTime = cols(3).toLong

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = currentDate

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    val allRDD = firstDayRDD.union(secondDayRDD).union(currentDayRDD)


    //TODO 分时
    val splitTimeRdd = allRDD

      .flatMap(line => {

        val cols = line.split("\t")

        //终端唯一码
        val sn = cols(0)
        //apk package
        val apkPackage = cols(1)

        val launchTime = cols(2).toLong

        val exitTime = cols(3).toLong

        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        val launchDate = cols(7)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", launchTime, exitTime)

        tmpArrayBuffer

      })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val province = cols(i)
        i = i + 1

        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        val sn = cols(i)
        i = i + 1

        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)
      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")
    //TODO 关联apkinfo 过滤
   //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  ap.sn,
        |  ap.apkPackage,
        |  ap.date,
        |  ap.hour,
        |  SUM(ap.launchCnt) launchCnts,
        |  SUM(ap.duration) dura
        |FROM
        |  tb_apk_extract ap
        |GROUP BY
        |  ap.sn,
        |  ap.apkPackage,
        |  ap.date,
        |  ap.hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //inner join hr.apkinfo ai
      //on (ap.apkPackage=ai.packagename)
      //TODO  写入到Hbase
      .foreachPartition(lines => {

//            val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
//            val mutator = HBaseUtils.getMutator("test_apk_active_fact")
            val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine, brand))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )
    sqlContext.uncacheTable("tb_apk_extract")
    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{SQLContext, Row}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJobGQ {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-10-26")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {


    //截取analysisDate的年月 加上当月一号作为过滤条件
    //    var checkTime = sdf2.parse(analysisDate.substring(0, 5) + "01").getTime()

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

//    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161026\\aowei_appStatus20161026.txt"

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)


    //提取部分数据
    val filteredRDD = validateFilter(checkTime, initRDD)

    //排序
    val sortedRDD = sortTimeData(checkTime, filteredRDD)

    //过滤时长
    val filtered2RDD = sortedRDD.filter(line => {

      true
    })


        val sqlContext = new SQLContext(sc)
//    val sqlContext = new HiveContext(sc)

    //    val snLicenseArr = sqlContext.sql("select distinct sn,license from hr.terminal where brand = 'COOCAA' and license" +
    //      " = 'yinhe' or license  = 'tencent'").collect()

    //    snLicenseArr.map { row => {
    //      println(row.toString())
    //    }
    //    }

    //    val snLicenseArrBV = sc.broadcast(snLicenseArr)

    //分时
    //    val splitTimeRdd = splitTimeByHour(analysisDate, filtered2RDD)
    val splitTimeRdd = filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      var apkPackage = cols(5)
      //      if (apkPackage.equals("com.tianci.movieplatform")) {
      //        snLicenseArrBV.value.foreach(ele => {
      //          if (sn.equals(ele.get(0))) {
      //            if (ele.get(1).equals("tencent")) {
      //              apkPackage = "腾讯launcher"
      //            }
      //
      //            if (ele.get(1).equals("yinhe")) {
      //              apkPackage = "爱奇艺launcher"
      //            }
      //          }
      //
      //        })
      //      }

      val launchDate = TimeUtils.convertTimeStamp2DateStr(cols(6).toLong, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })



    //映射成临时表中的行

    val cleanedRDDRows = map2Row(splitTimeRdd)

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")



    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |  hour,
        |  SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date,
        |  hour
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })
      //    写入到Hbase
      .foreachPartition(lines => {

      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
      val mutator = HBaseUtils.getMutator("tlog_apk_active_fact")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      //            val myConf = HBaseConfiguration.create()
      //            myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //            myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //            val hbaseConn = ConnectionFactory.createConnection(myConf);
      //            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact_test"))


      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" +
            duration + "\t" + currentDate

          //          println(sortedLine)


          mutator.mutate(HBaseUtils.getPut_apkGQ(sortedLine))

          //          accum += 1

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

    sqlContext.uncacheTable("tb_apk_extract")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }


  //////////////////////////////////过滤格式或错误数据//////////////////////////////////////////
  def validateFilter(checkTime: Long, initRDD: RDD[String]): RDD[String] = {
    initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })
  }

  /////////////////////////理清启动时间 退出时间和时长的顺序/////////////////////////////////////
  def sortTimeData(checkTime: Long, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

      var launchTime = sortedTimeList.apply(1)

      cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

    })
  }

  //////////////////////////////按小时拆分数据//////////////////////////////////////////////////////////////
  def splitTimeByHour(analysisDate: String, filteredRDD: RDD[String]): RDD[String] = {
    filteredRDD.flatMap(line => {

      val cols = line.split("\t")
      //省
      val province = cols(10)
      //市
      val city = cols(11)
      //终端唯一码
      val sn = cols(2)
      //apk package
      val apkPackage = cols(5)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(cols(6).toLong, TimeUtils.DAY_DATE_FORMAT_ONE)

      val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", cols(6).toLong, cols(7).toLong)

      tmpArrayBuffer

    })
  }

  ////////////////////////////////////映射成临时表的行////////////////////
  def map2Row(splitTimeRdd: RDD[String]): RDD[Row] = {
    splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        //省
        val province = cols(i)
        i = i + 1

        //市
        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)

      })
  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object ApkDataLoadJobGQ01 {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-11")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {


    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate

    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()

    val splitTimeRdd = initRDD

      .filter(line => {
        val cols = line.split('\t')

        //省 可以用ip库解析
        val province = cols(10)
        val provinceIsRight = !province.isEmpty()

        //市
        val city = cols(11)
        //      val cityIsRight = !city.isEmpty() && !city.equals("匹配")
        val cityIsRight = true

        //终端唯一码mac
        val sn = cols(2)
        val snIsRight = !sn.isEmpty()

        //apk package
        val apkPackage = cols(5)
        val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


        //启动日期
        var dateIsRight = ValidateUtils.isNumber(cols(6))
        if (dateIsRight) {
          //        dateIsRight = cols(6).toLong > 0
          dateIsRight = cols(6).toLong > 0
        }

        //退出时间
        var exitTimeIsRight = ValidateUtils.isNumber(cols(7))

        val durationIsRight = ValidateUtils.isNumber(cols(8))

        provinceIsRight && cityIsRight && dateIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

      })

      .map(line => {
        val cols = line.split('\t')

        val sortedTimeList = List[Long](cols(6).toLong, cols(7).toLong, cols(8).toLong).sorted

        var launchTime = sortedTimeList.apply(1)

        cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTime + "\t" + sortedTimeList.apply(2) + "\t" + sortedTimeList.apply(0) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)

      })


      .map(line => {

        val cols = line.split("\t")

        //终端唯一码
        val sn = cols(2)

        //apk package
        val apkPackage = cols(5)

        //日志日期
        val dim_date = TimeUtils.convertTimeStamp2DateStr(cols(6).toLong, TimeUtils.DAY_DATE_FORMAT_ONE)

        //次数
        val cnt = 1


        //时长
        val duration = (cols(7).toLong - cols(6).toLong) / 1000



        sn + "\t" + apkPackage + "\t" + dim_date + "\t" + cnt + "\t" + duration

      })



    //映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0

        //终端唯一码
        val sn = cols(i)
        i = i + 1

        //apk package
        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val dim_date = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1


        //时长
        val duration = cols(i).toLong




        //启动的当天日期
        //如果是之前日期的数据 统一改成当天时间
        //      val todayTimeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)
        //      val today = TimeUtils.convertTimeStamp2DateStr(todayTimeStamp, TimeUtils.DAY_DATE_FORMAT_ONE)

        Row(sn, apkPackage, dim_date, launchCnt, duration)

      })

    //    cleanedRDDRows.foreach(println(_))

    //映射成临时表
    val schema = StructType(
      Seq(
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")

    val sqlContext = new HiveContext(sc)

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    sqlContext.cacheTable("tb_apk_extract")

    //累加变量作为rowkey构成元素
    //    val accum = sc.accumulator(0L)


    //    //统计分析
    sqlContext.sql(
      """
        |SELECT
        |  sn,
        |  apkPackage,
        |  date,
        |   SUM(launchCnt) launchCnts,
        |  SUM(duration) dura
        |FROM
        |  tb_apk_extract
        |GROUP BY
        |  sn,
        |  apkPackage,
        |  date
      """.stripMargin).rdd
      //    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("gq_apk_active_fact")

      //            val myConf = HBaseConfiguration.create()
      //            myConf.set("hbase.zookeeper.quorum", "192.168.79.131")
      //            myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //            val hbaseConn = ConnectionFactory.createConnection(myConf);
      //            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact_test"))

      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString


          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + currentDate + "\t" +
            launchCnt + "\t" + duration


          //                    println(sortedLine)

          mutator.mutate(HBaseUtils.getPut_apk_gq(sortedLine))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )
    sqlContext.uncacheTable("tb_apk_extract")


  }


}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object ApkHourTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_apk_hoursql(analysisDate), HiveSql.tracker_tv_apk_hourtable, 25)

        val parsql =
            """
              |insert overwrite table hr.tracker_tv_apk_hour_partition partition(date)
              |select key,brand,license,province,city,apk,tv_date,tv_hour,terminal_cnt,tv_date from hr.tracker_tv_apk_hour
              |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
            """

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object ApkHourTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object ApkHourTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        HiveToHbase.write(sqlc, HiveSql.getTracker_total_tv_apk_hoursql(analysisDate), HiveSql.tracker_total_tv_apk_hourtable, 31)

        val parsql =
            """
              |insert overwrite table hr.tracker_total_tv_apk_hour_partition partition(date)
              |select key,brand,license,province,city,apk,tv_date,tv_hour,terminal_cnt,tv_date from hr.tracker_total_tv_apk_hour
              |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
            """

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.time

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将apk推总数据导入到分区表
  */
object ApkTimeTotal2Partition {

    def main(args: Array[String]) {
  val currentDate = "2017-01-01"
  val sql = "INSERT OVERWRITE TABLE hr.tracker_total_apk_fact_partition PARTITION (date='" + currentDate + "') " +
    "select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_total_apk_fact where dim_date='" + currentDate + "'"

  println(sql)

    }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    val sql = "INSERT OVERWRITE TABLE hr.tracker_total_apk_fact_partition PARTITION (date='" + currentDate + "') " +
      "select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_total_apk_fact where dim_date='" + currentDate + "'"

    println(sql)

    sqlContext.sql(sql)



    //加载三天前的数据
    //    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate,TimeUtils.DAY_DATE_FORMAT_ONE,-2)
    //    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime,TimeUtils.DAY_DATE_FORMAT_ONE)


    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_total_apk_fact_partition PARTITION (date='" + twoDaysAgoDate + "') " +
    //      "select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_total_apk_fact where dim_date='" + twoDaysAgoDate + "'")

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_total_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_total_apk_fact")

  }

}
package com.avcdata.spark.job.total.time

import com.avcdata.spark.job.total.Sql
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define APK次数和时长推总
  *
  */
object ApkTimeTotalJob {

  val log = Logger.getLogger(getClass.getName)

  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("ApkTimeTotalJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")
    sc.stop()

  }

  /////////////////////////////////////////////test//////////////////////////////////////////////////

  def run(sc: SparkContext, analysisDate: String) = {


    val hiveContext: HiveContext = new HiveContext(sc)


    val totalDF: DataFrame = hiveContext.sql(Sql.getTracker_total_apk_active_fact_HQL(analysisDate))

    //    totalDF.write.mode(SaveMode.Append).saveAsTable("hr.tracker_total_apk_fact")

    val totalRDD = totalDF.rdd

    //TODO 特殊应用 单独乘系数
    val totalDF2 = hiveContext.sql(Sql.getTracker_total_apk_active_fact_vsttj_HQL(analysisDate))

    val totalRDD2 = totalDF2.rdd


      //TODO 过滤异常值
      .filter(line=>{
      var i = 0

      val key = line(i).toString
      i = i + 1

      val sn = line(i).toString
      i = i + 1

      val apk = line(i).toString
      i = i + 1

      val date = line(i).toString
      i = i + 1

      val hour = line(i).toString
      i = i + 1

      val cnt = line(i).toString
      i = i + 1

      val duration = line(i).toString

      duration.toDouble >= 0

    })

    totalRDD.union(totalRDD2)
      .foreachPartition(lines => {

        val mutator = HBaseUtils.getMutator(Sql.tracker_total_apk_active_fact_HtableName)


        try {

          lines.foreach(line => {

            var i = 0

            val key = line(i).toString
            i = i + 1

            val sn = line(i).toString
            i = i + 1

            val apk = line(i).toString
            i = i + 1

            val date = line(i).toString
            i = i + 1

            val hour = line(i).toString
            i = i + 1

            val cnt = line(i).toString
            i = i + 1

            val duration = line(i).toString

            val sortedLine = key + "\t" + sn + "\t" + apk + "\t" + date + "\t" + hour + "\t" + cnt + "\t" + duration

            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_total_apk(sortedLine))
          })
          mutator.flush()

        } finally {
          mutator.close()
        }
      }

      )




    //加载数据到hive分区表
    //    hiveContext.sql("set hive.exec.dynamic.partition=true")
    //    hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //    hiveContext.sql("INSERT INTO TABLE hr.tracker_total_apk_fact_partition PARTITION (date='" + analysisDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_total_apk_fact where dim_date='" + analysisDate + "'")

  }


}package com.avcdata.spark.job.total.time

import com.avcdata.spark.job.total.Sql
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define APK次数和时长推总
  *
  */
object ApkTimeTotalJob_toMonth3 {

  val log = Logger.getLogger(getClass.getName)

  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("ApkTimeTotalJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")
    sc.stop()

  }

  /////////////////////////////////////////////test//////////////////////////////////////////////////

  def run(sc: SparkContext, analysisDate: String) = {


    val hiveContext: HiveContext = new HiveContext(sc)


    val totalDF: DataFrame = hiveContext.sql(Sql.getTracker_total_apk_active_fact_HQL(analysisDate))

    //    totalDF.write.mode(SaveMode.Append).saveAsTable("hr.tracker_total_apk_fact")

    val totalRDD = totalDF.rdd

    totalRDD
      .foreachPartition(lines => {

        val mutator = HBaseUtils.getMutator(Sql.tracker_total_apk_active_fact_HtableName)


        try {

          lines.foreach(line => {

            var i = 0

            val key = line(i).toString
            i = i + 1

            val sn = line(i).toString
            i = i + 1

            val apk = line(i).toString
            i = i + 1

            val date = line(i).toString
            i = i + 1

            val hour = line(i).toString
            i = i + 1

            val cnt = line(i).toString
            i = i + 1

            val duration = line(i).toString

            val sortedLine = key + "\t" + sn + "\t" + apk + "\t" + date + "\t" + hour + "\t" + cnt + "\t" + duration

            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_total_apk(sortedLine))
          })
          mutator.flush()

        } finally {
          mutator.close()
        }
      }

      )

    //TODO 特殊应用 单独乘系数
    val totalDF2: DataFrame = hiveContext.sql(Sql.getTracker_total_apk_active_fact_vsttj_HQL(analysisDate))

    val totalRDD2 = totalDF2.rdd

    totalRDD2
      .foreachPartition(lines => {

        val mutator = HBaseUtils.getMutator(Sql.tracker_total_apk_active_fact_HtableName)

        try {

          lines.foreach(line => {

            var i = 0

            val key = line(i).toString
            i = i + 1

            val sn = line(i).toString
            i = i + 1

            val apk = line(i).toString
            i = i + 1

            val date = line(i).toString
            i = i + 1

            val hour = line(i).toString
            i = i + 1

            val cnt = line(i).toString
            i = i + 1

            val duration = line(i).toString

            val sortedLine = key + "\t" + sn + "\t" + apk + "\t" + date + "\t" + hour + "\t" + cnt + "\t" + duration

            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_total_apk(sortedLine))
          })
          mutator.flush()

        } finally {
          mutator.close()
        }
      }

      )


    //加载数据到hive分区表
    //    hiveContext.sql("set hive.exec.dynamic.partition=true")
    //    hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //    hiveContext.sql("INSERT INTO TABLE hr.tracker_total_apk_fact_partition PARTITION (date='" + analysisDate + "') select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_total_apk_fact where dim_date='" + analysisDate + "'")

  }


}package com.avcdata.spark.job.total.tnumpre

import java.util.Date

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object ApkTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt - 1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt - 1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" + "'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt - 1).toString
            }

        }

        var month = ""
        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_apksql_daily(analysisDate), HiveSql.tracker_tv_apktable, 24)

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_apksql_weekly(analysisDate), HiveSql.tracker_tv_apktable, 24)
        }

        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_apksql_monthly(month, analysisDate), HiveSql.tracker_tv_apktable, 24)
        }

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_apksql_30days(analysisDate), HiveSql.tracker_tv_apktable, 24)

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_apksql_7days(analysisDate), HiveSql.tracker_tv_apktable, 24)

        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_apk_partition partition(date)
                  |select key,brand,license,province, city,apk,period, tv_date, terminal_cnt,tv_date from hr.tracker_tv_apk
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<='""" + analysisDate +"""'
                """
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_apk_partition partition(date)
                  |select key,brand,license,province, city,apk,period, tv_date, terminal_cnt,tv_date from hr.tracker_tv_apk
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 7) and tv_date<='""" + analysisDate +"""'
                """
        } else {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_apk_partition partition(date)
                  |select key,brand,license,province, city,apk,period, tv_date, terminal_cnt,tv_date from hr.tracker_tv_apk
                  |WHERE tv_date='""" + analysisDate +"""'
                """
        }

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object ApkTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object ApkTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt - 1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt - 1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" + "'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt - 1).toString
            }

        }

        var subDate = "0"
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            subDate = "32"
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            subDate = "7"
        }

//        HiveToHbase.write(sqlc, HiveSql.getTracker_total_tv_apksql(analysisDate, subDate), HiveSql.tracker_total_tv_apktable, 30)

        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_apk_partition partition(date)
                  |select key,brand,license,province,city,apk,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_apk
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<=DATE_SUB('""" + analysisDate +"""', 0)
                """
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_apk_partition partition(date)
                  |select key,brand,license,province,city,apk,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_apk
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 7) and tv_date<=DATE_SUB('""" + analysisDate +"""', 0)
                """
        } else {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_apk_partition partition(date)
                  |select key,brand,license,province,city,apk,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_apk
                  |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
                """
        }

        sqlc.sql(parsql.stripMargin)
    }


}

package com.avcdata;

import com.avcdata.config.ConfigInfo;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.builder.SpringApplicationBuilder;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.context.annotation.ComponentScan;

import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;


/**
 * @author Gary Russell
 * @since 4.2
 */
@Slf4j
@SpringBootApplication
@ComponentScan
@EnableAutoConfiguration
@EnableConfigurationProperties(ConfigInfo.class)
public class Application {

    @Autowired
    static ConfigInfo configInfo;

    public static void main(String[] args) throws Exception {
        ConfigurableApplicationContext context
                = new SpringApplicationBuilder(Application.class)
                .web(false)
                .run(args);

        DateFormat format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        log.debug("start程序>>>NO.3>>>>>>>>>>{}", format.format(new Date()));
        TransferByBatch transferByBatch = context.getBean("transferByBatch", TransferByBatch.class);
        transferByBatch.processMessagestart();
    }

}package com.avcdata.etl.common.udf

/**
  * 处理数据的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/12 12:40
  */
object ArrayUDF
{
  /*
   * 过滤数据的空值
   */
  val arrayFilterEmptyValue = (values: Seq[String]) =>
  {
    if (null != values)
    {
      values.filter(_ != null).map(_.trim).filter(_ != "")
    }
    else
    {
      null
    }
  }
}
package com.avcdata.etl.common.udf

/**
  * 处理数据的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/12 12:40
  */
object ArrayUDF
{
  /*
   * 过滤数据的空值
   */
  val arrayFilterEmptyValue = (values: Seq[String]) =>
  {
    if (null != values)
    {
      values.filter(_ != null).map(_.trim).filter(_ != "")
    }
    else
    {
      null
    }
  }
}
package com.avcdata.etl.common.udf

/**
  * 处理数据的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/12 12:40
  */
object ArrayUDF
{
  /*
   * 过滤数据的空值
   */
  val arrayFilterEmptyValue = (values: Seq[String]) =>
  {
    if (null != values)
    {
      values.filter(_ != null).map(_.trim).filter(_ != "")
    }
    else
    {
      null
    }
  }
}
package com.avcdata.etl.common.udf

/**
  * 处理数据的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/12 12:40
  */
object ArrayUDF
{
  /*
   * 过滤数据的空值
   */
  val arrayFilterEmptyValue = (values: Seq[String]) =>
  {
    if (null != values)
    {
      values.filter(_ != null).map(_.trim).filter(_ != "")
    }
    else
    {
      null
    }
  }
}
package com.avcdata;

import lombok.Data;

import java.util.Map;

/**
 * Created by wxy on 2/29/16.
 */
@Data
public class AttrVO {

    private String catId;
    private String url;
    private String platform;
    private String category1;
    private String collectiontime;

}
package com.avcdata.spark.job.common

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * Created by avc on 2017/4/5.
  * 新增终端
  * 近三个月内，第一次使用指定应用的终端数
  */
object Auconstitute {
    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("new")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimApkCol = Bytes.toBytes("dim_apk")
        val dimDateCol = Bytes.toBytes("dim_date")

        val date = analysisDate.substring(0, 7) + "-01"
        val currDateStart = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd") //统计月开始时间
        val currDateEnd = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd") //统计月结束时间

        val hiveContext = new HiveContext(sc)

        //当前周
        val sql1 =
            """
              |select dim_sn,sum(fact_duration) fact_duration,sum(fact_cnt) fact_cnt,appname,brand,license,province from
              |(select dim_sn,fact_duration,fact_cnt, dim_apk from hr.tracker_apk_fact_partition
              |where date>='2017-03-27' and date<'2017-04-03' and fact_duration>=0) a
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') b
              |join
              |(select sn,province,license,brand from hr.sample_terminal_three) c
              |on a.dim_sn=c.sn and a.dim_apk=b.packagename
              |group by dim_sn,appname,brand,license,province
            """
        val currWeek = hiveContext.sql(sql1.stripMargin).mapPartitions(items =>{
            items.map(item =>{
                (item(0).toString+"0x01"+item(3).toString, item(1).toString+"0x01"+item(2).toString)
            })
        })
        println("----currWeek :" + currWeek.count())

        //当前周往前推一周
        val sql2 =
            """
              |select dim_sn,appname,brand,license,province from
              |(select dim_sn,fact_duration,fact_cnt, dim_apk from hr.tracker_apk_fact_partition
              |where date>='2017-03-27' and date<'2017-04-03' and fact_duration>=0) a
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') b
              |join
              |(select sn,province,license,brand from hr.sample_terminal_three) c
              |on a.dim_sn=c.sn and a.dim_apk=b.packagename
            """

        val firstWeek = hiveContext.sql(sql2.stripMargin).mapPartitions(items =>{
            items.map(item =>{
                (item(0).toString+"0x01"+item(3).toString, item(1).toString+"0x01"+item(2).toString)
            })
        })
        println("----firstWeek :" + firstWeek.count())

        //当前周往前推二周
        val sql3 =
            """
              |select dim_sn, sum(fact_duration) as fact_duration ,sum(fact_cnt) as fact_cnt, dim_apk from hr.tracker_apk_fact_partition
              |where date>='2017-03-13' and
              |date<'2017-03-20' and fact_duration>=0
              |group by dim_sn,dim_apk
            """
        val secWeek = hiveContext.sql(sql3.stripMargin).mapPartitions(items =>{
            items.map(item =>{
                (item(0).toString+"0x01"+item(3).toString, item(1).toString+"0x01"+item(2).toString)
            })
        })
        println("----secWeek :" + secWeek.count())

        //当前周往前推三周
        val sql4 =
            """
              |select dim_sn, sum(fact_duration) as fact_duration ,sum(fact_cnt) as fact_cnt, dim_apk from hr.tracker_apk_fact_partition
              |where date>='2017-03-06' and
              |date<'2017-03-13' and fact_duration>=0
              |group by dim_sn,dim_apk
            """
        val thWeek = hiveContext.sql(sql4.stripMargin).mapPartitions(items =>{
            items.map(item =>{
                (item(0).toString+"0x01"+item(3).toString, item(1).toString+"0x01"+item(2).toString)
            })
        })
        println("----thWeek :" + thWeek.count())

        //当前周往前推四周
        val sql5 =
            """
              |select dim_sn, sum(fact_duration) as fact_duration ,sum(fact_cnt) as fact_cnt, dim_apk from hr.tracker_apk_fact_partition
              |where date>='2017-02-27' and
              |date<'2017-03-06' and fact_duration>=0
              |group by dim_sn,dim_apk
            """
        val fourWeek = hiveContext.sql(sql5.stripMargin).mapPartitions(items =>{
            items.map(item =>{
                (item(0).toString+"0x01"+item(3).toString, item(1).toString+"0x01"+item(2).toString)
            })
        })
        println("----fourWeek :" + fourWeek.count())

        val one = currWeek.leftOuterJoin(firstWeek).filter(x => !x._2.toString().contains("None")).mapPartitions(items =>{
            items.map(item =>{
                (item._1, item._2._1)
            })
        })
        println("one : " + one.count())

        val two = one.leftOuterJoin(secWeek).filter(x => !x._2.toString().contains("None")).mapPartitions(items =>{
            items.map(item =>{
                (item._1, item._2._1)
            })
        })
        println("two : " + two.count())

        val three = two.leftOuterJoin(thWeek).filter(x => !x._2.toString().contains("None")).mapPartitions(items =>{
            items.map(item =>{
                (item._1, item._2._1)
            })
        })
        println("three : " + three.count())

        val four = three.leftOuterJoin(fourWeek).filter(x => !x._2.toString().contains("None")).mapPartitions(items =>{
            items.map(item =>{
                (item._1, item._2._1)
            })
        })
        println("four : " + four.count())

        val resql =
            """
              |select b.brand,b.license,b.province,d.appname,'连续四周活跃'  as period,
              |count(distinct w.dim_sn) as uv ,sum(w.fact_cnt) as vv,sum(w.fact_duration)/3600 as time_length,
              |"2017-04-03" as date
              |from tmp4 w
              |join  apkInfo d
              |on w.dim_apk=d.packagename
              |join province b
              |on w.dim_sn=b.sn
              |group by b.brand,b.license,b.province,d.appname;
            """.stripMargin
    }

    def main(args: Array[String]): Unit = {
        val analysisDate = "2017-02-16"
        val date = analysisDate.substring(0, 7) + "-01"
        val currDateStart = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val currDateEnd = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val hisMaxDate = DateTime.parse(currDateStart).plusMonths(-1).toString("yyyyMM")
        val hisMinDate = DateTime.parse(currDateStart).plusMonths(-3).toString("yyyyMM")
        println(currDateStart + ", " + currDateEnd + ", " + hisMaxDate + ", " + hisMinDate)
    }
}
package com.avcdata.etl.common.udf

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import org.slf4j.LoggerFactory

import scala.collection.mutable

/**
  * 根据库名、表名、字段名获取可用时间周期信息
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/10 16:46
  */
object AvailableTimePeriodUDF
{
  private val logger = LoggerFactory.getLogger(AvailableTimePeriodUDF.getClass)

  private val availableTimePeriods = mutable.Map[(String, String, String, String, String, String), String]()

  /**
    * 根据HIVE数据库名、表名、字段名获取可用月份信息
    */
  val availableMonth = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String, yearOffset: Int, monthOffset: Int) =>
  {
    //获取最大月度
    val maxMonth = availableMaxValue(connUri, username, password, dbName, tblName, fieldName)

    val ca = Calendar.getInstance()

    val sdf = new SimpleDateFormat("yyyy.MM.dd")
    ca.setTime(sdf.parse(s"20$maxMonth.01"))

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)

    new SimpleDateFormat("yy.MM").format(ca.getTime)
  }

  /**
    * 从数据库或调度器获取月度
    */
  val dbOrCoordMonth = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                        , yearOffset: Int, monthOffset: Int, coordDate: String, useCoordDate: String) =>
  {
    if (useCoordDate.toBoolean)
    {
      val ca = Calendar.getInstance()

      val sdf = new SimpleDateFormat("yyyy-MM-dd")
      ca.setTime(sdf.parse(coordDate))

      ca.add(Calendar.YEAR, yearOffset)
      ca.add(Calendar.MONTH, monthOffset)

      new SimpleDateFormat("yy.MM").format(ca.getTime)
    }
    else
    {
      availableMonth(connUri, username, password, dbName, tblName, fieldName, yearOffset, monthOffset)
    }
  }

  /**
    * 根据HIVE数据库名、表名、字段名获取可用月份信息
    */
  val availableWeek = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String, yearOffset: Int, weekOffset: Int) =>
  {
    //获取最大周度
    val maxWeek = availableMaxValue(connUri, username, password, dbName, tblName, fieldName)

    DateTimeUDF.yearWeek(maxWeek, yearOffset, weekOffset)
  }

  /**
    * 从数据库或调度器获取周度
    */
  val dbOrCoordWeek = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                       , yearOffset: Int, weekOffset: Int, coordDate: String, useCoordDate: String) =>
  {
    if (useCoordDate.toBoolean)
    {
      DateTimeUDF.dateToYearWeek(coordDate, "yyyy-MM-dd", yearOffset, 0, 7 * weekOffset, "false")
    }
    else
    {
      availableWeek(connUri, username, password, dbName, tblName, fieldName, yearOffset, weekOffset)
    }
  }

  /**
    * 根据HIVE数据库名、表名、字段名获取可用日期信息
    */
  val availableDay = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String, dayOffset: Int) =>
  {
    //获取最大周度
    val maxDate = availableMaxValue(connUri, username, password, dbName, tblName, fieldName)

    calcOffsetDate(maxDate, dayOffset)
  }

  /**
    * 从数据库或调度器获取日期
    */
  val dbOrCoordDate = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                       , dayOffset: Int, coordDate: String, useCoordDate: String) =>
  {
    if (useCoordDate.toBoolean)
    {
      calcOffsetDate(coordDate, dayOffset)
    }
    else
    {
      availableDay(connUri, username, password, dbName, tblName, fieldName, dayOffset)
    }
  }

  /**
    * 统一获取时间周期
    */
  val dbOrCoordPeriod = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                         , yearOffset: Int, offset: Int, coordDate: String, useCoordDate: String, timeType: String, isBegin: String) =>
  {
    val periodValue = timeType.toUpperCase match
    {
      case "M" => dbOrCoordMonth(connUri, username, password, dbName, tblName, fieldName, yearOffset, offset, coordDate, useCoordDate)
      case "W" => dbOrCoordWeek(connUri, username, password, dbName, tblName, fieldName, yearOffset, offset, coordDate, useCoordDate)
      case "D" => dbOrCoordDate(connUri, username, password, dbName, tblName, fieldName, offset, coordDate, useCoordDate)
      case _ => throw new IllegalArgumentException(s"Unsupported timeType <$timeType>")
    }

    if (isBegin.toBoolean)
    {
      s"${periodValue.substring(0, periodValue.length - 2)}01"
    }
    else
    {
      periodValue
    }
  }

  private def calcOffsetDate(maxDate: String, dayOffset: Int): String =
  {
    //格式化日期
    val parsedDate = new SimpleDateFormat("yyyy-MM-dd").parse(maxDate)

    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(parsedDate)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    new SimpleDateFormat("yyyy-MM-dd").format(ca.getTime)
  }

  private def availableMaxValue(connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String): String =
  {
    val timeKey = (connUri, username, password, dbName, tblName, fieldName)
    availableTimePeriods.getOrElse(timeKey, availableTimePeriods.synchronized[String]
      {
        availableTimePeriods.getOrElse(timeKey,
          {
            logger.info(s"Obtain the available $fieldName period from DB, the key is $timeKey")

            //获取最大时间
            val availableTimeValue = LoanPattern.using(JDBCConnectionPool(connUri, username, password))
            { conn =>

              LoanPattern.using(conn.prepareStatement("SELECT max_value FROM avc_available_time_period " +
                " WHERE db_name = ? " +
                " AND tbl_name = ? " +
                " AND field_name = ?"))
              { ps =>

                ps.setString(1, dbName)
                ps.setString(2, tblName)
                ps.setString(3, fieldName)

                LoanPattern.using(ps.executeQuery())
                { rs =>

                  if (rs.next())
                  {
                    rs.getString("max_value")
                  }
                  else
                  {
                    throw new IllegalArgumentException(s"Can not find the max_value for primary key $dbName-$tblName-$fieldName")
                  }
                }
              }
            }

            availableTimePeriods.put(timeKey, availableTimeValue)

            availableTimeValue
          })
      })
  }
}
package com.avcdata.etl.common.udf

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import org.slf4j.LoggerFactory

import scala.collection.mutable

/**
  * 根据库名、表名、字段名获取可用时间周期信息
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/10 16:46
  */
object AvailableTimePeriodUDF
{
  private val logger = LoggerFactory.getLogger(AvailableTimePeriodUDF.getClass)

  private val availableTimePeriods = mutable.Map[(String, String, String, String, String, String), String]()

  /**
    * 根据HIVE数据库名、表名、字段名获取可用月份信息
    */
  val availableMonth = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String, yearOffset: Int, monthOffset: Int) =>
  {
    //获取最大月度
    val maxMonth = availableMaxValue(connUri, username, password, dbName, tblName, fieldName)

    val ca = Calendar.getInstance()

    val sdf = new SimpleDateFormat("yyyy.MM.dd")
    ca.setTime(sdf.parse(s"20$maxMonth.01"))

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)

    new SimpleDateFormat("yy.MM").format(ca.getTime)
  }

  /**
    * 从数据库或调度器获取月度
    */
  val dbOrCoordMonth = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                        , yearOffset: Int, monthOffset: Int, coordDate: String, useCoordDate: String) =>
  {
    if (useCoordDate.toBoolean)
    {
      val ca = Calendar.getInstance()

      val sdf = new SimpleDateFormat("yyyy-MM-dd")
      ca.setTime(sdf.parse(coordDate))

      ca.add(Calendar.YEAR, yearOffset)
      ca.add(Calendar.MONTH, monthOffset)

      new SimpleDateFormat("yy.MM").format(ca.getTime)
    }
    else
    {
      availableMonth(connUri, username, password, dbName, tblName, fieldName, yearOffset, monthOffset)
    }
  }

  /**
    * 根据HIVE数据库名、表名、字段名获取可用月份信息
    */
  val availableWeek = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String, yearOffset: Int, weekOffset: Int) =>
  {
    //获取最大周度
    val maxWeek = availableMaxValue(connUri, username, password, dbName, tblName, fieldName)

    DateTimeUDF.yearWeek(maxWeek, yearOffset, weekOffset)
  }

  /**
    * 从数据库或调度器获取周度
    */
  val dbOrCoordWeek = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                       , yearOffset: Int, weekOffset: Int, coordDate: String, useCoordDate: String) =>
  {
    if (useCoordDate.toBoolean)
    {
      DateTimeUDF.dateToYearWeek(coordDate, "yyyy-MM-dd", yearOffset, 0, 7 * weekOffset, "false")
    }
    else
    {
      availableWeek(connUri, username, password, dbName, tblName, fieldName, yearOffset, weekOffset)
    }
  }

  /**
    * 根据HIVE数据库名、表名、字段名获取可用日期信息
    */
  val availableDay = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String, dayOffset: Int) =>
  {
    //获取最大周度
    val maxDate = availableMaxValue(connUri, username, password, dbName, tblName, fieldName)

    calcOffsetDate(maxDate, dayOffset)
  }

  /**
    * 从数据库或调度器获取日期
    */
  val dbOrCoordDate = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                       , dayOffset: Int, coordDate: String, useCoordDate: String) =>
  {
    if (useCoordDate.toBoolean)
    {
      calcOffsetDate(coordDate, dayOffset)
    }
    else
    {
      availableDay(connUri, username, password, dbName, tblName, fieldName, dayOffset)
    }
  }

  /**
    * 统一获取时间周期
    */
  val dbOrCoordPeriod = (connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String
                         , yearOffset: Int, offset: Int, coordDate: String, useCoordDate: String, timeType: String, isBegin: String) =>
  {
    val periodValue = timeType.toUpperCase match
    {
      case "M" => dbOrCoordMonth(connUri, username, password, dbName, tblName, fieldName, yearOffset, offset, coordDate, useCoordDate)
      case "W" => dbOrCoordWeek(connUri, username, password, dbName, tblName, fieldName, yearOffset, offset, coordDate, useCoordDate)
      case "D" => dbOrCoordDate(connUri, username, password, dbName, tblName, fieldName, offset, coordDate, useCoordDate)
      case _ => throw new IllegalArgumentException(s"Unsupported timeType <$timeType>")
    }

    if (isBegin.toBoolean)
    {
      s"${periodValue.substring(0, periodValue.length - 2)}01"
    }
    else
    {
      periodValue
    }
  }

  private def calcOffsetDate(maxDate: String, dayOffset: Int): String =
  {
    //格式化日期
    val parsedDate = new SimpleDateFormat("yyyy-MM-dd").parse(maxDate)

    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(parsedDate)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    new SimpleDateFormat("yyyy-MM-dd").format(ca.getTime)
  }

  private def availableMaxValue(connUri: String, username: String, password: String, dbName: String, tblName: String, fieldName: String): String =
  {
    val timeKey = (connUri, username, password, dbName, tblName, fieldName)
    availableTimePeriods.getOrElse(timeKey, availableTimePeriods.synchronized[String]
      {
        availableTimePeriods.getOrElse(timeKey,
          {
            logger.info(s"Obtain the available $fieldName period from DB, the key is $timeKey")

            //获取最大时间
            val availableTimeValue = LoanPattern.using(JDBCConnectionPool(connUri, username, password))
            { conn =>

              LoanPattern.using(conn.prepareStatement("SELECT max_value FROM avc_available_time_period " +
                " WHERE db_name = ? " +
                " AND tbl_name = ? " +
                " AND field_name = ?"))
              { ps =>

                ps.setString(1, dbName)
                ps.setString(2, tblName)
                ps.setString(3, fieldName)

                LoanPattern.using(ps.executeQuery())
                { rs =>

                  if (rs.next())
                  {
                    rs.getString("max_value")
                  }
                  else
                  {
                    throw new IllegalArgumentException(s"Can not find the max_value for primary key $dbName-$tblName-$fieldName")
                  }
                }
              }
            }

            availableTimePeriods.put(timeKey, availableTimeValue)

            availableTimeValue
          })
      })
  }
}
package com.avcdata.bean;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;

@ConfigurationProperties(prefix = "avc", ignoreUnknownFields = true)
@Data
public class AVCConfigurationProperties {
    private String env;
    private String version;
}
package com.avcdata.etl.common.util.encryption.rsa

import java.io.ByteArrayInputStream
import java.io.ByteArrayOutputStream
import java.io.File
import java.io.FileInputStream
import java.io.FileOutputStream

import com.avcdata.etl.common.pattern.LoanPattern
import com.sun.jersey.core.util.Base64

object Base64Util
{
  private val CACHE_SIZE: Int = 1024

  @throws[Exception]
  def decode(base64: String): Array[Byte] =
  {
    Base64.decode(base64.getBytes)
  }

  @throws[Exception]
  def encode(bytes: Array[Byte]): String =
  {
    new String(Base64.encode(bytes))
  }

  @throws[Exception]
  def encodeFile(filePath: String): String =
  {
    val bytes: Array[Byte] = fileToByte(filePath)
    encode(bytes)
  }

  @throws[Exception]
  def decodeToFile(filePath: String, base64: String)
  {
    val bytes: Array[Byte] = decode(base64)
    byteArrayToFile(bytes, filePath)
  }

  @throws[Exception]
  def fileToByte(filePath: String): Array[Byte] =
  {
    var data: Array[Byte] = new Array[Byte](0)
    val file: File = new File(filePath)
    if (file.exists)
    {
      LoanPattern.using(new FileInputStream(file))
      {in =>

        LoanPattern.using(new ByteArrayOutputStream(2048))
        { out =>

          val cache: Array[Byte] = new Array[Byte](CACHE_SIZE)
          var nRead: Int = 0
          while (
          {
            nRead = in.read(cache)
            nRead != -1
          })
          {
            {
              out.write(cache, 0, nRead)
              out.flush()
            }
          }

          data = out.toByteArray
        }
      }
    }

    data
  }

  @throws[Exception]
  def byteArrayToFile(bytes: Array[Byte], filePath: String)
  {
    val destFile = new File(filePath)
    if (!destFile.getParentFile.exists) destFile.getParentFile.mkdirs
    destFile.createNewFile

    LoanPattern.using(new ByteArrayInputStream(bytes))
    { in =>

      LoanPattern.using(new FileOutputStream(destFile))
      { out =>

        val cache: Array[Byte] = new Array[Byte](CACHE_SIZE)
        var nRead: Int = 0
        while (
        {
          nRead = in.read(cache)
          nRead != -1
        })
        {
          {
            out.write(cache, 0, nRead)
            out.flush()
          }
        }
      }
    }
  }
}
package com.avcdata.etl.common.util.encryption.rsa

import java.io.ByteArrayInputStream
import java.io.ByteArrayOutputStream
import java.io.File
import java.io.FileInputStream
import java.io.FileOutputStream

import com.avcdata.etl.common.pattern.LoanPattern
import com.sun.jersey.core.util.Base64

object Base64Util
{
  private val CACHE_SIZE: Int = 1024

  @throws[Exception]
  def decode(base64: String): Array[Byte] =
  {
    Base64.decode(base64.getBytes)
  }

  @throws[Exception]
  def encode(bytes: Array[Byte]): String =
  {
    new String(Base64.encode(bytes))
  }

  @throws[Exception]
  def encodeFile(filePath: String): String =
  {
    val bytes: Array[Byte] = fileToByte(filePath)
    encode(bytes)
  }

  @throws[Exception]
  def decodeToFile(filePath: String, base64: String)
  {
    val bytes: Array[Byte] = decode(base64)
    byteArrayToFile(bytes, filePath)
  }

  @throws[Exception]
  def fileToByte(filePath: String): Array[Byte] =
  {
    var data: Array[Byte] = new Array[Byte](0)
    val file: File = new File(filePath)
    if (file.exists)
    {
      LoanPattern.using(new FileInputStream(file))
      {in =>

        LoanPattern.using(new ByteArrayOutputStream(2048))
        { out =>

          val cache: Array[Byte] = new Array[Byte](CACHE_SIZE)
          var nRead: Int = 0
          while (
          {
            nRead = in.read(cache)
            nRead != -1
          })
          {
            {
              out.write(cache, 0, nRead)
              out.flush()
            }
          }

          data = out.toByteArray
        }
      }
    }

    data
  }

  @throws[Exception]
  def byteArrayToFile(bytes: Array[Byte], filePath: String)
  {
    val destFile = new File(filePath)
    if (!destFile.getParentFile.exists) destFile.getParentFile.mkdirs
    destFile.createNewFile

    LoanPattern.using(new ByteArrayInputStream(bytes))
    { in =>

      LoanPattern.using(new FileOutputStream(destFile))
      { out =>

        val cache: Array[Byte] = new Array[Byte](CACHE_SIZE)
        var nRead: Int = 0
        while (
        {
          nRead = in.read(cache)
          nRead != -1
        })
        {
          {
            out.write(cache, 0, nRead)
            out.flush()
          }
        }
      }
    }
  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object BhRestdayChannelDistCnt {

  case class RestdayChannelDistCnt(
                                    sn: String,
                                    stat_date: String,
                                    period: String,
                                    channel: String,
                                    cnt: String,
                                    cluster_id: String
                                  )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("BhRestdayChannelDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._


    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)


      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snRestdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,restday_channel_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val restday_channel_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_CHANNEL_ARR)

      (sn + "\t" + stat_date + "\t" + period, restday_channel_dist_map)

    })

    val restdayChannelDistCntDF = snCidRDD.join(snRestdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val restday_channel_dist_map = line._2._2

      val resultArr = new Array[RestdayChannelDistCnt](restday_channel_dist_map.size)

      val channel_arr = restday_channel_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = RestdayChannelDistCnt(
          sn,
          stat_date,
          period,
          channel_arr(i),
          restday_channel_dist_map.get(channel_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    restdayChannelDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    restdayChannelDistCntDF.registerTempTable("restdayChannelDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.channel,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,channel,sum(cnt) as cnt  from restdayChannelDistCnt group by stat_date,period,cluster_id,channel
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from restdayChannelDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_restday_channel_dist_cnt_k" + clusterNum, false,
      SaveMode.Append)

  }


}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object BhRestdayOcTimeDistCnt {

  case class RestdayOcTimeDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   dim_hour: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("BhRestdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snRestdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,restday_oc_dist from hr.user_vector_all").rdd
      .map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val Restday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_OC_HOUR_ARR)

      (sn + "\t" + stat_date + "\t" + period, Restday_oc_dist_map)

    })

    val RestdayOcTimeDistCntDF = snCidRDD.join(snRestdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val Restday_oc_dist_map = line._2._2

      val resultArr = new Array[RestdayOcTimeDistCnt](Restday_oc_dist_map.size)

      val hour_arr = Restday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = RestdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          Restday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF


    //    //TODO 注册临时表
    RestdayOcTimeDistCntDF.registerTempTable("RestdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from RestdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from RestdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_restday_oc_time_dist_cnt_k"+ clusterNum, false,
      SaveMode.Append)



  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object BhWorkdayChannelDistCnt {

  case class WorkdayChannelDistCnt(
                                    sn: String,
                                    stat_date: String,
                                    period: String,
                                    channel: String,
                                    cnt: Double,
                                    cluster_id: String
                                  )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("BhWorkdayChannelDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)


      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_channel_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_channel_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_CHANNEL_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_channel_dist_map)

    })

    val workdayChannelDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_channel_dist_map = line._2._2

      val resultArr = new Array[WorkdayChannelDistCnt](workday_channel_dist_map.size)

      val channel_arr = workday_channel_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayChannelDistCnt(
          sn,
          stat_date,
          period,
          channel_arr(i),
          workday_channel_dist_map.get(channel_arr(i)).get.toDouble,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayChannelDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayChannelDistCntDF.registerTempTable("workdayChannelDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.channel,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,channel,sum(cnt) as cnt  from workdayChannelDistCnt group by stat_date,period,cluster_id,channel
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayChannelDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_workday_channel_dist_cnt_k"+ clusterNum, false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}


object BhWorkdayOcTimeDistCnt {

  case class WorkdayOcTimeDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   dim_hour: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("BhWorkdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_oc_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_OC_HOUR_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_oc_dist_map)

    })

    val workdayOcTimeDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_oc_dist_map = line._2._2

      val resultArr = new Array[WorkdayOcTimeDistCnt](workday_oc_dist_map.size)

      val hour_arr = workday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          workday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayOcTimeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayOcTimeDistCntDF.registerTempTable("workdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_workday_oc_time_dist_cnt_k"+ clusterNum, false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.konka

import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * Created by avc on 2016/12/30.
  * 品牌的处理，供KONKA对某个sn设置对应的品牌作参考用
  */
object BrandProcessJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val brandDuraCol = Bytes.toBytes("dura")

        val duraSnCol = Bytes.toBytes("sn")
        val duraAppnameCol = Bytes.toBytes("appname")
        val duraDurationCol = Bytes.toBytes("duration")
        val duraMonthCol = Bytes.toBytes("month")

        //val baseApkRdd = sc.textFile("F:/avc/docs/konka/activity.log.2016-08-16")
        val baseApkRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/activity.log." + analysisDate)
        //val baseApkRdd = sc.textFile("/user/hdfs/rsync/KONKA/history/activity.log.since_10-01")
                .filter(x => x.split('|')(8).substring(5, 7) == analysisDate.substring(5, 7))
        .filter(x => x.split('|')(11).toInt > 0).mapPartitions(items => {
            items.map(line => {
                val cols = line.split('|')
                val sn = cols(0)
                val apk = cols(6)
                val duration = cols(11)
                //(apk + "|" + sn, duration.toLong)
                (apk, sn + "|" + duration)
            })
        })

        //println(baseApkRdd.count())

        //baseApkRdd.take(100).foreach(println)
        //println(baseApkRdd.count())

        /*val apkInfoRdd = sc.textFile("F:/avc/docs/konka/apkinfo.csv")
                .filter(x => x.length > 10)
            .mapPartitions(items => {
            items.map(line => {
                val cols = line.split(",")
                val packagename = cols(0)
                val appname = cols(1)
                (packagename, appname)
            })
        }).collect()*/
        val hiveCon = new HiveContext(sc)
        val apkInfoRdd = hiveCon.sql("select * from hr.apkinfo").rdd.filter(x => {
                val appname = x.toString().split(",")(1)
                appname.equals("CIBN环球影视") || appname.equals("银河·奇异果") || appname.equals("腾讯视频TV端")
            }).mapPartitions(items => {
            items.map(line => {
                val cols = line.toString().split(",")
                val packagename = cols(0).substring(1, cols(0).length)
                val appname = cols(1)
                (packagename, appname)
            })
        }).collect()

        val apkInfoBrocast = sc.broadcast(apkInfoRdd)
        //apkInfoRdd.take(150).foreach(println)

        val reRdd = baseApkRdd.mapPartitions(items =>{
            items.map(item =>{
                bf(item, apkInfoBrocast.value)
            })
        }).filter(x => x._1.length > 2 && x._1.split('|').length == 2).reduceByKey(_+_).sortBy(x=> x._2,true)


        reRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_ko_brand_dura"))
            try {

                items.foreach(line => {
                    val sn = line._1.split('|')(0)
                    val appname = line._1.split('|')(1)
                    val duration = (line._2.toFloat/3600).toString

                    val put = new Put(Bytes.toBytes(sn + appname + duration + "KO"))
                    put.addColumn(brandDuraCol, duraSnCol, Bytes.toBytes(sn))
                    put.addColumn(brandDuraCol, duraAppnameCol, Bytes.toBytes(appname))
                    put.addColumn(brandDuraCol, duraDurationCol, Bytes.toBytes(duration))
                    put.addColumn(brandDuraCol, duraMonthCol, Bytes.toBytes(analysisDate.substring(5, 7))) //.substring(5, 7)

                    mutator.mutate(put)
                })
                mutator.flush()

            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }

    def bf(item: (String, String), vu: Array[(String, String)]) : (String, Long) ={
        var apk = ""
        var sndura = ""
        var appname = ""
        vu.foreach(ele => {
            if (item._1.equals(ele._1)) {
                apk = item._1
                sndura = item._2
                appname = ele._2
            }
        })
        var sn = ""
        var dura:Long = 0
        if (sndura != "") {
            sn = sndura.split('|')(0)
            dura = sndura.split('|')(1).toLong
        }

        //println("----" + apk + ", " + appname + ", " + sn + "," + dura)
        (sn + "|" + appname, dura)
    }
}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.coocaa._
import com.avcdata.spark.job.epg.EpgDataLoadJob
import org.apache.log4j.Logger


object CCDataCleanExecutor {

  object test{
    def main(args: Array[String]) {
      println("asdfasdfd")
    }
  }

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //TODO EPG数据清洗  2core 4G  5
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@EpgDataLoadJob start...")
      EpgDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@EpgDataLoadJob end....")

      //TODO 终端  4core 4G  5
      println(analysisDate + "@COOCAA-TerminalDataLoadJob start....")
      TerminalDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@COOCAA-TerminalDataLoadJob start....")

    }


    //TODO apk 8core 8G  8
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
      ApkDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    }



    //TODO  到剧信息清洗  8core 8G 10
//    if (executePart.charAt(3) == '1') {
//      println(analysisDate + "@COOCAA-PlaysDataLoadJob start....")
//      PlaysDataLoadJob.run(sc, analysisDate);
//      println(analysisDate + "@COOCAA-PlaysDataLoadJob end....")
//      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob start....")
//      //      PlaysUnpassDataLoadJob.run(sc, analysisDate);
//      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob end....")
//    }


    //TODO 到剧终端信息清洗
//    if (executePart.charAt(4) == '1') {
//      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob start ... ")
//      PlayerTerminalLoadJob.run(sc, analysisDate);
//      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob end ... ")
//    }

  }

}package com.avcdata.spark.job.changhong

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

import org.apache.spark.util.Utils

/**
  * Created by wxc on 10/27/16.
  * 长虹apk清洗,根据endTime过滤数据
  */

case class ApkOpen(mac: String, actions: mutable.MutableList[ApkOpenItem]) {
}

case class ApkOpenItem(apk: String, duration: String, count: String, date: String)

case class ApkOpenResultByHour(apk: String, date: String, hour: String, duration: Int, cnt: Int)

object ChApkDataLoadJob {

    def getApkOpenResultByHour(item: ApkOpen): mutable.MutableList[ApkOpenResultByHour] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        val result = mutable.MutableList[ApkOpenResultByHour]()

        for (i <- 0 until item.actions.size) {
            val duration = item.actions.get(i).get.duration
            val endTime = item.actions.get(i).get.date
            val count = item.actions.get(i).get.count

            val map = DataUtil.durationSplitByHour(duration, endTime)
            val apk = item.actions.get(i).get.apk

            //求每个时段次数的思路：从开始时间到结束时间次数每小时先置为1，不够的补0，，多了的又从开始平铺1，以此类推
            var k: Int = 0
            val len = map.size
            var x: Int = 0
            var y: Int = 0
            if (len == 0) {
                x = 0
                y = 0
            } else {
                x = count.toInt / len
                y = count.toInt % len
            }

            map.keys.foreach { j =>

                if (x == 0) {
                    if (k < y) {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 1)
                    } else {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 0)
                    }
                } else {
                    if (k < y) {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x + 1)
                    } else {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x)
                    }
                }

                k += 1
            }

        }

        result
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val digitRegex = """^\d+$""".r
        val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimApkCol = Bytes.toBytes("dim_apk")
        val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val factCountCol = Bytes.toBytes("fact_cnt")
        val factDurationCol = Bytes.toBytes("fact_duration")

        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

        //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
        val baseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")
        val fpRdd = baseRdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("""","""")
                cols
            }).filter(x => x(0).length > 8).flatMap(x => {
                val mac = x(0).substring(8, x(0).length)
                val mutableArr = ArrayBuffer[String]()
                for (i <- 1 until x.length)
                    mutableArr += x(i)
                val c = mutableArr.map(y => (mac, y))
                c
            })
        }).filter(x => !x._2.contains("#")).filter(x => {
            val tm = x._2.split("time")
            tm.length > 1 && tm(1).length >= 20 && dateRegex.findFirstMatchIn(tm(1).substring(1, 20)) != None

        }) //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
            .filter(x => x._2.split(';').length == 4)
            .filter(x => {
                val dura = x._2.split(';')(3).split('|')(0)
                dura.length < 25 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None && handlnArr(dura, "=").toLong != 0
            }) //过滤duration后是乱码的数据
            .filter(x => {
                    var date = x._2.split("time")(1).substring(1, 11)
                    date == analysisDate
                }) //date
            .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count
            //.filter(y => y._2.split(';')(2).split("=")(1).toInt != 0) //count != 0

        val sqlc = new HiveContext(sc)
        val apkinfoRdd = sqlc.sql("select * from hr.apkinfo").mapPartitions(items => {
            items.map(line => {
                (line(0).toString, 1)
            })
        }).collect()

        val apkRdd = fpRdd.mapPartitions(items => {
            items.map(str => {
                val mac = str._1 //mac
                val astrs = str._2.split(';')
                val pack = handlnArr(astrs(0), "=") //包名
                val count = handlnArr(astrs(2), "=") //次数
                //println("time : " + astrs(3))
                val duration = handlnArr(astrs(3).split('|')(0), "=") //时长
                val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
                val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

                val xs = new ApkOpenItem(pack, duration, count, time)

                val terminalApk = new ApkOpen(mac, mutable.MutableList(xs))
                //(mac + time.substring(0, 10), terminalApk)
                (duration, pack, terminalApk)
                //terminalApk
            })
        }).filter(x => x._1.toLong > 0).mapPartitions(items => {
            items.map(item =>{
                (item._2, item._3)
            })
        })

        /*val apkinfoRdd1 = sqlc.sql("select * from hr.apkinfo").mapPartitions(items => {
            items.map(line => {
                (line(0).toString, 1)
            })
        })

        val amRdd1 = apkRdd.leftOuterJoin(apkinfoRdd1).filter(x => !x._2.toString().contains("None"))
                .mapPartitions(items =>{
                    items.map(item => {
                        item._2._1
                    })
                })
        println("1 : : " + amRdd1.count())*/

        val apkInfoBrocast = sc.broadcast(apkinfoRdd)
        val amRdd = apkRdd.mapPartitions(items =>{
            items.map(item => {
                apkBro(item, apkInfoBrocast.value)
            })
        })
        //println("2 : : " + amRdd.filter(x => x!= null).count())

        amRdd.filter(x => x!= null).foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact")) //tlog_apk_active_fact

            try {
                items.foreach(item => {

                    val mac = item.mac

                    getApkOpenResultByHour(item).filter(x => x != null).foreach(r => {
                        //hbase key 也就是行
                        val put = new Put(Bytes.toBytes(mac + r.apk + r.date.substring(0, 10) + r.hour + "CH"))
                        //'列族', '列名称:', '值'
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(mac))
                        put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(r.apk))
                        put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(r.date.substring(0, 10)))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
                        put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(r.cnt.toString))
                        put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))
                        //if ((r.hour.toInt) < 10)
                        //println(mac + "\t" + item.date + "\t" + r.apk + "\t"
                        //        + r.date + "\t" + (r.hour.toInt).toString + "\t" + r.cnt + "\t" + r.duration)
                        mutator.mutate(put)
                    })
                    mutator.flush()
                })

            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }

    def apkBro(item: (String, ApkOpen), vu: Array[(String, Int)]) : ApkOpen ={
        var p:ApkOpen = null

        vu.foreach(ele => {
            if (item._1.equals(ele._1)) {
                val apk = item._2
                p = new ApkOpen(apk.mac, apk.actions)
            }
        })

        p
    }

    /**
      * 处理数组
      *
      * @param str
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            "0"
        } else {
            var ele = arr(1)
            if (ele == "" || ele == null) {
                ele = "0"
            }
            ele
        }
    }
}

object test {
    def main(args: Array[String]): Unit = {
        /*val date = """"reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=atvMain;version=1.1.1;count=1;duration=5457396|time:2015-02-01 08:01:15""""
        println(date.split("time")(1).substring(1, 20))

        val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
        val dd = "2017-12-12 00:00:00"*/
        println("2573802785".toLong)
    }
}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.changhong._
import com.avcdata.spark.job.common.Helper
import org.apache.log4j.Logger

object CHDataCleanExecutor {


  //参数
  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //TODO 终端  4core 5G 4
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@CH-ChTerminalDataLoadJob start....")
      ChTerminalDataLoadJob.run(sc, analysisDate)
      ChSampleTerminal.run(sc, analysisDate)
      println(analysisDate + "@CH-ChTerminalDataLoadJob end....")
    }



    //TODO 开关机 4core 5G 4
    if (executePart.charAt(1) == '0') {
      println(analysisDate + "@CH-ChTerminalPowerOnDataLoadJob start....")
      ChTerminalPowerOnDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@CH-ChTerminalPowerOnDataLoadJob end....")
    }


    //TODO 直播  8core  10G  10
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@CH-ChLiveDataLoadJob start....")
      Chliveterminal.run(sc, analysisDate)
      ChLiveDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@CH-ChLiveDataLoadJob end....")
    }


    //TODO apk 8core  10G  8
    if (executePart.charAt(3) == '2') {
      println(analysisDate + "@CH-ChApkDataLoadJob start....")
      ChApkDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@CH-ChApkDataLoadJob end....")
    }



    //TODO  到剧信息清洗  4core  5G  8
//    if (executePart.charAt(4) == '3') {
//      println(analysisDate + "@CH-ChTVDataLoadJob start....")
//      ChFilm.run(sc, analysisDate);
//      ChTVDataLoadJob.run(sc, analysisDate);
//      println(analysisDate + "@CH-ChTVDataLoadJob end....")
//      //      println(analysisDate + "@CH-PlaysUnpassDataLoadJob start....")
//      //      PlaysUnpassDataLoadJob.run(sc, analysisDate);
//      //      println(analysisDate + "@CH-PlaysUnpassDataLoadJob end....")
//    }





  }

}
package com.avcdata.spark.job.changhong


import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.konka.{HiveToMysql, Sql}
import org.apache.log4j.Logger

/**
  * Created by wxc on 10/27/16.
  */
object ChExecutor {
    def main(args: Array[String]): Unit = {

        val log = Logger.getLogger(getClass.getName)

        val analysisDate = Helper.parseOptions(args, 0, "2016-11-14") //10-24--07-24--2016-11-14

        val executePart = Helper.parseOptions(args, 1, "100000000000000000000")

        println(analysisDate + ", " + executePart)

        //clean table


        log.debug("start job for " + analysisDate)


        val sc = Helper.sparkContext

        if (executePart.charAt(4) == '1') {
            HiveToMysql.write(sc, Sql.vbox_tv_ratesql, Sql.vbox_tv_ratetable, Helper.mysqlConf)
        }

        //use new sc for every sparksql


        //sc.stop()
    }

}package com.avcdata.spark.job.changhong

import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext

import scala.collection.mutable.ArrayBuffer

/**
  * Created by avc on 2016/12/12.
  * 长虹到剧的film的处理
  */
object ChFilm {
    def run(sc: SparkContext, analysisDate: String) = {
        val filmFamilyCol = Bytes.toBytes("film")

        val filmCidCol = Bytes.toBytes("film_cid")
        val filmNameCol = Bytes.toBytes("film_name")
        val filmTitleCol = Bytes.toBytes("film_title")
        val filmVidCol = Bytes.toBytes("film_vid")
        val filmUpdateTimeCol = Bytes.toBytes("film_update_time")

        //val filmTextRdd = sc.textFile("F:/avc/docs/changhong/2016-12-08/film.txt")
        val filmTextRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/film.txt")
        val filmRdd = filmTextRdd.mapPartitions(items => {
            items.map(line => {
                val strs = line.split("""},""")
                strs
            })
        }).flatMap(x => {
            val mutableArr = ArrayBuffer[(String, String)]()
            for (i <- 0 until x.length) {
                val strs = x(i).split(""","""")
                val dl = strs(0).split("""":"""")(1).substring(0, strs(0).split("""":"""")(1).indexOf("""""""))
                val name = strs(1).split("""":"""")(1).substring(0, strs(1).split("""":"""")(1).indexOf("""""""))
                val title = strs(2).split("""":"""")(1).substring(0, strs(2).split("""":"""")(1).indexOf("""""""))
                val vid = strs(3).split("""":"""")(1).substring(0, strs(3).split("""":"""")(1).indexOf("""""""))
                val cid = strs(4).split("""":"""")(1).substring(0, strs(4).split("""":"""")(1).indexOf("""""""))
                val update_time = strs(5).split("""":"""")(1).substring(0, strs(5).split("""":"""")(1).indexOf("""""""))
                mutableArr.append((vid, dl + "0x01" + title + "0x01" + name + "0x01" + cid + "0x01" + update_time))
            }
            val c = mutableArr.map(y => (y._1, y._2))
            c
        })

        //println("count : " + filmRdd.count())
        filmRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_film_active_fact"))

            try {
                items.foreach(item => {
                    val vid = item._1
                    val dl = item._2.split("0x01")(0)
                    val title = item._2.split("0x01")(1)
                    val name = item._2.split("0x01")(2)
                    val cid = item._2.split("0x01")(3)
                    val update_time = item._2.split("0x01")(4)
                    //hbase key 也就是行
                    val put = new Put(Bytes.toBytes(vid + cid + dl + "CH"))

                    //'列族', '列名称:', '值'
                    put.addColumn(filmFamilyCol, filmCidCol, Bytes.toBytes(cid))
                    put.addColumn(filmFamilyCol, filmNameCol, Bytes.toBytes(name))
                    put.addColumn(filmFamilyCol, filmTitleCol, Bytes.toBytes(title))
                    put.addColumn(filmFamilyCol, filmVidCol, Bytes.toBytes(vid))
                    put.addColumn(filmFamilyCol, filmUpdateTimeCol, Bytes.toBytes(update_time))
                    //println(r.title + "\t" + r.mac + "\t" + r.part + "\t" + item.date + "\t"
                    //    + "\t" + r.hour + "\t" + r.vv + "\t" + r.duration)
                    //file.wrApknameToFile3(r.title + "\t" + r.mac + "\t" + part + "\t" + r.start + "\t" + r.end  + "\t" + item.date + "\t"
                    //+ "\t" + r.hour + "\t" + r.vv + "\t" + r.duration)
                    mutator.mutate(put)
                })

                mutator.flush()
            } finally {
                //file.writer.close()
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}
package com.avcdata.spark.job.changhong

import com.avcdata.spark.job.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

/**
  * Created by wxy on 8/29/16.
  * 长虹历史数据的处理
  */
object ChHistoryTerminal {
    /**
      * 处理数组
      *
      * @param str
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            "其他"
        } else {
            var ele = arr(1)
            if (ele.equals("null") || ele.equals("unknown")) {
                ele = "其他"
            }
            ele
        }
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")


        val mardd = sc.textFile("/user/hdfs/rsync/CH/deviceInfo-history.txt")
        val maSpRdd = mardd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("\t")
                val mac = cols(1)
                val model = cols(3)
                val province = cols(10) //省
                var city = cols(11) //市
                if (province == "未知") {
                    city = "其他"
                }

                val area = Codearea.getArea(province) //大区
                val procductName = cols(2) //机型名称
                val clevel = Codearea.getCl(city)

                (mac, area, province, city, procductName, clevel, model)
            })
        })

        maSpRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal"))

            try {
                items.foreach(line => {
                    val sn = line._1
                    val brand = "CH"
                    val model = line._7
                    val lastPowerOn = ""
                    val size = ""
                    val area = line._2
                    val province = line._3
                    val city = line._4
                    val procductName = line._5
                    val clevel = line._6
                    val put = new Put(Bytes.toBytes(sn + "CH"))
                    put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes("tencent"))
                    put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                    put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                    put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                    put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                    put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                    put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                    put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))
                    println(sn + "\t" + area + "\t" + province + "\t" + city + model)
                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}
package com.avcdata.etl.common.util

import net.sourceforge.pinyin4j.PinyinHelper
import net.sourceforge.pinyin4j.format.{HanyuPinyinCaseType, HanyuPinyinOutputFormat, HanyuPinyinToneType, HanyuPinyinVCharType}

/**
  * 中文转拼音
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/25 12:42
  */
object ChineseToEnglish
{
  // 将汉字转换为全拼
  def getPinYin(src: String): String =
  {
    val srcChars = src.toCharArray
    val pyOutput = new HanyuPinyinOutputFormat()

    pyOutput.setCaseType(HanyuPinyinCaseType.LOWERCASE)
    pyOutput.setToneType(HanyuPinyinToneType.WITHOUT_TONE)
    pyOutput.setVCharType(HanyuPinyinVCharType.WITH_V)
    val destBuilder = new StringBuilder()
    try
    {
      srcChars.foreach
      { c =>
        // 判断是否为汉字字符
        if (java.lang.Character.toString(c).matches("[\\u4E00-\\u9FA5]+"))
        {
          destBuilder.append(PinyinHelper.toHanyuPinyinStringArray(c, pyOutput)(0))
        }
        else
        {
          destBuilder.append(java.lang.Character.toString(c))
        }
      }
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }

    destBuilder.toString()
  }

  // 返回中文的首字母
  def getPinYinHeadChar(str: String): String =
  {
    val destBuilder = new StringBuilder()

    str.foreach(c =>
    {
      val pys = PinyinHelper.toHanyuPinyinStringArray(c)

      if (null != pys) destBuilder.append(pys(0).charAt(0)) else destBuilder.append(c)
    })

    destBuilder.toString()
  }
}
package com.avcdata.etl.common.util

import net.sourceforge.pinyin4j.PinyinHelper
import net.sourceforge.pinyin4j.format.{HanyuPinyinCaseType, HanyuPinyinOutputFormat, HanyuPinyinToneType, HanyuPinyinVCharType}

/**
  * 中文转拼音
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/25 12:42
  */
object ChineseToEnglish
{
  // 将汉字转换为全拼
  def getPinYin(src: String): String =
  {
    val srcChars = src.toCharArray
    val pyOutput = new HanyuPinyinOutputFormat()

    pyOutput.setCaseType(HanyuPinyinCaseType.LOWERCASE)
    pyOutput.setToneType(HanyuPinyinToneType.WITHOUT_TONE)
    pyOutput.setVCharType(HanyuPinyinVCharType.WITH_V)
    val destBuilder = new StringBuilder()
    try
    {
      srcChars.foreach
      { c =>
        // 判断是否为汉字字符
        if (java.lang.Character.toString(c).matches("[\\u4E00-\\u9FA5]+"))
        {
          destBuilder.append(PinyinHelper.toHanyuPinyinStringArray(c, pyOutput)(0))
        }
        else
        {
          destBuilder.append(java.lang.Character.toString(c))
        }
      }
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }

    destBuilder.toString()
  }

  // 返回中文的首字母
  def getPinYinHeadChar(str: String): String =
  {
    val destBuilder = new StringBuilder()

    str.foreach(c =>
    {
      val pys = PinyinHelper.toHanyuPinyinStringArray(c)

      if (null != pys) destBuilder.append(pys(0).charAt(0)) else destBuilder.append(c)
    })

    destBuilder.toString()
  }
}
package com.avcdata.spark.job.changhong

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

import scala.collection.mutable

/*
  *长虹直播的清洗
  */
case class TVWatching(sn: String, date: String, area: String, actions: mutable.MutableList[TVWatchedItem]) {
}

case class TVWatchedItem(tv: String, date: String, startTime: String, endTime: String)

case class TVWatchedResultByMinute(tv: String, date: String, startTime: String, endTime: String,
                                   hour: String, minute: String, duration: Int, cnt: Int)


object ChLiveDataLoadJob {
    //val file = new FileUtil("live")
    def getTVWatchedResultByMinute(item: TVWatching): mutable.MutableList[TVWatchedResultByMinute] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        val result = mutable.MutableList[TVWatchedResultByMinute]()

        for (i <- 0 until item.actions.size) {
            val tv = item.actions.get(i).get.tv
            val date = item.actions.get(i).get.date
            val startTime = item.actions.get(i).get.startTime
            val endTime = item.actions.get(i).get.endTime
            val list = DataUtil.durationSplitByMinute(startTime, endTime)
            for (l <- list) {
                var hour = l._1
                var min = l._2._1
                var dura = l._2._2.split(";")(0)
                var cnt = l._2._2.split(";")(1)
                result += new TVWatchedResultByMinute(tv, date, startTime, endTime,
                    hour, min, dura.toInt, cnt.toInt)
            }
        }

        result
    }

    def run(sc: SparkContext, analysisDate: String) = {

        //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimChannelCol = Bytes.toBytes("dim_channel")
        val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val dimMinCol = Bytes.toBytes("dim_min")
        val factCntCol = Bytes.toBytes("fact_cnt")
        val factTimeLenghtCol = Bytes.toBytes("fact_time_length")

        //val preDate = DateTime.parse(analysisDate).plus(-1).toString("yyyy-MM-dd")--- -1减1  1不变
        val NextDate = DateTime.parse(analysisDate).plusDays(1).toString("yyyy-MM-dd")
        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
        //println(preDate + ", " + yesBeforeDate)

        //val tvBaseRdd = sc.textFile("F:/avc/docs/changhong/2016-11-14/livebroadcast.txt")
        val tvBaseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/livebroadcast.txt")
            .filter(x => x.split('\t').length == 11)
            .filter(x => {
                var date = x.split('\t')(5).substring(0, 10)
                date == analysisDate || date == NextDate || date == preDate || date == yesBeforeDate
            })
            .filter(x => x.split('\t')(0).length >= 12)

        val tvRdd = tvBaseRdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split('\t')
                val tv = cols(3) //频道名称
                var sn = cols(0) //sn--mac
                if (sn.length == 12) //aabbccddeeff-->aa:bb:cc:dd:ee:ff
                {
                    sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
                        ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
                }

                val date = cols(5).substring(0, 10) //日期
                val startTime = cols(5) //开始时间
                val endTime = cols(6) //结束时间
                val area = cols(4) //地域省份
                val xs = new TVWatchedItem(tv, date, startTime, endTime)
                val terminalTV = new TVWatching(sn, date, area, mutable.MutableList(xs))
                //(sn + date, terminalTV)
                terminalTV
            })
        })
        /*.reduceByKey((left, right) => {
                    left.actions ++= right.actions
                    left
                }).mapPartitions(items => {
                    items.map(item => {
                        val sortedList = item._2.actions.sortBy(_.date).clone()
                        item._2.actions.clear()
                        item._2.actions ++= sortedList
                        item._2
                    })
                })*/

        val count = tvRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_active_fact02"))
            //val table = hbaseConn.getTable(TableName.valueOf("tracker_live_active_fact"))


            try {
                items.foreach(item => {

                    val sn = item.sn

                    getTVWatchedResultByMinute(item).foreach(r => {
                        val put = new Put(Bytes.toBytes(sn + r.tv + item.date + r.hour + r.minute + "CH"))
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimChannelCol, Bytes.toBytes(CHTVMapping.getTVStandName(r.tv)))
                        //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
                        put.addColumn(dimFamilyCol, dimMinCol, Bytes.toBytes((r.minute.toInt).toString))
                        put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(r.cnt.toString))
                        put.addColumn(factFamilyCol, factTimeLenghtCol, Bytes.toBytes(r.duration.toString))

                        mutator.mutate(put)
                    })
                })
                mutator.flush()
            } finally {
                //file.writer.close()
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

object CHTVMapping extends Serializable {
    private val mapping = mutable.Map(
        ("中央8台" -> "CCTV-8"),
        ("中央12台" -> "CCTV-12"),
        ("中央1台" -> "CCTV-1"),
        ("黑龙江卫视" -> "黑龙江卫视"),
        ("中央9台" -> "CCTV-9"),
        ("中央5台" -> "CCTV-5"),
        ("贵州卫视" -> "贵州卫视"),
        ("中央14台" -> "CCTV-14"),
        ("东方卫视" -> "上海东方卫视"),
        ("上海卫视" -> "上海东方卫视"),
        ("湖北卫视" -> "湖北卫视"),
        ("东南卫视" -> "东南卫视"),
        ("广东卫视" -> "广东卫视"),
        ("中央7台" -> "CCTV-7"),
        ("湖南卫视" -> "湖南卫视"),
        ("山东卫视" -> "山东卫视"),
        ("北京卫视" -> "北京卫视"),
        ("青海卫视" -> "青海卫视"),
        ("中央10台" -> "CCTV-10"),
        ("旅游卫视" -> "旅游卫视"),
        ("甘肃卫视" -> "甘肃卫视"),
        ("重庆卫视" -> "重庆卫视"),
        ("中央3台" -> "CCTV-3"),
        ("新疆卫视" -> "新疆卫视"),
        ("厦门卫视频道" -> "厦门卫视"),
        ("中央13台" -> "CCTV-13"),
        ("辽宁卫视" -> "辽宁卫视"),
        ("山西卫视" -> "山西卫视"),
        ("中央2台" -> "CCTV-2"),
        ("宁夏卫视" -> "宁夏卫视"),
        ("安徽卫视" -> "安徽卫视"),
        ("河北卫视" -> "河北卫视"),
        ("中央6台" -> "CCTV-6"),
        ("浙江卫视" -> "浙江卫视"),
        ("江西卫视" -> "江西卫视"),
        ("河南卫视" -> "河南卫视"),
        ("中央11台" -> "CCTV-11"),
        ("广西卫视" -> "广西卫视"),
        ("江苏卫视" -> "江苏卫视"),
        ("四川卫视" -> "四川卫视"),
        ("中央15台" -> "CCTV-15"),
        ("云南卫视" -> "云南卫视"),
        ("中央4台" -> "CCTV-4"),
        ("内蒙古卫视" -> "内蒙古卫视"),
        ("西藏卫视" -> "西藏卫视"),
        ("深圳卫视" -> "深圳卫视"),
        ("天津卫视" -> "天津卫视"),
        ("陕西卫视" -> "陕西卫视"),
        ("吉林卫视" -> "吉林卫视"),
        ("CCTV-1综合" -> "CCTV-1"),
        ("CCTV-2财经" -> "CCTV-2"),
        ("CCTV-3综艺" -> "CCTV-3"),
        ("CCTV-4中文国际" -> "CCTV-4"),
        ("CCTV-5体育" -> "CCTV-5"),
        ("CCTV-6电影" -> "CCTV-6"),
        ("CCTV-7军事农业" -> "CCTV-7"),
        ("CCTV-8电视剧" -> "CCTV-8"),
        ("CCTV-9纪录" -> "CCTV-9"),
        ("CCTV-10科教" -> "CCTV-10"),
        ("CCTV-11戏曲" -> "CCTV-11"),
        ("CCTV-12社会与法" -> "CCTV-12"),
        ("CCTV-13新闻" -> "CCTV-13"),
        ("CCTV-14少儿" -> "CCTV-14"),
        ("CCTV-15音乐" -> "CCTV-15"),
        ("厦门卫视" -> "厦门卫视")
    )

    def getTVStandName(name: String): String = {
        mapping.get(name).getOrElse("其他")
    }
}
package com.avcdata.spark.job.changhong

/**
  * Created by avc on 2016/12/5.
  */

import com.avcdata.spark.job.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * 长虹直播的终端处理，推总的时候用到
  */
object Chliveterminal {

    /**
      * 处理数组
      *
      * @param str   字符串
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            null
        } else {
            val ele = arr(1)
            ele
        }
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")

        //val tvBaseRdd = sc.textFile("F:/avc/docs/changhong/data/livebroadcast.txt")
        val tvBaseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/livebroadcast.txt")
            //.filter(x => x.split('\t')(5).substring(0, 10) == analysisDate)
            .filter(x => x.split('\t')(0).length >= 12)

        val baseRdd = tvBaseRdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split('\t')
                var sn = cols(0) //sn--mac
                if (sn.length == 12) //aabbccddeeff-->aa:bb:cc:dd:ee:ff
                {
                    sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
                        ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
                }

                val province = cols(4) //地域省份
                val area = Codearea.getArea(ProMap.getPro(province)) //大区

                (sn, area+","+province)
            })
        }).distinct()

        val hiveContext = new HiveContext(sc)
        val teRdd = hiveContext.sql("select sn, province, city from hr.terminal where brand = 'CH'").mapPartitions(items =>{
            items.map(item => {
                val sn = item(0).toString
                val province = item(1).toString
                val city = item(2).toString
                (sn, province+","+city)
            })
        })

        val tvRdd = baseRdd.leftOuterJoin(teRdd).filter(x => !x._2.toString().contains("None"))

        tvRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_terminal_test"))

            try {
                items.foreach(item => {
                    val sn = item._1
                    val brand = "CH"
                    val model = ""
                    val lastPowerOn = ""
                    val size = ""
                    val area = item._2._1.split(",")(0)
                    val province = item._2._2.get.split(",")(0)
                    val city = item._2._2.get.split(",")(1)
                    val clevel = ""
                    val procductName = ""
                    val put = new Put(Bytes.toBytes(sn + "CH"))
                    put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes("tencent"))
                    put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                    put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                    put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                    put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                    put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                    put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                    put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))

                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

object ProMap {

    val provinceMap = Map(
        ("北京" -> "北京市"),
        ("天津" -> "天津市"),
        ("上海" -> "上海市"),
        ("重庆" -> "重庆市"),
        ("香港" -> "香港特别行政区"),
        ("河北" -> "河北省"),
        ("山西" -> "山西省"),
        ("内蒙古" -> "内蒙古自治区"),
        ("辽宁" -> "辽宁省"),
        ("吉林" -> "吉林省"),
        ("黑龙江" -> "黑龙江省"),
        ("江苏" -> "江苏省"),
        ("浙江" -> "浙江省"),
        ("安徽" -> "安徽省"),
        ("福建" -> "福建省"),
        ("江西" -> "江西省"),
        ("山东" -> "山东省"),
        ("河南" -> "河南省"),
        ("湖北" -> "湖北省"),
        ("湖南" -> "湖南省"),
        ("广东" -> "广东省"),
        ("广西" -> "广西壮族自治区"),
        ("海南" -> "海南省"),
        ("四川" -> "四川省"),
        ("贵州" -> "贵州省"),
        ("云南" -> "云南省"),
        ("西藏" -> "西藏自治区"),
        ("陕西" -> "陕西省"),
        ("甘肃" -> "甘肃省"),
        ("青海" -> "青海省"),
        ("宁夏" -> "宁夏回族自治区"),
        ("新疆" -> "新疆维吾尔自治区"),
        ("台湾" -> "台湾省")
    )

    def getPro(pro: String) : String = {
        provinceMap.get(pro).getOrElse("其他")
    }
}


package com.avcdata.spark.job.changhong

import com.avcdata.spark.job.changhong.ChApkDataLoadJob.handlnArr
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wxc on 10/27/16.
  * 长虹样本终端的处理，推总的时候用到，采集最近三个月的
  */

object ChSampleTerminal {

    def run(sc: SparkContext, analysisDate: String) = {
        val digitRegex = """^\d+$""".r
        val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")

        //apk
        //val apkrdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
        val apkrdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")
        val fpRdd = apkrdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("""","""")
                cols
            }).filter(x => x(0).length > 8).flatMap(x => {
                val mac = x(0).substring(8, x(0).length)
                val mutableArr = ArrayBuffer[String]()
                for (i <- 1 until x.length)
                    mutableArr += x(i)
                val c = mutableArr.map(y => (mac, y))
                c
            })
        })//有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
            .filter(x => x._2.split(';').length == 4)
            .filter(x => {
                val dura = x._2.split(';')(3).split('|')(0)
                dura.length < 25 && handlnArr(dura, "=").toLong != 0 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None
            }) //过滤duration后是乱码的数据
            .filter(x => x._2.split(';')(2).split("=").length > 1)
            .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count


        val amRdd = fpRdd.mapPartitions(items => {
            items.map(str => {
                val mac = str._1 //mac
                (mac, 1)
            })
        }).distinct()

        //开关机
        val terRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/heartbeats.txt")
        //val terRdd = sc.textFile("F:/avc/docs/changhong/data/heart.txt")
            .filter(x => x.split("""":"""").length > 4)
            .filter(x => x.split("""":"""")(4).length >= 12)
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split("""":"""")
                    var sn = ""
                    if (cols(4).length > 16)
                        sn = cols(4).substring(0, 17)
                    else {
                        sn = cols(4).substring(0, 12)
                        sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
                            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
                    }
                    (sn, 1)
                })
            }).distinct()

        val apkocRdd = amRdd.union(terRdd)

        val sqlc = new HiveContext(sc)
        val hiveDataFrame = sqlc.sql("select * from hr.terminal where brand = 'CH'")
            .mapPartitions(items => {
                items.map(line => {
                    (line(1).toString, line)
                })
            })

        val sampleRdd = apkocRdd.leftOuterJoin(hiveDataFrame)
                    .filter(x => !x._2.toString().contains("None"))
        //println("count : " + sampleRdd.count())

        sampleRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("sample_terminal_four"))

            try {
                items.foreach(item => {
                    val sn = item._1
                    val cols = item._2._2.get.toString().split(",")
                    val brand = cols(2)
                    val license = cols(10).replace("]", "")
                    val model = cols(9)
                    val lastPowerOn = cols(3)
                    val size = cols(8)
                    val area = cols(4)
                    val province = cols(5)
                    val city = cols(6)
                    val clevel = cols(7)

                    val put = new Put(Bytes.toBytes(sn + "CH"))
                    put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes(license))
                    put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                    put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                    put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                    put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                    put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                    put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                    put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))
                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }

}

object testsa {
    def main(args: Array[String]): Unit = {
        val x = "tencent]"
        println(x.replaceAll("]", ""))
    }
}
package com.avcdata.spark.job.test

import org.apache.spark.{SparkContext, SparkConf}

object ChSnStat {

  def main(args: Array[String]) {
//    addMaoHao("123456")
        val conf = new SparkConf()
          .setMaster("local[1]")
          .setAppName("ApkTimeTotalJob")
        val sc = new SparkContext(conf)
        run(sc, "2016-11-15")
        sc.stop()

  }

  def run(sc: SparkContext, s: String) = {

    val initRDD = sc.textFile("E:\\aowei\\tracker-job\\doc\\test\\ch-no-sn.txt")

    initRDD.map(line => {

      addMaoHao(line)
    })
      .foreach(println(_))
//      .repartition(1).saveAsTextFile("doc\\chsnstat")

  }


  def addMaoHao(mac: String): String = {
    val sb = new StringBuilder

    val charArr = mac.toCharArray
    for (i <- 0 until charArr.length) {
      if (i % 2 == 0 && i != 0) {
        sb.append(
          ":" + charArr(i)
        )
      }else{
        sb.append(charArr(i))
      }
    }
    sb.toString
  }

}
package com.avcdata.spark.job.test

import org.apache.spark.{SparkConf, SparkContext}

object ChSnStat02 {

  def main(args: Array[String]) {
    //    addMaoHao("123456")
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("ApkTimeTotalJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")
    sc.stop()

  }

  def run(sc: SparkContext, s: String) = {

    val initRDD = sc.textFile("E:\\aowei\\tracker-job\\doc\\test\\ch-no-sn.csv")

    val allSnRDD = initRDD.map(line => {

      line.split(",")(0).toString
    })

    val sampleTerminalThreeRDD = initRDD.filter(_.split(",").length > 1).map(line => {
      line.split(",")(1).toString
    })

        allSnRDD.subtract(sampleTerminalThreeRDD)
          .foreach(println(_))


    //    val initRDD = sc.textFile("E:\\aowei\\tracker-job\\doc\\test\\ch-no-sn.txt")
    //
    //    initRDD.map(line => {
    //
    //      addMaoHao(line)
    //    })
    //      .foreach(println(_))
    ////      .repartition(1).saveAsTextFile("doc\\chsnstat")

  }


  def addMaoHao(mac: String): String = {
    val sb = new StringBuilder

    val charArr = mac.toCharArray
    for (i <- 0 until charArr.length) {
      if (i % 2 == 0 && i != 0) {
        sb.append(
          ":" + charArr(i)
        )
      } else {
        sb.append(charArr(i))
      }
    }
    sb.toString
  }

}
package com.avcdata.spark.job.changhong

import com.avcdata.spark.job.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

/**
  * Created by wxy on 8/29/16.
  * 总的长虹的终端数
  */
object ChTerminalDataLoadJob {

    val pro = List("北京市","天津市","上海市","重庆市","香港特别行政区","河北省",
        "山西省","内蒙古自治区","辽宁省","吉林省","黑龙江省","江苏省","浙江省",
        "安徽省","福建省","江西省","山东省","河南省","湖北省","湖南省",
        "广东省","广西壮族自治区","海南省","四川省","贵州省","云南省",
        "西藏自治区","陕西省","甘肃省","青海省","宁夏回族自治区","新疆维吾尔自治区","台湾省"
    )
    /**
      * 处理数组
      *
      * @param str
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            "其他"
        } else {
            var ele = arr(1)
            if (ele.equals("null") || ele.equals("unknown")) {
                ele = "其他"
            }
            ele
        }
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")


        //val mardd = sc.textFile("F:/avc/docs/changhong/data/deviceInfo")
        val mardd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/deviceInfo.txt")
            .filter(x => x.split("""Mac":"""")(1).substring(0, 4) != "null")
            .filter(x => x.split(""""Data":""")(1).contains("province"))
        val maSpRdd = mardd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("""","""") //以","来隔
                val mac = cols(0).substring(8, cols(0).length)
                val mstrs = cols(1).split(';')
                var province = handlnArr(mstrs(5), "=") //省
                var city = "" //市
                var district = "" //区
                if (!pro.contains(province)) {
                    province = "其他"
                    city = "其他"
                    district = "其他"
                }
                else {
                    city = handlnArr(mstrs(6), "=") //市
                    if (city == null || city.equals("")) {
                        city = "其他"
                    }
                    district = handlnArr(mstrs(7), "=") //区
                }
                //val city = handlnArr(mstrs(6), "=") //市
                //val district = handlnArr(mstrs(7), "=") //区
                val area = Codearea.getArea(province) //大区
                val procductName = handlnArr(mstrs(1), "=") //机型名称
                val clevel = Codearea.getCl(city)

                (mac, area, province, city, procductName, clevel)
            })
        })
        //maSpRdd.foreach(println)

        maSpRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal"))

            try {
                items.foreach(line => {
                    val sn = line._1
                    val brand = "CH"
                    //val model = ""
                    val lastPowerOn = ""
                    val size = ""
                    val area = line._2
                    val province = line._3
                    val city = line._4
                    val procductName = line._5
                    val clevel = line._6
                    val put = new Put(Bytes.toBytes(sn + "CH"))
                    put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes("tencent"))
                    put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(procductName))
                    put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                    put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                    put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                    put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                    put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                    put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))
                    //println(sn + "\t" + area + "\t" + province + "\t" + city)
                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}
package com.avcdata.spark.job.changhong

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.{HashPartitioner, SparkContext}
import org.joda.time.Seconds

import scala.collection.mutable

/*
**长虹开关机的清洗
 */
case class Heart(sn: String, actions: mutable.MutableList[HeartItem]) {
}

case class HeartItem(mac: String, timestamp: String)

case class HeartResultByMinute(date: String, hour: String, duration: Int, cnt: Int)

/**
  * Created by wxc on 11/07/16.
  */
object ChTerminalPowerOnDataLoadJob {
    /**
      * 处理数组
      *
      * @param str
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            null
        } else {
            val ele = arr(1)
            ele
        }
    }

    def getSecondsToHourEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.hours).withSecondOfMinute(0).withMinuteOfHour(0)).getSeconds.abs
    }

    def getSecondsToMinuteEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.minutes).withSecondOfMinute(0)).getSeconds.abs
    }

    def getSecondsAbs(from: DateTime, to: DateTime): Int = {
        Seconds.secondsBetween(from, to).getSeconds.abs
    }

    def getHeartResultByMinute(item: Heart): mutable.MutableList[HeartResultByMinute] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var startItem: HeartItem = null
        var startTime: DateTime = null
        var endItem: HeartItem = null
        var endTime: DateTime = null
        var tempItem: HeartItem = null
        var tempTime: DateTime = null
        var flag = 0
        val result = mutable.MutableList[HeartResultByMinute]()
        var firstTime:DateTime = null
        var heartTime:Long = 0

        for (i <- 0 until item.actions.size) {
            if (startItem == null) {
                startItem = item.actions.get(i).get
                startTime = DateTime.parse(DataUtil.timestampToDate(startItem.timestamp), format)
                tempItem = startItem
                tempTime = startTime
            } else {
                endItem = item.actions.get(i).get
                endTime = DateTime.parse(DataUtil.timestampToDate(endItem.timestamp), format)
                heartTime = (endItem.timestamp.toLong - tempItem.timestamp.toLong) / 1000

                //120 是2分钟 时间超过20分钟的间隔也算一次开机
                if (heartTime < 120 * 10) {
                    if (endItem.mac == tempItem.mac) {
                        val startHour = tempTime.getHourOfDay
                        val endHour = endTime.getHourOfDay

                        //当不在同一小时的时候，做对应的处理
                        if (startHour != endHour) {
                            if (flag == 0) {
                                flag = 1
                                if (DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)>=("00:00")
                                    && DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)<=("00:02")) {
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, getSecondsToHourEnd(startTime) / 60, 0)
                                } else {
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, getSecondsToHourEnd(startTime) / 60, 1)
                                }
                            } else {
                                result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, 60, 0)
                            }

                        } else if (startHour == endHour) {
                            if (firstTime == null || flag == 0) {
                                firstTime = tempTime
                            }
                        }

                        tempItem = endItem
                        tempTime = endTime
                    }
                    //日志跳到另外一个Mac中也算是一次开机,需要把时长存储起来
                    else if (endItem.mac != tempItem.mac) {
                        result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), endTime.getHourOfDay.toString, endTime.getMinuteOfHour, 0)
                        flag = 0
                    }
                } else if (heartTime >= 120 * 10) {
                    if (flag == 1) {
                        if (firstTime == null) {
                            // 同时段的处理或者类似18:50:01,52,54,19:02,04-->19:24:01的处理，时长2分钟
                            result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 0)
                        } else {
                            // start为2017-05-01 01:20:24，tmp是2017-05-01 01:26:24，endTime为2017-05-01 01:54:50,时长是6分钟
                            result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, (tempTime.getMinuteOfHour-firstTime.getMinuteOfHour), 0)
                            firstTime = null
                        }
                    } else {
                        if (DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)>=("00:00")
                            && DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)<=("00:02")) {
                            //比如start为2017-05-01 00:01:24，tmp是2017-05-01 00:03:24，再次为endTime为2017-05-01 13:04:50
                            //这个时候开机次数为0，而且开机时长为1分钟
                            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 0)
                        } else {
                            if (tempTime.getHourOfDay == endTime.getHourOfDay && result.length == 0) {
                                if (firstTime != null) {//比如start为2017-05-01 01:20:24，23-->endTime为2017-05-01 01:54:50,时长是2分钟
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, (tempTime.getMinuteOfHour - firstTime.getMinuteOfHour), 1)
                                    firstTime = null
                                } else {//比如start为2017-05-01 01:20:24-->endTime为2017-05-01 01:54:50,时长是2分钟
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, 2, 1)
                                }

                            } else {//比如start为为2017-05-01 01:20:24，23-->endTime为2017-05-01 11:34:50,时长是3分钟
                                result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 1)
                            }
                        }
                    }

                    //如果时间间隔大于20分钟，则算一次开关机,这个时候从头开始算
                    startItem = endItem
                    startTime = endTime
                    flag = 0
                    tempItem = endItem
                    tempTime = endTime
                }
            }
        }

        if (flag == 1) {
            //最后一个小时的处理，比如：2017-01-09 18:59:00--2017-01-09 19:01:00,这个时候2017-01-09 19:01:00的1分钟就在这处理
            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), endTime.getHourOfDay.toString, endTime.getMinuteOfHour, 0)
            flag = 0
        } else {
            if (heartTime < 120* 10) {
                if (firstTime != null) {
                    //比如只有23时的数据
                    result += new HeartResultByMinute(DataUtil.timestampToDate(endItem.timestamp), endTime.getHourOfDay.toString, (endTime.getMinuteOfHour - firstTime.getMinuteOfHour), 1)
                    firstTime = null
                }
            }
        }

        result
    }

    def run(sc: SparkContext, analysisDate: String) = {

        //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")

        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSeriesNoCol = Bytes.toBytes("sn")
        //val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimPowerONDateCol = Bytes.toBytes("power_on_day")
        val dimPowerONTimeCol = Bytes.toBytes("power_on_time")
        val factPowerLenghtCol = Bytes.toBytes("power_on_length")
        val factCntCol = Bytes.toBytes("cnt")
        val digitRegex = """^\d+$""".r


        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
        val baseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/heartbeats.txt")
        //val baseRdd = sc.textFile("/user/hdfs/rsync/CH/2017-01-09/oc/oc"+analysisDate)
        //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/oc05")
                .filter(x => x.split("""":"""").length >= 4)
                .filter(x => x.contains("Mac") && x.split("""Mac":"""").length == 2 && x.split("""Mac":"""")(1).length >= 12)
                /*.filter(x => {
                    var timest = x.split("""timestamp":""")(1).substring(1, 14)
                    if (digitRegex.findFirstMatchIn(timest) == None) {
                        timest = x.split("""timestamp":""")(1).substring(0, 13)
                    }

                    var date = ""
                    if (digitRegex.findFirstMatchIn(timest) != None)
                        date = DataUtil.timestampToDate(timest).substring(0, 10)

                    date == analysisDate || date == preDate || date == yesBeforeDate
                })*/
            .mapPartitions(items => {
                items.map(line => {
                    val macs = line.split("""Mac":"""")
                    var sn = ""
                    if (macs(1).length > 16)
                        sn = macs(1).substring(0, 17)
                    else {
                        sn = macs(1).substring(0, 12)
                        sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
                            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
                    }

                    var timest = line.split("""timestamp":""")(1).substring(1, 14)
                    if (digitRegex.findFirstMatchIn(timest) == None) {
                        timest = line.split("""timestamp":""")(1).substring(0, 13)
                    }
                    val timestamp = timest
                    val xs = new HeartItem(sn, timestamp)
                    val terminalTV = new Heart(sn, mutable.MutableList(xs))
                    (sn, terminalTV)
                })
            })

        val i = 0
        val ter1Rdd = baseRdd.repartition(baseRdd.getNumPartitions + 10).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        })

        val terRdd = ter1Rdd.mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.timestamp).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })

        val count = terRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal_active_fact")) //tracker_terminal_active_fact

            try {
                items.foreach(x => {
                    val sn = x.sn
                    getHeartResultByMinute(x).foreach(r => {
                        val lastPowerOnDate = r.date.substring(0, 10)  //date
                        val lastPowerOnTime = r.hour  //hour
                        val timeLenth = r.duration
                        val cnt = r.cnt

                        val put = new Put(Bytes.toBytes(sn + lastPowerOnDate + lastPowerOnTime + "CH"))
                        put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                        //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimPowerONDateCol, Bytes.toBytes(lastPowerOnDate))
                        put.addColumn(dimFamilyCol, dimPowerONTimeCol, Bytes.toBytes((lastPowerOnTime.toInt).toString))
                        put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(timeLenth.toString))
                        put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(cnt.toString))
                        mutator.mutate(put)
                    })
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

object heartTest {
    def main(args: Array[String]): Unit = {
        val x1 = """{"ip":"/1.12.1.150:39450","timestamp":"1483611481078","mac":"{"Mac":"d8:47:10:a8:82:8a"}"}	1"""
        val x2 = """{"ip":"/1.12.1.150:39460","timestamp":1483611599037,"mac":"{"Mac":"d8:47:10:a8:82:8a"}"}	1"""

        println(x1.split("""timestamp":""")(1).substring(1, 14))
        println(x2.split("""timestamp":""")(1).substring(0, 13))

        println(x1.split("""Mac":"""")(1).substring(0, 17))
        println(x2.split("""Mac":"""")(1).substring(0, 17))
        println("2017-04-30 23:00:40".substring(11, 13))

        val item1:Heart = new Heart("d8:47:10:a8:82:8a", mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483974060000")) ++ //01
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483974180000")) ++ //03
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483975440000")) ++  //24
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483975620000")) ++  //27
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483975740000")) //29
        )
        ChTerminalPowerOnDataLoadJob.getHeartResultByMinute(item1)
    }
}
package com.avcdata.spark.job.changhong

import java.util.UUID

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * Created by avc on 2016/12/8.
  * 长虹到剧的清洗
  */
case class Playing(vid: String,awcid:String,categorys:String,title: String, mac: String, date: String, actions: mutable.MutableList[PlayerItem]) {
}

case class PlayerItem(name: String, action: String, time: String, title: String, tt: String)

case class PlayerResultByHour(title: String, part: String, hour: String, duration: Int, vv: Int, start: String, end: String)

object ChTVDataLoadJob {
    val digitRegex = """^\d+$""".r
    val dateRegex = """^(?:(?!0000)[0-9]{4}(?:(?:0[1-9]|1[0-2])(?:0[1-9]|1[0-9]|2[0-8])|(?:0[13-9]|1[0-2])-(?:29|30)|(?:0[13578]|1[02])-31)|(?:[0-9]{2}(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)-02-29)$""".r
    val dateRegex1 = """^(?:(?!0000)[0-9]{4})$""".r

    /**
      * 获取name和title中相同字符的个数
      *
      * @param str1 name
      * @param str2 title
      * @return
      */
    def getSameLen(str1: String, str2: String): (Int, Int) = {
        var len = 0
        for (i <- 0 until str1.length) {
            //极限挑战之皇家宝藏--极限挑战
            if (i == str2.length) {
                return (len, 1)
            }
            if (str1.charAt(i) == str2.charAt(i)) {
                len += 1
            } else {
                return (len, 0)
            }
        }

        (len, 0)
    }

    /**
      * 根据处理一些特点不明显的title获取part
      *
      * @param title
      * @return
      */
    def getPartByTitle(title: String, name: String): String = {
        var p = ""
        var part = ""

        for (i <- 0 until title.length) {
            if (digitRegex.findFirstMatchIn(title.charAt(i).toString) != None) {
                p = p + title.charAt(i).toString
            } else if (digitRegex.findFirstMatchIn(title.charAt(i).toString) == None && p != ""
                && title.charAt(i) == ' ' || title.charAt(i).toString.equals("集") || title.charAt(i) == '_'
                || title.charAt(i) == '期' || title.charAt(i) == ':' || title.charAt(i) == '：') {

                return part
            } else if (dateRegex1.findFirstMatchIn(part) != None && p != ""){
                //小小智慧树/[SD]小小智慧树2011年11月第2期
                if (title.contains(part + "年")) {
                    part = ""
                    return part
                }
            } else if (p != "" && ((name.length+i) <= (title.length - i)) && title.substring(i, name.length+i).equals(name)) {
                //超级飞侠大百科/24超级飞侠大百科仙人掌为什么会有刺0826
                part = p
                return part
            } else if (name.contains(p)) {
                return ""
            }
        }

        part = p
        if (dateRegex1.findFirstMatchIn(part) != None) {
            if (title.contains(part + "年")) {
                //println("year")
            }
        } else if (part.length > 8) {
            part = ""
        } else if (name.contains(part)) {
            part = ""
        }

        part
    }

    /**
      * 获取集数
      *
      * @param title
      * @param name
      * @return
      */
    def getPart(title: String, name: String): (String, String) = {
        var part = ""
        var ti = name + "/" + title

        if (title.contains("先导集") || title.contains("先导集上") || title.contains("先导集下")) {
            return (ti, "1")
        }

        //"name":"仲夏夜魔法","title":"仲夏夜魔法"
        if (title.equals(name)) {
            part = ""
        } else {
            val same = getSameLen(name, title)
            val sameLen = same._1
            val flag = same._2

            if (sameLen >= 2 && flag == 1) {
                //"name":"极限挑战之皇家宝藏","title":"极限挑战"
                part = ""

            } else {
                if (title.contains("_")) {
                    //"name":"超级育儿师","title":"[SD]超级育儿师20131220_"
                    if (title.split("_").length < 2 || title.contains("集锦") || title.contains("录像") || title.contains("花絮")
                        || title.contains("得分") || title.contains("扣篮") || title.contains("助攻")
                        || title.contains("原声")) {
                        return (ti, "")
                    }
                    //"name":"大仙衙门","title":"大仙衙门_01"
                    //"name":"大仙衙门","title":"大仙衙门_1"
                    else if (digitRegex.findFirstMatchIn(title.split("_")(1).substring(0, title.split("_")(1).length)) != None) {
                        part = title.split("_")(1)
                    } else if (title.contains("集")) {
                        val parts1 = title.substring(title.indexOf("集") - 2, title.indexOf("集"))
                        val parts2 = title.substring(title.indexOf("集") - 1, title.indexOf("集"))
                        //"name":"小马宝莉友谊的魔力第4季","title":"小马宝莉友谊的魔力第4季第10集：云宝落幕"
                        if (digitRegex.findFirstMatchIn(parts1) != None) {
                            part = parts1
                        }
                        //"name":"小马宝莉友谊的魔力第4季","title":"小马宝莉友谊的魔力第4季第4集：无畏天马的真相"
                        else if (digitRegex.findFirstMatchIn(parts2) != None
                            && digitRegex.findFirstMatchIn(parts1) == None) {
                            part = parts2
                        }
                    } else {
                        part = getPartByTitle(title, name)

                    }
                } else if (title.contains("集")) {
                    if (title.split("集")(0).length == 0) {
                        return (ti, "")
                    }

                    val dipart = title.split("集")(0).charAt(title.split("集")(0).length-1).toString
                    if (digitRegex.findFirstMatchIn(dipart) != None) {
                        val parts1 = title.substring(title.indexOf("集") - 2, title.indexOf("集"))
                        val parts2 = title.substring(title.indexOf("集") - 1, title.indexOf("集"))
                        //"name":"小马宝莉友谊的魔力 第4季","title":"小马宝莉友谊的魔力第4季第10集：云宝落幕"
                        if (digitRegex.findFirstMatchIn(parts1) != None) {
                            part = parts1
                        }
                        //"name":"小马宝莉友谊的魔力 第4季","title":"小马宝莉友谊的魔力第4季第4集：无畏天马的真相"
                        else if (digitRegex.findFirstMatchIn(parts2) != None
                            && digitRegex.findFirstMatchIn(parts1) == None) {
                            part = parts2
                        }
                    } else {
                        part = ""
                    }
                } else if (title.contains("：")) {
                    if (title.split("：").length < 2) {
                        return (ti, "")
                    }

                    if (digitRegex.findFirstMatchIn(title.split("：")(1).substring(0, title.split("：")(1).length)) != None) {
                        part = title.split("：")(1)
                    }
                    //拍客纪实/《拍客纪实》第14期：男子月入3000寄回家 穴居山洞对家人称住宾馆
                    //
                    else {
                        part = getPartByTitle(title.split("：")(0), name)
                    }

                } else {
                    //"name":"勇士115-98快船","title":"【集锦】勇士115-98快船 库里19分7抢断格里芬上演惊天隔扣"
                    if (title.contains("集锦") || title.contains("录像") || title.contains("花絮")
                        || title.contains("得分") || title.contains("扣篮") || title.contains("助攻")
                        || title.contains("原声")) {
                        return (ti, "")
                    }

                    part = getPartByTitle(title, name)
                }
            }
        }

        //年份的处理
        if (dateRegex.findFirstMatchIn(part) != None) {
            part = ""
        } else if (dateRegex1.findFirstMatchIn(part) != None) {
            if (title.contains(part + "年"))
                part = ""
        }

        (ti, part)
    }

    /**
      * 中文数据的处理
      * @param title 标题
      * @return
      */
    def numProcess(title : String) : String = {
        var pn = title
        val filterStr = Array("一季","二季","三季","四季","五季","六季","七季","八季","九季")
        val numStr = Array("1季","2季","3季","4季","5季","6季","7季","8季","9季")
        var i = 0
        for (i<- 0 to filterStr.length-1) {
            if (title.contains(filterStr(i))) {
                pn = title.replace(filterStr(i), numStr(i))

                return pn
            }
        }

        pn
    }

    /**
      *第*季的处理
      * @param str name or title
      * @return
      */
    def seasonProcess(str: String) : String = {

        for (i <- 0 until str.length-2) {
            if (str.charAt(i).toString.equals("第") && str.charAt(i+2).toString.equals("季")) {
                return str.charAt(i+1).toString
            }
        }

        null
    }

    /**
      *第*季的处理
      * @param str name or title
      * @return
      */
    def seasonProcess1(str: String) : String = {

        for (i <- 0 until str.length-2) {
            if (str.charAt(i).toString.equals("第") && str.charAt(i+2).toString.equals("季")) {
                return str.substring(0, str.indexOf(str.charAt(i+2).toString)+1)
            }
        }

        str
    }

    /**
      * 过滤掉花絮
      * @param title 剧集的集
      * @return
      */
    def filterProcess(title: String) : Int = {
        val filterStr = Array("剧透","预告","抢先看","片花","片段","花絮","插曲","片尾曲","主题曲",
            "广场舞","图书","独家策划","原创","搞笑","海报","将映","剧照")
        for (fs <- filterStr) {
            if (title.contains(fs)) {
                return 0
            }
        }

        1
    }

    /**
      * 剧名和具体剧集中的季度要对应上
      * @param title 剧集的集
      * @param na 剧集名称
      * @return
      */
    def filterProcess1(title: String, na: String) : Int = {
        var ti = title
        var name = na

        if (ti.contains("一") || ti.contains("二") || ti.contains("三") || ti.contains("四") || ti.contains("五")
            || ti.contains("六") || ti.contains("七") || ti.contains("八") || ti.contains("九")) {
            ti = numProcess(ti)
        }

        if (name.contains("一") || name.contains("二") || name.contains("三") || name.contains("四") || name.contains("五")
            || name.contains("六") || name.contains("七") || name.contains("八") || name.contains("九")) {
            name = numProcess(name)
        }

        var tiNum = seasonProcess(ti)
        var nameNum = seasonProcess(name)

        if(tiNum!=null && nameNum != null) {

            if (tiNum.equals(nameNum)){
                return 1
            } else {
                return 0
            }
        }

        1
    }

    /**
      * 版本处理
      * @param name 名称
      * @return
      */
    def versionProcess(name: String) : String = {
        var pn = name
        val filterStr = Array("未删减版", "[未删减版]", "完整版", "全集",
            "合集", "完全版", "[TV版]", "精华版", "（国语）","乐高版", " 精华版", "精华版",
            "（国语版）", "国语版", "国语中字", "国语", "（英语版）","Q版","（一月）","二月）","（三月）","（四月）","（五月）","（六月）",
            "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "日播版","（七月）","（八月）","（九月）","（十月）",
            "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "粤语", "（日语版）","日语版", "日语","（十一月）","（十二月）",
            "TV中文版", "（中文版）", "中文版", "[韩语版]", "韩语中字","韩语版", "四川话版", "云南话版", "东北话版",
            "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版",
            "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版",
            "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版",
            "（4K）", "VR版", "（VR）", "【3D版】", "（新3D版）", "3D版", "（3D）", "3D", "标清版", "_标清",
            "（蓝光真高清）", "蓝光真高清", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版",
            "纯享版", "精简版", "（加长版）", "加长版", "（加长重映版）", "精编版", "重制版", "双语字幕版", "字幕版",
            "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "（免费版）", " （免费版）","[免费版]", "免费版", "（原声）",
            "原声高清版", "英文原声高清版", "原声", "生肖特别版", "圣诞特别版", "特别版", "完全版", " (环绕声版)", "(环绕声版)", "环绕声版"
        )

        for (fs <- filterStr) {
            if (name.contains(fs)) {
                pn = name.replace(fs, "")

                return pn
            }
        }

        pn
    }

    /**
      * 获取每个小时的数据
      *
      * @param item
      * @return
      */
    def getPlayerResultByHour(item: Playing): mutable.MutableList[PlayerResultByHour] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var startItem: PlayerItem = null
        var startTime: DateTime = null
        var quitItem: PlayerItem = null
        var quitTime: DateTime = null
        var tempItem: PlayerItem = null
        var tempTime: DateTime = null
        var result = mutable.MutableList[PlayerResultByHour]()
        var preResult: PlayerResultByHour = null
        var flag = 0
        var vvSameHour = 0
        var dura = 0

        for (i <- 0 until item.actions.size) {

            val title = item.actions.get(i).get.title
            val name = item.actions.get(i).get.name
            val pt = getPart(title, name)
            if (startItem == null) {
                startItem = item.actions.get(i).get
                startTime = DateTime.parse(startItem.time, format)
            }
            else {
                quitItem = item.actions.get(i).get
                quitTime = DateTime.parse(quitItem.time, format)

                if (startItem.action.equals("Start") && quitItem.action.equals("Quit")
                    && startItem.title.equals(quitItem.title)) {
                    val duration = DataUtil.dateTotimestamp(quitItem.time) - DataUtil.dateTotimestamp(startItem.time)
                    val map = DataUtil.durationSplitByHour(duration.toString, quitItem.time)
                    map.keys.foreach { x =>
                        //当前的记录是否和之前的最后一个记录在同一个小时段,而且是同一个节目
                        if (map.size == 1 && preResult != null) {
                            if (preResult.title.equals(pt._1) && preResult.part.equals(pt._2) &&
                                preResult.hour.equals(x)) {
                                vvSameHour += 1
                                dura = dura + (map(x).toLong / 1000).toString.toInt
                                result += new PlayerResultByHour(pt._1, pt._2, x, dura, vvSameHour, startItem.time, quitItem.time)
                            }
                        } else {
                            if (flag == 0) {
                                if ((map(x).toLong / 1000).toString.toInt != 0) {
                                    result += new PlayerResultByHour(pt._1, pt._2, x, (map(x).toLong / 1000).toString.toInt, 1, startItem.time, quitItem.time)
                                }
                                vvSameHour += 1
                                flag = 1
                            } else {
                                if ((map(x).toLong / 1000).toString.toInt != 0) {
                                    result += new PlayerResultByHour(pt._1, pt._2, x, (map(x).toLong / 1000).toString.toInt, 0, startItem.time, quitItem.time)
                                }
                            }

                            if (map.size == 1 && result.length != 0) {
                                preResult = result.last //记住最后一个的数据
                                dura = preResult.duration
                            } else {
                                vvSameHour = 0
                                dura = 0
                            }
                        }
                    }

                    flag = 0
                    startItem = null
                } else {
                    flag = 0
                    startItem = null
                }
            }
        }

        vvSameHour = 0

        result
    }

    /**
      * 获取title标准名称
      * @param title 标题
      * @return
      */
    def getStartandTitle(title: String, categorys: String) : String = {
        var ti = ""

        if (title.contains("【") && title.contains("】") && categorys.equals("动画片")) {
            ti = title.substring(title.indexOf("【")+1, title.indexOf(("】")))
        }
        //小马宝莉友谊的魔力第6季第2集 --小马宝莉友谊的魔力第6季 第2集
        else if (title.contains("第") && title.contains("集") && title.contains("季")) {
            ti = title.split("第")(0)+"第"+title.split("第")(1)
            if (ti != "" && ti.charAt(ti.length - 1).equals(" ")) {
                ti = title.substring(0, title.length - 2)
            }
        } else if (title.contains("第") && title.contains("集") && !title.contains("季")) {
            //小马宝莉友谊的魔力 第2集
            ti = title.split("第")(0)
            if (ti != "" && ti.charAt(ti.length - 1).equals(" ")) {
                ti = title.substring(0, title.length - 2)
            }
        } else if (title.contains("_")) {
            ti = title.split("_")(0)
            if (ti.contains("[SD]") || ti.contains("[HD]")) {
                ti = ti.substring(ti.indexOf("D]")+2, ti.length)
            }

            if (digitRegex.findFirstMatchIn(ti) != None) {
                ti = title.split("_")(1)
            } else if (ti != "" && ti.charAt(ti.length - 1).equals(" ")) {
                ti = title.substring(0, title.length - 2)
            } else if (digitRegex.findFirstMatchIn(ti) != None) {
                ti = title.split("_")(0)
            }
        } else if (title.contains(" ")) {
            val strs = title.split(" ")
            if(strs.length < 2) {
                ti = title.replaceAll(" ", "")
            } else if (digitRegex.findFirstMatchIn(strs(1)) != None) {
                ti = title.split(" ")(0)
            }
        } else {
            for (i <- 0 until title.length) {
                if (digitRegex.findFirstMatchIn(title.charAt(i).toString) == None) {
                    ti = ti + title.charAt(i).toString
                }
            }
            ti = title
        }

        //星际宝贝：神奇大冒险第二季_10
        if (ti.contains("：")) {
            ti = title.split("：")(0)
        }

        //去除空格和，！· 。 一些不需要的字符
        ti = ti.replaceAll(" ", "")
        ti = ti.replaceAll("，", "")
        ti = ti.replaceAll("！", "")
        ti = ti.replaceAll("·", "")
        ti = ti.replaceAll("。", "")
        ti = ti.replaceAll("\\(", "")
        ti = ti.replaceAll("\\)", "")
        ti = versionProcess(ti)

        if (ti.contains("[SD]") || ti.contains("[HD]")) {
            ti = ti.substring(ti.indexOf("D]")+2, ti.length)
        }

        if (ti.contains("《") && ti.contains("》")) {
            ti = ti.substring(ti.indexOf("《")+1, ti.indexOf(("》")))
        }

        if (ti.contains("一") || ti.contains("二") || ti.contains("三") || ti.contains("四") || ti.contains("五")
            || ti.contains("六") || ti.contains("七") || ti.contains("八") || ti.contains("九")) {
            ti = numProcess(ti)
        }

        /*if (ti.contains("季") && ti.contains("第") && ((ti.lastIndexOf("季") - ti.lastIndexOf("第")) == 2)){
            ti = ti.substring(0, ti.lastIndexOf("第")) + ti.charAt(ti.lastIndexOf("季")-1)
        }*/
        ti = seasonProcess1(ti)

        ti
    }

    /**
      * 获取name标准名称
      * @param name 标题
      * @return
      */
    def getStartandName(name: String) : String = {
        var nm = ""

        nm = name

        if (nm.contains("：")) {
            nm = nm.split("：")(0)
        }

        //去除空格和，！· 。 一些不需要的字符
        nm = nm.replaceAll(" ", "")
        nm = nm.replaceAll("，", "")
        nm = nm.replaceAll("！", "")
        nm = nm.replaceAll("·", "")
        nm = nm.replaceAll("。", "")
        nm = nm.replaceAll("\\(", "")
        nm = nm.replaceAll("\\)", "")
        nm = versionProcess(nm)

        if (nm.contains("《") && nm.contains("》")) {
            nm = nm.substring(nm.indexOf("《")+1, nm.indexOf(("》")))
        }

        if (nm.contains("一") || nm.contains("二") || nm.contains("三") || nm.contains("四") || nm.contains("五")
            || nm.contains("六") || nm.contains("七") || nm.contains("八") || nm.contains("九")) {
            nm = numProcess(nm)
        }

        /*if (nm.contains("季") && nm.contains("第") && ((nm.lastIndexOf("季") - nm.lastIndexOf("第")) == 2)){
            nm = nm.substring(0, nm.lastIndexOf("第"))+nm.charAt(nm.lastIndexOf("季")-1)
        }*/
        nm = seasonProcess1(nm)

        nm
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimNameCol = Bytes.toBytes("dim_name")
        val dimTitleCol = Bytes.toBytes("dim_title")
        val dimAwcidCol = Bytes.toBytes("dim_awcid")
        val dimPartCol = Bytes.toBytes("dim_part")
        val dimYearCol = Bytes.toBytes("dim_year")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val dimModelCol = Bytes.toBytes("dim_model")
        val dimCrowdCol = Bytes.toBytes("dim_crowd")
        val dimRegionCol = Bytes.toBytes("dim_region")
        val factVvCol = Bytes.toBytes("fact_vv")
        val factDurationCol = Bytes.toBytes("fact_duration")

        //val filmTextRdd = sc.textFile("F:/avc/docs/changhong/2017-01-15/film1.txt")
        //val filmTextRdd = sc.textFile("/user/hdfs/rsync/CH/film1.txt")
        val filmTextRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/film.txt")
        val filmRdd = filmTextRdd.mapPartitions(items => {
            items.map(line => {
                val strs = line.split("""},""")
                strs
            })
        }).flatMap(x => {
            val mutableArr = ArrayBuffer[(String, String)]()
            for (i <- 0 until x.length) {
                val strs = x(i).split(""","""")
                if (strs.length == 7) {
                    val name = strs(1).split("""":"""")(1).substring(0, strs(1).split("""":"""")(1).indexOf("""""""))
                    val title = strs(2).split("""":"""")(1).substring(0, strs(2).split("""":"""")(1).indexOf("""""""))
                    val vid = strs(3).split("""":"""")(1).substring(0, strs(3).split("""":"""")(1).indexOf("""""""))
                    val categorys = strs(6).split("""":"""")(1).substring(0, strs(6).split("""":"""")(1).indexOf("""""""))
                    mutableArr.append((vid, title + "0x01" + name + "0x01" + categorys))
                }
            }
            val c = mutableArr.map(y => (y._1, y._2))
            c
        }).filter(x => filterProcess(x._2.split("0x01")(0)) == 1)
            .filter(x => filterProcess1(x._2.split("0x01")(0), x._2.split("0x01")(1)) == 1)

        //val tencentRdd = sc.textFile("F:/avc/docs/changhong/2017-01-15/tencent1.txt")
        val tencentRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/tencent.txt")

        val tenRdd = tencentRdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("""","""")
                cols
            }).filter(x => x(0).split("""":"""").length > 1).flatMap(x => {
                val mac = x(0).split("""":"""")(1).substring(0, x(0).split("""":"""")(1).length)
                val mutableArr = ArrayBuffer[String]()
                for (i <- 1 until x.length)
                    mutableArr += x(i)
                val c = mutableArr.map(y => (mac, y))
                c
            })
        }).filter(x => x._2.split(";").length == 5)
            .filter(x => x._2.split("time:").length > 1)
            .filter(x => x._2.split("time:")(1).length >= 19)
            .filter(x => x._2.contains("vid"))
            .filter(x => x._2.split("time:")(1).substring(0, 10) == analysisDate)

        val ten = tenRdd.mapPartitions(items => {
            items.map(line => {
                val mac = line._1
                val astrs = line._2.split(";")
                val vid = handlnArr(astrs(2), "=")
                val action = handlnArr(astrs(3), "=") //action
                val time = astrs(4).split("time:")(1).substring(0, 19) //time

                (vid, mac + "," + action + "," + time)
            })
        }).distinct()

        //得到时间 action等
        val djRdd = ten.leftOuterJoin(filmRdd).filter(x => x._2.toString().split("0x01").length == 3)

        //存储Tencent和film没有匹配上的数据
        val unMatchedRdd = ten.leftOuterJoin(filmRdd).filter(x => x._2.toString().split("0x01").length != 3)
        //unMatchedRdd.map(x=>(x._1, 1)).reduceByKey(_+_).saveAsTextFile("hdfs:///user/hdfs/player/tenfiml-" + analysisDate + ".txt")

        val tfRdd = djRdd.mapPartitions(items => {
            items.map(item => {
                val mac = item._2._1.split(",")(0)
                val action = item._2._1.split(",")(1)
                val time = item._2._1.split(",")(2)
                val title = item._2._2.get.split("0x01")(0)
                var name = item._2._2.get.split("0x01")(1)

                var categorys = item._2._2.get.split("0x01")(2)
                if (categorys.equals("动漫")) {
                    categorys = "动画片"
                }

                //处理时间相同的Start和Quit，把Start放在前面
                var ac = ""
                if(action.equals("Start")) {
                    ac = "cStart"
                } else if (action.equals("Quit")) {
                    ac = "Quit"
                }
                val xs = new PlayerItem(name, action, time, title, name+title+time+ac)

                val film = new Playing(item._1, "", categorys, title, mac, time.substring(0, 10), mutable.MutableList(xs))


                (name+"0x01"+categorys, film)
            })
        })


        //内容库
        val sqlc = new HiveContext(sc)
        val hiveDataFrame = sqlc.sql("select * from hr.film_properties")
        val tagRdd = hiveDataFrame.mapPartitions(items =>
            {
                items.map(item => {
                    val awcid = item(0).toString
                    val originalName = item(1).toString
                    val standardName = item(2).toString
                    val year = item(3).toString
                    val model = item(4).toString
                    val crowd = item(5).toString
                    val region = item(6).toString
                    (originalName+"0x01"+model, awcid + "0x01" + year + "0x01" + model + "0x01" + crowd + "0x01" + region + "0x01" + standardName)
                })
            })

        val ttRdd = tfRdd.leftOuterJoin(tagRdd)

        //1、先匹配name + origin
        val noReRdd = ttRdd.filter(x => !x._2.toString().contains("None"))

        //2、然后匹配name + standard
        val nsBaseRdd = ttRdd.filter(x => x._2.toString().contains("None"))
            .mapPartitions(items => {
                items.map(item => {
                    val categorys = item._1.split("0x01")(1)
                    val film = item._2._1
                    var name = item._1.split("0x01")(0)
                    name = getStartandName(name)

                    (name+"0x01"+categorys, item._2._1)
                })
            })

        //内容库的标准名称
        val sdnRdd = hiveDataFrame.mapPartitions(items =>
            {
                items.map(item => {
                    val awcid = item(0).toString
                    val originalName = item(1).toString
                    val standardName = item(2).toString
                    val year = item(3).toString
                    val model = item(4).toString
                    val crowd = item(5).toString
                    val region = item(6).toString

                    (standardName+"0x01"+model, awcid + "0x01" + year + "0x01" + model + "0x01" + crowd + "0x01" + region + "0x01" + standardName)
                })
            })

        //
        val nsRdd = nsBaseRdd.leftOuterJoin(sdnRdd)
        val nsReRdd = nsRdd.filter(x => !x._2.toString().contains("None"))

        //3、name 匹配不上的就去重新匹配title+originalname
        val titleOriRdd = nsRdd.filter(x => x._2.toString().contains("None"))
            .mapPartitions(items => {
            items.map(item => {
                val categorys = item._1.split("0x01")(1)
                val film = item._2._1
                var title = item._2._1.title
                title = getStartandTitle(title, categorys)

                (title+"0x01"+categorys, film)
            })
        })
        val toRdd = titleOriRdd.leftOuterJoin(tagRdd)
        val toReRdd = toRdd.filter(x => !x._2.toString().contains("None"))

        //4、name 匹配不上的就去重新匹配title+standardname
        val titleStRdd = toRdd.filter(x => x._2.toString().contains("None"))
            .mapPartitions(items => {
                items.map(item => {
                    val categorys = item._1.split("0x01")(1)
                    val film = item._2._1
                    var title = item._2._1.title
                    title = getStartandTitle(title, categorys)

                    (title+"0x01"+categorys, film)
                })
            })
        //title + standardName
        val tsBaseRdd = titleStRdd.leftOuterJoin(sdnRdd)
        val tsReRdd = tsBaseRdd.filter(x => !x._2.toString().contains("None"))

        ///////////////////////////////////////////////////////////////////////////////////////
        //不需要category的匹配方式再来一遍
        //5 name+origin
        val noRdd1 = tsBaseRdd.filter(x => x._2.toString().contains("None"))
                .mapPartitions(items => {
                        items.map(item => {
                        var name = item._1.split("0x01")(0)
                        name = getStartandName(name)

                        (name, item._2._1)
                    })
                })

        val sdnRdd1 = sdnRdd.mapPartitions({items =>
            items.map({item =>
                (item._1.split("0x01")(0), item._2)
            })
        })
        val noBaseRdd1 = noRdd1.leftOuterJoin(sdnRdd1)
        val noReRdd1 = noBaseRdd1.filter(!_._2.toString().contains("None"))

        //6 name + standard
        val nsRdd1 = noBaseRdd1.filter(_._2.toString().contains("None"))
            .mapPartitions(items => {
                items.map(item => {
                    var name = item._1.split("0x01")(0)
                    name = getStartandName(name)

                    (name, item._2._1)
                })
            })

        val tagRdd1 = tagRdd.mapPartitions({items =>
            items.map({item =>
                (item._1.split("0x01")(0), item._2)
            })
        })
        val nsBaseRdd1 = nsRdd1.leftOuterJoin(tagRdd1)
        val nsReRdd1 = nsBaseRdd1.filter(!_._2.toString().contains("None"))

        //7 title+origin
        val toRdd1 = nsBaseRdd1.filter(_._2.toString().contains("None"))
            .mapPartitions(items => {
                items.map(item => {
                    val film = item._2._1
                    var title = item._2._1.title
                    title = getStartandTitle(title, item._2._1.categorys)

                    (title, film)
                })
            })
        val toBaseRdd1 = toRdd1.leftOuterJoin(tagRdd1)
        val toReRdd1 = toBaseRdd1.filter(!_._2.toString().contains("None"))

        //8 title+standard
        val tsRdd1 = toBaseRdd1.filter(_._2.toString().contains("None"))
            .mapPartitions(items => {
                items.map(item => {
                    val film = item._2._1
                    var title = item._2._1.title
                    title = getStartandTitle(title, item._2._1.categorys)

                    (title, film)
                })
            })
        val tsBaseRdd1 = tsRdd1.leftOuterJoin(sdnRdd1)
        val tsReRdd1 = tsBaseRdd1.filter(!_._2.toString().contains("None"))

        val unMatched1Rdd = tsBaseRdd1.filter(x => x._2.toString().contains("None"))
        //unMatched1Rdd.map(x => (x._1 ,1)).reduceByKey(_+_).saveAsTextFile("hdfs:///user/hdfs/player/tfproperty-" + analysisDate + ".txt")

        val unionRdd = noReRdd.union(nsReRdd).union(toReRdd).union(tsReRdd)
                .union(noReRdd1).union(nsReRdd1).union(toReRdd1).union(tsReRdd1).mapPartitions(items => {
            items.map(item => {
                val play = item._2._1
                val awcid = item._2._2.get
                val p = new Playing(play.vid, awcid, "", play.title, play.mac, play.date, play.actions)
                val vid = play.vid
                (vid + play.mac + awcid.split("0x01")(0) , p)
            })
        })

        val reRdd = unionRdd.repartition(unionRdd.getNumPartitions + 10).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        }).mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.tt).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })

        //reRdd.map(x => x.vid + "," + x.mac + "," + x.awcid + "," + x.title).saveAsTextFile("hdfs:///user/hdfs/player/aa-" + analysisDate + ".txt")
        reRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_player_active_fact")) //tracker_player_active_fact

            try {
                items.foreach(item => {
                    getPlayerResultByHour(item).foreach(r => {
                        val uuid = UUID.randomUUID().toString
                        //hbase key 也就是行
                        val put = new Put(Bytes.toBytes(item.awcid.split("0x01")(0) + r.title.split("/")(1) + item.mac + r.part + item.date + r.hour + "CH"))
                        var part = ""
                        if (r.part != "") {
                            part = r.part.toInt.toString
                        }

                        val eles = item.awcid.split("0x01")

                        //'列族', '列名称:', '值'
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(item.mac))
                        put.addColumn(dimFamilyCol, dimNameCol, Bytes.toBytes(r.title))
                        put.addColumn(dimFamilyCol, dimTitleCol, Bytes.toBytes(eles(5)))
                        put.addColumn(dimFamilyCol, dimAwcidCol, Bytes.toBytes(eles(0)))
                        put.addColumn(dimFamilyCol, dimPartCol, Bytes.toBytes(part))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
                        put.addColumn(dimFamilyCol, dimYearCol, Bytes.toBytes(eles(1)))
                        put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(eles(2)))
                        put.addColumn(dimFamilyCol, dimCrowdCol, Bytes.toBytes(eles(3)))
                        put.addColumn(dimFamilyCol, dimRegionCol, Bytes.toBytes(eles(4)))
                        put.addColumn(factFamilyCol, factVvCol, Bytes.toBytes(r.vv.toString))
                        put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))

                        mutator.mutate(put)
                    })
                    mutator.flush()
                })

            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }

    def bf(item: (String, Playing), vu: Array[(String, String)]) : (String, Playing) ={
        var p:Playing = null
        var vid = ""
        var mac = ""
        vu.foreach(ele => {
            if (item._1.equals(ele._1)) {
                val play = item._2
                val awcid = ele._2
                p = new Playing(play.vid, awcid, "", "", play.mac, play.date, play.actions)
                vid = play.vid
                mac = play.mac
            }
        })

        (vid + mac, p)
    }
    /**
      * 处理数组
      *
      * @param str   字符串
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            null
        } else {
            val ele = arr(1)
            ele
        }
    }

    def main(args: Array[String]): Unit = {
        //println(filterProcess1("芭比之梦想豪宅第6季第13集","芭比之梦想豪宅 第7季"))
        //println(getStartandName("芭比之梦想豪宅 第7季"))
    }
}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.coocaa.{OCData2Partition, ApkData2Partition, Helper}
import org.apache.log4j.Logger

object Clean2PartitionExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext

    //    到分区表1-oc  4core 5G 4
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@OCData2Partition start ... ")
      OCData2Partition.run(sc, analysisDate);
      println(analysisDate + "@OCData2Partition end ... ")
    }



    //    到分区表1-live 4core 5G 4
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@OCData2Partition start ... ")
      OCData2Partition.run(sc, analysisDate);
      println(analysisDate + "@OCData2Partition end ... ")
    }




    //    到分区表1-apk 4core 5G 4
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@ApkData2Partition start ... ")
      ApkData2Partition.run(sc, analysisDate);
      println(analysisDate + "@ApkData2Partition end ... ")
    }



    //    到分区表1-到剧 10core 10G 10



  }

}
package com.avcdata.etl.launcher.util

import com.avcdata.etl.common.util.HdfsFileUtil
import com.avcdata.etl.common.util.encryption.StringEncryptorUtil
import com.avcdata.etl.export.config.{ExportCsvConfigOption, ExportJDBCConfig, ExportJDBCConfigOption}
import org.apache.commons.cli._

import scala.collection.mutable

/**
  * 命令行解析
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 9:46
  */
object CliParser
{
  def parse(args: Array[String]): CommandLine =
  {
    val parser = new PosixParser()

    parser.parse(options(), args)
  }

  /**
    * 创建等解析类的Options对象
    *
    * @return Options
    */
  def options(): Options =
  {
    // 创建 Options 对象
    val options = new Options()

    val exportTypeOption = new Option("T", "export-type", true, "The target type(none, jdbc, mongo, csv, hive, kafka, k2m) of export, default is jdbc.")
    exportTypeOption.setArgName("export-type")

    val rcFileOption = new Option(null, "rc-file", true, "Pre execute script file from hdfs.")
    rcFileOption.setArgName("filename")

    val filenameOption = new Option("f", "sql-file", true, "Script file from hdfs.")
    filenameOption.setArgName("filename")

    val exportOption = new Option(null, "export-config-file", true, "Export config file from hdfs.")
    exportOption.setArgName("filename")

    val connectUriOption = configTupleToOption(ExportJDBCConfigOption.CONNECT_URI)
    connectUriOption.setArgName("jdbc-uri")

    val usernameOption = configTupleToOption(ExportJDBCConfigOption.USERNAME)
    usernameOption.setArgName("username")

    val tableOption = configTupleToOption(ExportJDBCConfigOption.TABLE)
    tableOption.setArgName("table-name")

    val columnsOption = configTupleToOption(ExportJDBCConfigOption.COLUMNS)
    columnsOption.setArgName("col,col,col…")

    val passwordOption = configTupleToOption(ExportJDBCConfigOption.PASSWORD)
    passwordOption.setArgName("password")

    val updateKeyOption = configTupleToOption(ExportJDBCConfigOption.UPDATE_KEY)
    updateKeyOption.setArgName("col-name")

    val deleteKeyOption = configTupleToOption(ExportJDBCConfigOption.DELETE_KEY)
    deleteKeyOption.setArgName("col-name")

    val insertSQLOption = configTupleToOption(ExportJDBCConfigOption.INSERT_SQL)
    insertSQLOption.setArgName("insert-sql")

    val batchOption = configTupleToOption(ExportJDBCConfigOption.BATCH)
    batchOption.setArgName("batch")

    val batchSizeOption = configTupleToOption(ExportJDBCConfigOption.BATCH_SIZE)
    batchSizeOption.setArgName("batch-size")

    val sqlParamsOption = new Option("p", "sql-param", true, "Sql params for execute.")
    sqlParamsOption.setArgName("name=value")
    sqlParamsOption.setValueSeparator('=')
    sqlParamsOption.setArgs(2)

    val verboseOption = configTupleToOption(ExportJDBCConfigOption.VERBOSE)
    verboseOption.setArgName("verbose")

    val csvSavePathOption = configTupleToOption(ExportCsvConfigOption.SAVE_PATH)
    csvSavePathOption.setArgName("save-path")

    val csvHeaderOption = configTupleToOption(ExportCsvConfigOption.HEADER)
    csvHeaderOption.setArgName("true or false")

    val csvFieldDelimiterOption = configTupleToOption(ExportCsvConfigOption.DELIMITER)
    csvFieldDelimiterOption.setArgName("field-delimiter")

    options.addOption("h", "help", false, "Lists short help.")
    options.addOption(exportTypeOption)
    options.addOption(rcFileOption)
    options.addOption(filenameOption)
    options.addOption(exportOption)
    options.addOption(connectUriOption)
    options.addOption(usernameOption)
    options.addOption(passwordOption)
    options.addOption(tableOption)
    options.addOption(columnsOption)
    options.addOption(updateKeyOption)
    options.addOption(deleteKeyOption)
    options.addOption(insertSQLOption)
    options.addOption(batchOption)
    options.addOption(batchSizeOption)
    options.addOption(sqlParamsOption)
    options.addOption(verboseOption)
    options.addOption(csvSavePathOption)
    options.addOption(csvHeaderOption)
    options.addOption(csvFieldDelimiterOption)
  }

  /**
    * 将配置tuple转换为CLI Option
    *
    * @param tuple 配置Tuple
    * @return Option
    */
  private def configTupleToOption(tuple: (String, String, Boolean, String)): Option =
  {
    new Option(tuple._1, tuple._2, tuple._3, tuple._4)
  }

  /**
    * 获取数据库配置信息对象,使用命令行参数覆盖配置文件参数
    *
    * @param mergedParams 合并后的参数
    * @return 数据库导入配置对象
    */
  def readJDBCExportConfig(mergedParams: Map[String, String]): ExportJDBCConfig =
  {
    val connectUri = mergedParams(ExportJDBCConfigOption.CONNECT_URI._2)
    val username = mergedParams.getOrElse(ExportJDBCConfigOption.USERNAME._2, null)
    val password = mergedParams.getOrElse(ExportJDBCConfigOption.PASSWORD._2, null)
    val table = mergedParams(ExportJDBCConfigOption.TABLE._2)
    val updateKey = mergedParams.getOrElse(ExportJDBCConfigOption.UPDATE_KEY._2, null)
    val deleteKey = mergedParams.getOrElse(ExportJDBCConfigOption.DELETE_KEY._2, null)
    val columns = mergedParams.getOrElse(ExportJDBCConfigOption.COLUMNS._2, null)
    val insertSQL = mergedParams.getOrElse(ExportJDBCConfigOption.INSERT_SQL._2, null)
    val transactional = mergedParams.getOrElse("need-transaction", "true").toBoolean
    val batch = mergedParams.getOrElse(ExportJDBCConfigOption.BATCH._2, "true").toBoolean
    val batchSize = mergedParams.getOrElse(ExportJDBCConfigOption.BATCH_SIZE._2, "1000").toInt
    val verbose = mergedParams.getOrElse(ExportJDBCConfigOption.VERBOSE._2, "false").toBoolean

    ExportJDBCConfig(connectUri, username, password, table, updateKey, deleteKey, columns, insertSQL
      , transactional, batch, batchSize, verbose)
  }

  def readParamsFromExportConfig(cl: CommandLine): Map[String, String] =
  {
    val fileConfigs = if (cl.hasOption("export-config-file"))
    {
      HdfsFileUtil.readPropertiesToMap(cl.getOptionValue("export-config-file"))
    }
    else
    {
      Map[String, String]()
    }

    val efficentParams = mutable.Map[String, String]()

    val batchValue = efficentParamValue(ExportJDBCConfigOption.BATCH._2, fileConfigs, cl)
    if (!batchValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.BATCH._2, batchValue)

    val batchSizeValue = efficentParamValue(ExportJDBCConfigOption.BATCH_SIZE._2, fileConfigs, cl)
    if (!batchSizeValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.BATCH_SIZE._2, batchSizeValue)

    val columnsValue = efficentParamValue(ExportJDBCConfigOption.COLUMNS._2, fileConfigs, cl)
    if (!columnsValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.COLUMNS._2, columnsValue)

    val connectUriValue = efficentParamValue(ExportJDBCConfigOption.CONNECT_URI._2, fileConfigs, cl)
    if (!connectUriValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.CONNECT_URI._2, connectUriValue)

    val passwordValue = efficentParamValue(ExportJDBCConfigOption.PASSWORD._2, fileConfigs, cl)
    val encryptPasswordValue = efficentParamValue(ExportJDBCConfigOption.ENCRYPT_PASSWORD._2, fileConfigs, cl)
    if (!passwordValue.isEmpty && !encryptPasswordValue.isEmpty)
      throw new IllegalArgumentException(s"Both `${ExportJDBCConfigOption.PASSWORD._2}` and `${ExportJDBCConfigOption.ENCRYPT_PASSWORD._2}` param are set, please specific one to use.")
    else if (!passwordValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.PASSWORD._2, passwordValue)
    else if (!encryptPasswordValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.PASSWORD._2, StringEncryptorUtil.decrypt(encryptPasswordValue))

    val tableValue = efficentParamValue(ExportJDBCConfigOption.TABLE._2, fileConfigs, cl)
    if (!tableValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.TABLE._2, tableValue)

    val updateKeyValue = efficentParamValue(ExportJDBCConfigOption.UPDATE_KEY._2, fileConfigs, cl)
    if (!updateKeyValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.UPDATE_KEY._2, updateKeyValue)

    val deleteKeyValue = efficentParamValue(ExportJDBCConfigOption.DELETE_KEY._2, fileConfigs, cl)
    if (!deleteKeyValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.DELETE_KEY._2, deleteKeyValue)

    val insertSQLValue = efficentParamValue(ExportJDBCConfigOption.INSERT_SQL._2, fileConfigs, cl)
    if (!insertSQLValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.INSERT_SQL._2, insertSQLValue)

    val usernameValue = efficentParamValue(ExportJDBCConfigOption.USERNAME._2, fileConfigs, cl)
    if (!usernameValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.USERNAME._2, usernameValue)

    val verboseValue = efficentParamValue(ExportJDBCConfigOption.VERBOSE._2, fileConfigs, cl)
    if (!verboseValue.isEmpty) efficentParams.put(ExportJDBCConfigOption.VERBOSE._2, verboseValue)

    fileConfigs ++ efficentParams.toMap
  }

  /**
    * 获取生效的参数配置
    *
    * @param paramName   参数名称
    * @param fileConfigs 文件配置对象
    * @param cli         命令行参数
    * @return 配置参数值
    */
  private def efficentParamValue(paramName: String, fileConfigs: Map[String, String], cli: CommandLine): String =
  {
    if (cli.hasOption(paramName)) cli.getOptionValue(paramName)
    else if (fileConfigs.contains(paramName)) fileConfigs(paramName)
    else ""
  }

  /**
    * 从命令行获取导出CSV的配置
    *
    * @param cl 命令行配置
    * @return 导出CSV配置
    */
  def readCsvExportConfig(cl: CommandLine): Map[String, String] =
  {
    val csvExportConfig = mutable.Map[String, String]()

    if (cl.hasOption(ExportCsvConfigOption.HEADER._2)) csvExportConfig += ("header" -> cl.getOptionValue(ExportCsvConfigOption.HEADER._2))
    if (cl.hasOption(ExportCsvConfigOption.DELIMITER._2)) csvExportConfig += ("delimiter" -> cl.getOptionValue(ExportCsvConfigOption.DELIMITER._2))

    csvExportConfig.toMap
  }

  /**
    * 打印命令行帮助
    */
  def printHelp() =
  {
    val hf = new HelpFormatter()

    hf.printHelp("Spark4HiveQLExecutor", options())
  }
}
package com.avcdata.spark.job.mllib

import java.sql.DriverManager

import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkConf, SparkContext}

object ClusterEvaluateIndex1 {


  def main(args: Array[String]) {

    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("ClusterEvaluateIndex1")
    val sc = new SparkContext(conf)

    run(sc, "2017-03-15", "15")

    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    testRun(sc, "2017-03-15", "15", 4)
    testRun(sc, "2017-03-15", "15", 8)
    testRun(sc, "2017-03-15", "15", 12)
    testRun(sc, "2017-03-15", "15", 16)

  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, k: Int) = {


    //TODO 聚类-评估指标1-聚合度

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-UserVectorAllETL")
    //////////////test///////////////
    //    val vec = sc.textFile("UserVectorAllETL.txt")
    //////////////test///////////////
    //data training points
    val vec = initRDD.map(line => {
      val cols = line.split("\t")
      //TODO 合并向量字符串
      val sb = new StringBuilder
      for (i <- 3 until cols.length) {
        if (i < (cols.length - 1)) {
          sb.append(cols(i) + ",")
        } else {
          sb.append(cols(i))
        }

      }

      //TODO 转换成大向量
      val vector = Vectors.dense(sb.toString.split(",")
        .map(_.toDouble))

      vector
    })


    //TODO 模型训练
    // 类簇的个数 number of clusters
    //    val k = 8
    // 设置最大迭代次数 maxIterations max number of iterations
    val dataModelTrainTimes = 30
    // 运行3次选出最优解 runs number of parallel runs, defaults to 1. The best model is returned
    val runs = 3
    // 初始聚类中心的选取为k-means++ initializationMode initialization model, either "random" or "k-means||" (default).
    val initMode = "k-means||"

    //TODO 生成模型
    val model_k = KMeans.train(vec, k, dataModelTrainTimes, runs, initMode)

    //TODO 计算集合内方差和，数值越小说明聚类效果越好
    val ssd = model_k.computeCost(vec)
    //评估结果存入数据库
    mysqlDB.index_resultTODB(k, ssd)

    //TODO 聚类中心点打印
    val centers = model_k.clusterCenters
//    centers.foreach(println(_))

    //////////////////////////////////////////////////////////////////////////////////////
    //TODO 保存预测结果

    val label_feature_vec = initRDD.map(line => {

      val cols = line.split("\t")

      var i = 0
      val sn = cols(i)
      i = i + 1
      val stat_date = cols(i)
      i = i + 1
      val period = cols(i)
      i = i + 1

      val brand = cols(i)
      i = i + 1
      val province = cols(i)
      i = i + 1
      val price = cols(i)
      i = i + 1
      val size = cols(i)
      i = i + 1

      val workday_oc_dist = cols(i)
      i = i + 1
      val restday_oc_dist = cols(i)
      i = i + 1
      val workday_channel_dist = cols(i)
      i = i + 1
      val restday_channel_dist = cols(i)
      i = i + 1
      val pg_subject_dist = cols(i)
      i = i + 1
      val pg_year_dist = cols(i)
      i = i + 1
      val pg_region_dist = cols(i)


      //TODO 合并向量字符串
      val sb = new StringBuilder
      for (i <- 3 until cols.length) {
        if (i < (cols.length - 1)) {
          sb.append(cols(i) + ",")
        } else {
          sb.append(cols(i))
        }
      }
      val vectorStr = sb.toString

      //TODO 所有信息+向量合并字符串
      (sn, stat_date, period, brand, province, price, size, workday_oc_dist, restday_oc_dist, workday_channel_dist, restday_channel_dist, pg_subject_dist, pg_year_dist, pg_region_dist, vectorStr)

    })

    //TODO 保存预测结果（所有信息+类别ID)
    val cluster_result = label_feature_vec.map(x => {
      val v = Vectors.dense(x._15.split(",")
        .map(_.toDouble))

      //预测 生成类别ID
      val cluster_id = model_k.predict(v)

      x._1 + "\t" + x._2 + "\t" + x._3 + "\t" + x._4 + "\t" + x._5 + "\t" + x._6 + "\t" + x._7 + "\t" + x._8 + "\t" + x._9 + "\t" + x._10 + "\t" + x._11 + "\t" + x._12 + "\t" + x._13 + "\t" + x._14 + "\t" + cluster_id

    })

      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + k)


    /////////////////////////////////////////////////////////////////////////////////////
    //TODO 聚类-评估指标2-类别ID与家庭构成类别ID的差异
    val samples =
      sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-FamilyVectorDataExport")
        //////////////test//////////////////////
        //      sc.textFile("2017-03-15-15-FamilyVectorDataExport.txt")
        //////////////test//////////////////////
        .map(line => {
        val arr = line.split("\t")
        val sn = arr(0)

        val member_num = arr(12)

        val has_child = arr(14)
        val has_old = arr(15)

        //TODO 合并向量字符串
        val sb = new StringBuilder
        for (i <- 1 until arr.length - 4)
          if (i < (arr.length - 5)) {
            sb.append(arr(i) + ",")
          } else {
            sb.append(arr(i))
          }

        //var sample_vectorStr = ""
        //          //向量字段
        //          for (i <- 1 until arr.length - 1)
        //            sample_vectorStr = sample_vectorStr + "," + arr(i)
        //
        //          val sample_vector = Vectors.dense(sample_vectorStr.split(",")
        //            .map(_.toDouble))

        //带有sn号和向量的字段，样本向量，样本中的家庭构成
        (sn, sb.toString, member_num, has_child, has_old)
      })

    //样本数据结果
    val sample_label_vec = samples.map(x => {
      val v = Vectors.dense(x._2.split(",")
        .map(_.toDouble))
      //TODO 预测 生成类别ID
      val cluster_id = model_k.predict(v)
      (x._1, cluster_id, x._3, x._4, x._5)
    })

    //TODO 清空指标数据表
//    JdbcUtils.truncate(mysqlDB.url, "sample_cluster_result_k" + k, Helper.mysqlConf)

//    classOf[com.mysql.jdbc.Driver]
//    val conn = DriverManager.getConnection(mysqlDB.url, "root", "new.1234")
//    try {
//      val statement = conn.createStatement()
//      statement.executeUpdate("TRUNCATE " + "sample_cluster_result_k" + k);
//    }
//    catch {
//      case e: Exception => e.printStackTrace
//    }
//    finally {
//      conn.close
//    }


    //TODO 样本数据聚类结果及家庭组成  并写入到Mysql
    sample_label_vec.foreachPartition(items => {
      val conn = DriverManager.getConnection(mysqlDB.url, "root", "new.1234")
      val sql = "insert into sample_allTags_cluster_result_k" + k + "(sn,cluster_id,family_compose, has_child,has_old) values (?,?,?,?,?)"
      val ps = conn.prepareStatement(sql)
      items.foreach(item => {
        ps.setString(1, item._1)
        ps.setInt(2, item._2)
        ps.setString(3, item._3)
        ps.setString(4, item._4)
        ps.setString(5, item._5)
        ps.executeUpdate()
      })
      conn.close()

    })


  }


}
package com.avcdata.spark.job.mllib

import java.sql.DriverManager

import com.avcdata.spark.job.common.Helper
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors

object ClusterEvaluateIndex2 {

//  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

  def main(args: Array[String]) {
    //    val trainPath = "/user/hdfs/rsync/uservector/2017-03-15-15-UserVectorAllETL"
    //    val testPath = "2017-03-15-15-FamilyVectorDataExport.txt"

    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SparkTest")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "30")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    testRun(sc, "2017-03-15", "15", 4)
    testRun(sc, "2017-03-15", "15", 8)
    testRun(sc, "2017-03-15", "15", 12)
    testRun(sc, "2017-03-15", "15", 16)

  }

  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, k: Int) = {

    //data training points
    val vec = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-UserVectorAllETL")
      //////////////test///////////////
      //    val vec = sc.textFile("UserVectorAllETL.txt")
      //////////////test///////////////
      .map(line => {
      val cols = line.split("\t")
      //TODO 合并向量字符串
      val sb = new StringBuilder
      for (i <- 3 until cols.length) {
        if (i < (cols.length - 1)) {
          sb.append(cols(i) + ",")
        } else {
          sb.append(cols(i))
        }
      }

      //TODO 转换成大向量
      val vector = Vectors.dense(sb.toString.split(",")
        .map(_.toDouble))

      vector
    })

    /////////////////////////////////////////////////////////////////////////////////////////
    //TODO 模型训练
    // 类簇的个数 number of clusters
    //    val k = 8
    // 设置最大迭代次数 maxIterations max number of iterations
    val dataModelTrainTimes = 30
    ///////////test/////////////////////////
    //    val dataModelTrainTimes = 1
    // 运行3次选出最优解 runs number of parallel runs, defaults to 1. The best model is returned
    val runs = 3
    /////////////test//////////////
    //    val runs = 1
    ////////////test///////////////
    // 初始聚类中心的选取为k-means++ initializationMode initialization model, either "random" or "k-means||" (default).
    val initMode = "k-means||"

    //TODO 生成模型
    val model_k = KMeans.train(vec, k, dataModelTrainTimes, runs, initMode)

    //TODO 计算集合内方差和，数值越小说明聚类效果越好
    val ssd = model_k.computeCost(vec)
    //评估结果存入数据库
    //    mysqlDB.index_resultTODB(k, ssd)

    //TODO 聚类中心点打印
    val centers = model_k.clusterCenters
    //    centers.foreach(println(_))



  }
}
package com.avcdata.spark.job.mllib

import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkConf, SparkContext}

object ClusterEvaluateIndex3 {

//  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

  def main(args: Array[String]) {
    //    val trainPath = "/user/hdfs/rsync/uservector/2017-03-15-15-UserVectorAllETL"
    //    val testPath = "2017-03-15-15-FamilyVectorDataExport.txt"

    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SparkTest")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "30")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    testRun(sc, "2017-03-15", "15", 4)
    testRun(sc, "2017-03-15", "15", 8)
    testRun(sc, "2017-03-15", "15", 12)
    testRun(sc, "2017-03-15", "15", 16)

  }

  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, k: Int) = {

    //TODO 小时交叉维度与之前的大向量合并





  }
}
package com.avcdata.spark.job.common

/**
  * Created by avc on 2016/11/16.
  */
object Codearea {

    val provinceMap = Map(
        ("北京市" -> "华北"),
        ("天津市" -> "华北"),
        ("上海市" -> "华东"),
        ("重庆市" -> "西南"),
        ("香港特别行政区"-> "港澳台"),
        ("河北省" -> "华北"),
        ("山西省" -> "华北"),
        ("内蒙古自治区" -> "华北"),
        ("辽宁省" -> "华北"),
        ("吉林省" -> "东北"),
        ("黑龙江省" -> "东北"),
        ("江苏省" -> "华东"),
        ("浙江省" -> "华东"),
        ("安徽省" -> "华东"),
        ("福建省" -> "华东"),
        ("江西省" -> "华东"),
        ("山东省" -> "华东"),
        ("河南省" -> "华中"),
        ("湖北省"-> "华中"),
        ("湖南省" -> "华中"),
        ("广东省" -> "华南"),
        ("广西壮族自治区" -> "华南"),
        ("海南省" -> "西南"),
        ("四川省" -> "西南"),
        ("贵州省" -> "西南"),
        ("云南省" -> "西南"),
        ("西藏自治区" -> "西南"),
        ("陕西省" -> "西北"),
        ("甘肃省" -> "西北"),
        ("青海省" -> "西北"),
        ("宁夏回族自治区" -> "西北"),
        ("新疆维吾尔自治区" -> "西北"),
        ("台湾省" -> "港澳台")
    )

    /**
      * 根据省份获取大区
      * @param province
      * @return
      */
    def getArea(province: String) : String = {
        provinceMap.get(province).getOrElse("其他")
    }

    val areaMap = Map(
        ("110000" -> "华北,北京市,北京市"),
        ("120000" -> "华北,天津市,天津市"),
        ("310000" -> "华东,上海市,上海市"),
        ("500000" -> "西南,重庆市,重庆市"),
        ("810000" -> "港澳台,香港特别行政区,香港特别行政区"),
        ("130000" -> "华北,河北省,其他"),
        ("130100" -> "华北,河北省,石家庄市"),
        ("130200" -> "华北,河北省,唐山市"),
        ("130300" -> "华北,河北省,秦皇岛市"),
        ("130400" -> "华北,河北省,邯郸市"),
        ("130500" -> "华北,河北省,邢台市"),
        ("130600" -> "华北,河北省,保定市"),
        ("130700" -> "华北,河北省,张家口市"),
        ("130800" -> "华北,河北省,承德市"),
        ("130900" -> "华北,河北省,沧州市"),
        ("131000" -> "华北,河北省,廊坊市"),
        ("131100" -> "华北,河北省,衡水市"),
        ("140000" -> "华北,山西省,其他"),
        ("140100" -> "华北,山西省,太原市"),
        ("140200" -> "华北,山西省,大同市"),
        ("140300" -> "华北,山西省,阳泉市"),
        ("140400" -> "华北,山西省,长治市"),
        ("140500" -> "华北,山西省,晋城市"),
        ("140600" -> "华北,山西省,朔州市"),
        ("140700" -> "华北,山西省,晋中市"),
        ("140800" -> "华北,山西省,运城市"),
        ("140900" -> "华北,山西省,忻州市"),
        ("141000" -> "华北,山西省,临汾市"),
        ("141100" -> "华北,山西省,吕梁市"),
        ("150000" -> "华北,内蒙古自治区,其他"),
        ("150100" -> "华北,内蒙古自治区,呼和浩特市"),
        ("150200" -> "华北,内蒙古自治区,包头市"),
        ("150300" -> "华北,内蒙古自治区,乌海市"),
        ("150400" -> "华北,内蒙古自治区,赤峰市"),
        ("150500" -> "华北,内蒙古自治区,通辽市"),
        ("150600" -> "华北,内蒙古自治区,鄂尔多斯市"),
        ("150700" -> "华北,内蒙古自治区,呼伦贝尔市"),
        ("150800" -> "华北,内蒙古自治区,巴彦淖尔市"),
        ("150900" -> "华北,内蒙古自治区,乌兰察布市"),
        ("152200" -> "华北,内蒙古自治区,兴安盟"),
        ("152500" -> "华北,内蒙古自治区,锡林郭勒盟"),
        ("152900" -> "华北,内蒙古自治区,阿拉善盟"),
        ("210000" -> "华北,辽宁省,其他"),
        ("210100" -> "东北,辽宁省,沈阳市"),
        ("210200" -> "东北,辽宁省,大连市"),
        ("210300" -> "东北,辽宁省,鞍山市"),
        ("210400" -> "东北,辽宁省,抚顺市"),
        ("210500" -> "东北,辽宁省,本溪市"),
        ("210600" -> "东北,辽宁省,丹东市"),
        ("210700" -> "东北,辽宁省,锦州市"),
        ("210800" -> "东北,辽宁省,营口市"),
        ("210900" -> "东北,辽宁省,阜新市"),
        ("211000" -> "东北,辽宁省,辽阳市"),
        ("211100" -> "东北,辽宁省,盘锦市"),
        ("211200" -> "东北,辽宁省,铁岭市"),
        ("211300" -> "东北,辽宁省,朝阳市"),
        ("211400" -> "东北,辽宁省,葫芦岛市"),
        ("220000" -> "东北,吉林省,其他"),
        ("220100" -> "东北,吉林省,长春市"),
        ("220200" -> "东北,吉林省,吉林市"),
        ("220300" -> "东北,吉林省,四平市"),
        ("220400" -> "东北,吉林省,辽源市"),
        ("220500" -> "东北,吉林省,通化市"),
        ("220600" -> "东北,吉林省,白山市"),
        ("220700" -> "东北,吉林省,松原市"),
        ("220800" -> "东北,吉林省,白城市"),
        ("222400" -> "东北,吉林省,延边朝鲜族自治州"),
        ("230000" -> "东北,黑龙江省,其他"),
        ("230100" -> "东北,黑龙江省,哈尔滨市"),
        ("230200" -> "东北,黑龙江省,齐齐哈尔市"),
        ("230300" -> "东北,黑龙江省,鸡西市"),
        ("230400" -> "东北,黑龙江省,鹤岗市"),
        ("230500" -> "东北,黑龙江省,双鸭山市"),
        ("230600" -> "东北,黑龙江省,大庆市"),
        ("230700" -> "东北,黑龙江省,伊春市"),
        ("230800" -> "东北,黑龙江省,佳木斯市"),
        ("230900" -> "东北,黑龙江省,七台河市"),
        ("231000" -> "东北,黑龙江省,牡丹江市"),
        ("231100" -> "东北,黑龙江省,黑河市"),
        ("231200" -> "东北,黑龙江省,绥化市"),
        ("232700" -> "东北,黑龙江省,大兴安岭地区"),
        ("320000" -> "华东,江苏省,其他"),
        ("320100" -> "华东,江苏省,南京市"),
        ("320200" -> "华东,江苏省,无锡市"),
        ("320300" -> "华东,江苏省,徐州市"),
        ("320400" -> "华东,江苏省,常州市"),
        ("320500" -> "华东,江苏省,苏州市"),
        ("320600" -> "华东,江苏省,南通市"),
        ("320700" -> "华东,江苏省,连云港市"),
        ("320800" -> "华东,江苏省,淮安市"),
        ("320900" -> "华东,江苏省,盐城市"),
        ("321000" -> "华东,江苏省,扬州市"),
        ("321100" -> "华东,江苏省,镇江市"),
        ("321200" -> "华东,江苏省,泰州市"),
        ("321300" -> "华东,江苏省,宿迁市"),
        ("330000" -> "华东,浙江省,其他"),
        ("330100" -> "华东,浙江省,杭州市"),
        ("330200" -> "华东,浙江省,宁波市"),
        ("330300" -> "华东,浙江省,温州市"),
        ("330400" -> "华东,浙江省,嘉兴市"),
        ("330500" -> "华东,浙江省,湖州市"),
        ("330600" -> "华东,浙江省,绍兴市"),
        ("330700" -> "华东,浙江省,金华市"),
        ("330800" -> "华东,浙江省,衢州市"),
        ("330900" -> "华东,浙江省,舟山市"),
        ("331000" -> "华东,浙江省,台州市"),
        ("331100" -> "华东,浙江省,丽水市"),
        ("340000" -> "华东,安徽省,其他"),
        ("340100" -> "华东,安徽省,合肥市"),
        ("340200" -> "华东,安徽省,芜湖市"),
        ("340300" -> "华东,安徽省,蚌埠市"),
        ("340400" -> "华东,安徽省,淮南市"),
        ("340500" -> "华东,安徽省,马鞍山市"),
        ("340600" -> "华东,安徽省,淮北市"),
        ("340700" -> "华东,安徽省,铜陵市"),
        ("340800" -> "华东,安徽省,安庆市"),
        ("341000" -> "华东,安徽省,黄山市"),
        ("341100" -> "华东,安徽省,滁州市"),
        ("341200" -> "华东,安徽省,阜阳市"),
        ("341300" -> "华东,安徽省,宿州市"),
        ("341500" -> "华东,安徽省,六安市"),
        ("341600" -> "华东,安徽省,亳州市"),
        ("341700" -> "华东,安徽省,池州市"),
        ("341800" -> "华东,安徽省,宣城市"),
        ("350000" -> "华东,福建省,其他"),
        ("350100" -> "华东,福建省,福州市"),
        ("350200" -> "华东,福建省,厦门市"),
        ("350300" -> "华东,福建省,莆田市"),
        ("350400" -> "华东,福建省,三明市"),
        ("350500" -> "华东,福建省,泉州市"),
        ("350600" -> "华东,福建省,漳州市"),
        ("350700" -> "华东,福建省,南平市"),
        ("350800" -> "华东,福建省,龙岩市"),
        ("350900" -> "华东,福建省,宁德市"),
        ("360000" -> "华东,江西省,其他"),
        ("360100" -> "华东,江西省,南昌市"),
        ("360200" -> "华东,江西省,景德镇市"),
        ("360300" -> "华东,江西省,萍乡市"),
        ("360400" -> "华东,江西省,九江市"),
        ("360500" -> "华东,江西省,新余市"),
        ("360600" -> "华东,江西省,鹰潭市"),
        ("360700" -> "华东,江西省,赣州市"),
        ("360800" -> "华东,江西省,吉安市"),
        ("360900" -> "华东,江西省,宜春市"),
        ("361000" -> "华东,江西省,抚州市"),
        ("361100" -> "华东,江西省,上饶市"),
        ("370000" -> "华东,山东省,其他"),
        ("370100" -> "华东,山东省,济南市"),
        ("370200" -> "华东,山东省,青岛市"),
        ("370300" -> "华东,山东省,淄博市"),
        ("370400" -> "华东,山东省,枣庄市"),
        ("370500" -> "华东,山东省,东营市"),
        ("370600" -> "华东,山东省,烟台市"),
        ("370700" -> "华东,山东省,潍坊市"),
        ("370800" -> "华东,山东省,济宁市"),
        ("370900" -> "华东,山东省,泰安市"),
        ("371000" -> "华东,山东省,威海市"),
        ("371100" -> "华东,山东省,日照市"),
        ("371200" -> "华东,山东省,莱芜市"),
        ("371300" -> "华东,山东省,临沂市"),
        ("371400" -> "华东,山东省,德州市"),
        ("371500" -> "华东,山东省,聊城市"),
        ("371600" -> "华东,山东省,滨州市"),
        ("371700" -> "华东,山东省,菏泽市"),
        ("410000" -> "华中,河南省,其他"),
        ("410100" -> "华中,河南省,郑州市"),
        ("410200" -> "华中,河南省,开封市"),
        ("410300" -> "华中,河南省,洛阳市"),
        ("410400" -> "华中,河南省,平顶山市"),
        ("410500" -> "华中,河南省,安阳市"),
        ("410600" -> "华中,河南省,鹤壁市"),
        ("410700" -> "华中,河南省,新乡市"),
        ("410800" -> "华中,河南省,焦作市"),
        ("410900" -> "华中,河南省,濮阳市"),
        ("411000" -> "华中,河南省,许昌市"),
        ("411100" -> "华中,河南省,漯河市"),
        ("411200" -> "华中,河南省,三门峡市"),
        ("411300" -> "华中,河南省,南阳市"),
        ("411400" -> "华中,河南省,商丘市"),
        ("411500" -> "华中,河南省,信阳市"),
        ("411600" -> "华中,河南省,周口市"),
        ("411700" -> "华中,河南省,驻马店市"),
        ("419000" -> "华中,河南省,省直辖县级行政区划"),
        ("420000" -> "华中,湖北省,其他"),
        ("420100" -> "华中,湖北省,武汉市"),
        ("420200" -> "华中,湖北省,黄石市"),
        ("420300" -> "华中,湖北省,十堰市"),
        ("420500" -> "华中,湖北省,宜昌市"),
        ("420600" -> "华中,湖北省,襄阳市"),
        ("420700" -> "华中,湖北省,鄂州市"),
        ("420800" -> "华中,湖北省,荆门市"),
        ("420900" -> "华中,湖北省,孝感市"),
        ("421000" -> "华中,湖北省,荆州市"),
        ("421100" -> "华中,湖北省,黄冈市"),
        ("421200" -> "华中,湖北省,咸宁市"),
        ("421300" -> "华中,湖北省,随州市"),
        ("422800" -> "华中,湖北省,恩施土家族苗族自治州"),
        ("429000" -> "华中,湖北省,省直辖县级行政区划"),
        ("430000" -> "华中,湖南省,其他"),
        ("430100" -> "华中,湖南省,长沙市"),
        ("430200" -> "华中,湖南省,株洲市"),
        ("430300" -> "华中,湖南省,湘潭市"),
        ("430400" -> "华中,湖南省,衡阳市"),
        ("430500" -> "华中,湖南省,邵阳市"),
        ("430600" -> "华中,湖南省,岳阳市"),
        ("430700" -> "华中,湖南省,常德市"),
        ("430800" -> "华中,湖南省,张家界市"),
        ("430900" -> "华中,湖南省,益阳市"),
        ("431000" -> "华中,湖南省,郴州市"),
        ("431100" -> "华中,湖南省,永州市"),
        ("431200" -> "华中,湖南省,怀化市"),
        ("431300" -> "华中,湖南省,娄底市"),
        ("433100" -> "华中,湖南省,湘西土家族苗族自治州"),
        ("440000" -> "华南,广东省,其他"),
        ("440100" -> "华南,广东省,广州市"),
        ("440200" -> "华南,广东省,韶关市"),
        ("440300" -> "华南,广东省,深圳市"),
        ("440400" -> "华南,广东省,珠海市"),
        ("440500" -> "华南,广东省,汕头市"),
        ("440600" -> "华南,广东省,佛山市"),
        ("440700" -> "华南,广东省,江门市"),
        ("440800" -> "华南,广东省,湛江市"),
        ("440900" -> "华南,广东省,茂名市"),
        ("441200" -> "华南,广东省,肇庆市"),
        ("441300" -> "华南,广东省,惠州市"),
        ("441400" -> "华南,广东省,梅州市"),
        ("441500" -> "华南,广东省,汕尾市"),
        ("441600" -> "华南,广东省,河源市"),
        ("441700" -> "华南,广东省,阳江市"),
        ("441800" -> "华南,广东省,清远市"),
        ("441900" -> "华南,广东省,东莞市"),
        ("442000" -> "华南,广东省,中山市"),
        ("445100" -> "华南,广东省,潮州市"),
        ("445200" -> "华南,广东省,揭阳市"),
        ("445300" -> "华南,广东省,云浮市"),
        ("450000" -> "华南,广西壮族自治区,其他"),
        ("450100" -> "华南,广西壮族自治区,南宁市"),
        ("450200" -> "华南,广西壮族自治区,柳州市"),
        ("450300" -> "华南,广西壮族自治区,桂林市"),
        ("450400" -> "华南,广西壮族自治区,梧州市"),
        ("450500" -> "华南,广西壮族自治区,北海市"),
        ("450600" -> "华南,广西壮族自治区,防城港市"),
        ("450700" -> "华南,广西壮族自治区,钦州市"),
        ("450800" -> "华南,广西壮族自治区,贵港市"),
        ("450900" -> "华南,广西壮族自治区,玉林市"),
        ("451000" -> "华南,广西壮族自治区,百色市"),
        ("451100" -> "华南,广西壮族自治区,贺州市"),
        ("451200" -> "华南,广西壮族自治区,河池市"),
        ("451300" -> "华南,广西壮族自治区,来宾市"),
        ("451400" -> "华南,广西壮族自治区,崇左市"),
        ("460000" -> "西南,海南省,其他"),
        ("460100" -> "西南,海南省,海口市"),
        ("460200" -> "西南,海南省,三亚市"),
        ("460300" -> "西南,海南省,三沙市"),
        ("469000" -> "西南,海南省,省直辖县级行政区划"),
        ("510000" -> "西南,四川省,其他"),
        ("510100" -> "西南,四川省,成都市"),
        ("510300" -> "西南,四川省,自贡市"),
        ("510400" -> "西南,四川省,攀枝花市"),
        ("510500" -> "西南,四川省,泸州市"),
        ("510600" -> "西南,四川省,德阳市"),
        ("510700" -> "西南,四川省,绵阳市"),
        ("510800" -> "西南,四川省,广元市"),
        ("510900" -> "西南,四川省,遂宁市"),
        ("511000" -> "西南,四川省,内江市"),
        ("511100" -> "西南,四川省,乐山市"),
        ("511300" -> "西南,四川省,南充市"),
        ("511400" -> "西南,四川省,眉山市"),
        ("511500" -> "西南,四川省,宜宾市"),
        ("511600" -> "西南,四川省,广安市"),
        ("511700" -> "西南,四川省,达州市"),
        ("511800" -> "西南,四川省,雅安市"),
        ("511900" -> "西南,四川省,巴中市"),
        ("512000" -> "西南,四川省,资阳市"),
        ("513200" -> "西南,四川省,阿坝藏族羌族自治州"),
        ("513300" -> "西南,四川省,甘孜藏族自治州"),
        ("513400" -> "西南,四川省,凉山彝族自治州"),
        ("520000" -> "西南,贵州省,其他"),
        ("520100" -> "西南,贵州省,贵阳市"),
        ("520200" -> "西南,贵州省,六盘水市"),
        ("520300" -> "西南,贵州省,遵义市"),
        ("520400" -> "西南,贵州省,安顺市"),
        ("520500" -> "西南,贵州省,毕节市"),
        ("520600" -> "西南,贵州省,铜仁市"),
        ("522300" -> "西南,贵州省,黔西南布依族苗族自治州"),
        ("522600" -> "西南,贵州省,黔东南苗族侗族自治州"),
        ("522700" -> "西南,贵州省,黔南布依族苗族自治州"),
        ("530000" -> "西南,云南省,其他"),
        ("530100" -> "西南,云南省,昆明市"),
        ("530300" -> "西南,云南省,曲靖市"),
        ("530400" -> "西南,云南省,玉溪市"),
        ("530500" -> "西南,云南省,保山市"),
        ("530600" -> "西南,云南省,昭通市"),
        ("530700" -> "西南,云南省,丽江市"),
        ("530800" -> "西南,云南省,普洱市"),
        ("530900" -> "西南,云南省,临沧市"),
        ("532300" -> "西南,云南省,楚雄彝族自治州"),
        ("532500" -> "西南,云南省,红河哈尼族彝族自治州"),
        ("532600" -> "西南,云南省,文山壮族苗族自治州"),
        ("532800" -> "西南,云南省,西双版纳傣族自治州"),
        ("532900" -> "西南,云南省,大理白族自治州"),
        ("533100" -> "西南,云南省,德宏傣族景颇族自治州"),
        ("533300" -> "西南,云南省,怒江傈僳族自治州"),
        ("533400" -> "西南,云南省,迪庆藏族自治州"),
        ("540000" -> "西南,西藏自治区,其他"),
        ("540100" -> "西南,西藏自治区,拉萨市"),
        ("542100" -> "西南,西藏自治区,昌都地区"),
        ("542200" -> "西南,西藏自治区,山南地区"),
        ("542300" -> "西南,西藏自治区,日喀则地区"),
        ("542400" -> "西南,西藏自治区,那曲地区"),
        ("542500" -> "西南,西藏自治区,阿里地区"),
        ("542600" -> "西南,西藏自治区,林芝地区"),
        ("610000" -> "西北,陕西省,其他"),
        ("610100" -> "西北,陕西省,西安市"),
        ("610200" -> "西北,陕西省,铜川市"),
        ("610300" -> "西北,陕西省,宝鸡市"),
        ("610400" -> "西北,陕西省,咸阳市"),
        ("610500" -> "西北,陕西省,渭南市"),
        ("610600" -> "西北,陕西省,延安市"),
        ("610700" -> "西北,陕西省,汉中市"),
        ("610800" -> "西北,陕西省,榆林市"),
        ("610900" -> "西北,陕西省,安康市"),
        ("611000" -> "西北,陕西省,商洛市"),
        ("620000" -> "西北,甘肃省,其他"),
        ("620100" -> "西北,甘肃省,兰州市"),
        ("620200" -> "西北,甘肃省,嘉峪关市"),
        ("620300" -> "西北,甘肃省,金昌市"),
        ("620400" -> "西北,甘肃省,白银市"),
        ("620500" -> "西北,甘肃省,天水市"),
        ("620600" -> "西北,甘肃省,武威市"),
        ("620700" -> "西北,甘肃省,张掖市"),
        ("620800" -> "西北,甘肃省,平凉市"),
        ("620900" -> "西北,甘肃省,酒泉市"),
        ("621000" -> "西北,甘肃省,庆阳市"),
        ("621100" -> "西北,甘肃省,定西市"),
        ("621200" -> "西北,甘肃省,陇南市"),
        ("622900" -> "西北,甘肃省,临夏回族自治州"),
        ("623000" -> "西北,甘肃省,甘南藏族自治州"),
        ("630000" -> "西北,青海省,其他"),
        ("630100" -> "西北,青海省,西宁市"),
        ("630200" -> "西北,青海省,海东市"),
        ("632200" -> "西北,青海省,海北藏族自治州"),
        ("632300" -> "西北,青海省,黄南藏族自治州"),
        ("632500" -> "西北,青海省,海南藏族自治州"),
        ("632600" -> "西北,青海省,果洛藏族自治州"),
        ("632700" -> "西北,青海省,玉树藏族自治州"),
        ("632800" -> "西北,青海省,海西蒙古族藏族自治州"),
        ("640000" -> "西北,宁夏回族自治区,其他"),
        ("640100" -> "西北,宁夏回族自治区,银川市"),
        ("640200" -> "西北,宁夏回族自治区,石嘴山市"),
        ("640300" -> "西北,宁夏回族自治区,吴忠市"),
        ("640400" -> "西北,宁夏回族自治区,固原市"),
        ("640500" -> "西北,宁夏回族自治区,中卫市"),
        ("650000" -> "西北,新疆维吾尔自治区,其他"),
        ("650100" -> "西北,新疆维吾尔自治区,乌鲁木齐市"),
        ("650200" -> "西北,新疆维吾尔自治区,克拉玛依市"),
        ("652100" -> "西北,新疆维吾尔自治区,吐鲁番地区"),
        ("652200" -> "西北,新疆维吾尔自治区,哈密地区"),
        ("652300" -> "西北,新疆维吾尔自治区,昌吉回族自治州"),
        ("652700" -> "西北,新疆维吾尔自治区,博尔塔拉蒙古自治州"),
        ("652800" -> "西北,新疆维吾尔自治区,巴音郭楞蒙古自治州"),
        ("652900" -> "西北,新疆维吾尔自治区,阿克苏地区"),
        ("653000" -> "西北,新疆维吾尔自治区,克孜勒苏柯尔克孜自治州"),
        ("653100" -> "西北,新疆维吾尔自治区,喀什地区"),
        ("653200" -> "西北,新疆维吾尔自治区,和田地区"),
        ("654000" -> "西北,新疆维吾尔自治区,伊犁哈萨克自治州"),
        ("654200" -> "西北,新疆维吾尔自治区,塔城地区"),
        ("654300" -> "西北,新疆维吾尔自治区,阿勒泰地区"),
        ("659000" -> "西北,新疆维吾尔自治区,自治区直辖县级行政区划"),
        ("710000" -> "港澳台,台湾省,其他"),
        ("710100" -> "港澳台,台湾省,台北市"),
        ("710200" -> "港澳台,台湾省,高雄市"),
        ("710300" -> "港澳台,台湾省,台南市"),
        ("710400" -> "港澳台,台湾省,台中市"),
        ("710700" -> "港澳台,台湾省,基隆市"),
        ("710800" -> "港澳台,台湾省,新竹市"),
        ("710900" -> "港澳台,台湾省,嘉义市")
    )

    /**
      * 根据地域码获取大区、省份、城市
      * @param code
      * @return
      */
    def getPc(code: String) : String = {
        areaMap.get(code).getOrElse("其他,其他,其他")
    }

    val cityLevel = Map(
        ("北京市"->"特级城市"),
        ("上海市"->"特级城市"),
        ("广州市"->"特级城市"),
        ("深圳市"->"特级城市"),
        ("成都市"->"一线城市"),
        ("杭州市"->"一线城市"),
        ("武汉市"->"一线城市"),
        ("天津市"->"一线城市"),
        ("南京市"->"一线城市"),
        ("重庆市"->"一线城市"),
        ("西安市"->"一线城市"),
        ("长沙市"->"一线城市"),
        ("青岛市"->"一线城市"),
        ("沈阳市"->"一线城市"),
        ("大连市"->"一线城市"),
        ("厦门市"->"一线城市"),
        ("苏州市"->"一线城市"),
        ("宁波市"->"一线城市"),
        ("无锡市"->"一线城市"),
        ("福州市"->"二线城市"),
        ("合肥市"->"二线城市"),
        ("郑州市"->"二线城市"),
        ("哈尔滨市"->"二线城市"),
        ("佛山市"->"二线城市"),
        ("济南市"->"二线城市"),
        ("东莞市"->"二线城市"),
        ("昆明市"->"二线城市"),
        ("太原市"->"二线城市"),
        ("南昌市"->"二线城市"),
        ("南宁市"->"二线城市"),
        ("温州市"->"二线城市"),
        ("石家庄市"->"二线城市"),
        ("长春市"->"二线城市"),
        ("泉州市"->"二线城市"),
        ("贵阳市"->"二线城市"),
        ("常州市"->"二线城市"),
        ("珠海市"->"二线城市"),
        ("金华市"->"二线城市"),
        ("烟台市"->"二线城市"),
        ("海口市"->"二线城市"),
        ("惠州市"->"二线城市"),
        ("乌鲁木齐市"->"二线城市"),
        ("徐州市"->"二线城市"),
        ("嘉兴市"->"二线城市"),
        ("潍坊市"->"二线城市"),
        ("洛阳市"->"二线城市"),
        ("南通市"->"二线城市"),
        ("扬州市"->"二线城市"),
        ("汕头市"->"二线城市"),
        ("兰州市"->"三线城市"),
        ("桂林市"->"三线城市"),
        ("三亚市"->"三线城市"),
        ("呼和浩特市"->"三线城市"),
        ("绍兴市"->"三线城市"),
        ("泰州市"->"三线城市"),
        ("银川市"->"三线城市"),
        ("中山市"->"三线城市"),
        ("保定市"->"三线城市"),
        ("西宁市"->"三线城市"),
        ("芜湖市"->"三线城市"),
        ("赣州市"->"三线城市"),
        ("绵阳市"->"三线城市"),
        ("漳州市"->"三线城市"),
        ("莆田市"->"三线城市"),
        ("威海市"->"三线城市"),
        ("邯郸市"->"三线城市"),
        ("临沂市"->"三线城市"),
        ("唐山市"->"三线城市"),
        ("台州市"->"三线城市"),
        ("宜昌市"->"三线城市"),
        ("湖州市"->"三线城市"),
        ("包头市"->"三线城市"),
        ("济宁市"->"三线城市"),
        ("盐城市"->"三线城市"),
        ("鞍山市"->"三线城市"),
        ("廊坊市"->"三线城市"),
        ("衡阳市"->"三线城市"),
        ("秦皇岛市"->"三线城市"),
        ("吉林市"->"三线城市"),
        ("大庆市"->"三线城市"),
        ("淮安市"->"三线城市"),
        ("丽江市"->"三线城市"),
        ("揭阳市"->"三线城市"),
        ("荆州市"->"三线城市"),
        ("连云港市"->"三线城市"),
        ("张家口市"->"三线城市"),
        ("遵义市"->"三线城市"),
        ("上饶市"->"三线城市"),
        ("龙岩市"->"三线城市"),
        ("衢州市"->"三线城市"),
        ("赤峰市"->"三线城市"),
        ("湛江市"->"三线城市"),
        ("运城市"->"三线城市"),
        ("鄂尔多斯市"->"三线城市"),
        ("岳阳市"->"三线城市"),
        ("安阳市"->"三线城市"),
        ("株洲市"->"三线城市"),
        ("镇江市"->"三线城市"),
        ("淄博市"->"三线城市"),
        ("郴州市"->"三线城市"),
        ("南平市"->"三线城市"),
        ("齐齐哈尔市"->"三线城市"),
        ("常德市"->"三线城市"),
        ("柳州市"->"三线城市"),
        ("咸阳市"->"三线城市"),
        ("南充市"->"三线城市"),
        ("泸州市"->"三线城市"),
        ("蚌埠市"->"三线城市"),
        ("邢台市"->"三线城市"),
        ("舟山市"->"三线城市"),
        ("宝鸡市"->"三线城市"),
        ("常德市"->"三线城市"),
        ("抚顺市"->"三线城市"),
        ("宜宾市"->"三线城市"),
        ("宜春市"->"三线城市"),
        ("怀化市"->"三线城市"),
        ("榆林市"->"三线城市"),
        ("梅州市"->"三线城市"),
        ("呼伦贝尔市"->"三线城市")
    )

    /**
      * 根据城市获取城市级别
      * @param code
      * @return
      */
    def getCl(code: String) : String = {
        cityLevel.get(code).getOrElse("其他")
    }
}

object chareatest {
    def main(args: Array[String]): Unit = {
        println(Codearea.getArea("四川省"))
    }
}package com.avcdata.vbox.common

/**
  * Created by avc on 2016/11/16.
  */
object Codearea {
    val prolist:List[String] = List("上海市","云南省","内蒙古自治区","北京市","吉林省","四川省","天津市",
        "宁夏回族自治区","安徽省","山东省","山西省","广东省","广西壮族自治区","新疆维吾尔自治区","江苏省",
        "江西省","河北省","河南省","浙江省","海南省","湖北省","湖南省",
        "甘肃省","福建省","西藏自治区","贵州省","辽宁省","重庆市","陕西省","青海省","黑龙江省")

    val provinceMap = Map(
        ("北京市" -> "华北"),
        ("天津市" -> "华北"),
        ("上海市" -> "华东"),
        ("重庆市" -> "西南"),
        ("香港特别行政区"-> "港澳台"),
        ("河北省" -> "华北"),
        ("山西省" -> "华北"),
        ("内蒙古自治区" -> "华北"),
        ("辽宁省" -> "华北"),
        ("吉林省" -> "东北"),
        ("黑龙江省" -> "东北"),
        ("江苏省" -> "华东"),
        ("浙江省" -> "华东"),
        ("安徽省" -> "华东"),
        ("福建省" -> "华东"),
        ("江西省" -> "华东"),
        ("山东省" -> "华东"),
        ("河南省" -> "华中"),
        ("湖北省"-> "华中"),
        ("湖南省" -> "华中"),
        ("广东省" -> "华南"),
        ("广西壮族自治区" -> "华南"),
        ("海南省" -> "西南"),
        ("四川省" -> "西南"),
        ("贵州省" -> "西南"),
        ("云南省" -> "西南"),
        ("西藏自治区" -> "西南"),
        ("陕西省" -> "西北"),
        ("甘肃省" -> "西北"),
        ("青海省" -> "西北"),
        ("宁夏回族自治区" -> "西北"),
        ("新疆维吾尔自治区" -> "西北"),
        ("台湾省" -> "港澳台")
    )

    /**
      * 根据省份获取大区
      * @param province
      * @return
      */
    def getArea(province: String) : String = {
        provinceMap.get(province).getOrElse("其他")
    }

    val areaMap = Map(
        ("110000" -> "华北,北京市,北京市"),
        ("120000" -> "华北,天津市,天津市"),
        ("310000" -> "华东,上海市,上海市"),
        ("500000" -> "西南,重庆市,重庆市"),
        ("810000" -> "港澳台,香港特别行政区,香港特别行政区"),
        ("130000" -> "华北,河北省,其他"),
        ("130100" -> "华北,河北省,石家庄市"),
        ("130200" -> "华北,河北省,唐山市"),
        ("130300" -> "华北,河北省,秦皇岛市"),
        ("130400" -> "华北,河北省,邯郸市"),
        ("130500" -> "华北,河北省,邢台市"),
        ("130600" -> "华北,河北省,保定市"),
        ("130700" -> "华北,河北省,张家口市"),
        ("130800" -> "华北,河北省,承德市"),
        ("130900" -> "华北,河北省,沧州市"),
        ("131000" -> "华北,河北省,廊坊市"),
        ("131100" -> "华北,河北省,衡水市"),
        ("140000" -> "华北,山西省,其他"),
        ("140100" -> "华北,山西省,太原市"),
        ("140200" -> "华北,山西省,大同市"),
        ("140300" -> "华北,山西省,阳泉市"),
        ("140400" -> "华北,山西省,长治市"),
        ("140500" -> "华北,山西省,晋城市"),
        ("140600" -> "华北,山西省,朔州市"),
        ("140700" -> "华北,山西省,晋中市"),
        ("140800" -> "华北,山西省,运城市"),
        ("140900" -> "华北,山西省,忻州市"),
        ("141000" -> "华北,山西省,临汾市"),
        ("141100" -> "华北,山西省,吕梁市"),
        ("150000" -> "华北,内蒙古自治区,其他"),
        ("150100" -> "华北,内蒙古自治区,呼和浩特市"),
        ("150200" -> "华北,内蒙古自治区,包头市"),
        ("150300" -> "华北,内蒙古自治区,乌海市"),
        ("150400" -> "华北,内蒙古自治区,赤峰市"),
        ("150500" -> "华北,内蒙古自治区,通辽市"),
        ("150600" -> "华北,内蒙古自治区,鄂尔多斯市"),
        ("150700" -> "华北,内蒙古自治区,呼伦贝尔市"),
        ("150800" -> "华北,内蒙古自治区,巴彦淖尔市"),
        ("150900" -> "华北,内蒙古自治区,乌兰察布市"),
        ("152200" -> "华北,内蒙古自治区,兴安盟"),
        ("152500" -> "华北,内蒙古自治区,锡林郭勒盟"),
        ("152900" -> "华北,内蒙古自治区,阿拉善盟"),
        ("210000" -> "华北,辽宁省,其他"),
        ("210100" -> "东北,辽宁省,沈阳市"),
        ("210200" -> "东北,辽宁省,大连市"),
        ("210300" -> "东北,辽宁省,鞍山市"),
        ("210400" -> "东北,辽宁省,抚顺市"),
        ("210500" -> "东北,辽宁省,本溪市"),
        ("210600" -> "东北,辽宁省,丹东市"),
        ("210700" -> "东北,辽宁省,锦州市"),
        ("210800" -> "东北,辽宁省,营口市"),
        ("210900" -> "东北,辽宁省,阜新市"),
        ("211000" -> "东北,辽宁省,辽阳市"),
        ("211100" -> "东北,辽宁省,盘锦市"),
        ("211200" -> "东北,辽宁省,铁岭市"),
        ("211300" -> "东北,辽宁省,朝阳市"),
        ("211400" -> "东北,辽宁省,葫芦岛市"),
        ("220000" -> "东北,吉林省,其他"),
        ("220100" -> "东北,吉林省,长春市"),
        ("220200" -> "东北,吉林省,吉林市"),
        ("220300" -> "东北,吉林省,四平市"),
        ("220400" -> "东北,吉林省,辽源市"),
        ("220500" -> "东北,吉林省,通化市"),
        ("220600" -> "东北,吉林省,白山市"),
        ("220700" -> "东北,吉林省,松原市"),
        ("220800" -> "东北,吉林省,白城市"),
        ("222400" -> "东北,吉林省,延边朝鲜族自治州"),
        ("230000" -> "东北,黑龙江省,其他"),
        ("230100" -> "东北,黑龙江省,哈尔滨市"),
        ("230200" -> "东北,黑龙江省,齐齐哈尔市"),
        ("230300" -> "东北,黑龙江省,鸡西市"),
        ("230400" -> "东北,黑龙江省,鹤岗市"),
        ("230500" -> "东北,黑龙江省,双鸭山市"),
        ("230600" -> "东北,黑龙江省,大庆市"),
        ("230700" -> "东北,黑龙江省,伊春市"),
        ("230800" -> "东北,黑龙江省,佳木斯市"),
        ("230900" -> "东北,黑龙江省,七台河市"),
        ("231000" -> "东北,黑龙江省,牡丹江市"),
        ("231100" -> "东北,黑龙江省,黑河市"),
        ("231200" -> "东北,黑龙江省,绥化市"),
        ("232700" -> "东北,黑龙江省,大兴安岭地区"),
        ("320000" -> "华东,江苏省,其他"),
        ("320100" -> "华东,江苏省,南京市"),
        ("320200" -> "华东,江苏省,无锡市"),
        ("320300" -> "华东,江苏省,徐州市"),
        ("320400" -> "华东,江苏省,常州市"),
        ("320500" -> "华东,江苏省,苏州市"),
        ("320600" -> "华东,江苏省,南通市"),
        ("320700" -> "华东,江苏省,连云港市"),
        ("320800" -> "华东,江苏省,淮安市"),
        ("320900" -> "华东,江苏省,盐城市"),
        ("321000" -> "华东,江苏省,扬州市"),
        ("321100" -> "华东,江苏省,镇江市"),
        ("321200" -> "华东,江苏省,泰州市"),
        ("321300" -> "华东,江苏省,宿迁市"),
        ("330000" -> "华东,浙江省,其他"),
        ("330100" -> "华东,浙江省,杭州市"),
        ("330200" -> "华东,浙江省,宁波市"),
        ("330300" -> "华东,浙江省,温州市"),
        ("330400" -> "华东,浙江省,嘉兴市"),
        ("330500" -> "华东,浙江省,湖州市"),
        ("330600" -> "华东,浙江省,绍兴市"),
        ("330700" -> "华东,浙江省,金华市"),
        ("330800" -> "华东,浙江省,衢州市"),
        ("330900" -> "华东,浙江省,舟山市"),
        ("331000" -> "华东,浙江省,台州市"),
        ("331100" -> "华东,浙江省,丽水市"),
        ("340000" -> "华东,安徽省,其他"),
        ("340100" -> "华东,安徽省,合肥市"),
        ("340200" -> "华东,安徽省,芜湖市"),
        ("340300" -> "华东,安徽省,蚌埠市"),
        ("340400" -> "华东,安徽省,淮南市"),
        ("340500" -> "华东,安徽省,马鞍山市"),
        ("340600" -> "华东,安徽省,淮北市"),
        ("340700" -> "华东,安徽省,铜陵市"),
        ("340800" -> "华东,安徽省,安庆市"),
        ("341000" -> "华东,安徽省,黄山市"),
        ("341100" -> "华东,安徽省,滁州市"),
        ("341200" -> "华东,安徽省,阜阳市"),
        ("341300" -> "华东,安徽省,宿州市"),
        ("341500" -> "华东,安徽省,六安市"),
        ("341600" -> "华东,安徽省,亳州市"),
        ("341700" -> "华东,安徽省,池州市"),
        ("341800" -> "华东,安徽省,宣城市"),
        ("350000" -> "华东,福建省,其他"),
        ("350100" -> "华东,福建省,福州市"),
        ("350200" -> "华东,福建省,厦门市"),
        ("350300" -> "华东,福建省,莆田市"),
        ("350400" -> "华东,福建省,三明市"),
        ("350500" -> "华东,福建省,泉州市"),
        ("350600" -> "华东,福建省,漳州市"),
        ("350700" -> "华东,福建省,南平市"),
        ("350800" -> "华东,福建省,龙岩市"),
        ("350900" -> "华东,福建省,宁德市"),
        ("360000" -> "华东,江西省,其他"),
        ("360100" -> "华东,江西省,南昌市"),
        ("360200" -> "华东,江西省,景德镇市"),
        ("360300" -> "华东,江西省,萍乡市"),
        ("360400" -> "华东,江西省,九江市"),
        ("360500" -> "华东,江西省,新余市"),
        ("360600" -> "华东,江西省,鹰潭市"),
        ("360700" -> "华东,江西省,赣州市"),
        ("360800" -> "华东,江西省,吉安市"),
        ("360900" -> "华东,江西省,宜春市"),
        ("361000" -> "华东,江西省,抚州市"),
        ("361100" -> "华东,江西省,上饶市"),
        ("370000" -> "华东,山东省,其他"),
        ("370100" -> "华东,山东省,济南市"),
        ("370200" -> "华东,山东省,青岛市"),
        ("370300" -> "华东,山东省,淄博市"),
        ("370400" -> "华东,山东省,枣庄市"),
        ("370500" -> "华东,山东省,东营市"),
        ("370600" -> "华东,山东省,烟台市"),
        ("370700" -> "华东,山东省,潍坊市"),
        ("370800" -> "华东,山东省,济宁市"),
        ("370900" -> "华东,山东省,泰安市"),
        ("371000" -> "华东,山东省,威海市"),
        ("371100" -> "华东,山东省,日照市"),
        ("371200" -> "华东,山东省,莱芜市"),
        ("371300" -> "华东,山东省,临沂市"),
        ("371400" -> "华东,山东省,德州市"),
        ("371500" -> "华东,山东省,聊城市"),
        ("371600" -> "华东,山东省,滨州市"),
        ("371700" -> "华东,山东省,菏泽市"),
        ("410000" -> "华中,河南省,其他"),
        ("410100" -> "华中,河南省,郑州市"),
        ("410200" -> "华中,河南省,开封市"),
        ("410300" -> "华中,河南省,洛阳市"),
        ("410400" -> "华中,河南省,平顶山市"),
        ("410500" -> "华中,河南省,安阳市"),
        ("410600" -> "华中,河南省,鹤壁市"),
        ("410700" -> "华中,河南省,新乡市"),
        ("410800" -> "华中,河南省,焦作市"),
        ("410900" -> "华中,河南省,濮阳市"),
        ("411000" -> "华中,河南省,许昌市"),
        ("411100" -> "华中,河南省,漯河市"),
        ("411200" -> "华中,河南省,三门峡市"),
        ("411300" -> "华中,河南省,南阳市"),
        ("411400" -> "华中,河南省,商丘市"),
        ("411500" -> "华中,河南省,信阳市"),
        ("411600" -> "华中,河南省,周口市"),
        ("411700" -> "华中,河南省,驻马店市"),
        ("419000" -> "华中,河南省,省直辖县级行政区划"),
        ("420000" -> "华中,湖北省,其他"),
        ("420100" -> "华中,湖北省,武汉市"),
        ("420200" -> "华中,湖北省,黄石市"),
        ("420300" -> "华中,湖北省,十堰市"),
        ("420500" -> "华中,湖北省,宜昌市"),
        ("420600" -> "华中,湖北省,襄阳市"),
        ("420700" -> "华中,湖北省,鄂州市"),
        ("420800" -> "华中,湖北省,荆门市"),
        ("420900" -> "华中,湖北省,孝感市"),
        ("421000" -> "华中,湖北省,荆州市"),
        ("421100" -> "华中,湖北省,黄冈市"),
        ("421200" -> "华中,湖北省,咸宁市"),
        ("421300" -> "华中,湖北省,随州市"),
        ("422800" -> "华中,湖北省,恩施土家族苗族自治州"),
        ("429000" -> "华中,湖北省,省直辖县级行政区划"),
        ("430000" -> "华中,湖南省,其他"),
        ("430100" -> "华中,湖南省,长沙市"),
        ("430200" -> "华中,湖南省,株洲市"),
        ("430300" -> "华中,湖南省,湘潭市"),
        ("430400" -> "华中,湖南省,衡阳市"),
        ("430500" -> "华中,湖南省,邵阳市"),
        ("430600" -> "华中,湖南省,岳阳市"),
        ("430700" -> "华中,湖南省,常德市"),
        ("430800" -> "华中,湖南省,张家界市"),
        ("430900" -> "华中,湖南省,益阳市"),
        ("431000" -> "华中,湖南省,郴州市"),
        ("431100" -> "华中,湖南省,永州市"),
        ("431200" -> "华中,湖南省,怀化市"),
        ("431300" -> "华中,湖南省,娄底市"),
        ("433100" -> "华中,湖南省,湘西土家族苗族自治州"),
        ("440000" -> "华南,广东省,其他"),
        ("440100" -> "华南,广东省,广州市"),
        ("440200" -> "华南,广东省,韶关市"),
        ("440300" -> "华南,广东省,深圳市"),
        ("440400" -> "华南,广东省,珠海市"),
        ("440500" -> "华南,广东省,汕头市"),
        ("440600" -> "华南,广东省,佛山市"),
        ("440700" -> "华南,广东省,江门市"),
        ("440800" -> "华南,广东省,湛江市"),
        ("440900" -> "华南,广东省,茂名市"),
        ("441200" -> "华南,广东省,肇庆市"),
        ("441300" -> "华南,广东省,惠州市"),
        ("441400" -> "华南,广东省,梅州市"),
        ("441500" -> "华南,广东省,汕尾市"),
        ("441600" -> "华南,广东省,河源市"),
        ("441700" -> "华南,广东省,阳江市"),
        ("441800" -> "华南,广东省,清远市"),
        ("441900" -> "华南,广东省,东莞市"),
        ("442000" -> "华南,广东省,中山市"),
        ("445100" -> "华南,广东省,潮州市"),
        ("445200" -> "华南,广东省,揭阳市"),
        ("445300" -> "华南,广东省,云浮市"),
        ("450000" -> "华南,广西壮族自治区,其他"),
        ("450100" -> "华南,广西壮族自治区,南宁市"),
        ("450200" -> "华南,广西壮族自治区,柳州市"),
        ("450300" -> "华南,广西壮族自治区,桂林市"),
        ("450400" -> "华南,广西壮族自治区,梧州市"),
        ("450500" -> "华南,广西壮族自治区,北海市"),
        ("450600" -> "华南,广西壮族自治区,防城港市"),
        ("450700" -> "华南,广西壮族自治区,钦州市"),
        ("450800" -> "华南,广西壮族自治区,贵港市"),
        ("450900" -> "华南,广西壮族自治区,玉林市"),
        ("451000" -> "华南,广西壮族自治区,百色市"),
        ("451100" -> "华南,广西壮族自治区,贺州市"),
        ("451200" -> "华南,广西壮族自治区,河池市"),
        ("451300" -> "华南,广西壮族自治区,来宾市"),
        ("451400" -> "华南,广西壮族自治区,崇左市"),
        ("460000" -> "西南,海南省,其他"),
        ("460100" -> "西南,海南省,海口市"),
        ("460200" -> "西南,海南省,三亚市"),
        ("460300" -> "西南,海南省,三沙市"),
        ("469000" -> "西南,海南省,省直辖县级行政区划"),
        ("510000" -> "西南,四川省,其他"),
        ("510100" -> "西南,四川省,成都市"),
        ("510300" -> "西南,四川省,自贡市"),
        ("510400" -> "西南,四川省,攀枝花市"),
        ("510500" -> "西南,四川省,泸州市"),
        ("510600" -> "西南,四川省,德阳市"),
        ("510700" -> "西南,四川省,绵阳市"),
        ("510800" -> "西南,四川省,广元市"),
        ("510900" -> "西南,四川省,遂宁市"),
        ("511000" -> "西南,四川省,内江市"),
        ("511100" -> "西南,四川省,乐山市"),
        ("511300" -> "西南,四川省,南充市"),
        ("511400" -> "西南,四川省,眉山市"),
        ("511500" -> "西南,四川省,宜宾市"),
        ("511600" -> "西南,四川省,广安市"),
        ("511700" -> "西南,四川省,达州市"),
        ("511800" -> "西南,四川省,雅安市"),
        ("511900" -> "西南,四川省,巴中市"),
        ("512000" -> "西南,四川省,资阳市"),
        ("513200" -> "西南,四川省,阿坝藏族羌族自治州"),
        ("513300" -> "西南,四川省,甘孜藏族自治州"),
        ("513400" -> "西南,四川省,凉山彝族自治州"),
        ("520000" -> "西南,贵州省,其他"),
        ("520100" -> "西南,贵州省,贵阳市"),
        ("520200" -> "西南,贵州省,六盘水市"),
        ("520300" -> "西南,贵州省,遵义市"),
        ("520400" -> "西南,贵州省,安顺市"),
        ("520500" -> "西南,贵州省,毕节市"),
        ("520600" -> "西南,贵州省,铜仁市"),
        ("522300" -> "西南,贵州省,黔西南布依族苗族自治州"),
        ("522600" -> "西南,贵州省,黔东南苗族侗族自治州"),
        ("522700" -> "西南,贵州省,黔南布依族苗族自治州"),
        ("530000" -> "西南,云南省,其他"),
        ("530100" -> "西南,云南省,昆明市"),
        ("530300" -> "西南,云南省,曲靖市"),
        ("530400" -> "西南,云南省,玉溪市"),
        ("530500" -> "西南,云南省,保山市"),
        ("530600" -> "西南,云南省,昭通市"),
        ("530700" -> "西南,云南省,丽江市"),
        ("530800" -> "西南,云南省,普洱市"),
        ("530900" -> "西南,云南省,临沧市"),
        ("532300" -> "西南,云南省,楚雄彝族自治州"),
        ("532500" -> "西南,云南省,红河哈尼族彝族自治州"),
        ("532600" -> "西南,云南省,文山壮族苗族自治州"),
        ("532800" -> "西南,云南省,西双版纳傣族自治州"),
        ("532900" -> "西南,云南省,大理白族自治州"),
        ("533100" -> "西南,云南省,德宏傣族景颇族自治州"),
        ("533300" -> "西南,云南省,怒江傈僳族自治州"),
        ("533400" -> "西南,云南省,迪庆藏族自治州"),
        ("540000" -> "西南,西藏自治区,其他"),
        ("540100" -> "西南,西藏自治区,拉萨市"),
        ("542100" -> "西南,西藏自治区,昌都地区"),
        ("542200" -> "西南,西藏自治区,山南地区"),
        ("542300" -> "西南,西藏自治区,日喀则地区"),
        ("542400" -> "西南,西藏自治区,那曲地区"),
        ("542500" -> "西南,西藏自治区,阿里地区"),
        ("542600" -> "西南,西藏自治区,林芝地区"),
        ("610000" -> "西北,陕西省,其他"),
        ("610100" -> "西北,陕西省,西安市"),
        ("610200" -> "西北,陕西省,铜川市"),
        ("610300" -> "西北,陕西省,宝鸡市"),
        ("610400" -> "西北,陕西省,咸阳市"),
        ("610500" -> "西北,陕西省,渭南市"),
        ("610600" -> "西北,陕西省,延安市"),
        ("610700" -> "西北,陕西省,汉中市"),
        ("610800" -> "西北,陕西省,榆林市"),
        ("610900" -> "西北,陕西省,安康市"),
        ("611000" -> "西北,陕西省,商洛市"),
        ("620000" -> "西北,甘肃省,其他"),
        ("620100" -> "西北,甘肃省,兰州市"),
        ("620200" -> "西北,甘肃省,嘉峪关市"),
        ("620300" -> "西北,甘肃省,金昌市"),
        ("620400" -> "西北,甘肃省,白银市"),
        ("620500" -> "西北,甘肃省,天水市"),
        ("620600" -> "西北,甘肃省,武威市"),
        ("620700" -> "西北,甘肃省,张掖市"),
        ("620800" -> "西北,甘肃省,平凉市"),
        ("620900" -> "西北,甘肃省,酒泉市"),
        ("621000" -> "西北,甘肃省,庆阳市"),
        ("621100" -> "西北,甘肃省,定西市"),
        ("621200" -> "西北,甘肃省,陇南市"),
        ("622900" -> "西北,甘肃省,临夏回族自治州"),
        ("623000" -> "西北,甘肃省,甘南藏族自治州"),
        ("630000" -> "西北,青海省,其他"),
        ("630100" -> "西北,青海省,西宁市"),
        ("630200" -> "西北,青海省,海东市"),
        ("632200" -> "西北,青海省,海北藏族自治州"),
        ("632300" -> "西北,青海省,黄南藏族自治州"),
        ("632500" -> "西北,青海省,海南藏族自治州"),
        ("632600" -> "西北,青海省,果洛藏族自治州"),
        ("632700" -> "西北,青海省,玉树藏族自治州"),
        ("632800" -> "西北,青海省,海西蒙古族藏族自治州"),
        ("640000" -> "西北,宁夏回族自治区,其他"),
        ("640100" -> "西北,宁夏回族自治区,银川市"),
        ("640200" -> "西北,宁夏回族自治区,石嘴山市"),
        ("640300" -> "西北,宁夏回族自治区,吴忠市"),
        ("640400" -> "西北,宁夏回族自治区,固原市"),
        ("640500" -> "西北,宁夏回族自治区,中卫市"),
        ("650000" -> "西北,新疆维吾尔自治区,其他"),
        ("650100" -> "西北,新疆维吾尔自治区,乌鲁木齐市"),
        ("650200" -> "西北,新疆维吾尔自治区,克拉玛依市"),
        ("652100" -> "西北,新疆维吾尔自治区,吐鲁番地区"),
        ("652200" -> "西北,新疆维吾尔自治区,哈密地区"),
        ("652300" -> "西北,新疆维吾尔自治区,昌吉回族自治州"),
        ("652700" -> "西北,新疆维吾尔自治区,博尔塔拉蒙古自治州"),
        ("652800" -> "西北,新疆维吾尔自治区,巴音郭楞蒙古自治州"),
        ("652900" -> "西北,新疆维吾尔自治区,阿克苏地区"),
        ("653000" -> "西北,新疆维吾尔自治区,克孜勒苏柯尔克孜自治州"),
        ("653100" -> "西北,新疆维吾尔自治区,喀什地区"),
        ("653200" -> "西北,新疆维吾尔自治区,和田地区"),
        ("654000" -> "西北,新疆维吾尔自治区,伊犁哈萨克自治州"),
        ("654200" -> "西北,新疆维吾尔自治区,塔城地区"),
        ("654300" -> "西北,新疆维吾尔自治区,阿勒泰地区"),
        ("659000" -> "西北,新疆维吾尔自治区,自治区直辖县级行政区划"),
        ("710000" -> "港澳台,台湾省,其他"),
        ("710100" -> "港澳台,台湾省,台北市"),
        ("710200" -> "港澳台,台湾省,高雄市"),
        ("710300" -> "港澳台,台湾省,台南市"),
        ("710400" -> "港澳台,台湾省,台中市"),
        ("710700" -> "港澳台,台湾省,基隆市"),
        ("710800" -> "港澳台,台湾省,新竹市"),
        ("710900" -> "港澳台,台湾省,嘉义市")
    )

    /**
      * 根据地域码获取大区、省份、城市
      * @param code
      * @return
      */
    def getPc(code: String) : String = {
        areaMap.get(code).getOrElse("其他,其他,其他")
    }

    val cityLevel = Map(
        ("北京市"->"1线城市"),
        ("广州市"->"1线城市"),
        ("上海市"->"1线城市"),
        ("上海市市辖区"->"1线城市"),
        ("深圳市"->"1线城市"),
        ("成都市"->"2线城市"),
        ("大连市"->"2线城市"),
        ("东莞市"->"2线城市"),
        ("佛山市"->"2线城市"),
        ("福州市"->"2线城市"),
        ("哈尔滨市"->"2线城市"),
        ("杭州市"->"2线城市"),
        ("杭州市"->"2线城市"),
        ("合肥市"->"2线城市"),
        ("济南市"->"2线城市"),
        ("昆明市"->"2线城市"),
        ("南昌市"->"2线城市"),
        ("南京市"->"2线城市"),
        ("南宁市"->"2线城市"),
        ("宁波市"->"2线城市"),
        ("青岛市"->"2线城市"),
        ("厦门市"->"2线城市"),
        ("沈阳市"->"2线城市"),
        ("石家庄市"->"2线城市"),
        ("苏州市"->"2线城市"),
        ("太原市"->"2线城市"),
        ("唐山市"->"2线城市"),
        ("天津市"->"2线城市"),
        ("天津市市辖区"->"2线城市"),
        ("温州市"->"2线城市"),
        ("无锡市"->"2线城市"),
        ("武汉市"->"2线城市"),
        ("西安市"->"2线城市"),
        ("烟台市"->"2线城市"),
        ("长春市"->"2线城市"),
        ("长沙市"->"2线城市"),
        ("郑州市"->"2线城市"),
        ("郑州市"->"2线城市"),
        ("重庆市"->"2线城市"),
        ("重庆市市辖区"->"2线城市"),
        ("淄博市"->"2线城市"),
        ("鞍山市"->"3线城市"),
        ("包头市"->"3线城市"),
        ("保定市"->"3线城市"),
        ("沧州市"->"3线城市"),
        ("常德市"->"3线城市"),
        ("常州市"->"3线城市"),
        ("大庆市"->"3线城市"),
        ("德州市"->"3线城市"),
        ("东营市"->"3线城市"),
        ("鄂尔多斯市"->"3线城市"),
        ("贵阳市"->"3线城市"),
        ("海口市"->"3线城市"),
        ("邯郸市"->"3线城市"),
        ("衡阳市"->"3线城市"),
        ("呼和浩特市"->"3线城市"),
        ("淮安市"->"3线城市"),
        ("惠州市"->"3线城市"),
        ("吉林市"->"3线城市"),
        ("济宁市"->"3线城市"),
        ("嘉兴市"->"3线城市"),
        ("江门市"->"3线城市"),
        ("金华市"->"3线城市"),
        ("兰州市"->"3线城市"),
        ("廊坊市"->"3线城市"),
        ("聊城市"->"3线城市"),
        ("临沂市"->"3线城市"),
        ("柳州市"->"3线城市"),
        ("洛阳市"->"3线城市"),
        ("茂名市"->"3线城市"),
        ("南通市"->"3线城市"),
        ("南阳市"->"3线城市"),
        ("泉州市"->"3线城市"),
        ("三亚市"->"3线城市"),
        ("绍兴市"->"3线城市"),
        ("台州市"->"3线城市"),
        ("泰安市"->"3线城市"),
        ("泰州市"->"3线城市"),
        ("威海市"->"3线城市"),
        ("潍坊市"->"3线城市"),
        ("乌鲁木齐市"->"3线城市"),
        ("芜湖市"->"3线城市"),
        ("西宁市"->"3线城市"),
        ("襄阳市"->"3线城市"),
        ("徐州市"->"3线城市"),
        ("盐城市"->"3线城市"),
        ("扬州市"->"3线城市"),
        ("宜昌市"->"3线城市"),
        ("银川市"->"3线城市"),
        ("榆林市"->"3线城市"),
        ("岳阳市"->"3线城市"),
        ("湛江市"->"3线城市"),
        ("漳州市"->"3线城市"),
        ("镇江市"->"3线城市"),
        ("中山市"->"3线城市"),
        ("珠海市"->"3线城市"),
        ("安庆市"->"4线城市"),
        ("安阳市"->"4线城市"),
        ("宝鸡市"->"4线城市"),
        ("本溪市"->"4线城市"),
        ("毕节市"->"4线城市"),
        ("滨州市"->"4线城市"),
        ("朝阳市"->"4线城市"),
        ("郴州市"->"4线城市"),
        ("承德市"->"4线城市"),
        ("赤峰市"->"4线城市"),
        ("滁州市"->"4线城市"),
        ("达州市"->"4线城市"),
        ("丹东市"->"4线城市"),
        ("德阳市"->"4线城市"),
        ("抚顺市"->"4线城市"),
        ("阜阳市"->"4线城市"),
        ("赣州市"->"4线城市"),
        ("桂林市"->"4线城市"),
        ("菏泽市"->"4线城市"),
        ("衡水市"->"4线城市"),
        ("呼伦贝尔市"->"4线城市"),
        ("湖州市"->"4线城市"),
        ("怀化市"->"4线城市"),
        ("黄冈市"->"4线城市"),
        ("黄石市"->"4线城市"),
        ("吉安市"->"4线城市"),
        ("焦作市"->"4线城市"),
        ("揭阳市"->"4线城市"),
        ("锦州市"->"4线城市"),
        ("晋城市"->"4线城市"),
        ("晋中市"->"4线城市"),
        ("荆门市"->"4线城市"),
        ("荆州市"->"4线城市"),
        ("九江市"->"4线城市"),
        ("开封市"->"4线城市"),
        ("拉萨市"->"4线城市"),
        ("乐山市"->"4线城市"),
        ("连云港市"->"4线城市"),
        ("凉山彝族自治州"->"4线城市"),
        ("辽阳市"->"4线城市"),
        ("临汾市"->"4线城市"),
        ("六安市"->"4线城市"),
        ("龙岩市"->"4线城市"),
        ("娄底市"->"4线城市"),
        ("泸州市"->"4线城市"),
        ("吕梁市"->"4线城市"),
        ("马鞍山市"->"4线城市"),
        ("绵阳市"->"4线城市"),
        ("牡丹江市"->"4线城市"),
        ("南充市"->"4线城市"),
        ("南平市"->"4线城市"),
        ("内江市"->"4线城市"),
        ("宁德市"->"4线城市"),
        ("盘锦市"->"4线城市"),
        ("平顶山市"->"4线城市"),
        ("莆田市"->"4线城市"),
        ("濮阳市"->"4线城市"),
        ("齐齐哈尔市"->"4线城市"),
        ("秦皇岛市"->"4线城市"),
        ("清远市"->"4线城市"),
        ("衢州市"->"4线城市"),
        ("曲靖市"->"4线城市"),
        ("日照市"->"4线城市"),
        ("三门峡市"->"4线城市"),
        ("三明市"->"4线城市"),
        ("汕头市"->"4线城市"),
        ("商丘市"->"4线城市"),
        ("上饶市"->"4线城市"),
        ("邵阳市"->"4线城市"),
        ("十堰市"->"4线城市"),
        ("四平市"->"4线城市"),
        ("松原市"->"4线城市"),
        ("绥化市"->"4线城市"),
        ("铁岭市"->"4线城市"),
        ("通辽市"->"4线城市"),
        ("渭南市"->"4线城市"),
        ("咸阳市"->"4线城市"),
        ("湘潭市"->"4线城市"),
        ("孝感市"->"4线城市"),
        ("新乡市"->"4线城市"),
        ("信阳市"->"4线城市"),
        ("邢台市"->"4线城市"),
        ("宿迁市"->"4线城市"),
        ("宿州市"->"4线城市"),
        ("许昌市"->"4线城市"),
        ("延安市"->"4线城市"),
        ("宜宾市"->"4线城市"),
        ("宜春市"->"4线城市"),
        ("益阳市"->"4线城市"),
        ("营口市"->"4线城市"),
        ("永州市"->"4线城市"),
        ("玉林市"->"4线城市"),
        ("玉溪市"->"4线城市"),
        ("运城市"->"4线城市"),
        ("枣庄市"->"4线城市"),
        ("张家口市"->"4线城市"),
        ("长治市"->"4线城市"),
        ("肇庆市"->"4线城市"),
        ("周口市"->"4线城市"),
        ("株洲市"->"4线城市"),
        ("驻马店市"->"4线城市"),
        ("资阳市"->"4线城市"),
        ("遵义市"->"4线城市"),
        ("NULL"->"5线城市及其他"),
        ("unknown"->"5线城市及其他"),
        ("阿坝藏族羌族自治州"->"5线城市及其他"),
        ("阿克苏地区"->"5线城市及其他"),
        ("阿拉尔市"->"5线城市及其他"),
        ("阿拉善盟"->"5线城市及其他"),
        ("阿勒泰地区"->"5线城市及其他"),
        ("阿里地区"->"5线城市及其他"),
        ("安康市"->"5线城市及其他"),
        ("安顺市"->"5线城市及其他"),
        ("巴彦淖尔市"->"5线城市及其他"),
        ("巴音郭楞蒙古自治州"->"5线城市及其他"),
        ("巴中市"->"5线城市及其他"),
        ("白城市"->"5线城市及其他"),
        ("白沙黎族自治县"->"5线城市及其他"),
        ("白山市"->"5线城市及其他"),
        ("白银市"->"5线城市及其他"),
        ("百色市"->"5线城市及其他"),
        ("蚌埠市"->"5线城市及其他"),
        ("保山市"->"5线城市及其他"),
        ("保亭黎族苗族自治县"->"5线城市及其他"),
        ("北海市"->"5线城市及其他"),
        ("亳州市"->"5线城市及其他"),
        ("博尔塔拉蒙古自治州"->"5线城市及其他"),
        ("昌都地区"->"5线城市及其他"),
        ("昌吉回族自治州"->"5线城市及其他"),
        ("昌江黎族自治县"->"5线城市及其他"),
        ("潮州市"->"5线城市及其他"),
        ("澄迈县"->"5线城市及其他"),
        ("池州市"->"5线城市及其他"),
        ("崇左市"->"5线城市及其他"),
        ("楚雄彝族自治州"->"5线城市及其他"),
        ("大理白族自治州"->"5线城市及其他"),
        ("大同市"->"5线城市及其他"),
        ("大兴安岭地区"->"5线城市及其他"),
        ("儋州市"->"5线城市及其他"),
        ("德宏傣族景颇族自治州"->"5线城市及其他"),
        ("迪庆藏族自治州"->"5线城市及其他"),
        ("定安县"->"5线城市及其他"),
        ("定西市"->"5线城市及其他"),
        ("东方市"->"5线城市及其他"),
        ("鄂州市"->"5线城市及其他"),
        ("恩施土家族苗族自治州"->"5线城市及其他"),
        ("防城港市"->"5线城市及其他"),
        ("抚州市"->"5线城市及其他"),
        ("阜新市"->"5线城市及其他"),
        ("甘南藏族自治州"->"5线城市及其他"),
        ("甘孜藏族自治州"->"5线城市及其他"),
        ("固原市"->"5线城市及其他"),
        ("广安市"->"5线城市及其他"),
        ("广元市"->"5线城市及其他"),
        ("贵港市"->"5线城市及其他"),
        ("果洛藏族自治州"->"5线城市及其他"),
        ("哈密地区"->"5线城市及其他"),
        ("海北藏族自治州"->"5线城市及其他"),
        ("海东地区"->"5线城市及其他"),
        ("海东市"->"5线城市及其他"),
        ("海南藏族自治州"->"5线城市及其他"),
        ("海南省直辖县级行政单位"->"5线城市及其他"),
        ("海西蒙古族藏族自治州"->"5线城市及其他"),
        ("汉中市"->"5线城市及其他"),
        ("和田地区"->"5线城市及其他"),
        ("河池市"->"5线城市及其他"),
        ("河南省直辖县级行政单位"->"5线城市及其他"),
        ("河源市"->"5线城市及其他"),
        ("贺州市"->"5线城市及其他"),
        ("鹤壁市"->"5线城市及其他"),
        ("鹤岗市"->"5线城市及其他"),
        ("黑河市"->"5线城市及其他"),
        ("红河哈尼族彝族自治州"->"5线城市及其他"),
        ("葫芦岛市"->"5线城市及其他"),
        ("湖北省直辖县级行政单位"->"5线城市及其他"),
        ("淮北市"->"5线城市及其他"),
        ("淮南市"->"5线城市及其他"),
        ("黄南藏族自治州"->"5线城市及其他"),
        ("黄山市"->"5线城市及其他"),
        ("鸡西市"->"5线城市及其他"),
        ("济源市"->"5线城市及其他"),
        ("佳木斯市"->"5线城市及其他"),
        ("嘉峪关市"->"5线城市及其他"),
        ("金昌市"->"5线城市及其他"),
        ("景德镇市"->"5线城市及其他"),
        ("酒泉市"->"5线城市及其他"),
        ("喀什地区"->"5线城市及其他"),
        ("克拉玛依市"->"5线城市及其他"),
        ("克孜勒苏柯尔克孜自治州"->"5线城市及其他"),
        ("来宾市"->"5线城市及其他"),
        ("莱芜市"->"5线城市及其他"),
        ("乐东黎族自治县"->"5线城市及其他"),
        ("丽江市"->"5线城市及其他"),
        ("丽水市"->"5线城市及其他"),
        ("辽源市"->"5线城市及其他"),
        ("林芝地区"->"5线城市及其他"),
        ("临沧市"->"5线城市及其他"),
        ("临高县"->"5线城市及其他"),
        ("临夏回族自治州"->"5线城市及其他"),
        ("陵水黎族自治县"->"5线城市及其他"),
        ("六盘水市"->"5线城市及其他"),
        ("陇南市"->"5线城市及其他"),
        ("漯河市"->"5线城市及其他"),
        ("眉山市"->"5线城市及其他"),
        ("梅州市"->"5线城市及其他"),
        ("那曲地区"->"5线城市及其他"),
        ("怒江傈僳族自治州"->"5线城市及其他"),
        ("攀枝花市"->"5线城市及其他"),
        ("平凉市"->"5线城市及其他"),
        ("萍乡市"->"5线城市及其他"),
        ("普洱市"->"5线城市及其他"),
        ("七台河市"->"5线城市及其他"),
        ("潜江市"->"5线城市及其他"),
        ("黔东南苗族侗族自治州"->"5线城市及其他"),
        ("黔南布依族苗族自治州"->"5线城市及其他"),
        ("黔西南布依族苗族自治州"->"5线城市及其他"),
        ("钦州市"->"5线城市及其他"),
        ("庆阳市"->"5线城市及其他"),
        ("琼海市"->"5线城市及其他"),
        ("琼中黎族苗族自治县"->"5线城市及其他"),
        ("日喀则地区"->"5线城市及其他"),
        ("山南地区"->"5线城市及其他"),
        ("汕尾市"->"5线城市及其他"),
        ("商洛市"->"5线城市及其他"),
        ("韶关市"->"5线城市及其他"),
        ("神农架林区"->"5线城市及其他"),
        ("石河子市"->"5线城市及其他"),
        ("石嘴山市"->"5线城市及其他"),
        ("双鸭山市"->"5线城市及其他"),
        ("朔州市"->"5线城市及其他"),
        ("随州市"->"5线城市及其他"),
        ("遂宁市"->"5线城市及其他"),
        ("塔城地区"->"5线城市及其他"),
        ("天门市"->"5线城市及其他"),
        ("天水市"->"5线城市及其他"),
        ("通化市"->"5线城市及其他"),
        ("铜川市"->"5线城市及其他"),
        ("铜陵市"->"5线城市及其他"),
        ("铜仁市"->"5线城市及其他"),
        ("图木舒克市"->"5线城市及其他"),
        ("吐鲁番地区"->"5线城市及其他"),
        ("屯昌县"->"5线城市及其他"),
        ("万宁市"->"5线城市及其他"),
        ("文昌市"->"5线城市及其他"),
        ("文山壮族苗族自治州"->"5线城市及其他"),
        ("乌海市"->"5线城市及其他"),
        ("乌兰察布市"->"5线城市及其他"),
        ("吴忠市"->"5线城市及其他"),
        ("梧州市"->"5线城市及其他"),
        ("五家渠市"->"5线城市及其他"),
        ("五指山市"->"5线城市及其他"),
        ("武威市"->"5线城市及其他"),
        ("西双版纳傣族自治州"->"5线城市及其他"),
        ("锡林郭勒盟"->"5线城市及其他"),
        ("仙桃市"->"5线城市及其他"),
        ("咸宁市"->"5线城市及其他"),
        ("湘西土家族苗族自治州"->"5线城市及其他"),
        ("忻州市"->"5线城市及其他"),
        ("新疆维吾尔自治区直辖县级行政单位"->"5线城市及其他"),
        ("新余市"->"5线城市及其他"),
        ("兴安盟"->"5线城市及其他"),
        ("宣城市"->"5线城市及其他"),
        ("雅安市"->"5线城市及其他"),
        ("延边朝鲜族自治州"->"5线城市及其他"),
        ("阳江市"->"5线城市及其他"),
        ("阳泉市"->"5线城市及其他"),
        ("伊春市"->"5线城市及其他"),
        ("伊犁哈萨克自治州"->"5线城市及其他"),
        ("鹰潭市"->"5线城市及其他"),
        ("玉树藏族自治州"->"5线城市及其他"),
        ("云浮市"->"5线城市及其他"),
        ("张家界市"->"5线城市及其他"),
        ("张掖市"->"5线城市及其他"),
        ("昭通市"->"5线城市及其他"),
        ("中卫市"->"5线城市及其他"),
        ("舟山市"->"5线城市及其他"),
        ("自贡市"->"5线城市及其他"),
        ("其他"->"5线城市及其他"),
        ("未匹配"->"5线城市及其他")
    )

    /**
      * 根据城市获取城市级别
      * @param code
      * @return
      */
    def getCl(code: String) : String = {
        cityLevel.get(code).getOrElse("5线城市及其他")
    }
}

object chareatest {
    def main(args: Array[String]): Unit = {
        println(Codearea.getArea("四川省"))
    }
}package com.avcdata.etl.common.compiler

import scala.tools.nsc.{Global, Settings}
import scala.reflect.internal.util.BatchSourceFile
import tools.nsc.io.{AbstractFile, VirtualDirectory}
import tools.nsc.interpreter.AbstractFileClassLoader
import java.security.MessageDigest
import java.math.BigInteger
import java.io.File

import scala.collection.mutable

/**
  * Scala编译器
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/30 10:06
  */
class Compiler(targetDir: Option[File])
{
  val target = targetDir match
  {
    case Some(dir) => AbstractFile.getDirectory(dir)
    case None => new VirtualDirectory("(memory)", None)
  }

  val classCache = mutable.Map[String, Class[_]]()

  private val settings = new Settings()
  settings.deprecation.value = true // enable detailed deprecation warnings
  settings.unchecked.value = true // enable detailed unchecked warnings
  settings.outputDirs.setSingleOutput(target)
  settings.usejavacp.value = true

  private val global = new Global(settings)
  private lazy val run = new global.Run

  val classLoader = new AbstractFileClassLoader(target, this.getClass.getClassLoader)

  /** Compiles the code as a class into the class loader of this compiler.
    */
  def compile(code: String) =
  {
    val className = classNameForCode(code)
    findClass(className).getOrElse
    {
      val sourceFiles = List(new BatchSourceFile("(inline)", wrapCodeInClass(className, code)))
      run.compileSources(sourceFiles)
      findClass(className).get
    }
  }

  /** Compiles the source string into the class loader and
    * evaluates it.
    */
  def eval[T](code: String): T =
  {
    val cls = compile(code)
    cls.getConstructor().newInstance().asInstanceOf[() => Any].apply().asInstanceOf[T]
  }

  def findClass(className: String): Option[Class[_]] =
  {
    synchronized
    {
      classCache.get(className).orElse
      {
        try
        {
          val cls = classLoader.loadClass(className)
          classCache(className) = cls
          Some(cls)
        }
        catch
        {
          case e: ClassNotFoundException => None
        }
      }
    }
  }

  protected def classNameForCode(code: String): String =
  {
    val digest = MessageDigest.getInstance("SHA-1").digest(code.getBytes)
    "sha" + new BigInteger(1, digest).toString(16)
  }

  /*
  * Wrap source code in a new class with an apply method.
  */
  private def wrapCodeInClass(className: String, code: String) =
  {
    "class " + className + " extends (() => Any) {\n" +
      "  def apply() = {\n" +
      code + "\n" +
      "  }\n" +
      "}\n"
  }
}package com.avcdata.etl.common.compiler

import scala.tools.nsc.{Global, Settings}
import scala.reflect.internal.util.BatchSourceFile
import tools.nsc.io.{AbstractFile, VirtualDirectory}
import tools.nsc.interpreter.AbstractFileClassLoader
import java.security.MessageDigest
import java.math.BigInteger
import java.io.File

import scala.collection.mutable

/**
  * Scala编译器
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/30 10:06
  */
class Compiler(targetDir: Option[File])
{
  val target = targetDir match
  {
    case Some(dir) => AbstractFile.getDirectory(dir)
    case None => new VirtualDirectory("(memory)", None)
  }

  val classCache = mutable.Map[String, Class[_]]()

  private val settings = new Settings()
  settings.deprecation.value = true // enable detailed deprecation warnings
  settings.unchecked.value = true // enable detailed unchecked warnings
  settings.outputDirs.setSingleOutput(target)
  settings.usejavacp.value = true

  private val global = new Global(settings)
  private lazy val run = new global.Run

  val classLoader = new AbstractFileClassLoader(target, this.getClass.getClassLoader)

  /** Compiles the code as a class into the class loader of this compiler.
    */
  def compile(code: String) =
  {
    val className = classNameForCode(code)
    findClass(className).getOrElse
    {
      val sourceFiles = List(new BatchSourceFile("(inline)", wrapCodeInClass(className, code)))
      run.compileSources(sourceFiles)
      findClass(className).get
    }
  }

  /** Compiles the source string into the class loader and
    * evaluates it.
    */
  def eval[T](code: String): T =
  {
    val cls = compile(code)
    cls.getConstructor().newInstance().asInstanceOf[() => Any].apply().asInstanceOf[T]
  }

  def findClass(className: String): Option[Class[_]] =
  {
    synchronized
    {
      classCache.get(className).orElse
      {
        try
        {
          val cls = classLoader.loadClass(className)
          classCache(className) = cls
          Some(cls)
        }
        catch
        {
          case e: ClassNotFoundException => None
        }
      }
    }
  }

  protected def classNameForCode(code: String): String =
  {
    val digest = MessageDigest.getInstance("SHA-1").digest(code.getBytes)
    "sha" + new BigInteger(1, digest).toString(16)
  }

  /*
  * Wrap source code in a new class with an apply method.
  */
  private def wrapCodeInClass(className: String, code: String) =
  {
    "class " + className + " extends (() => Any) {\n" +
      "  def apply() = {\n" +
      code + "\n" +
      "  }\n" +
      "}\n"
  }
}package com.avcdata.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;

/**
 * Created by dev on 15-12-19.
 */
@Data
@ConfigurationProperties(prefix="avc")
public class ConfigInfo {


    private String driver;
    private String url;
    private String username;
    private String password;


    private String redishost;
    private String redisport;

}
package com.avcdata.spark.job.util

import scala.collection.mutable

/**
  * Created by avc on 2016/12/23.
  */
object Constant {
  val luomaNumMap = mutable.Map(
    "Ⅰ" -> "1",
    "Ⅱ" -> "2",
    "Ⅲ" -> "3",
    "Ⅳ" -> "4",
    "Ⅴ" -> "5",
    "Ⅵ" -> "6",
    "Ⅶ" -> "7",
    "Ⅷ" -> "8",
    "Ⅸ" -> "9",
    "Ⅹ" -> "10",
    "Ⅺ" -> "11",
    "Ⅻ" -> "12"
  )
  //"XIII","XIV","XV","XVI","XVII","XVIII","XIX","XX"

  val zhNumMap = mutable.Map(
    "一" -> "1",
    "二" -> "2",
    "三" -> "3",
    "四" -> "4",
    "五" -> "5",
    "六" -> "6",
    "七" -> "7",
    "八" -> "8",
    "九" -> "9",
    "十" -> "10",
    "十一" -> "11",
    "十三" -> "12",
    "十四" -> "12",
    "十五" -> "12",
    "十六" -> "12",
    "十七" -> "12",
    "十八" -> "12",
    "十九" -> "12",
    "二十" -> "12",
    "二十一" -> "12",
    "二十二" -> "22",
    "二十三" -> "23",
    "二十四" -> "24",
    "二十五" -> "25",
    "二十六" -> "26"
  )



}
package com.avcdata.spark.job.util

import scala.collection.mutable

/**
  * Created by avc on 2016/12/23.
  */
object Constant {
  val luomaNumMap = mutable.Map(
    "Ⅰ" -> "1",
    "Ⅱ" -> "2",
    "Ⅲ" -> "3",
    "Ⅳ" -> "4",
    "Ⅴ" -> "5",
    "Ⅵ" -> "6",
    "Ⅶ" -> "7",
    "Ⅷ" -> "8",
    "Ⅸ" -> "9",
    "Ⅹ" -> "10",
    "Ⅺ" -> "11",
    "Ⅻ" -> "12"
  )
  //"XIII","XIV","XV","XVI","XVII","XVIII","XIX","XX"

  val zhNumMap = mutable.Map(
    "一" -> "1",
    "二" -> "2",
    "三" -> "3",
    "四" -> "4",
    "五" -> "5",
    "六" -> "6",
    "七" -> "7",
    "八" -> "8",
    "九" -> "9",
    "十" -> "10",
    "十一" -> "11",
    "十三" -> "12",
    "十四" -> "12",
    "十五" -> "12",
    "十六" -> "12",
    "十七" -> "12",
    "十八" -> "12",
    "十九" -> "12",
    "二十" -> "12",
    "二十一" -> "12",
    "二十二" -> "22",
    "二十三" -> "23",
    "二十四" -> "24",
    "二十五" -> "25",
    "二十六" -> "26"
  )



}
package com.avcdata.vbox.util

import scala.collection.mutable
import scala.util.matching.Regex

object Constant {
  val luomaNumMap = mutable.Map(
    "Ⅰ" -> "1",
    "Ⅱ" -> "2",
    "Ⅲ" -> "3",
    "Ⅳ" -> "4",
    "Ⅴ" -> "5",
    "Ⅵ" -> "6",
    "Ⅶ" -> "7",
    "Ⅷ" -> "8",
    "Ⅸ" -> "9",
    "Ⅹ" -> "10",
    "Ⅺ" -> "11",
    "Ⅻ" -> "12"
  )
  //"XIII","XIV","XV","XVI","XVII","XVIII","XIX","XX"

  val zhNumMap = mutable.Map(
    "一" -> "1",
    "二" -> "2",
    "三" -> "3",
    "四" -> "4",
    "五" -> "5",
    "六" -> "6",
    "七" -> "7",
    "八" -> "8",
    "九" -> "9",
    "十" -> "10",
    "十一" -> "11",
    "十三" -> "12",
    "十四" -> "12",
    "十五" -> "12",
    "十六" -> "12",
    "十七" -> "12",
    "十八" -> "12",
    "十九" -> "12",
    "二十" -> "12",
    "二十一" -> "12",
    "二十二" -> "22",
    "二十三" -> "23",
    "二十四" -> "24",
    "二十五" -> "25",
    "二十六" -> "26"
  )


  val versionArr = Array[String](
    "未删减版", "[未删减版]", "完整版", "全集", "完全版", "[TV版]", "TV版", "tv版", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "(1080P)", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "美版", "（美版）", "[DVD版]", "-TV版", "（四川版）", "(环绕声版)", "独家精华版", "日文版", "周播版", "（北美版）", "（东北版）", "英文版", "(英文版)", "潮汕版", "精编版纪念版", "合集版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "春节贺岁版", "周末版", "周间版", "日播版", "典藏版", "教学版", "独家未播版", "VR", "特别版", "重制版", "国际版", "独家抢鲜版", "影院版", "免费版", "短剧版", "搜狐版", "幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "新编集版", "标准版", "原声版", "（午间版）", "数字集版"
  )

  val partWord = Array[String](
    "数字集版", "第.*集", "\\s*第.*集", "-第.*集", "(第.*集)", "_\\d*", "大结局", "先导集", "先导集上", "先导集下", "上集", "下集", "第.*期", "空格第.*期", "-第.*期", "(第.*期)"
  )



}
package com.avc.spark.mllib

import org.apache.spark.mllib.linalg.{Vector, Vectors}

/**
  * Created by Administrator on 2017/5/19.
  */
object CosineSimilarity {

  case class newVector(point: Vector) {

    def *(a: Double): newVector = {
      var res = new Array[Double](point.size)
      for (i <- 0 until point.size) {
        res(i) = a * point.toArray.apply(i)
      }
      newVector(Vectors.dense(res))
    }

    // 计算两个向量的余弦距离
    def distance(vector1: Vector, vector2: Vector): Double = {
      var dot = 0.0
      for (i <- 0 until vector1.size) {
        val temp = vector1.apply(i) * vector2.apply(i)
        dot = dot + temp
      }
      val v1 = Vectors.norm(vector1.toSparse, 2.0)
      val v2 = Vectors.norm(vector2.toSparse, 2.0)
      val similarity = dot / (v1 * v2)
      return similarity
    }
  }

}
package com.avcdata.vbox.launcher

import com.avcdata.vbox.clean.apk._
import com.avcdata.vbox.clean.coocaa.{DataCleanCCTerminal, DataCleanKOTerminal}
import com.avcdata.vbox.clean.epg.DataCleanEpg
import com.avcdata.vbox.clean.live.{DataCleanCHLive, DataCleanKOLive, LiveData2Partition}
import com.avcdata.vbox.clean.oc._
import com.avcdata.vbox.clean.play.DataCleanCHPlay
import com.avcdata.vbox.clean.terminal.{DataCleanTCLTerminal, DataCleanCHTerminal}
import com.avcdata.vbox.common.Helper
import org.apache.log4j.Logger

object DailyExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    val sc = Helper.sparkContext

    println("===============?????????????????????=======================")

    ///////////////////////////////////epg////////////////////////////
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@DataCleanEpg start...")
      DataCleanEpg.run(sc, analysisDate)
      println(analysisDate + "@DataCleanEpg end....")
    }

    //////////////////////////////////终端数////////////////////////////
    //TODO 酷开终端信息清洗
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@DataCleanCCTerminal start....")
      DataCleanCCTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCCTerminal end....")
    }

    //TODO 长虹终端信息清洗
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@DataCleanCHTerminal start....")
      DataCleanCHTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCHTerminal end....")
    }

    //TODO 康佳终端信息清洗
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@DataCleanKOTerminal start....")
      DataCleanKOTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanKOTerminal end....")
    }




    ////////////////////////搜索指数/////////////////////////////////////////
//    //TODO 爬虫搜索指数清洗
//    if (executePart.charAt(5) == '1') {
//      println(analysisDate + "@DataCleanPlaySearchIndex start....")
//      DataCleanPlaySearchIndex.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanPlaySearchIndex end....")
//    }
//
//
//    //////////////////////////开关机////////////////////////////////////
//


    // TODO 康佳开关机清洗
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@DataCleanKOPowerOn start....")
      DataCleanKOPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOPowerOn end....")
    }

    //TODO TCL开关机日志清洗
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@DataCleanTCLPowerOn start....")
      DataCleanTCLPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLPowerOn end....")
    }


    // TODO 长虹开关机清洗
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@DataCleanCHPowerOn start....")
      DataCleanCHPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPowerOn end....")
    }

//    ///////////////////////////直播///////////////////////////////////

    // TODO 康佳直播清洗
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@DataCleanKOLive start....")
      DataCleanKOLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOLive end....")
    }

    // TODO 长虹直播清洗-8月28日
//    if (executePart.charAt(10) == '1') {
//      println(analysisDate + "@DataCleanCHLiveOld start....")
//      DataCleanCHLiveOld.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCHLiveOld end....")
//    }
    //8月28日-今
    if (executePart.charAt(10) == '1') {
      println(analysisDate + "@DataCleanCHLive start....")
      DataCleanCHLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHLive end....")
    }

//
//    ///////////////////////////apk///////////////////////////////////////

    //TODO 康佳apk清洗
    if (executePart.charAt(11) == '1') {
      println(analysisDate + "@DataCleanKOApk start....")
      DataCleanKOApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOApk end....")
    }

    // TODO 长虹apk清洗
    if (executePart.charAt(12) == '1') {
      println(analysisDate + "@DataCleanCHApk start....")
      DataCleanCHApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHApk end....")
    }


    //TODO TCL APk日志清洗
    if (executePart.charAt(13) == '1') {
      println(analysisDate + "@DataCleanTCLApk start....")
      DataCleanTCLApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLApk end....")
    }


    // TODO 酷开apk清洗
    if (executePart.charAt(14) == '1') {
      println(analysisDate + "@DataCleanCCApk start....")
      DataCleanCCApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCCApk end....")
    }


    ////////////////////////导入分区表////////////////////////////////////

    if(1==1){
      //TODO 开关机（非TCL)导入分区表
      if (executePart.charAt(15) == '1') {
        println(analysisDate + "@OCData2Partition start....")
        OCData2Partition.run(sc, analysisDate);
        println(analysisDate + "@OCData2Partition end....")
      }

      //TODO apk（非TCL)导入分区表
      if (executePart.charAt(17) == '1') {
        println(analysisDate + "@ApkData2Partition start....")
        ApkData2Partition.run(sc, analysisDate);
        println(analysisDate + "@ApkData2Partition end....")
      }

      //TODO 直播（非TCL)导入分区表
      if (executePart.charAt(19) == '1') {
        println(analysisDate + "@LiveData2Partition start....")
        LiveData2Partition.run(sc, analysisDate);
        println(analysisDate + "@LiveData2Partition end....")
      }
    }


    //////////////////////////剧集//////////////////////

    //TODO 长虹到剧清洗
    if (executePart.charAt(20) == '1') {
      println(analysisDate + "@DataCleanCHPlay start....")
      DataCleanCHPlay.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPlay end....")
    }


    //TODO 酷开到剧清洗
//    if (executePart.charAt(21) == '1') {
//      println(analysisDate + "@DataCleanCCPlay start....")
//      DataCleanCCPlay.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCCPlay end....")
//    }


    //TODO TCL终端信息清洗
//    if (executePart.charAt(4) == '1') {
//      println(analysisDate + "@DataCleanTCLTerminal start....")
//      DataCleanTCLTerminal.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanTCLTerminal end....")
//    }




    ///////////////////////////////////////////////////
    //TODO 计算月沉默终端数
//    if (executePart.charAt(22) == '1') {
//      println(analysisDate + "@SilentTerminal start....")
//      SilentTerminal.run(sc, analysisDate);
//      println(analysisDate + "@SilentTerminal end....")
//    }
//
//    //TODO 月沉默终端数推总
//    if (executePart.charAt(23) == '1') {
//      println(analysisDate + "@SilentTerminalTotal start....")
//      SilentTerminalTotal.run(sc, analysisDate);
//      println(analysisDate + "@SilentTerminalTotal end....")
//    }





   sc.stop()
  }

}
package com.avcdata.vbox.launcher

import com.avcdata.vbox.clean.apk._
import com.avcdata.vbox.clean.coocaa.{DataCleanCCTerminal, DataCleanKOTerminal}
import com.avcdata.vbox.clean.epg.DataCleanEpg
import com.avcdata.vbox.clean.live.{DataCleanCHLive, DataCleanKOLive, LiveData2Partition}
import com.avcdata.vbox.clean.oc._
import com.avcdata.vbox.clean.play.DataCleanCHPlay
import com.avcdata.vbox.clean.terminal.DataCleanCHTerminal
import com.avcdata.vbox.common.Helper
import org.apache.log4j.Logger

object DailyExecutor20170918 {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    val sc = Helper.sparkContext

    println("===============?????????????????????=======================")

    ///////////////////////////////////epg////////////////////////////
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@DataCleanEpg start...")
      DataCleanEpg.run(sc, analysisDate)
      println(analysisDate + "@DataCleanEpg end....")
    }

    //////////////////////////////////终端数////////////////////////////
    //TODO 酷开终端信息清洗
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@DataCleanCCTerminal start....")
      DataCleanCCTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCCTerminal end....")
    }

    //TODO 长虹终端信息清洗
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@DataCleanCHTerminal start....")
      DataCleanCHTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCHTerminal end....")
    }

    //TODO 康佳终端信息清洗
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@DataCleanKOTerminal start....")
      DataCleanKOTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanKOTerminal end....")
    }




    ////////////////////////搜索指数/////////////////////////////////////////
//    //TODO 爬虫搜索指数清洗
//    if (executePart.charAt(5) == '1') {
//      println(analysisDate + "@DataCleanPlaySearchIndex start....")
//      DataCleanPlaySearchIndex.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanPlaySearchIndex end....")
//    }
//
//
//    //////////////////////////开关机////////////////////////////////////
//


    // TODO 康佳开关机清洗
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@DataCleanKOPowerOn start....")
      DataCleanKOPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOPowerOn end....")
    }

    //TODO TCL开关机日志清洗
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@DataCleanTCLPowerOn start....")
      DataCleanTCLPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLPowerOn end....")
    }


    // TODO 长虹开关机清洗
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@DataCleanCHPowerOn start....")
      DataCleanCHPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPowerOn end....")
    }

//    ///////////////////////////直播///////////////////////////////////

    // TODO 康佳直播清洗
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@DataCleanKOLive start....")
      DataCleanKOLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOLive end....")
    }

    // TODO 长虹直播清洗-8月28日
//    if (executePart.charAt(10) == '1') {
//      println(analysisDate + "@DataCleanCHLiveOld start....")
//      DataCleanCHLiveOld.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCHLiveOld end....")
//    }
    //8月28日-今
    if (executePart.charAt(10) == '1') {
      println(analysisDate + "@DataCleanCHLive start....")
      DataCleanCHLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHLive end....")
    }

//
//    ///////////////////////////apk///////////////////////////////////////

    //TODO 康佳apk清洗
    if (executePart.charAt(11) == '1') {
      println(analysisDate + "@DataCleanKOApk start....")
      DataCleanKOApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOApk end....")
    }

    // TODO 长虹apk清洗
    if (executePart.charAt(12) == '1') {
      println(analysisDate + "@DataCleanCHApk start....")
      DataCleanCHApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHApk end....")
    }


    //TODO TCL APk日志清洗
    if (executePart.charAt(13) == '1') {
      println(analysisDate + "@DataCleanTCLApk start....")
      DataCleanTCLApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLApk end....")
    }


    // TODO 酷开apk清洗
    if (executePart.charAt(14) == '1') {
      println(analysisDate + "@DataCleanCCApk start....")
      DataCleanCCApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCCApk end....")
    }


    ////////////////////////导入分区表////////////////////////////////////

    if(1==1){
      //TODO 开关机（非TCL)导入分区表
      if (executePart.charAt(15) == '1') {
        println(analysisDate + "@OCData2Partition start....")
        OCData2Partition.run(sc, analysisDate);
        println(analysisDate + "@OCData2Partition end....")
      }

      //TODO 开关机（TCL)导入分区表
      if (executePart.charAt(16) == '1') {
        println(analysisDate + "@OCData2PartitionTCL start....")
        OCData2PartitionTCL.run(sc, analysisDate);
        println(analysisDate + "@OCData2PartitionTCL end....")
      }


      //TODO apk（非TCL)导入分区表
      if (executePart.charAt(17) == '1') {
        println(analysisDate + "@ApkData2Partition start....")
        ApkData2Partition.run(sc, analysisDate);
        println(analysisDate + "@ApkData2Partition end....")
      }
      //TODO apk（TCL)导入分区表
      if (executePart.charAt(18) == '1') {
        println(analysisDate + "@ApkData2PartitionTCL start....")
        ApkData2PartitionTCL.run(sc, analysisDate);
        println(analysisDate + "@ApkData2PartitionTCL end....")
      }

      //TODO 直播（非TCL)导入分区表
      if (executePart.charAt(19) == '1') {
        println(analysisDate + "@LiveData2Partition start....")
        LiveData2Partition.run(sc, analysisDate);
        println(analysisDate + "@LiveData2Partition end....")
      }
    }


    //////////////////////////剧集//////////////////////

    //TODO 长虹到剧清洗
    if (executePart.charAt(20) == '1') {
      println(analysisDate + "@DataCleanCHPlay start....")
      DataCleanCHPlay.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPlay end....")
    }


    //TODO 酷开到剧清洗
//    if (executePart.charAt(21) == '1') {
//      println(analysisDate + "@DataCleanCCPlay start....")
//      DataCleanCCPlay.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCCPlay end....")
//    }


    //TODO TCL终端信息清洗
//    if (executePart.charAt(4) == '1') {
//      println(analysisDate + "@DataCleanTCLTerminal start....")
//      DataCleanTCLTerminal.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanTCLTerminal end....")
//    }




    ///////////////////////////////////////////////////
    //TODO 计算月沉默终端数
//    if (executePart.charAt(22) == '1') {
//      println(analysisDate + "@SilentTerminal start....")
//      SilentTerminal.run(sc, analysisDate);
//      println(analysisDate + "@SilentTerminal end....")
//    }
//
//    //TODO 月沉默终端数推总
//    if (executePart.charAt(23) == '1') {
//      println(analysisDate + "@SilentTerminalTotal start....")
//      SilentTerminalTotal.run(sc, analysisDate);
//      println(analysisDate + "@SilentTerminalTotal end....")
//    }





   sc.stop()
  }

}
package com.avcdata.vbox.tmp

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  *字段及要求如表格所示：
  * 品牌：长虹 、酷开，其中长虹以新的清洗规则（与长虹核对后的规则）数据进行计算，酷开以原数据（目前使用的）计算
  * 省：31省
  * 日期：日度数据计算2017年7月24日-7月30日每天的数据，周度数据计算7月17-23日、7月24-7月30日两周的数据，月度数据计算7月
  * 9家应用：见列表
  *

  * @author zhangyongtian
@define 长虹apk新清洗规则统计
  */

object DataCleanAll9ApkCnt {

//  case class DataCleanCHApkCntResult(
//                                date: String,
//                                dateType: String,
//                                province: String,
//                                appname: String,
//                                acnt: Long,
//                                tcnt: Double,
//                                ucnt: Long
//                              )

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    val hiveContext = new HiveContext(sc)

//    val apkArr = Array[String](
//      "银河·奇异果", "腾讯视频TV端", "CIBN环球影视", "芒果TV", "CIBN微视听", "云视听·泰捷", "CIBN聚体育", "CIBN聚精彩", "CIBN悦厅TV"
//    )

    //用到的表
    //    tracker_apk_fact_ch
    //    key	dim_sn	dim_apk	dim_date	dim_hour	fact_cnt	fact_duration
    //
    //    sample_terminal_three
    //    key	sn	brand	last_poweron	area	province	city	citylevel	size	model	license

    //TODO 日-分省
    val chDailyDF = hiveContext.sql(
      """
        select
         t.brand,
         t.license,
          t.province as province,
        	afc.dim_date as date,
        	ai.appname as appname,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
         where dim_date between '2017-07-24' and '2017-07-30' and ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by  t.brand,t.license,t.province,afc.dim_date,ai.appname
      """.stripMargin)


    val ccOrKoDailyDF =  hiveContext.sql(
      """
        select
         t.brand,
         t.license,
          t.province as province,
        	afc.date as date,
        	ai.appname as appname,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_partition afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
         where date between '2017-07-24' and '2017-07-30' and  (afc.key like '%KO%' or  afc.key like '%CC%')  and ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by  t.brand,t.license,t.province,afc.date,ai.appname
      """.stripMargin)



    /////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 周-分省
    val chWeeklyDF = hiveContext.sql(
      """
        select
        	  t.brand,
            t.license,
            t.province as province,
        	weekofyear(afc.dim_date) as date,
        	ai.appname as appname,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where dim_date between '2017-07-17' and '2017-07-30' and ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,weekofyear(afc.dim_date),ai.appname
      """.stripMargin)

    val ccOrKoWeeklyDF =  hiveContext.sql(
      """
        select
        	  t.brand,
            t.license,
            t.province as province,
        	weekofyear(afc.date) as date,
        	ai.appname as appname,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_partition afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where dim_date between '2017-07-17' and '2017-07-30' and  (afc.key like '%KO%' or  afc.key like '%CC%') and ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,weekofyear(afc.date),ai.appname
      """.stripMargin)



    ////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 月-分省
    val chMonthlyDF = hiveContext.sql(
      """
        select
          t.brand,
          t.license,
          t.province as province,
        	month(afc.dim_date) as date,
        	ai.appname as appname,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,month(afc.dim_date),ai.appname
      """.stripMargin)



    val ccOrKoMonthlyDF  = hiveContext.sql(
      """
        select
          t.brand,
          t.license,
          t.province as province,
        	month(afc.date) as date,
        	ai.appname as appname,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_partition afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where date like '2017-07%'  and  (afc.key like '%KO%' or  afc.key like '%CC%') and ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by  t.brand,t.license,t.province,month(afc.date),ai.appname
      """.stripMargin)


    val allDF = chDailyDF.unionAll(ccOrKoDailyDF)
      .unionAll(chWeeklyDF).unionAll(ccOrKoWeeklyDF)
      .unionAll(chMonthlyDF).unionAll(ccOrKoMonthlyDF)

    //      .rdd.saveAsTextFile("/tmp/DataCleanCHApkCnt")

    JdbcUtils.writeDF2Mysql(sc, allDF, Helper.mysqlConf, "vboxDB", "DataCleanAll9ApkCnt", true,
      SaveMode.Append)

    ///////////////////////////////end of methond/////////////////////////////////////////
  }


}
package com.avcdata.vbox.tmp

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  * 字段及要求如表格所示：
  * 品牌：长虹 、酷开，其中长虹以新的清洗规则（与长虹核对后的规则）数据进行计算，酷开以原数据（目前使用的）计算
  * 省：31省
  * 日期：日度数据计算2017年7月24日-7月30日每天的数据，周度数据计算7月17-23日、7月24-7月30日两周的数据，月度数据计算7月
  * 将视频应用作为一个整体，统计以下各指标
  *

  * @author zhangyongtian
@define apk新清洗规则数据支持统计
  */

object DataCleanAllApkOTTCnt {

//  case class DataCleanCHApkCntResult(
//                                date: String,
//                                dateType: String,
//                                province: String,
//                                appname: String,
//                                acnt: Long,
//                                tcnt: Double,
//                                ucnt: Long
//                              )

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    val hiveContext = new HiveContext(sc)

//    val apkArr = Array[String](
//      "银河·奇异果", "云视听极光", "CIBN环球影视", "芒果TV", "CIBN微视听", "云视听·泰捷", "CIBN聚体育", "CIBN聚精彩", "CIBN悦厅TV"
//    )

    //用到的表
    //    tracker_apk_fact_ch
    //    key	dim_sn	dim_apk	dim_date	dim_hour	fact_cnt	fact_duration
    //
    //    sample_terminal_three
    //    key	sn	brand	last_poweron	area	province	city	citylevel	size	model	license


//    val all_acnt = all_acnt_arr(0).getLong(0).toString

    //TODO 日
    val chDailyDF = hiveContext.sql(
      s"""
        select
          t.brand,
          t.license,
        	t.province as province,
        	afc.dim_date as date,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where dim_date between '2017-07-24' and '2017-07-30' and ai.onelevel = '视频' and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,afc.dim_date
      """.stripMargin)
    /////////////////////////////////////////////////////////////////////////////////////////////


    val ccOrKoDailyDF = hiveContext.sql(
      s"""
        select
          t.brand,
          t.license,
        	t.province as province,
        	afc.date as date,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_partition afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where date between '2017-07-24' and '2017-07-30' and  (afc.key like '%KO%' or  afc.key like '%CC%') and ai.onelevel = '视频' and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,afc.date
      """.stripMargin)





    //TODO 周
    val chWeeklyDF = hiveContext.sql(
      s"""
         select
            t.brand,
           t.license,
           t.province as province,
        	weekofyear(afc.dim_date) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where dim_date between '2017-07-17' and '2017-07-30' and  ai.onelevel = '视频' and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,weekofyear(afc.dim_date)
      """.stripMargin)

    val ccOrKoWeeklyDF = hiveContext.sql(
      s"""
         select
            t.brand,
           t.license,
           t.province as province,
        	weekofyear(afc.dim_date) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_partition afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where dim_date between '2017-07-17' and '2017-07-30' and  (afc.key like '%KO%' or  afc.key like '%CC%') and  ai.onelevel = '视频' and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,weekofyear(afc.dim_date)
      """.stripMargin)


    ////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 月-分省
    val chMonthlyDF = hiveContext.sql(
      s"""
         select
           t.brand,
           t.license,
           t.province as province,
        	month(afc.dim_date) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
          where  ai.onelevel = '视频'
          and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,month(afc.dim_date)
      """.stripMargin)

    val ccOrKoMonthlyDF = hiveContext.sql(
      s"""
         select
           t.brand,
           t.license,
           t.province as province,
        	month(afc.dim_date) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_partition afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
          where date like '2017-07%'  and  (afc.key like '%KO%' or  afc.key like '%CC%') and  ai.onelevel = '视频'
          and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.license,t.province,month(afc.dim_date)
      """.stripMargin)





    val allDF = chDailyDF.unionAll(ccOrKoDailyDF)
      .unionAll(chWeeklyDF).unionAll(ccOrKoWeeklyDF)
      .unionAll(chMonthlyDF).unionAll(ccOrKoMonthlyDF)

//      .rdd.saveAsTextFile("/tmp/DataCleanCHApkCnt")

    JdbcUtils.writeDF2Mysql(sc, allDF, Helper.mysqlConf, "vboxDB", "DataCleanAllApkOTTCnt", true,
      SaveMode.Append)




    ///////////////////////////////end of methond/////////////////////////////////////////
  }


}
package com.avcdata.vbox.tmp

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  *
  * 字段及要求如表格所示：
  * 品牌：长虹 、康佳，其中长虹以新的清洗规则（与长虹核对后的规则）数据进行计算，康佳以原数据（目前使用的）计算
  * 省：31省
  * 日期：日度数据计算2017年7月24日-7月30日每天的数据，周度数据计算7月17-23日、7月24-7月30日两周的数据，月度数据计算7月
  *

  * @author zhangyongtian
  * @define 开关机新清洗规则统计
  */
object DataCleanAllPowerOnCnt {

  def main(args: Array[String]) {

    val all_acnt = 100

    //    val sql =
    //      s"""
    //        select
    //          ''$all_acnt'' as all_acnt,
    //          '智能电视开机' as t_name,
    //        	t.province as province,
    //        	afc.power_on_day as date,
    //        	'daily' as dateType,
    //        	count(distinct t.sn) as acnt,
    //        	sum(afc.power_on_length)/60 as tcnt,
    //        	sum(afc.cnt) as ucnt
    //        from hr.sample_terminal_three t
    //        join hr.tracker_oc_fact_ch afc
    //        on
    //        (t.sn=afc.sn)
    //        where t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
    //        group by t.province,afc.power_on_day
    //      """.stripMargin
    //
    //    println(sql)

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("DataCleanCHPowerOnForCH")
    //    //      .set("spark.driver.allowMultipleContexts", "true");
    //    val sc = new SparkContext(conf)
    //
    //
    //
    //    run(sc, "2017-07-02")
    //
    //
    //    sc.stop()


  }

  def run(sc: SparkContext, analysisDate: String) = {
    val hiveContext = new HiveContext(sc)

    //    val apkArr = Array[String](
    //      "银河·奇异果", "云视听极光", "CIBN环球影视", "芒果TV", "CIBN微视听", "云视听·泰捷", "CIBN聚体育", "CIBN聚精彩", "CIBN悦厅TV"
    //    )

    //用到的表
    //    tracker_oc_fact_ch
    //    	afc.key	sn	power_on_day	power_on_time	power_on_length	cnt
    //
    //    sample_terminal_three
    //    afc.key	sn	brand	last_poweron	area	province	city	citylevel	size	model	license


    //    select province,count(distinct sn) from hr.sample_terminal_three where afc.key like '%CH%' group by province


    //TODO 日-分省
    val chDailyDF = hiveContext.sql(
      s"""
       select
         t.brand,
          t.province as province,
          afc.power_on_day as date,
          'daily' as dateType,
          count(distinct t.sn) as acnt,
          sum(afc.power_on_length)/60 as tcnt,
          sum(afc.cnt) as ucnt
         from hr.sample_terminal_three t
         join hr.tracker_oc_fact_ch afc
         on
         (t.sn=afc.sn)
         where power_on_day between '2017-07-24' and '2017-07-30' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
         group by t.brand,t.province,afc.power_on_day
      """.stripMargin)

    val ccDailyDF = hiveContext.sql(
      s"""
       select
         t.brand,
          t.province as province,
          afc.date as date,
          'daily' as dateType,
          count(distinct t.sn) as acnt,
          sum(afc.power_on_length)/60 as tcnt,
          sum(afc.cnt) as ucnt
         from hr.sample_terminal_three t
         join hr.tracker_oc_fact_partition afc
         on
         (t.sn=afc.sn)
         where date between '2017-07-24' and '2017-07-30' and  afc.key like '%CC%' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
         group by t.brand,t.province,afc.date
      """.stripMargin)


    val koDailyDF = hiveContext.sql(
      s"""
       select
          t.brand,
          t.province as province,
          afc.date as date,
          'daily' as dateType,
          count(distinct t.sn) as acnt,
          sum(afc.power_on_length)/60 as tcnt,
          sum(afc.cnt) as ucnt
         from hr.sample_terminal_three t
         join hr.tracker_oc_fact_partition afc
         on
         (t.sn=afc.sn)
         where date between '2017-07-24' and '2017-07-30' and  afc.key like '%KO%' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by  t.brand,t.province,afc.date
      """.stripMargin)


    /////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 周-分省
    val chWeeklyDF = hiveContext.sql(
      s"""
         select
           t.brand,
        	t.province as province,
        	weekofyear(afc.power_on_day) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_ch afc
        on (t.sn=afc.sn)
        where  power_on_day between '2017-07-17' and '2017-07-30' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by  t.brand,t.province,weekofyear(afc.power_on_day)
      """.stripMargin)

    val ccWeeklyDF = hiveContext.sql(
      s"""
         select
           t.brand,
        	t.province as province,
        	weekofyear(afc.date) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_partition afc
        on (t.sn=afc.sn)
        where  date between '2017-07-17' and '2017-07-30'  and  afc.key like '%CC%' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by  t.brand,t.province,weekofyear(afc.date)
      """.stripMargin)

    val koWeeklyDF = hiveContext.sql(
      s"""
         select
           t.brand,
        	t.province as province,
        	weekofyear(afc.date) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_partition afc
        on (t.sn=afc.sn)
        where  date between '2017-07-17' and '2017-07-30'  and  afc.key like '%KO%' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.province,weekofyear(afc.date)
      """.stripMargin)



    ////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 月-分省
    val chMonthlyDF = hiveContext.sql(
      s"""
         select
            t.brand,
        	t.province as province,
        	month(afc.power_on_day) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_ch afc
        on (t.sn=afc.sn)
          where  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.province,month(afc.power_on_day)
      """.stripMargin)

    val ccMonthlyDF = hiveContext.sql(
      s"""
         select
           t.brand,
        	t.province as province,
        	month(afc.date) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_partition afc
        on (t.sn=afc.sn)
          where date like '2017-07%' and  afc.key like '%CC%' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.province,month(afc.date)
      """.stripMargin)


    val koMonthlyDF = hiveContext.sql(
      s"""
         select
             t.brand,
        	t.province as province,
        	month(afc.date) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_partition afc
        on (t.sn=afc.sn)
          where date like '2017-07%' and afc.key like '%KO%' and  t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.brand,t.province,month(afc.date)
      """.stripMargin)



    val allDF =
      chDailyDF.unionAll(ccDailyDF).unionAll(koDailyDF)
        .unionAll(chWeeklyDF).unionAll(ccWeeklyDF).unionAll(koWeeklyDF)
        .unionAll(chMonthlyDF).unionAll(ccMonthlyDF).unionAll(koMonthlyDF)

    //      .rdd.saveAsTextFile("/tmp/DataCleanCHApkCnt")

    JdbcUtils.writeDF2Mysql(sc, allDF, Helper.mysqlConf, "vboxDB", "DataCleanAllPowerOnCnt", true,
      SaveMode.Append)







    //////////////////////////////////end of method ////////////////////////////////////
    //    sc.stop()
  }


}
package com.avcdata.vbox.clean.apk

import com.avcdata.vbox.util.{HBaseUtils, TimeUtils, ValidateUtils}
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._

/**
  * @author zhangyongtian
  * @define 酷开apk数据清洗
  */
object DataCleanCCApk {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2016-11-20")
    //
    //     sc.stop()

  }

  //49.82.43.21	COOCAAOS_TV	fca38688dcd6	appStatus	1492910088401						江苏省	淮安市	E3500	8S62
  def run(sc: SparkContext, currentDate: String) = {

    //TODO 原始日志格式
    //    113.249.247.212	COOCAAOS_TV	fca386edf206	appStatus	1483372857784	com.tianci.appstore	1483372726640	1483372736868	10228	2017-01-02 23:58:56	重庆市	重庆市	E5	8S82

    //TODO 转换当前日期为yyyyMMdd
    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //TODO HDFS文件->RDD
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_appStatus" + analysisDate
    val initRDD = sc.textFile(hdfsPath).distinct()

    //////////////////test///////////////////////////////////
    //wxc
    //val frdd = initRDD.map(x => (x.split('\t')(5), 1)).reduceByKey(_ + _)
    //frdd.foreach(x => file.wrApknameToFile(x))
    //file.writer.close()
    //    frdd.saveAsTextFile("hdfs:///user/hdfs/apk/CC-" + currentDate + ".txt")


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()

    //测试 将时间戳转换为时间字符串 方便查看
    //    val testInitRDD = initRDD.map(line => {
    //      val cols = line.split('\t')
    //
    //      val launchTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(6).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      val exitTimeStr = TimeUtils.convertTimeStamp2DateStr(cols(7).toString.toLong, TimeUtils.DAY_DATE_FORMAT_ONE)
    //
    //
    //      val result = cols(0) + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3) + "\t" + cols(4) + "\t" + cols(5) + "\t" + launchTimeStr + "\t" + exitTimeStr + "\t" + cols(8) + "\t" + cols(9) + "\t" + cols(10) + "\t" + cols(11)
    //
    //      println(result)
    //    })
    //////////////////test///////////////////////////////////

    //    补充终端信息到hbase terminal表
    //    load2terminal(initRDD)

    //TODO 提取
    val filteredRDD = initRDD
      .map(line => {
        val cols = line.split('\t')
        //终端唯一码mac
        val sn = cols(2).trim
        //apk package
        val apkPackage = cols(5).trim
        //启动日期
        val launchTime = cols(6).trim
        //退出时间
        val exitTime = cols(7).trim
        val duration = cols(8).trim
        //省 可以用ip库解析
        val province = cols(10).trim
        //市
        val city = cols(11).trim

        sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city
      }).distinct()
      //TODO 过滤
      .filter(line => {
      val cols = line.split('\t')

      //终端唯一码mac
      val sn = cols(0)
      val snIsRight = !sn.isEmpty()

      //apk package
      val apkPackage = cols(1)
      val apkIsRight = !apkPackage.isEmpty() && !apkPackage.equals("com.tianci.tv")


      //启动日期
      var launchTimeIsRight = ValidateUtils.isNumber(cols(2))
      if (launchTimeIsRight) {
        //        dateIsRight = cols(6).toLong > 0
        launchTimeIsRight = cols(2).toLong > 0
      }

      //退出时间
      var exitTimeIsRight = ValidateUtils.isNumber(cols(3))
      if (exitTimeIsRight) {
        exitTimeIsRight = cols(3).toLong > 0
      }

      var durationIsRight = ValidateUtils.isNumber(cols(4))
      if (durationIsRight) {
        durationIsRight = cols(4).toLong > 0
      }

      //省 可以用ip库解析
      val province = cols(5)
      val provinceIsRight = !province.isEmpty()

      //市
      val city = cols(6)
      //      val cityIsRight = !city.isEmpty() && !city.equals("未匹配")
      val cityIsRight = true

      provinceIsRight && cityIsRight && launchTimeIsRight && exitTimeIsRight && snIsRight && apkIsRight && durationIsRight

    })

    //TODO 理清启动时间 退出时间和时长的顺序
    val sortedRDD = filteredRDD.map(line => {
      val cols = line.split('\t')

      val sortedTimeList = List[Long](cols(2).toLong, cols(3).toLong, cols(4).toLong).sorted
      //终端唯一码mac
      val sn = cols(0)
      //apk package
      val apkPackage = cols(1)
      val launchTime = sortedTimeList.apply(1)
      val exitTime = sortedTimeList.apply(2)
      val duration = sortedTimeList.apply(0)
      //省 可以用ip库解析
      val province = cols(5)
      //市
      val city = cols(6)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })

    //    val sqlContext = new SQLContext(sc)
    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    //TODO 根据牌照修改com.tianci.movieplatform包名
    val sn2LicenseRDD = hiveContext.sql("select distinct sn,license from hr.terminal where brand = 'CC' " +
      "and license = 'yinhe' or license  = 'tencent'").map(line => {
      val sn = line(0).toString
      val license = line(1).toString

      (sn, license)
    })

    val sn2otherMovieplatformRDD = filteredRDD
      .filter(line => {
        val cols = line.split("\t")
        cols(1).equals("com.tianci.movieplatform")
      })
      .map(line => {
        val cols = line.split('\t')
        val sn = cols(0)
        val launchTime = cols(2)
        val exitTime = cols(3)
        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        //不包含包名
        val other = sn + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

        (sn, other)
      })

    val changedPackageRDD = sn2otherMovieplatformRDD.join(sn2LicenseRDD).map(line => {

      var apkPackage = "unknow"
      if (line._2._2.equals("tencent")) {
        apkPackage = "腾讯launcher"
      }

      if (line._2._2.equals("yinhe")) {
        apkPackage = "爱奇艺launcher"
      }

      val other = line._2._1

      val cols = other.split("\t")

      val sn = cols(0)
      val launchTime = cols(1)
      val exitTime = cols(2)
      val duration = cols(3)
      val province = cols(4)
      val city = cols(5)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" + city

    })


    val noMovieplatformRDD = filteredRDD.filter(line => {
      val cols = line.split("\t")
      !cols(1).equals("com.tianci.movieplatform")
    })


    //合并
    val compactRDD = changedPackageRDD.union(noMovieplatformRDD).distinct()

    //TODO 分近三天


    val currentDayStartTime = TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE)


    val ffDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -3)

    val firstDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val secondDayStartTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -1)

    val currentDateStartTime = TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    val currentDateEndTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, 1)

    //    val currentDateEndTime =  new DateTime(currentDayStartTime).plusDays(1).getMillis

    //    val firstDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 2)
    //    val secondDayStartTime = currentDayStartTime - (24 * 60 * 60 * 1000 * 1)

    //TODO 第一天
    val firstDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime >= ffDayStartTime && launchTime < secondDayStartTime && launchTime < exitTime && exitTime > firstDayStartTime && exitTime < currentDateStartTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      var exitTime = cols(3).toLong

      if (launchTime < firstDayStartTime) {
        launchTime = firstDayStartTime
      }

      if (exitTime > secondDayStartTime) {
        exitTime = secondDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(firstDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    println("firstDayRDD count:" + firstDayRDD.count())

    //TODO 第二天
    val secondDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime >= firstDayStartTime && launchTime < currentDateStartTime && launchTime < exitTime && exitTime > secondDayStartTime && exitTime < currentDateEndTime

      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < secondDayStartTime) {
        launchTime = secondDayStartTime
      }

      var exitTime = cols(3).toLong
      if (exitTime > currentDayStartTime) {
        exitTime = currentDayStartTime
      }

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = TimeUtils.convertTimeStamp2DateStr(secondDayStartTime, TimeUtils.DAY_DATE_FORMAT_ONE)

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    println("secondDayRDD count:" + secondDayRDD.count())

    //TODO 当天
    val currentDayRDD = compactRDD.filter(line => {
      val cols = line.split('\t')
      val launchTime = cols(2).toLong
      val exitTime = cols(3).toLong
      val launchTimeIsRight = launchTime >= secondDayStartTime && launchTime < currentDateEndTime && launchTime <
        exitTime && exitTime > currentDateStartTime && exitTime <= currentDateEndTime
      launchTimeIsRight
    }).map(line => {
      val cols = line.split('\t')

      val sn = cols(0)
      val apkPackage = cols(1)

      var launchTime = cols(2).toLong
      if (launchTime < currentDayStartTime) {
        launchTime = currentDayStartTime
      }

      val exitTime = cols(3).toLong

      val duration = cols(4)
      val province = cols(5)
      val city = cols(6)

      val launchDate = currentDate

      sn + "\t" + apkPackage + "\t" + launchTime + "\t" + exitTime + "\t" + duration + "\t" + province + "\t" +
        city + "\t" + launchDate
    })

    println("currentDayRDD count:" + currentDayRDD.count())

    val allRDD = firstDayRDD.union(secondDayRDD).union(currentDayRDD)


    //TODO 分时
    val splitTimeRdd = allRDD

      .flatMap(line => {

        val cols = line.split("\t")

        //终端唯一码
        val sn = cols(0)
        //apk package
        val apkPackage = cols(1)

        val launchTime = cols(2).toLong

        val exitTime = cols(3).toLong

        val duration = cols(4)
        val province = cols(5)
        val city = cols(6)

        val launchDate = cols(7)

        val tmpArrayBuffer = TimeUtils.splitTimeByHour(province + "\t" + city + "\t" + sn + "\t" + apkPackage + "\t" + launchDate + "\t", launchTime, exitTime)

        tmpArrayBuffer

      })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val province = cols(i)
        i = i + 1

        var city = cols(i)
        if (city.equals("未匹配")) {
          city = "unknow"
        }
        i = i + 1

        val sn = cols(i)
        i = i + 1

        val apkPackage = cols(i)
        i = i + 1

        //启动的原始日期
        val date = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        Row(province, city, sn, apkPackage, date, hour, launchCnt, duration)
      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("province", StringType, false),
        StructField("city", StringType, false),
        StructField("sn", StringType, false),
        StructField("apkPackage", StringType, false),
        StructField("date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false)
      )
    )

    /////////////////////////使用sql统计////////////////////////////////////
    println("sql on temptable ...")

    hiveContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_apk_extract")
    hiveContext.cacheTable("tb_apk_extract")
    //TODO 关联apkinfo 过滤
    //统计分析
    val resultDF = hiveContext.sql(
      """
        |SELECT
        |  ap.sn,
        |  ap.apkPackage,
        |  ap.date,
        |  ap.hour,
        |  SUM(ap.launchCnt) launchCnts,
        |  SUM(ap.duration) dura
        |FROM
        |  tb_apk_extract ap
        |GROUP BY
        |  ap.sn,
        |  ap.apkPackage,
        |  ap.date,
        |  ap.hour
      """.stripMargin).rdd

      //inner join hr.apkinfo ai
      //on (ap.apkPackage=ai.packagename)

      //再次过滤
      .filter(line => {
      line(5).toString().toLong <= 3600
      //          line(6).toString().toLong <= (24 * 3600)
    })


    //TODO  写入到Hbase
    resultDF.foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")
//      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact01")
      //   val mutator = HBaseUtils.getMutator("test_apk_active_fact")
      //   val mutator = HBaseUtils.getMutator("tracker_apk_active_fact_test")

      try {

        lines.foreach(line => {
          var i = 0

          val sn = line(i).toString
          i = i + 1

          val apkPackage = line(i).toString
          i = i + 1

          val date = line(i).toString
          i = i + 1


          val hour = line(i).toString
          i = i + 1

          val launchCnt = line(i).toString
          i = i + 1

          val duration = line(i).toString
          i = i + 1

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "CC"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine, brand))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })
    hiveContext.uncacheTable("tb_apk_extract")
    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}

//验证
//select dim_apk,count(distinct dim_sn) sn_num, sum(fact_cnt) as cnt ,sum(fact_duration)/3600 as dura,sum(fact_duration)/3600/count(distinct dim_sn) tcntavgclient  from tracker_apk_fact_test apt where apt.dim_date = '2016-12-21' and apt.key like '%CC'
//group by dim_apk
package com.avcdata.vbox.clean.play

import com.avcdata.vbox.util.{HBaseUtils, TimeUtils, ValidateUtils}
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object DataCleanCCPlay {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-DataCleanCCPlay")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable


  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)



    /////////////////////////test//////////////////////////
    //        val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121     \\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    //TODO 读取数据文件
    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate + "/aowei_PlayerControl" + analysisDate + "_2.txt"
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate
    println(hdfsPath)
    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取 过滤
    val filterRdd = initRDD.distinct().map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct()

      //TODO 过滤
      .filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //      val timeStamp = cols(1).toLong
      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val isTimestampRight = ValidateUtils.isNumber(cols(1)) && !(cols(1).toLong < checkTime)


      val dim_title = cols(2)
      val dim_titleIsRight = !cols(2).isEmpty
      //      && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(3).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(4)
      val isDurationRight = !cols(4).isEmpty && ValidateUtils.isNumber(cols(4)) && cols(4).toLong >= 0

      isTimestampRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )



    //TODO 配对 合并
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp)

    })

    //    startRDD.foreach(println(_))

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val pairRDD = startRDD.join(stopRDD)

    //笛卡尔积配对并合并 =>阶段一完成
    val stageOneRDD = pairRDD
      //过滤startTime大于stopTime的
      .filter(tuple => {
      val valueLeftCols = tuple._2._1.split("\t")
      val starTime = valueLeftCols(0)

      val valueRightCols = tuple._2._2.split("\t")
      val stopTime = valueRightCols(0)

      starTime <= stopTime
    })
      .map(
        tuple => {
          val keyCols = tuple._1.split("\t")
          val sn = keyCols(0)
          val dim_title = keyCols(1)

          val valueLeftCols = tuple._2._1.split("\t")
          val starTime = valueLeftCols(0)

          val valueRightCols = tuple._2._2.split("\t")
          val stopTime = valueRightCols(0)

          val fact_duration = valueRightCols(1)

          (sn + "\t" + dim_title + "\t" + starTime, starTime + "\t" + stopTime + "\t" + fact_duration)
        }
      ).reduceByKey((pre, after) => {
      val preCols = pre.split("\t")
      val preDiff = preCols(1).toString.toLong - preCols(0).toString.toLong

      val afterCols = after.split("\t")
      val afterDiff = afterCols(1).toString.toLong - afterCols(0).toString.toLong

      var min = ""
      if (preDiff <= afterDiff) {
        min = pre
      } else {
        min = after
      }
      min
    }).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_title = keyCols(1)

        val valueCols = tuple._2.split("\t")
        val starTime = valueCols(0)

        val stopTime = valueCols(1)

        val fact_duration = valueCols(2)

        (sn + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    )
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO  清理剧名

    val keywordArr = Array[String]("剧透", "预告", "抢先看", "片花", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "海报", "将映", "剧照", "mv", "mp4")


    //TODO 广播 film_properties表

    //////////////////////////////////test//////////////////////////////////////
    //    val sqlContext = new SQLContext(sc)
    //////////////////////////////////test//////////////////////////////////////

    ////////////////////////test/////////////////////////////
    //    val filmInfoArr = sc.textFile("E:\\aowei\\tracker-job\\doc\\data\\plays\\film_properties.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val id = cols(0)
    //        val original_name = cols(1)
    //        val standard_name = cols(2)
    //        val year = cols(3)
    //        val model = cols(4)
    //        val crowd = cols(5)
    //        val region = cols(6)
    //        Row(original_name, standard_name, model, id, year, crowd, region)
    //      }).collect
    ////////////////////////test/////////////////////////////

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect

    //    println("filmInfoArr.size=" + filmInfoArr.size)
    //    for(ele<-filmInfoArr){
    //      println(ele)
    //    }
    //    val filmInfoArrBV = sc.broadcast(filmInfoArr)

    //fca3864d91ad		锦绣未央 第19集预告	1479700448481	1479700448481	65530
    val stageTwoRDD = stageOneRDD

      //TODO 过滤特定关键词
      .filter(line => {
      val cols = line.split("\t")
      !ValidateUtils.isContainsSpecWords(cols(1), keywordArr)
    }).distinct()
      .coalesce(1000, shuffle = false)

      .mapPartitions(lines => {

        val filmInfo = filmInfoArr
        lines
          //TODO 提取集数和日期
          //    fca3864d91ad	_yinhe_203364601	锦绣未央 第19集预告	日期  开始时间  70368(时长)
          .map(line => {
          val cols = line.split("\t")

          //终端sn
          val dim_sn = cols(0)

          //日志的视频名称
          val dim_name = cols(1)

          //TODO 通过资源名称获取其他信息
          var dim_title = "#"
          var dim_model = "#"
          var dim_awcid = "#"
          var dim_year = "#"
          var dim_crowd = "#"
          var dim_region = "#"
          var dim_part = "unknow"

          val filmProp = DataCleanCCPlayHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfo)
          if (!filmProp.equals("#")) {
            val filmPropCols = filmProp.split("\t")
            //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
            dim_title = filmPropCols(0)
            dim_model = filmPropCols(1)
            dim_awcid = filmPropCols(2)
            dim_year = filmPropCols(3)
            dim_crowd = filmPropCols(4)
            dim_region = filmPropCols(5)
            dim_part = filmPropCols(6)
          }


          //集数
          //      val dim_part = DataCleanCCPlayHelper.extractVideoPartOFCooCaa(cols(1))

          //日期
          val dim_date = currentDate

          //开始时间
          val startTime = cols(2).toLong

          //时长
          val fact_duration = cols(4).toLong

          //结束时间=开始时间+时长
          val stopTime = startTime + fact_duration

          dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + startTime + "\t" + stopTime + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

        })

      })


    //TODO 过滤掉不匹配的数据
    val stageTwoPassRDD = stageTwoRDD.filter(line => {
      val cols = line.split("\t")
      !cols(1).equals("#")
    })

    //    //TODO 不匹配的数据统计
    //    val stageTwoUnPassRDD = stageTwoRDD.filter(line => {
    //      val cols = line.split("\t")
    //      cols(1).equals("#")
    //    }).map(line => {
    //      val cols = line.split("\t")
    //      val dim_name = cols(11)
    //      (dim_name, 1)
    //    }).reduceByKey(_ + _).repartition(1).saveAsTextFile("/user/hdfs/rsync/playunpass/" + currentDate)

    // TODO 其他（提取资源名称 单独存放)

    //        stageTwoRDD.foreach(println(_))
    //
    //    println("清洗前记录数：" + initRDD.count())
    //    println("清洗后记录数：：" + stageTwoRDD.count())


    //TODO 获得Awcid
    //    val keyword2awcidArr = sc.textFile("/user/hive/warehouse/hr.db/tracker_plays_tag/tracker_plays_tag.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        (cols(4), cols(0))
    //      })
    //      .collect()

    //TODO 分时
    val splitTimeRdd = stageTwoPassRDD.flatMap(line => {
      val cols = line.split("\t")

      val dim_sn = cols(0)

      val dim_title = cols(1)

      //      val keyword = ValidateUtils.convertTitle2Keyword(cols(1))
      //
      //      for (keword2cid <- keyword2awcidArr) {
      //        if (keyword.equals(keword2cid._1)) {
      //          dim_awcid = keword2cid._2
      //        }
      //      }

      val dim_part = cols(2)
      val dim_date = cols(3)
      val startTime = cols(4).toLong
      val stopTime = cols(5).toLong
      val dim_model = cols(6)
      val dim_awcid = cols(7)
      val dim_year = cols(8)
      val dim_crowd = cols(9)
      val dim_region = cols(10)

      val dim_name = cols(11)

      //+ "\t" + dim_model+ "\t" + dim_awcid+ "\t" + dim_year+ "\t" + dim_crowd+ "\t" + dim_region


      val tmpArrayBuffer = TimeUtils.splitTimeByHour(dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year ++ "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name + "\t", startTime, stopTime)

      tmpArrayBuffer

    })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val dim_sn = cols(i)

        i = i + 1

        //市
        val dim_title = cols(i)
        i = i + 1


        val dim_part = cols(i)
        i = i + 1

        val dim_date = cols(i)
        i = i + 1
        val dim_model = cols(i)
        i = i + 1
        val dim_awcid = cols(i)
        i = i + 1
        val dim_year = cols(i)
        i = i + 1
        val dim_crowd = cols(i)
        i = i + 1
        val dim_region = cols(i)
        i = i + 1

        val dim_name = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        val other = dim_awcid + ";" + dim_model + ";" + dim_year + ";" + dim_crowd + ";" + dim_region + ";" + dim_name

        Row(dim_sn, dim_title, dim_part, dim_date, hour, launchCnt, duration, other)

      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("dim_sn", StringType, false),
        StructField("dim_title", StringType, false),
        StructField("dim_part", StringType, false),
        StructField("dim_date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false),
        StructField("other", StringType, false)
      )
    )


    //TODO 使用sql统计次数和整点时长
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_tmp")
    sqlContext.cacheTable("tb_tmp")

    val hTableName = "tracker_player_active_fact"
    //      val hTableName ="tracker_player_active_fact_test"

    //统计分析
    sqlContext.sql(
      """
       SELECT
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
           other,
         SUM (launchCnt),
         SUM (duration)
         FROM
         tb_tmp
         GROUP BY
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
          other
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      val duration = line(7).toString().toLong
      duration > 0 && duration <= 3600
    })

      /////////////////////////////////////////test///////////////////
      //      .foreach(println(_))

      /////////////////////////////////////////test///////////////////
      //TODO 写入到Hbase
      .foreachPartition(lines => {


      val mutator = HBaseUtils.getMutator(hTableName)


      try {

        lines.foreach(line => {
          var i = 0

          val dim_sn = line(i).toString
          i = i + 1

          val dim_title = line(i).toString
          i = i + 1

          val dim_part = line(i).toString
          i = i + 1

          val dim_date = line(i).toString
          i = i + 1

          val dim_hour = line(i).toString
          i = i + 1

          val other = line(i).toString
          val cols = other.split(";")
          val dim_awcid = cols(0)
          val dim_model = cols(1)
          var dim_year = cols(2)
          if (dim_year.equals("unknow")) dim_year = ""
          val dim_crowd = cols(3)
          var dim_region = cols(4)
          if (dim_region.equals("unknow")) dim_region = ""
          var dim_name = cols(5)

          i = i + 1

          val fact_vv = line(i).toString
          i = i + 1

          //dim_model,dim_year,dim_crowd,dim_region
          val fact_duration = line(i).toString


          //关联terminal 加上brand last_poweron area province city siz model  license citylevel

          val sortedLine = dim_sn + "\t" + dim_title + "\t" + dim_awcid + "\t" + dim_part + "\t" + dim_date + "\t" + dim_hour + "\t" + fact_vv + "\t" + fact_duration + "\t" + dim_model + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

          //          println(sortedLine)
          val brand = "CC"
          mutator.mutate(HBaseUtils.getPut_Plays(brand, sortedLine))
        })
        mutator.flush()
      } finally {
        mutator.close()
      }
    }
    )
    println("#####写入到Hbase######hbase table" + hTableName)
    sqlContext.uncacheTable("tb_tmp")
  }
}
package com.avcdata.vbox.clean.play

import java.util.regex.Pattern

import com.avcdata.vbox.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object DataCleanCCPlayHelper {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))


    //TODO 规则01
//    if (!DataCleanCCPlayRuler.RULER_01.findFirstMatchIn(titleArr(i)).isEmpty) {
//      println("综艺")
//    }
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {


    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- Constant.versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.|~|;|、|《|》|（|）|(|)|【|】|\\s+", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)

      //TODO 直接匹配原始名称和标准名称
      for (i <- 0 until titleArr.length) {
        if (original_name.equals(titleArr(i)) || standard_name.equals(titleArr(i))) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
      }


      /////////////////////////////////////////////////////////////////////////////////
      var handledTitle = log_dim_title

      handledTitle = DataCleanCCPlayRuler.ruler_01(module,handledTitle)
      handledTitle = DataCleanCCPlayRuler.ruler_02(module,handledTitle)
      handledTitle = DataCleanCCPlayRuler.ruler_03(module,handledTitle)
      handledTitle = DataCleanCCPlayRuler.ruler_04(module,handledTitle)
      handledTitle = DataCleanCCPlayRuler.ruler_05(module,handledTitle)

      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") ) {

        val plays_start = System.currentTimeMillis()

        if (original_name.equals(handledTitle) || standard_name.equals(handledTitle)) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
        val plays_end = System.currentTimeMillis()
        //        println("电视剧匹配执行时间" + (plays_end - plays_start))
      }

      ///////////////////////////////////////////////////////////////////////////

      //电影 【电影名称】【版本】 //不包含集数和第
      if (module.equals("电影")) {
        val movie_start = System.currentTimeMillis()
        if (original_name.equals(handledTitle) || standard_name.equals(handledTitle)) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
        val movie_end = System.currentTimeMillis()
        //        println("电影匹配执行时间" + (movie_end - movie_start))
      }



      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////


      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {

        val variety_start = System.currentTimeMillis()
        if (original_name.equals(handledTitle) || standard_name.equals(handledTitle)) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
        val variety_end = System.currentTimeMillis()

        //        println("综艺匹配执行时间" + (variety_end - variety_start))
      }


      //////////////////////////////////////////////////////////////////////////////////

      //动画片
      if (module.equals("动画片")) {

        val cartoon_start = System.currentTimeMillis()

        if (original_name.equals(handledTitle) || standard_name.equals(handledTitle)) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
        val cartoon_end = System.currentTimeMillis()

        //        println("动画片匹配执行时间" + (cartoon_end - cartoon_start))

      }
    })

    return "#"
  }



















































  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "#"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result = result.replaceAll("\\s", "")
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim

    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }

    result = result.replaceAll("\\s", "")

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第") + 1, result.indexOf("季"))
    } else {
      result = ""
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    //去掉以0开头的数字
    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.vbox.clean.play

import java.util.regex.Pattern

import com.avcdata.vbox.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object DataCleanCCPlayHelper01 {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "TV版", "tv版", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }


      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)

      //直接匹配原始名称和标准名称
      for (i <- 0 until titleArr.length) {
        if (original_name.equals(titleArr(i)) || standard_name.equals(titleArr(i))) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
      }

      ///////////////////////////////////////////////////////////////////////////

      //电影 【电影名称】【版本】 //不包含集数和第
      if (module.equals("电影") && "(第.+集)|(\\d集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        val movie_start = System.currentTimeMillis()

        for (i <- 0 until titleArr.length) {



          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFMovie(titleArr(i))

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }
        val movie_end = System.currentTimeMillis()
//        println("电影匹配执行时间" + (movie_end - movie_start))
      }



      /////////////////////////////////////////////////////////////////////////////////

      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") && !"([^0-9]+[0-9]+)|([ ]\\d*)|(_\\d*)|(-第\\d*集)|(第\\d*集)|(\\(第\\d*集\\))|(大结局)|(先导集)".r.findFirstMatchIn(log_dim_title).isEmpty) {

        val plays_start = System.currentTimeMillis()

        for (i <- 0 until titleArr.length if i != 3) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFPlays(titleArr(i), standard_name)

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }
        val plays_end = System.currentTimeMillis()
//        println("电视剧匹配执行时间" + (plays_end - plays_start))
      }





      //////////////////////////////////////////////////////////////////////////////////


      //动画片
      if (module.equals("动画片")) {

        val cartoon_start = System.currentTimeMillis()


        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }


          val simpleName = getSimpleVideoNameOFCartoon(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

        }

        val cartoon_end = System.currentTimeMillis()

//        println("动画片匹配执行时间" + (cartoon_end - cartoon_start))

      }





      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////


      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {

        val variety_start = System.currentTimeMillis()


        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFVariety(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }


        val variety_end = System.currentTimeMillis()

//        println("综艺匹配执行时间" + (variety_end - variety_start))
      }


    })

    return "#"
  }

  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "#"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result = result.replaceAll("\\s", "")
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim

    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }

    result = result.replaceAll("\\s", "")

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第") + 1, result.indexOf("季"))
    } else {
      result = ""
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    //去掉以0开头的数字
    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.vbox.clean.play

import java.util.regex.Pattern

import com.avcdata.vbox.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  *酷开匹配规则
	规则1
		格式如：①6位或8位数字+剧名/②剧名+6位或8位数字/③剧名+之+8位数字

			格式的特点
				头部6位或8位数字+剧名；剧名+6位或8位数字；剧名+之+6位或8位数字
				举例：150724 极速前进第2季；一站到底之20160411；弈棋耍大牌170324；20170220 我是大美人

     处理方式
				将数字去掉
					此类特征是综艺节目的特点，处理完匹配时可只跟综艺库匹配


	规则2
		格式如：剧名+第X季/部(+无关字段）
			格式特点
				剧名+第×季；剧名+空格+第×季；剧名+第×部；剧名+空格+第×部；剧名+罗马数字I、II；剧名+空格+罗马数字I、II：x的可能格式为：①阿拉伯数字1234...；②汉字一二三...③罗马数字I II III
				举例：权力的游戏第4季；《旋转轮胎》第二季；权利的游戏7；权利的游戏III
			处理方式
				全部处理为：剧名+X阿拉伯数字格式
					案例：魔法俏佳人第3季第5集保存为：魔法俏佳人3
					案例：魔法俏佳人 第三季 保存为：魔法俏佳人3
					案例：马大帅III保存为：马大帅3
					案例：马大帅第3部保存为：马大帅3
				含无关字段：季、部后面若有无关字段也全部去掉
					樱桃小丸子 第2季 国语版(537集-984集)保存为：樱桃小丸子2

	规则3
		格式如：剧名+版本（+无关字段）
			版本字段格式特点
				版本
					含有：未删减版；[未删减版]；[TV版]；粤语版；精华版；湖南卫视版；国语；（国语）......后期补全
						【具体见Word附件】
					举例：锦绣未央未删减版；锦绣未央[DVD版]
				集数、期数
					第*集；空格第*集；-第*集；*；空格*；(第*集)；_*；大结局；先导集；先导集上；先导集下；上集；下集；数字集版
						【具体见Word附件】
					举例：剧名+35集版；射雕英雄传 第3集；中国有嘻哈 第五期 17/07/22
			处理方式
				含所列版本格式、集数、期数信息都处理掉，只保留剧名：剧名+版本信息=剧名
				含无关字段：版本字段后面可能含有其他字段，也全部去掉
					举例：锦绣未央第13集-表里不一 未央吃醋 保存为：锦绣未央
					 举例：粉红猪小妹4 DVD发行宣传片 保存为：粉红猪小妹4
					举例：《黑猫警长》沪语版发布保存为：《黑猫警长》

规则4
		格式如：剧名+空格+年份(+无关信息）/剧名+括号+年份（+无关信息）
			年份字段特点
				剧名+空格+四位数字年份；剧名+括号+四位数字年份
				举例：军情解码 2016；睡美人（2014）
			处理方式
				将年份信息去掉匹配
				含无关字段：年份后面可能含有其他字段
	规则5
		符号处理
			①去符号处理
				•   中间的点号去掉
					举例：安娜•卡列尼娜=安娜卡列尼娜
				。去掉
					举例：你的名字。=你的名字
				《》书名号、()括号、~、空格等都去掉；【列全所有可能出现的符号，见附件Word】
					举例：《少年神探狄仁杰》=少年神探狄仁杰
					【具体见Word附件】
			②符号转换处理
				，！：全部保存为英文格式
					举例：嘿，老头！；海底总动员2：多莉去哪儿=海底总动员2:多莉去哪儿
	匹配规则说明
		2.按照下列清洗规则，同爬虫影视库的"原始名称"/“标准名称”匹配。存储上爬虫影视库的“标准名称”，匹配工作完成
		3.同名问题：①能匹配到多条“原始名称”，且“原始名称”指向的“标准名称”不一致，选择最新的年份；②能匹配上多条，且对应的模块不唯一，则选择优先级为：电视剧>综艺>电影>动画片。
			举例：最强大脑能匹配上最强大脑1、最强大脑2、最强大脑3；最终选择匹配最强大脑3
		剧名中的包含季数/集数信息的按照下列规则处理为阿拉伯数字。剧名本身包含数字的就按照原始格式不变。
  *
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object DataCleanCCPlayHelper02 {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "完全版", "[TV版]", "TV版", "tv版", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "(1080P)", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "美版", "（美版）", "[DVD版]", "-TV版", "（四川版）", "(环绕声版)", "独家精华版", "日文版", "周播版", "（北美版）", "（东北版）", "英文版", "(英文版)", "潮汕版", "精编版纪念版", "合集版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "春节贺岁版", "周末版", "周间版", "日播版", "典藏版", "教学版", "独家未播版", "VR", "特别版", "重制版", "国际版", "独家抢鲜版", "影院版", "免费版", "短剧版", "搜狐版", "幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "新编集版", "标准版", "原声版", "（午间版）", "数字集版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }


      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.|~|;|、|《|》|（|）|(|)|【|】|\\s+", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)

      //TODO 直接匹配原始名称和标准名称
      for (i <- 0 until titleArr.length) {
        if (original_name.equals(titleArr(i)) || standard_name.equals(titleArr(i))) {
          return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
            dim_part
        }
      }

      ///////////////////////////////////////////////////////////////////////////

      //电影 【电影名称】【版本】 //不包含集数和第
      if (module.equals("电影")) {
        val movie_start = System.currentTimeMillis()

        for (i <- 0 until titleArr.length) {

          //TODO 包含匹配
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }



          //TODO 提取匹配
          val simpleName = getSimpleVideoNameOFMovie(titleArr(i))

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }
        val movie_end = System.currentTimeMillis()
        //        println("电影匹配执行时间" + (movie_end - movie_start))
      }



      /////////////////////////////////////////////////////////////////////////////////

      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") ) {

        val plays_start = System.currentTimeMillis()

        for (i <- 0 until titleArr.length if i != 3) {

          //TODO 包含匹配
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }




          //TODO 提取匹配
          val simpleName = getSimpleVideoNameOFPlays(titleArr(i), standard_name)

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }
        val plays_end = System.currentTimeMillis()
        //        println("电视剧匹配执行时间" + (plays_end - plays_start))
      }





      //////////////////////////////////////////////////////////////////////////////////


      //动画片
      if (module.equals("动画片")) {

        val cartoon_start = System.currentTimeMillis()



        for (i <- 0 until titleArr.length) {

          //TODO 包含匹配
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }


          //TODO 提取匹配
          val simpleName = getSimpleVideoNameOFCartoon(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

        }

        val cartoon_end = System.currentTimeMillis()

        //        println("动画片匹配执行时间" + (cartoon_end - cartoon_start))

      }





      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////


      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {

        val variety_start = System.currentTimeMillis()

        for (i <- 0 until titleArr.length) {



          //TODO 包含匹配
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {

            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }


          //TODO 提取匹配
          val simpleName = getSimpleVideoNameOFVariety(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }


        val variety_end = System.currentTimeMillis()

        //        println("综艺匹配执行时间" + (variety_end - variety_start))
      }


    })

    return "#"
  }

  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "#"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result = result.replaceAll("\\s", "")
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim

    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }

    result = result.replaceAll("\\s", "")

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第") + 1, result.indexOf("季"))
    } else {
      result = ""
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    //去掉以0开头的数字
    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    //判断标准名称是否包含 季
    if ("[0-9]{1,2}$".r.findFirstIn(standard_name) != None) {
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s", "")
    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.vbox.clean.play

import java.util.regex.Pattern

import com.avcdata.vbox.util.Constant

import scala.util.Random
import scala.util.matching.Regex

object DataCleanCCPlayRuler {

  /**
    * 规则1
    * 格式如：①6位或8位数字+剧名/②剧名+6位或8位数字/③剧名+之+8位数字

    * 格式的特点
    * 头部6位或8位数字+剧名；剧名+6位或8位数字；剧名+之+6位或8位数字
    * 举例：150724 极速前进第2季；一站到底之20160411；弈棋耍大牌170324；20170220 我是大美人

    * 处理方式
    *
    * 将数字去掉
    *
    * 此类特征是综艺节目的特点，处理完匹配时可只跟综艺库匹配
    */

  def ruler_01(module: String, log_title: String): String = {

    var res = log_title

    if (module.equals("综艺")) {
      res = log_title.replaceAll("[0-9]{6,8}", "")
    }

    res.trim
  }

  /**
    * 规则2
    * 格式如：剧名+第X季/部(+无关字段）
    * 格式特点
    * 剧名+第×季；剧名+空格+第×季；剧名+第×部；剧名+空格+第×部；剧名+罗马数字I、II；剧名+空格+罗马数字I、II：x的可能格式为：①阿拉伯数字1234...；②汉字一二三...③罗马数字I II III
    * 举例：权力的游戏第4季；《旋转轮胎》第二季；权利的游戏7；权利的游戏III
    *
    * 处理方式
    * 全部处理为：剧名+X阿拉伯数字格式
    * 案例：魔法俏佳人第3季第5集保存为：魔法俏佳人3
    * 案例：魔法俏佳人 第三季 保存为：魔法俏佳人3
    * 案例：马大帅III保存为：马大帅3
    * 案例：马大帅第3部保存为：马大帅3
    * 含无关字段：季、部后面若有无关字段也全部去掉
    * 樱桃小丸子 第2季 国语版(537集-984集)保存为：樱桃小丸子2

    */

  def ruler_02(module: String, log_title: String): String = {

    //数字转换匹配名称
    var res = log_title
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      res = res.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      res = res.replaceAll(i, zhNumMap.get(i).get)
    )
    res.trim
  }


  /**
    * 规则3
    * 格式如：剧名+版本（+无关字段）
    * 版本字段格式特点
    * 版本
    * 含有：未删减版；[未删减版]；[TV版]；粤语版；精华版；湖南卫视版；国语；（国语）......后期补全
    * 【具体见Word附件】
    * 举例：锦绣未央未删减版；锦绣未央[DVD版]
    * 集数、期数
    * 第*集；空格第*集；-第*集；*；空格*；(第*集)；_*；大结局；先导集；先导集上；先导集下；上集；下集；数字集版
    *
    * 【具体见Word附件】
    * 举例：剧名+35集版；射雕英雄传 第3集；中国有嘻哈 第五期 17/07/22
    *
    * 处理方式
    *
    * 含所列版本格式、集数、期数信息都处理掉，只保留剧名：剧名+版本信息=剧名
    *
    * 含无关字段：版本字段后面可能含有其他字段，也全部去掉
    *
    * 举例：锦绣未央第13集-表里不一 未央吃醋 保存为：锦绣未央
    * 举例：粉红猪小妹4 DVD发行宣传片 保存为：粉红猪小妹4
    * 举例：《黑猫警长》沪语版发布保存为：《黑猫警长》
    */

  def ruler_03(module: String, log_title: String): String = {

    var res = log_title

    //提取季数
    if (res.contains("第") && res.contains("季") && res.indexOf("第") < res.indexOf("季")) {
      val seasonNum = res.substring(res.indexOf("第") + 1, res.indexOf("季")).trim
      val pg = res.substring(0, res.indexOf("第")).trim

      res = pg + seasonNum
    }


    //去版本
    for (version <- Constant.versionArr) {
      if (res.contains(version)) {
        res = res.replaceAll(version, "")
      }
    }

    //去集数、期数
    for (partWord <- Constant.partWord) {
      val pattern = new Regex(partWord)

      val find = pattern.findFirstIn(res)

      if (!find.isEmpty) {
        res = res.substring(0, res.indexOf(find.get))
      }
    }


    res.trim
  }


  /**
    * 规则4
    * 格式如：剧名+空格+年份(+无关信息）/剧名+括号+年份（+无关信息）
    * 年份字段特点
    * 剧名+空格+四位数字年份；剧名+括号+四位数字年份
    * 举例：军情解码 2016；睡美人（2014）
    *
    * 处理方式
    * 将年份信息去掉匹配
    * 含无关字段：年份后面可能含有其他字段
    */

  def ruler_04(module: String, log_title: String): String = {

    var res = log_title

    res = res.replaceAll("19|20{1}\\d{2}", "")
    res = res.replaceAll("（19|20{1}\\d{2}）", "")
    res = res.replaceAll("(19|20{1}\\d{2})", "")

    res.trim
  }


  /**
    * 规则5
    * 符号处理
    * ①去符号处理
    * •   中间的点号去掉
    * 举例：安娜•卡列尼娜=安娜卡列尼娜
    * 。去掉
    * 举例：你的名字。=你的名字
    * 《》书名号、()括号、~、空格等都去掉；【列全所有可能出现的符号，见附件Word】
    * 举例：《少年神探狄仁杰》=少年神探狄仁杰
    * 【具体见Word附件】
    * ②符号转换处理
    * ，！：全部保存为英文格式
    * 举例：嘿，老头！；海底总动员2：多莉去哪儿=海底总动员2:多莉去哪儿
    */

  def ruler_05(module: String, log_title: String): String = {

    var res = log_title

    res = res.replaceAll(":|,|!|。|：|，|！|•   |.|~|;|、|《|》|（|）|(|)|【|】|\\s+", "")
    res = res.replaceAll("!", "！")
    res = res.replaceAll(",", "，")

    res.trim
  }


  //  * 匹配规则说明
  //    * 2.按照下列清洗规则，同爬虫影视库的"原始名称"/“标准名称”匹配。存储上爬虫影视库的“标准名称”，匹配工作完成
  //  * 3.同名问题：
  //  * ①能匹配到多条“原始名称”，且“原始名称”指向的“标准名称”不一致，选择最新的年份；
  //  * ②能匹配上多条，且对应的模块不唯一，则选择优先级为：电视剧>综艺>电影>动画片。
  //  * 举例：最强大脑能匹配上最强大脑1、最强大脑2、最强大脑3；最终选择匹配最强大脑3
  //  * 剧名中的包含季数/集数信息的按照下列规则处理为阿拉伯数字。剧名本身包含数字的就按照原始格式不变

  def main(args: Array[String]) {

    var handledTitle = "中国有嘻哈 第三期 17/07/08"
//    val handledTitle = "150717 极速前进第2季"
//    val handledTitle = "权力的游戏第2季"
//    val handledTitle = "《偶滴歌神啊》第二季第三期"
//    val handledTitle = "纸牌屋第二季归来 史派西再续教科书级表演"
//    val handledTitle = "第二滴血（2007）"
//    val handledTitle = "夏目友人帐 第4季"

//    log_dim_title=

//    var handledTitle = log_dim_title

    handledTitle = DataCleanCCPlayRuler.ruler_01("",handledTitle)
    handledTitle = DataCleanCCPlayRuler.ruler_02("",handledTitle)
    handledTitle = DataCleanCCPlayRuler.ruler_03("",handledTitle)
    handledTitle = DataCleanCCPlayRuler.ruler_04("",handledTitle)
    handledTitle = DataCleanCCPlayRuler.ruler_05("",handledTitle)

    println(handledTitle)

  }

  //  #按照酷开新规则，应当匹配却未匹配上的：
  //  #
  //  #150717 极速前进第2季
  //  #权力的游戏第2季
  //  #《偶滴歌神啊》第二季第三期
  //  #电击小子 第1季
  //  #纸牌屋第二季归来 史派西再续教科书级表演
  //  #第二滴血（2007）
  //  #中国有嘻哈 第三期 17/07/08
  //  #夏目友人帐 第4季


}package com.avcdata.vbox.clean.play

import com.avcdata.vbox.util.ValidateUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object DataCleanCCPlayTest {


  def main(args: Array[String]) {

    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-DataCleanCCPlayTest")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String) = {
    //TODO 读取数据文件
    val hdfsPath = "/user/hdfs/rsync/play_test/cc_new_pg.txt"

    val initRDD = sc.textFile(hdfsPath).map(_.trim)

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect

    val filmInfoArrBV = sc.broadcast(filmInfoArr)


    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //TODO 过滤特定关键词
    val stageTwoRDD = initRDD.filter(line => {
      !ValidateUtils.isContainsSpecWords(line, keywordArr)
    }).repartition(1000).distinct
      //提取剧名
      .map(line => {
      //日志的视频名称
      val dim_name = line

      //TODO 通过资源名称获取其他信息
      var dim_title = "#"
      var dim_model = "#"
      var dim_awcid = "#"
      var dim_year = "#"
      var dim_crowd = "#"
      var dim_region = "#"
      var dim_part = "unknow"

      val filmProp = DataCleanCCPlayHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfoArrBV.value)
      if (!filmProp.equals("#")) {
        val filmPropCols = filmProp.split("\t")
        //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
        dim_title = filmPropCols(0)
        dim_model = filmPropCols(1)
        dim_awcid = filmPropCols(2)
        dim_year = filmPropCols(3)
        dim_crowd = filmPropCols(4)
        dim_region = filmPropCols(5)
        dim_part = filmPropCols(6)
      }
      dim_title + "\t" + dim_part + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name
    })

    //TODO 过滤掉不匹配的数据
//        val stageTwoPassRDD = stageTwoRDD.filter(line => {
//          val cols = line.split("\t")
//          !cols(0).equals("#")
//        })
//      .repartition(1)
//      .saveAsTextFile("/user/hdfs/rsync/playunpass/pass/" + currentDate + System.currentTimeMillis())


    // TODO 不匹配的数据统计
    val stageTwoUnPassRDD = stageTwoRDD.
      filter(line => {
        val cols = line.split("\t")
        cols(0).equals("#")
      }).map(line => {
      val cols = line.split("\t")
      val dim_name = cols(7)
      dim_name
    }).distinct
//      .reduceByKey(_ + _)
      .repartition(1)
      .saveAsTextFile("/user/hdfs/rsync/playunpass/" + currentDate + System.currentTimeMillis())
  }

}package com.avcdata.vbox.clean.play

import com.avcdata.vbox.util.ValidateUtils
import org.apache.spark.sql.Row
import org.apache.spark.{SparkConf, SparkContext}

object DataCleanCCPlayTestLocal {


  def main(args: Array[String]) {

    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-DataCleanCCPlayTest")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String) = {
    //TODO 读取数据文件
    /////////////////////////////////test//////////////////////////////

    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\cc_new_pg-2.txt"

    ///////////////////////////////////test/////////////////////////////


    //    val hdfsPath = "/user/hdfs/rsync/play_test/cc_new_pg.txt"

    val initRDD = sc.textFile(hdfsPath).map(_.trim)


    ///////////////////test///////////////////////////

    val sqlContext = new org.apache.spark.sql.SQLContext(sc)


    ///////////////////test///////////////////////////

    //    val sqlContext = new HiveContext(sc)

    //    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect

    ///////////////////////////////test//////////////////////////////////////
    val filmInfoArr = sc.textFile("E:\\aowei\\tracker-clean\\doc\\film_properties.csv")
      .filter(line => {
        line.split('|').length > 5
      })
      .map(line => {
        val cols = line.split('|')
        val id = cols(0)
        val original_name = cols(1)
        val standard_name = cols(2)
        val year = cols(3)
        val model = cols(4)
        val crowd = cols(5)
        val region = cols(6)
        Row(original_name, standard_name, model, id, year, crowd, region)
      }).collect

    ////////////////////////test/////////////////////////////////////////////

    val filmInfoArrBV = sc.broadcast(filmInfoArr)


    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //TODO 过滤特定关键词
    val stageTwoRDD = initRDD.filter(line => {
      !ValidateUtils.isContainsSpecWords(line, keywordArr)
    }).distinct
      //提取剧名
      .map(line => {
      //日志的视频名称
      val dim_name = line

      //TODO 通过资源名称获取其他信息
      var dim_title = "#"
      var dim_model = "#"
      var dim_awcid = "#"
      var dim_year = "#"
      var dim_crowd = "#"
      var dim_region = "#"
      var dim_part = "unknow"

      val filmProp = DataCleanCCPlayHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfoArrBV.value)
      if (!filmProp.equals("#")) {
        val filmPropCols = filmProp.split("\t")
        //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
        dim_title = filmPropCols(0)
        dim_model = filmPropCols(1)
        dim_awcid = filmPropCols(2)
        dim_year = filmPropCols(3)
        dim_crowd = filmPropCols(4)
        dim_region = filmPropCols(5)
        dim_part = filmPropCols(6)
      }
      dim_title + "\t" + dim_part + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name
    })

    //TODO 过滤掉不匹配的数据
    //    val stageTwoPassRDD = stageTwoRDD.filter(line => {
    //      val cols = line.split("\t")
    //      !cols(0).equals("#")
    //    })

    // TODO 不匹配的数据统计
    val stageTwoUnPassRDD = stageTwoRDD.
      filter(line => {
        val cols = line.split("\t")
        cols(0).equals("#")
      }).map(line => {
      val cols = line.split("\t")
      val dim_name = cols(7)
      (dim_name, 1)
    }).reduceByKey(_ + _).repartition(1).foreach(println(_))

//      .saveAsTextFile("/user/hdfs/rsync/playunpass/" + currentDate + System.currentTimeMillis())


  }

}package com.avcdata.vbox.clean.coocaa

import com.avcdata.vbox.common.Codearea
import com.avcdata.vbox.util.{ValidateUtils, HBaseUtils, TimeUtils}
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.control.Breaks._

/**
  * @author zhangyongtian
  * @define 酷开终端数据清洗
  * @deprecated 2017-07-05
  */
object DataCleanCCTerminal {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val provinArr = Array[String](
      "河北省", "山西省", "辽宁省", "吉林省", "黑龙江省", "江苏省",
      "浙江省", "安徽省", "福建省", "江西省", "山东省", "河南省",
      "湖北省", "湖南省", "广东省", "海南省", "四川省", "贵州省",
      "云南省", "陕西省", "甘肃省", "青海省", "台湾省",
      "内蒙古自治区", "广西壮族自治区", "西藏自治区", "宁夏回族自治区",
      "新疆维吾尔自治区", "北京市", "天津市", "上海市",
      "重庆市", "香港特别行政区", "澳门特别行政区"
    )

//    val htableName = "tracker_terminal"
    val htableName = "tracker_terminal"
    //    val htableName = "sample_terminal_three"
    println(htableName)

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate)

    //history
//    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/all_deviceinfo_20170801.txt")

    //    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate + "/aowei_deviceinfo" + analysisDate + ".txt")

    //TODO 城市级别映射表
    val hiveContext = new HiveContext(sc)

    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    val pccArrBroadcast = sc.broadcast(pccArr)


    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      var province = cols(1)

      ValidateUtils.isNumber(cols(5)) && cols(5).toInt > 0
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator(htableName)

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = cols(6)

          var province = cols(1)


          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""

          //型号
          val model = cols(3) + "_" + cols(4)
          val size = cols(5)
          val city = cols(2)
          val sn = cols(0)

          //大区
          val area = Codearea.getArea(province)

          //城市级别
          //          val citylevel = Codearea.getCl(city)

//          val arr = Array[Int](1, 2, 3, 4, 5, 6, 7)
//
//          for (i <- 0 until arr.length) {
//            breakable(
//              if (arr(i) == 3) {
//                break
//              }
//            )
//          }


          //TODO 根据映射表获取城市级别
          var citylevel = "港澳台及国外"
          val pccArrValue = pccArrBroadcast.value
          breakable(
            for (i <- 0 until pccArrValue.length) {
              if (city.equals(pccArrValue(i).getString(1))) {
                citylevel = pccArrValue(i).getString(2)
                break
              }
            }
          )
          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )


    //    val sqlContext: HiveContext = new HiveContext(sc)

    //    //加载数据到hive分区表
    //    sqlContext.sql("set hive.exec.dynamic.partition=true")
    //    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //    //重复的不会覆盖
    //    sqlContext.sql("INSERT OVERWRITE table hr.terminal_partition partition(br) SELECT key,sn,brand,last_poweron,area,province,city,citylevel,size,model,license,brand as br FROM hr.terminal ")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.vbox.clean.apk

import com.avcdata.vbox.clean.apk.DataCleanCHApk.{ApkOpen, ApkOpenItem}
import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

import org.apache.spark.util.Utils

/**
  * Created by wxc on 10/27/16.
  * 长虹apk清洗,根据endTime过滤数据
  */

object DataCleanCHApk {

    case class ApkOpen(mac: String, actions: mutable.MutableList[ApkOpenItem]) {
    }

    case class ApkOpenItem(apk: String, duration: String, count: String, date: String)

    case class ApkOpenResultByHour(apk: String, date: String, hour: String, duration: Int, cnt: Int)

    def getApkOpenResultByHour(item: ApkOpen): mutable.MutableList[ApkOpenResultByHour] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        val result = mutable.MutableList[ApkOpenResultByHour]()

        for (i <- 0 until item.actions.size) {
            val duration = item.actions.get(i).get.duration
            val endTime = item.actions.get(i).get.date
            val count = item.actions.get(i).get.count

            val map = DataUtil.durationSplitByHour(duration, endTime)
            val apk = item.actions.get(i).get.apk

            //求每个时段次数的思路：从开始时间到结束时间次数每小时先置为1，不够的补0，，多了的又从开始平铺1，以此类推
            var k: Int = 0
            val len = map.size
            var x: Int = 0
            var y: Int = 0
            if (len == 0) {
                x = 0
                y = 0
            } else {
                x = count.toInt / len
                y = count.toInt % len
            }

            map.keys.foreach { j =>

                if (x == 0) {
                    if (k < y) {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 1)
                    } else {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 0)
                    }
                } else {
                    if (k < y) {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x + 1)
                    } else {
                        result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x)
                    }
                }

                k += 1
            }

        }

        result
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val digitRegex = """^\d+$""".r
        val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimApkCol = Bytes.toBytes("dim_apk")
        val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val factCountCol = Bytes.toBytes("fact_cnt")
        val factDurationCol = Bytes.toBytes("fact_duration")

        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

        //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
        val baseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")
        val fpRdd = baseRdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("""","""")
                cols
            }).filter(x => x(0).length > 8).flatMap(x => {
                val mac = x(0).substring(8, x(0).length)
                val mutableArr = ArrayBuffer[String]()
                for (i <- 1 until x.length)
                    mutableArr += x(i)
                val c = mutableArr.map(y => (mac, y))
                c
            })
        }).filter(x => !x._2.contains("#")).filter(x => {
            val tm = x._2.split("time")
            tm.length > 1 && tm(1).length >= 20 && dateRegex.findFirstMatchIn(tm(1).substring(1, 20)) != None

        }) //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
            .filter(x => x._2.split(';').length == 4)
            .filter(x => {
                val dura = x._2.split(';')(3).split('|')(0)
                dura.length < 25 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None && handlnArr(dura, "=").toLong != 0
            }) //过滤duration后是乱码的数据
            .filter(x => {
                    var date = x._2.split("time")(1).substring(1, 11)
                    date == analysisDate
                }) //date
            .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count
            //.filter(y => y._2.split(';')(2).split("=")(1).toInt != 0) //count != 0

        val sqlc = new HiveContext(sc)
        val apkinfoRdd = sqlc.sql("select * from hr.apkinfo").mapPartitions(items => {
            items.map(line => {
                (line(0).toString, 1)
            })
        }).collect()

        val apkRdd = fpRdd.mapPartitions(items => {
            items.map(str => {
                val mac = str._1 //mac
                val astrs = str._2.split(';')
                val pack = handlnArr(astrs(0), "=") //包名
                val count = handlnArr(astrs(2), "=") //次数
                //println("time : " + astrs(3))
                val duration = handlnArr(astrs(3).split('|')(0), "=") //时长
                val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
                val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

                val xs = new ApkOpenItem(pack, duration, count, time)

                val terminalApk = new ApkOpen(mac, mutable.MutableList(xs))
                //(mac + time.substring(0, 10), terminalApk)
                (duration, pack, terminalApk)
                //terminalApk
            })
        }).filter(x => x._1.toLong > 0).mapPartitions(items => {
            items.map(item =>{
                (item._2, item._3)
            })
        })

        /*val apkinfoRdd1 = sqlc.sql("select * from hr.apkinfo").mapPartitions(items => {
            items.map(line => {
                (line(0).toString, 1)
            })
        })

        val amRdd1 = apkRdd.leftOuterJoin(apkinfoRdd1).filter(x => !x._2.toString().contains("None"))
                .mapPartitions(items =>{
                    items.map(item => {
                        item._2._1
                    })
                })
        println("1 : : " + amRdd1.count())*/

        val apkInfoBrocast = sc.broadcast(apkinfoRdd)
        val amRdd = apkRdd.mapPartitions(items =>{
            items.map(item => {
                apkBro(item, apkInfoBrocast.value)
            })
        })
        //println("2 : : " + amRdd.filter(x => x!= null).count())

        amRdd.filter(x => x!= null).foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact")) //tlog_apk_active_fact

            try {
                items.foreach(item => {

                    val mac = item.mac

                    getApkOpenResultByHour(item).filter(x => x != null).foreach(r => {
                        //hbase key 也就是行
                        val put = new Put(Bytes.toBytes(mac + r.apk + r.date.substring(0, 10) + r.hour + "CH"))
                        //'列族', '列名称:', '值'
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(mac))
                        put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(r.apk))
                        put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(r.date.substring(0, 10)))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
                        put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(r.cnt.toString))
                        put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))
                        //if ((r.hour.toInt) < 10)
                        //println(mac + "\t" + item.date + "\t" + r.apk + "\t"
                        //        + r.date + "\t" + (r.hour.toInt).toString + "\t" + r.cnt + "\t" + r.duration)
                        mutator.mutate(put)
                    })
                    mutator.flush()
                })

            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }

    def apkBro(item: (String, ApkOpen), vu: Array[(String, Int)]) : ApkOpen ={
        var p:ApkOpen = null

        vu.foreach(ele => {
            if (item._1.equals(ele._1)) {
                val apk = item._2
                p = new ApkOpen(apk.mac, apk.actions)
            }
        })

        p
    }

    /**
      * 处理数组
      *
      * @param str
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            "0"
        } else {
            var ele = arr(1)
            if (ele == "" || ele == null) {
                ele = "0"
            }
            ele
        }
    }
}

object test {
    def main(args: Array[String]): Unit = {
        val a=Array(1,2,3,4,5,6)
        for (x <- a) {
            val xs = new ApkOpenItem(x.toString, "1", "2", "3")

            val terminalApk = new ApkOpen("mac", mutable.MutableList(xs))
            println(terminalApk)
        }
    }
}
package com.avcdata.vbox.tmp

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  * 数据维度：分日、周、月；分地区（全国、各省份)；9家APK的终端数、次数和时长等数据，具体指标见下表。
  * 日期维度：7月份，2017.07.01-2017.7.31
  * Apk：银河奇异果、腾讯视频TV端、CIBN环球影视、芒果TV、CIBN微视听、云视听·泰捷、CIBN聚体育、CIBN聚精彩、CIBN悦厅TV
  *
  * @author zhangyongtian
@define 长虹apk新清洗规则统计
  */

object DataCleanCHApkCnt {

//  case class DataCleanCHApkCntResult(
//                                date: String,
//                                dateType: String,
//                                province: String,
//                                appname: String,
//                                acnt: Long,
//                                tcnt: Double,
//                                ucnt: Long
//                              )

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    val hiveContext = new HiveContext(sc)

//    val apkArr = Array[String](
//      "银河·奇异果", "腾讯视频TV端", "CIBN环球影视", "芒果TV", "CIBN微视听", "云视听·泰捷", "CIBN聚体育", "CIBN聚精彩", "CIBN悦厅TV"
//    )

    //用到的表
    //    tracker_apk_fact_ch
    //    key	dim_sn	dim_apk	dim_date	dim_hour	fact_cnt	fact_duration
    //
    //    sample_terminal_three
    //    key	sn	brand	last_poweron	area	province	city	citylevel	size	model	license

    //TODO 日-分省
    val dailyProvinceDF = hiveContext.sql(
      """
        select
        	t.province as province,
        	afc.dim_date as date,
        	ai.appname as appname,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,afc.dim_date,ai.appname
      """.stripMargin)

    dailyProvinceDF.registerTempTable("daily_tmp")

    //TODO 日-全国
    val dailyNationDF = hiveContext.sql(
      """
       select
        	'全国' as province,
        	date,
        	appname,
        	'daily' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM daily_tmp dt
        GROUP BY date,appname

      """.stripMargin)
    /////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 周-分省
    val weeklyProvinceDF = hiveContext.sql(
      """
        select
        	t.province as province,
        	weekofyear(afc.dim_date) as date,
        	ai.appname as appname,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,weekofyear(afc.dim_date),ai.appname
      """.stripMargin)

    weeklyProvinceDF.registerTempTable("weekly_tmp")


    //TODO 周-全国
    val weeklyNationDF = hiveContext.sql(
      """
       select
        	'全国' as province,
        	date,
        	appname,
        	'weekly' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM weekly_tmp dt
        GROUP BY date,appname

      """.stripMargin)


    ////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 月-分省
    val monthlyProvinceDF = hiveContext.sql(
      """
        select
        	t.province as province,
        	month(afc.dim_date) as date,
        	ai.appname as appname,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where ai.appname in ( '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', '芒果TV', 'CIBN微视听', '云视听·泰捷', 'CIBN聚体育', 'CIBN聚精彩', 'CIBN悦厅TV')
        and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,month(afc.dim_date),ai.appname
      """.stripMargin)

    monthlyProvinceDF.registerTempTable("monthly_tmp")


    //TODO 月-全国
    val monthlyNationDF = hiveContext.sql(
      """
       select
        	'全国' as province,
        	dt.date,
        	appname,
        	'monthly' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM monthly_tmp dt
        GROUP BY dt.date,appname

      """.stripMargin)



    val allDF = dailyProvinceDF.unionAll(dailyNationDF).unionAll(weeklyProvinceDF).unionAll(weeklyNationDF).unionAll(monthlyProvinceDF).unionAll(monthlyNationDF)

//      .rdd.saveAsTextFile("/tmp/DataCleanCHApkCnt")

    JdbcUtils.writeDF2Mysql(sc, allDF, Helper.mysqlConf, "vboxDB", "DataCleanCHApkCnt", true,
      SaveMode.Append)




    ///////////////////////////////end of methond/////////////////////////////////////////
  }


}
package com.avcdata.vbox.test

import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * 规则1： 应用日志清洗：对每天每个应用汇总后的次数时长进行异常数据过滤
  *
  * 规则2：汇总后的时长次数，有效数据标准： sum_count > 0 AND sum_count <= 10 AND sum_time > 0  AND sum_time <= 8，即次数在1-10次之间并且时长0-8小时之间
  *
  * @author zhangyongtian
@define 长虹apk清洗 ,根据endTime过滤数据
  */

object DataCleanCHApkForCH {

  case class ApkOpen(sn: String, actions: mutable.ArrayBuffer[ApkOpenItem]) {
  }

  case class ApkOpenItem(apk: String, duration: String, count: String, time: String)

  case class ApkOpenResultByHour(apk: String, time: String, hour: String, duration: Int, cnt: Int)


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

    val apkOpen = ApkOpen("18:99:f5:97:73:90", ArrayBuffer(ApkOpenItem("中国互联网电视", "24957", "2", "2017-07-01 12:09:00")))
    getApkOpenResultByHour(apkOpen).foreach(println(_))

  }

  def run(sc: SparkContext, analysisDate: String) = {
    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //TODO 原始日志格式
    //    {"Mac":"00:00:00:00:00:00","Data":["reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=Android系统;version=5.1.1-3.0.12;count=1;duration=415|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=银河·奇异果;version=6.3;count=5;duration=9575999|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.93;count=2;duration=3052434|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=atvMain;version=1.1.1;count=2;duration=83173|time:2017-06-12 15:32:27"]}	1


    //TODO 读取文件

    //////////////////////test//////////////////////
    //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
    //////////////////////test//////////////////////
    val initRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")

    val filteredRDD =
      initRdd
        .map(line => {
          val cols = line.split("""","""")
          cols
        })
        .filter(x => x(0).length > 8)

        //TODO 转换
        .flatMap(x => {
        val mac = x(0).substring(8, x(0).length)
        val mutableArr = ArrayBuffer[String]()
        for (i <- 1 until x.length)
          mutableArr += x(i)
        mutableArr.map(y => (mac, y))
      })
        .filter(x => !x._2.contains("#")).filter(x => {
        val tm = x._2.split("time")
        tm.length > 1 && tm(1).length >= 20 && dateRegex.findFirstMatchIn(tm(1).substring(1, 20)) != None

      }) //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
        .filter(x => x._2.split(';').length == 4)
        .filter(x => {
          val dura = x._2.split(';')(3).split('|')(0)
          //        dura.length < 25 && handlnArr(dura, "=").toLong != 0 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None
          dura.length < 25 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None && handlnArr(dura, "=").toLong !=
            0
        }) //过滤duration后是乱码的数据
        .filter(x => {
        var date = x._2.split("time")(1).substring(1, 11)
        date == analysisDate
      }) //date
        .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count
    //.filter(y => y._2.split(';')(2).split("=")(1).toInt != 0) //count != 0

    val hiveContext = new HiveContext(sc)
    val apkinfoRdd = hiveContext.sql("select distinct packagename from hr.apkinfo").collect()
    val apkInfoBrocast = sc.broadcast(apkinfoRdd)


    //    (00:00:00:00:00:00,reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.117;count=1;duration=0|time:2017-06-12 12:30:03	)

    val rdd01 =
      filteredRDD
        .map(str => {
          val sn = str._1 //mac
          val astrs = str._2.split(';')
          val apk = handlnArr(astrs(0), "=") //包名
          val cnt = handlnArr(astrs(2), "=").toLong //次数
          //println("time : " + astrs(3))
          val duration = handlnArr(astrs(3).split('|')(0), "=").toLong //时长
          val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
          val date = time.substring(0, 10)
          //          val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

          //        new ApkOpen(mac, mutable.ArrayBuffer(new ApkOpenItem(pack, duration, count, time)))
          //(mac + time.substring(0, 10), terminalApk)
          //(pack, terminalApk)
          //terminalApk

          ((sn, date, apk), (time, duration.toString, cnt.toString, duration, cnt))
        })


    val rdd02 = rdd01.reduceByKey((pre, post) => {
      ((pre._1 + "#" + post._1), (pre._2 + "#" + post._2), (pre._3 + "#" + post._3), (pre._4 + post._4), (pre._5 + post._5))
    })

      .filter(line => {
        val isApkInfo = apkInfoBrocast.value.map(_.getString(0)).contains(line._1._3)
        val durations = line._2._4
        val cnts = line._2._5
        //TODO 新规则
        val duraIsRight = (durations > 0 && durations <= (8 * 3600 * 1000))
        val cntIsRight = (cnts > 0 && cnts <= 10)

        isApkInfo && duraIsRight && cntIsRight
      })
      .map(line => {
        val sn = line._1._1
        val date = line._1._2
        val apk = line._1._3

        val timeArr = line._2._1.split("#")
        val durationArr = line._2._2.split("#")
        val cntArr = line._2._3.split("#")

        val actions = new mutable.ArrayBuffer[ApkOpenItem]()

        for (i <- 0 until timeArr.length) {
          actions.+=(ApkOpenItem(apk: String, durationArr(i): String, cntArr(i): String, timeArr(i): String))
        }

        ApkOpen(sn: String, actions: mutable.ArrayBuffer[ApkOpenItem])
      })

    //          .saveAsTextFile("/tmp/DataCleanCHApkForCH" + System.currentTimeMillis())


    //TODO 写入HBase
    rdd02.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_fact_ch")) //tlog_apk_active_fact

      try {
        items.foreach(item => {

          val mac = item.sn

          getApkOpenResultByHour(item).filter(x => x != null).foreach(r => {
            //hbase key 也就是行
            val put = new Put(Bytes.toBytes(mac + r.apk + r.time.substring(0, 10) + r.hour + "CH"))
            //'列族', '列名称:', '值'
            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(mac))
            put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(r.apk))
            put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(r.time.substring(0, 10)))
            put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
            put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(r.cnt.toString))
            put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))
            //if ((r.hour.toInt) < 10)
            //println(mac + "\t" + item.date + "\t" + r.apk + "\t"
            //        + r.date + "\t" + (r.hour.toInt).toString + "\t" + r.cnt + "\t" + r.duration)
            mutator.mutate(put)
          })
          mutator.flush()
        })

      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })


    ///////////////////////////////end of methond/////////////////////////////////////////
  }


  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      "0"
    } else {
      var ele = arr(1)
      if (ele == "" || ele == null) {
        ele = "0"
      }
      ele
    }
  }

  def getApkOpenResultByHour(item: ApkOpen): mutable.MutableList[ApkOpenResultByHour] = {

    //    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    val result = mutable.MutableList[ApkOpenResultByHour]()

    for (i <- 0 until item.actions.size) {
      val duration = item.actions(i).duration
      val endTime = item.actions(i).time
      val count = item.actions(i).count

      val map = DataUtil.durationSplitByHour(duration, endTime)
      val apk = item.actions(i).apk

      //求每个时段次数的思路：从开始时间到结束时间次数每小时先置为1，不够的补0，，多了的又从开始平铺1，以此类推
      var k: Int = 0
      val len = map.size
      var x: Int = 0
      var y: Int = 0
      if (len == 0) {
        x = 0
        y = 0
      } else {
        x = count.toInt / len
        y = count.toInt % len
      }

      map.keys.foreach { j =>

        if (x == 0) {
          if (k < y) {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 1)
          } else {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 0)
          }
        } else {
          if (k < y) {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x + 1)
          } else {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x)
          }
        }

        k += 1
      }

    }

    result
  }

}
package com.avcdata.vbox.test

import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * 规则1： 应用日志清洗：对每天每个应用汇总后的次数时长进行异常数据过滤
  *
  * 规则2：汇总后的时长次数，有效数据标准： sum_count > 0 AND sum_count <= 10 AND sum_time > 0  AND sum_time <= 8，即次数在1-10次之间并且时长0-8小时之间
  *
  * @author zhangyongtian
@define 长虹apk清洗 ,根据endTime过滤数据
  */

object DataCleanCHApkForCH011 {

  case class ApkOpen(mac: String, actions: mutable.MutableList[ApkOpenItem]) {
  }

  case class ApkOpenItem(apk: String, duration: String, count: String, date: String)

  case class ApkOpenTest1ResultByHour(apk: String, date: String, hour: String, duration: Int, cnt: Int)


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {
    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //TODO 原始日志格式
    //    {"Mac":"00:00:00:00:00:00","Data":["reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=Android系统;version=5.1.1-3.0.12;count=1;duration=415|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=银河·奇异果;version=6.3;count=5;duration=9575999|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.93;count=2;duration=3052434|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=atvMain;version=1.1.1;count=2;duration=83173|time:2017-06-12 15:32:27"]}	1


    //TODO 读取文件

    //////////////////////test//////////////////////
    //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
    //////////////////////test//////////////////////
    val initRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")



    val filteredRDD = initRdd

      //TODO 过滤mac
      .filter(line => {
      //按照","分隔
      val cols = line.split("""","""")
      cols(0).length > 8
    })

      //TODO 提取
      .flatMap(line => {
      val cols = line.split("""","""")
      val mac = cols(0).substring(8).trim
      val apkArr = ArrayBuffer[String]()
      for (i <- 1 until cols.length)
        apkArr += cols(i)
      apkArr.map((mac, _))
    })
      //(sn,apkArr)


      //TODo 过滤apk信息中的异常数据
      .filter(line => {
      val apkInfo = line._2
      val cols = apkInfo.split("time")

      val isRight01 = !apkInfo.contains("#")
      val isRight02 = cols.length > 1
      val isRight03 = cols(1).length >= 20
      val isRight04 = !"""^\d+$""".r.findFirstMatchIn(cols(1).substring(1, 20)).isEmpty

      //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
      val isRight05 = apkInfo.split(';').length == 4

      ////过滤duration后是乱码的数据
      val dura = apkInfo.split(';')(3).split('|')(0)
      val isRight06 = dura.length < 25 && !digitRegex.findFirstMatchIn(handlnArr(dura, "=")).isEmpty && handlnArr(dura, "=").toLong != 0

      val date = apkInfo.split("time")(1).substring(1, 11)
      val isRight07 = (date == analysisDate)


      val isRight08 = (digitRegex.findFirstMatchIn(apkInfo.split(';')(2).split("=")(1)) != None)


      val astrs = apkInfo.split(';')
      val count = handlnArr(astrs(2), "=").toLong //次数
      val duration = handlnArr(astrs(3).split('|')(0), "=").toLong //时长
      val isRight09 = (count > 0 && count <= 10 && duration > 0)

      isRight01 && isRight02 && isRight03 && isRight04 && isRight05 && isRight06 && isRight07 && isRight08 && isRight09

    })

    val hiveContext = new HiveContext(sc)
    val apkinfoRdd = hiveContext.sql("select distinct packagename from hr.apkinfo")
      .map(line => {
        (line(0).toString, 1)
      })
      .collect()
    val apkInfoBrocast = sc.broadcast(apkinfoRdd)


    //    (00:00:00:00:00:00,reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.117;count=1;duration=0|time:2017-06-12 12:30:03	)

    val resultRDD = filteredRDD.map(str => {
      val mac = str._1 //mac
      val astrs = str._2.split(';')
      val pack = handlnArr(astrs(0), "=") //包名
      val count = handlnArr(astrs(2), "=") //次数
      //println("time : " + astrs(3))
      val duration = handlnArr(astrs(3).split('|')(0), "=") //时长
      val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
      val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

      val xs = new ApkOpenItem(pack, duration, count, time)

      val terminalApk = new ApkOpen(mac, mutable.MutableList(xs))
      //(mac + time.substring(0, 10), terminalApk)
      (pack, terminalApk)
      //terminalApk
    }).map(line => {
      apkBro(line, apkInfoBrocast.value)
    })
      .filter(x => x._2 > 0 && x._2 <= 8)
      .map(x => x._1)
      .filter(x => x != null)


    //TODO 写入HBase
    resultRDD.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_apk_active_fact")) //tlog_apk_active_fact

      try {
        items.foreach(item => {

          val mac = item.mac

          getApkOpenTest1ResultByHour(item).filter(x => x != null).foreach(r => {
            //hbase key 也就是行
            val put = new Put(Bytes.toBytes(mac + r.apk + r.date.substring(0, 10) + r.hour + "CH"))
            //'列族', '列名称:', '值'
            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(mac))
            put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(r.apk))
            put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(r.date.substring(0, 10)))
            put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
            put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(r.cnt.toString))
            put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))
            //if ((r.hour.toInt) < 10)
            //println(mac + "\t" + item.date + "\t" + r.apk + "\t"
            //        + r.date + "\t" + (r.hour.toInt).toString + "\t" + r.cnt + "\t" + r.duration)
            mutator.mutate(put)
          })
          mutator.flush()
        })

      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })



    ///////////////////////////////end of methond/////////////////////////////////////////
  }


  def apkBro(item: (String, ApkOpen), vu: Array[(String, Int)]): (ApkOpen, Int) = {
    var p: ApkOpen = null
    var count = 0

    vu.foreach(ele => {
      if (item._1.equals(ele._1)) {
        p = new ApkOpen(item._2.mac, item._2.actions)
      }
    })
    if (p != null) {
      for (i <- 0 until p.actions.size) {
        count += p.actions.get(i).get.count.toInt
      }
    }

    (p, count)
  }

  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      "0"
    } else {
      var ele = arr(1)
      if (ele == "" || ele == null) {
        ele = "0"
      }
      ele
    }
  }

  def getApkOpenTest1ResultByHour(item: ApkOpen): mutable.MutableList[ApkOpenTest1ResultByHour] = {

    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    val result = mutable.MutableList[ApkOpenTest1ResultByHour]()

    for (i <- 0 until item.actions.size) {
      val duration = item.actions.get(i).get.duration
      val endTime = item.actions.get(i).get.date
      val count = item.actions.get(i).get.count

      val map = DataUtil.durationSplitByHour(duration, endTime)
      val apk = item.actions.get(i).get.apk

      //求每个时段次数的思路：从开始时间到结束时间次数每小时先置为1，不够的补0，，多了的又从开始平铺1，以此类推
      var k: Int = 0
      val len = map.size
      var x: Int = 0
      var y: Int = 0
      if (len == 0) {
        x = 0
        y = 0
      } else {
        x = count.toInt / len
        y = count.toInt % len
      }

      map.keys.foreach { j =>

        if (x == 0) {
          if (k < y) {
            result += new ApkOpenTest1ResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 1)
          } else {
            result += new ApkOpenTest1ResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 0)
          }
        } else {
          if (k < y) {
            result += new ApkOpenTest1ResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x + 1)
          } else {
            result += new ApkOpenTest1ResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x)
          }
        }

        k += 1
      }

    }

    result
  }

}
package com.avcdata.vbox.test

import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * 规则1： 应用日志清洗：对每天每个应用汇总后的次数时长进行异常数据过滤
  *
  * 规则2：汇总后的时长次数，有效数据标准： sum_count > 0 AND sum_count <= 10 AND sum_time > 0  AND sum_time <= 8，即次数在1-10次之间并且时长0-8小时之间
  *
  * @author zhangyongtian
@define 长虹apk清洗 ,根据endTime过滤数据
  */

object DataCleanCHApkForCH02 {

  case class ApkOpen(sn: String, actions: mutable.ArrayBuffer[ApkOpenItem]) {
  }

  case class ApkOpenItem(apk: String, duration: String, count: String, time: String)

  case class ApkOpenResultByHour(apk: String, time: String, hour: String, duration: Int, cnt: Int)


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()
//
//    val apkOpen = ApkOpen("18:99:f5:97:73:90",ArrayBuffer(ApkOpenItem("中国互联网电视","24957","2","2017-07-01 12:09:00")))
//    getApkOpenResultByHour(apkOpen).foreach(println(_))

  }

  def run(sc: SparkContext, analysisDate: String) = {
    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //TODO 原始日志格式
    //    {"Mac":"00:00:00:00:00:00","Data":["reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=Android系统;version=5.1.1-3.0.12;count=1;duration=415|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=银河·奇异果;version=6.3;count=5;duration=9575999|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.93;count=2;duration=3052434|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=atvMain;version=1.1.1;count=2;duration=83173|time:2017-06-12 15:32:27"]}	1


    //TODO 读取文件

    //////////////////////test//////////////////////
    //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
    //////////////////////test//////////////////////
    val initRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")

    val filteredRDD =
      initRdd
        .map(line => {
          val cols = line.split("""","""")
          cols
        })
        .filter(x => x(0).length > 8)

        //TODO 转换
        .flatMap(x => {
        val mac = x(0).substring(8, x(0).length)
        val mutableArr = ArrayBuffer[String]()
        for (i <- 1 until x.length)
          mutableArr += x(i)
        mutableArr.map(y => (mac, y))
      })
        .filter(x => !x._2.contains("#")).filter(x => {
        val tm = x._2.split("time")
        tm.length > 1 && tm(1).length >= 20 && dateRegex.findFirstMatchIn(tm(1).substring(1, 20)) != None

      }) //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
        .filter(x => x._2.split(';').length == 4)
        .filter(x => {
          val dura = x._2.split(';')(3).split('|')(0)
          //        dura.length < 25 && handlnArr(dura, "=").toLong != 0 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None
          dura.length < 25 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None && handlnArr(dura, "=").toLong !=
            0
        }) //过滤duration后是乱码的数据
        .filter(x => {
        var date = x._2.split("time")(1).substring(1, 11)
        date == analysisDate
      }) //date
        .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count
    //.filter(y => y._2.split(';')(2).split("=")(1).toInt != 0) //count != 0

//    val hiveContext = new HiveContext(sc)
//    val apkinfoRdd = hiveContext.sql("select distinct packagename from hr.apkinfo").collect()
//    val apkInfoBrocast = sc.broadcast(apkinfoRdd)
//

    //    (00:00:00:00:00:00,reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.117;count=1;duration=0|time:2017-06-12 12:30:03	)

    val rdd01 =
      filteredRDD
        .map(str => {
          val sn = str._1 //mac
          val astrs = str._2.split(';')
          val apk = handlnArr(astrs(0), "=") //包名
          val cnt = handlnArr(astrs(2), "=").toLong //次数
          //println("time : " + astrs(3))
          val duration = handlnArr(astrs(3).split('|')(0), "=").toLong //时长
          val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
          val date = time.substring(0, 10)
          //          val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

          //        new ApkOpen(mac, mutable.ArrayBuffer(new ApkOpenItem(pack, duration, count, time)))
          //(mac + time.substring(0, 10), terminalApk)
          //(pack, terminalApk)
          //terminalApk

          ((sn, date, apk), (time, duration.toString, cnt.toString, duration, cnt))
        })





    //TODO 写入HBase
//    rdd02.foreachPartition(items => {
//      val myConf = HBaseConfiguration.create()
//      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
//      myConf.set("hbase.zookeeper.property.clientPort", "2181")
//      val hbaseConn = ConnectionFactory.createConnection(myConf)
//      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_fact_ch")) //tlog_apk_active_fact
//
//      try {
//        items.foreach(item => {
//
//          val mac = item.sn
//
//          getApkOpenResultByHour(item).filter(x => x != null).foreach(r => {
//            //hbase key 也就是行
//            val put = new Put(Bytes.toBytes(mac + r.apk + r.time.substring(0, 10) + r.hour + "CH"))
//            //'列族', '列名称:', '值'
//            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(mac))
//            put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(r.apk))
//            put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
//            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(r.time.substring(0, 10)))
//            put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
//            put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(r.cnt.toString))
//            put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))
//            //if ((r.hour.toInt) < 10)
//            //println(mac + "\t" + item.date + "\t" + r.apk + "\t"
//            //        + r.date + "\t" + (r.hour.toInt).toString + "\t" + r.cnt + "\t" + r.duration)
//            mutator.mutate(put)
//          })
//          mutator.flush()
//        })
//
//      } finally {
//        mutator.close()
//        hbaseConn.close()
//      }
//    })


    ///////////////////////////////end of methond/////////////////////////////////////////
  }


  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      "0"
    } else {
      var ele = arr(1)
      if (ele == "" || ele == null) {
        ele = "0"
      }
      ele
    }
  }

  def getApkOpenResultByHour(item: ApkOpen): mutable.MutableList[ApkOpenResultByHour] = {

//    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    val result = mutable.MutableList[ApkOpenResultByHour]()

    for (i <- 0 until item.actions.size) {
      val duration = item.actions(i).duration
      val endTime = item.actions(i).time
      val count = item.actions(i).count

      val map = DataUtil.durationSplitByHour(duration, endTime)
      val apk = item.actions(i).apk

      //求每个时段次数的思路：从开始时间到结束时间次数每小时先置为1，不够的补0，，多了的又从开始平铺1，以此类推
      var k: Int = 0
      val len = map.size
      var x: Int = 0
      var y: Int = 0
      if (len == 0) {
        x = 0
        y = 0
      } else {
        x = count.toInt / len
        y = count.toInt % len
      }

      map.keys.foreach { j =>

        if (x == 0) {
          if (k < y) {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 1)
          } else {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 0)
          }
        } else {
          if (k < y) {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x + 1)
          } else {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x)
          }
        }

        k += 1
      }

    }

    result
  }

}
package com.avcdata.vbox.test

import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * 规则1： 应用日志清洗：对每天每个应用汇总后的次数时长进行异常数据过滤
  *
  * 规则2：汇总后的时长次数，有效数据标准： sum_count > 0 AND sum_count <= 10 AND sum_time > 0  AND sum_time <= 8，即次数在1-10次之间并且时长0-8小时之间
  *
  * @author zhangyongtian
@define 长虹apk清洗 ,根据endTime过滤数据
  */

object DataCleanCHApkForCH03 {

  case class ApkOpen(sn: String, actions: mutable.ArrayBuffer[ApkOpenItem]) {
  }

  case class ApkOpenItem(apk: String, duration: String, count: String, time: String)

  case class ApkOpenResultByHour(apk: String, time: String, hour: String, duration: Int, cnt: Int)


  def main(args: Array[String]) {
        val conf = new SparkConf()
          .setMaster("local[1]")
          .setAppName("coocaa-ApkDataLoadJob")
        val sc = new SparkContext(conf)
        run(sc, "2017-06-12")

         sc.stop()

//    val apkOpen = ApkOpen("18:99:f5:97:73:90",ArrayBuffer(ApkOpenItem("中国互联网电视","24957","2","2017-07-01 12:09:00")))
//    getApkOpenResultByHour(apkOpen).foreach(println(_))

  }

  def run(sc: SparkContext, analysisDate: String) = {
    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //TODO 原始日志格式
    //    {"Mac":"00:00:00:00:00:00","Data":["reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=Android系统;version=5.1.1-3.0.12;count=1;duration=415|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=银河·奇异果;version=6.3;count=5;duration=9575999|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.93;count=2;duration=3052434|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=atvMain;version=1.1.1;count=2;duration=83173|time:2017-06-12 15:32:27"]}	1


    //TODO 读取文件

    //////////////////////test//////////////////////
    //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
    //////////////////////test//////////////////////
//    val initRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")
    val initRdd = sc.textFile("E:\\aowei\\tracker-clean\\doc\\usagestatistics.txt")

    val filteredRDD =
      initRdd
        .map(line => {
          val cols = line.split("""","""")
          cols
        })
        .filter(x => x(0).length > 8)

        //TODO 转换
        .flatMap(x => {
        val mac = x(0).substring(8, x(0).length)
        val mutableArr = ArrayBuffer[String]()
        for (i <- 1 until x.length)
          mutableArr += x(i)
        mutableArr.map(y => (mac, y))
      })
        .filter(x => !x._2.contains("#")).filter(x => {
        val tm = x._2.split("time")
        tm.length > 1 && tm(1).length >= 20 && dateRegex.findFirstMatchIn(tm(1).substring(1, 20)) != None

      }) //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
        .filter(x => x._2.split(';').length == 4)
        .filter(x => {
          val dura = x._2.split(';')(3).split('|')(0)
          //        dura.length < 25 && handlnArr(dura, "=").toLong != 0 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None
          dura.length < 25 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None && handlnArr(dura, "=").toLong !=
            0
        }) //过滤duration后是乱码的数据
        .filter(x => {
        var date = x._2.split("time")(1).substring(1, 11)
        date == analysisDate
      }) //date
        .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count
    //.filter(y => y._2.split(';')(2).split("=")(1).toInt != 0) //count != 0

//    val hiveContext = new HiveContext(sc)
//    val apkinfoRdd = hiveContext.sql("select distinct packagename from hr.apkinfo").collect()
//    val apkInfoBrocast = sc.broadcast(apkinfoRdd)


    //    (00:00:00:00:00:00,reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.117;count=1;duration=0|time:2017-06-12 12:30:03	)

    val rdd01 =
      filteredRDD
        .map(str => {
          val sn = str._1 //mac
          val astrs = str._2.split(';')
          val apk = handlnArr(astrs(0), "=") //包名
          val cnt = handlnArr(astrs(2), "=").toLong //次数
          //println("time : " + astrs(3))
          val duration = handlnArr(astrs(3).split('|')(0), "=").toLong //时长
          val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
          val date = time.substring(0, 10)
          //          val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

          //        new ApkOpen(mac, mutable.ArrayBuffer(new ApkOpenItem(pack, duration, count, time)))
          //(mac + time.substring(0, 10), terminalApk)
          //(pack, terminalApk)
          //terminalApk

          ((sn, date, apk), (time, duration.toString, cnt.toString, duration, cnt))
        })

//    rdd01.saveAsTextFile("/tmp/rdd01"+System.currentTimeMillis())


    val rdd02 = rdd01.reduceByKey((pre, post) => {
      ((pre._1 + "#" + post._1), (pre._2 + "#" + post._2), (pre._3 + "#" + post._3), (pre._4 + post._4), (pre._5 + post._5))
    })

      .filter(line => {
//        val isApkInfo = apkInfoBrocast.value.map(_.getString(0)).contains(line._1._3)
        val durations = line._2._4
        val cnts = line._2._5
        //TODO 新规则
        val duraIsRight = (durations > 0 && durations <= (8 * 3600 * 1000))
        val cntIsRight = (cnts > 0 && cnts <= 10)

        println(line._1+"\t"+durations+"\t"+cnts)
//        isApkInfo && duraIsRight && cntIsRight
          duraIsRight && cntIsRight

//        true
      })
      .map(line => {
        val sn = line._1._1
        val date = line._1._2
        val apk = line._1._3

        val timeArr = line._2._1.split("#")
        val durationArr = line._2._2.split("#")
        val cntArr = line._2._3.split("#")

        val actions = new mutable.ArrayBuffer[ApkOpenItem]()

        for (i <- 0 until timeArr.length) {
          actions.+=(ApkOpenItem(apk: String, durationArr(i): String, cntArr(i): String, timeArr(i): String))
        }

        ApkOpen(sn: String, actions: mutable.ArrayBuffer[ApkOpenItem])
      })


//          .saveAsTextFile("/tmp/DataCleanCHApkForCH" + System.currentTimeMillis())

        .foreach(println(_))
    //TODO 写入HBase
//    rdd02.foreachPartition(items => {
//      val myConf = HBaseConfiguration.create()
//      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
//      myConf.set("hbase.zookeeper.property.clientPort", "2181")
//      val hbaseConn = ConnectionFactory.createConnection(myConf)
//      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_fact_ch")) //tlog_apk_active_fact
//
//      try {
//        items.foreach(item => {
//
//          val mac = item.sn
//
//          getApkOpenResultByHour(item).filter(x => x != null).foreach(r => {
//            //hbase key 也就是行
//            val put = new Put(Bytes.toBytes(mac + r.apk + r.time.substring(0, 10) + r.hour + "CH"))
//            //'列族', '列名称:', '值'
//            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(mac))
//            put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(r.apk))
//            put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
//            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(r.time.substring(0, 10)))
//            put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
//            put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(r.cnt.toString))
//            put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))
//            //if ((r.hour.toInt) < 10)
//            //println(mac + "\t" + item.date + "\t" + r.apk + "\t"
//            //        + r.date + "\t" + (r.hour.toInt).toString + "\t" + r.cnt + "\t" + r.duration)
//            mutator.mutate(put)
//          })
//          mutator.flush()
//        })
//
//      } finally {
//        mutator.close()
//        hbaseConn.close()
//      }
//    })


    ///////////////////////////////end of methond/////////////////////////////////////////
  }


  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      "0"
    } else {
      var ele = arr(1)
      if (ele == "" || ele == null) {
        ele = "0"
      }
      ele
    }
  }

  def getApkOpenResultByHour(item: ApkOpen): mutable.MutableList[ApkOpenResultByHour] = {

//    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    val result = mutable.MutableList[ApkOpenResultByHour]()

    for (i <- 0 until item.actions.size) {
      val duration = item.actions(i).duration
      val endTime = item.actions(i).time
      val count = item.actions(i).count

      val map = DataUtil.durationSplitByHour(duration, endTime)
      val apk = item.actions(i).apk

      //求每个时段次数的思路：从开始时间到结束时间次数每小时先置为1，不够的补0，，多了的又从开始平铺1，以此类推
      var k: Int = 0
      val len = map.size
      var x: Int = 0
      var y: Int = 0
      if (len == 0) {
        x = 0
        y = 0
      } else {
        x = count.toInt / len
        y = count.toInt % len
      }

      map.keys.foreach { j =>

        if (x == 0) {
          if (k < y) {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 1)
          } else {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, 0)
          }
        } else {
          if (k < y) {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x + 1)
          } else {
            result += new ApkOpenResultByHour(apk, endTime, j, (map(j).toLong / 1000).toString.toInt, x)
          }
        }

        k += 1
      }

    }

    result
  }

}
package com.avcdata.vbox.tmp

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * 规则1： 应用日志清洗：对每天每个应用汇总后的次数时长进行异常数据过滤
  *
  * 规则2：汇总后的时长次数，有效数据标准： sum_count > 0 AND sum_count <= 10 AND sum_time > 0  AND sum_time <= 8，即次数在1-10次之间并且时长0-8小时之间
  *
  * @author zhangyongtian
@define 长虹apk清洗 ,根据endTime过滤数据
  */

object DataCleanCHApkForCH_GetPackage {

  case class ApkOpen(mac: String, actions: mutable.MutableList[ApkOpenItem]) {
  }

  case class ApkOpenItem(apk: String, duration: String, count: String, date: String)

  case class ApkOpenTest1ResultByHour(apk: String, date: String, hour: String, duration: Int, cnt: Int)


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //TODO 原始日志格式
    //    {"Mac":"00:00:00:00:00:00","Data":["reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=Android系统;version=5.1.1-3.0.12;count=1;duration=415|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=银河·奇异果;version=6.3;count=5;duration=9575999|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=主场景;version=2.93;count=2;duration=3052434|time:2017-06-12 15:32:27","reportType:action|saveType:append|sort:systemInfo|subClass:usagestatistics|reportInfo:package=atvMain;version=1.1.1;count=2;duration=83173|time:2017-06-12 15:32:27"]}	1


    //TODO 读取文件

    //////////////////////test//////////////////////
    //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
    //////////////////////test//////////////////////

    //    val baseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/usagestatistics.txt")
    val baseRdd = sc.textFile("/user/hdfs/rsync/CH/2017-07*/usagestatistics.txt")

    val fpRdd = baseRdd.mapPartitions(items => {
      items.map(line => {
        val cols = line.split("""","""")
        cols
      }).filter(x => x(0).length > 8).flatMap(x => {
        val mac = x(0).substring(8, x(0).length)
        val mutableArr = ArrayBuffer[String]()
        for (i <- 1 until x.length)
          mutableArr += x(i)
        val c = mutableArr.map(y => (mac, y))
        c
      })
    }).filter(x => !x._2.contains("#")).filter(x => {
      val tm = x._2.split("time")
      tm.length > 1 && tm(1).length >= 20 && dateRegex.findFirstMatchIn(tm(1).substring(1, 20)) != None

    }) //有些记录time后面是空的 && 有些记录time后面不是标准的时间,比如只有2016&&2017-0M8&��~12:35 乱码
      .filter(x => x._2.split(';').length == 4)
      .filter(x => {
        val dura = x._2.split(';')(3).split('|')(0)
        dura.length < 25 && digitRegex.findFirstMatchIn(handlnArr(dura, "=")) != None && handlnArr(dura, "=").toLong !=
          0
      }) //过滤duration后是乱码的数据
      .filter(x => {
      var date = x._2.split("time")(1).substring(1, 11)
      date == analysisDate
    }) //date
      .filter(x => digitRegex.findFirstMatchIn(x._2.split(';')(2).split("=")(1)) != None) //count
    //.filter(y => y._2.split(';')(2).split("=")(1).toInt != 0) //count != 0


    fpRdd.mapPartitions(items => {
      items.map(str => {
        //        val mac = str._1 //mac
        val astrs = str._2.split(';')
        val pack = handlnArr(astrs(0), "=") //包名
        //        val count = handlnArr(astrs(2), "=") //次数
        //        //println("time : " + astrs(3))
        //        val duration = handlnArr(astrs(3).split('|')(0), "=") //时长
        //        val time = handlnArr(astrs(3).split('|')(1), "e:").substring(0, 19) //日志时间
        //        val hour = handlnArr(astrs(3).split('|')(1), "e:").substring(11, 13) //小时

        //        val xs = new apk.ApkOpenItem(pack, duration, count, time)
        //
        //        val terminalApk = new apk.ApkOpen(mac, mutable.MutableList(xs))
        //(mac + time.substring(0, 10), terminalApk)
        pack
        //terminalApk
      })
    }).distinct.saveAsTextFile("/tmp/ch_apk/" + analysisDate)

    //    val hiveContext = new HiveContext(sc)

    //    hiveContext.sql(
    //      """
    //        |SELECT t.date, 'CH' AS brand, appname, COUNT(DISTINCT t.dim_sn), SUM(t.dura)
    //        |	, SUM(t.u_cnt)
    //        |FROM (SELECT tp.date, tp.dim_sn, ai.appname, SUM(tp.fact_duration) / 3600 AS dura, SUM(tp.fact_cnt) AS u_cnt
    //        |	FROM hr.tracker_apk_fact_partition tp
    //        |		JOIN hr.apkinfo ai ON tp.dim_apk = ai.packagename
    //        |	WHERE date IN (
    //        |			'2017-07-16',
    //        |			'2017-07-18'
    //        |		AND key LIKE '%CH'
    //        |	GROUP BY tp.date, ai.appname, tp.dim_sn
    //        |	) t
    //        |WHERE t.u_cnt > 0
    //        |	AND t.u_cnt <= 10
    //        |	AND t.dura > 0
    //        |	AND t.dura <= 8
    //        |GROUP BY t.date, appname
    //      """.stripMargin)


    /**
      * DROP TABLE `hr`.`apkpack2name`;
      * CREATE TABLE `hr`.`apkpack2name` (packagename string,appname string)
      * ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
      * WITH SERDEPROPERTIES (
      * "separatorChar" = ",",
      * "quoteChar"     = "'",
      * "escapeChar"    = "\\"
      * )
      * STORED AS TEXTFILE;
      * load data  inpath '/user/hdfs/rsync/apkpack2name.csv' into table apkpack2name;
      */


    ///////////////////////////////end of methond/////////////////////////////////////////
  }

  /**
    * 处理数组
    *
    * @param str
    * @param split 分隔符
    * @return
    */
  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      "0"
    } else {
      var ele = arr(1)
      if (ele == "" || ele == null) {
        ele = "0"
      }
      ele
    }
  }
}
package com.avcdata.vbox.tmp

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  *数据维度：分日、周、月；分地区（全国、各省份)；ott端的终端数、次数、时长、每终端时长、开机率等指标数据，具体指标见下表

  * 日期维度：7月份，2017.07.01-2017.7.31
  *
  * @author zhangyongtian
@define 长虹apk新清洗规则统计
  */

object DataCleanCHApkOTTCnt {

//  case class DataCleanCHApkCntResult(
//                                date: String,
//                                dateType: String,
//                                province: String,
//                                appname: String,
//                                acnt: Long,
//                                tcnt: Double,
//                                ucnt: Long
//                              )

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-06-12")
    //
    //     sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String) = {

    val hiveContext = new HiveContext(sc)

//    val apkArr = Array[String](
//      "银河·奇异果", "云视听极光", "CIBN环球影视", "芒果TV", "CIBN微视听", "云视听·泰捷", "CIBN聚体育", "CIBN聚精彩", "CIBN悦厅TV"
//    )

    //用到的表
    //    tracker_apk_fact_ch
    //    key	dim_sn	dim_apk	dim_date	dim_hour	fact_cnt	fact_duration
    //
    //    sample_terminal_three
    //    key	sn	brand	last_poweron	area	province	city	citylevel	size	model	license


    val all_acnt_arr = hiveContext.sql("select count(distinct sn) from hr.sample_terminal_three where key like '%CH%'")
      .collect

    val all_acnt = all_acnt_arr(0).getLong(0).toString

    //TODO 日-分省
    val dailyProvinceDF = hiveContext.sql(
      s"""
        select
          '$all_acnt' as all_acnt,
          'OTT端' as t_name,
        	t.province as province,
        	afc.dim_date as date,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where  ai.onelevel = '视频'
          and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,afc.dim_date
      """.stripMargin)

    dailyProvinceDF.registerTempTable("daily_tmp")

    //TODO 日-全国
    val dailyNationDF = hiveContext.sql(
      s"""
         select
           '$all_acnt' as all_acnt,
          'OTT端' as t_name,
        	'全国' as province,
        	date,
        	'daily' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM daily_tmp dt
        GROUP BY date

      """.stripMargin)
    /////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 周-分省
    val weeklyProvinceDF = hiveContext.sql(
      s"""
         select
           '$all_acnt' as all_acnt,
          'OTT端' as t_name,
        	t.province as province,
        	weekofyear(afc.dim_date) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
        where  ai.onelevel = '视频'
          and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,weekofyear(afc.dim_date)
      """.stripMargin)

    weeklyProvinceDF.registerTempTable("weekly_tmp")


    //TODO 周-全国
    val weeklyNationDF = hiveContext.sql(
      s"""
         select
           '$all_acnt' as all_acnt,
         'OTT端' as t_name,
        	'全国' as province,
        	date,
        	'weekly' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM weekly_tmp dt
        GROUP BY date

      """.stripMargin)


    ////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 月-分省
    val monthlyProvinceDF = hiveContext.sql(
      s"""
         select
          '$all_acnt' as all_acnt,
         'OTT端' as t_name,
        	t.province as province,
        	month(afc.dim_date) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.fact_duration)/3600 as tcnt,
        	sum(afc.fact_cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_apk_fact_ch afc
        on
        (t.sn=afc.dim_sn)
        join hr.apkinfo ai
        on
        (afc.dim_apk = ai.packagename)
          where  ai.onelevel = '视频'
          and t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,month(afc.dim_date)
      """.stripMargin)

    monthlyProvinceDF.registerTempTable("monthly_tmp")


    //TODO 月-全国
    val monthlyNationDF = hiveContext.sql(
      s"""
         select
         '$all_acnt' as all_acnt,
         'OTT端' as t_name,
        	'全国' as province,
        	date,
        	'monthly' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM monthly_tmp dt
        GROUP BY date

      """.stripMargin)



    val allDF = dailyProvinceDF.unionAll(dailyNationDF).unionAll(weeklyProvinceDF).unionAll(weeklyNationDF).unionAll(monthlyProvinceDF).unionAll(monthlyNationDF)

//      .rdd.saveAsTextFile("/tmp/DataCleanCHApkCnt")

    JdbcUtils.writeDF2Mysql(sc, allDF, Helper.mysqlConf, "vboxDB", "DataCleanCHApkOTTCnt", true,
      SaveMode.Append)




    ///////////////////////////////end of methond/////////////////////////////////////////
  }


}
package com.avcdata.vbox.clean.live

import com.avcdata.vbox.util.{DataUtil, TimeUtils}
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

import scala.collection.mutable

/*
  *长虹直播的清洗
  *
  * 2017-08-29 00:00:00,1000,ChangHong,null,cctv7,CCTV-7军事农业,11,null,HDMI,447544026,124.88.66.130,1.0,18:99:f5:53:6d:59,18:99:F5:36:42:48,89fef0a25391486b00e1a43bcd700127d09c1a3e,CH-ZLM65HiS2-DTV-00-ICNTV,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,HDMI,445544944,123.232.157.191,1.0,00:6c:fd:69:ca:49,00:6C:FD:A0:09:03,af07ae0bd71c9d48f403c95f803eccfc5117c9a3,CH-ZLM65HiS2-DTV-00-ICNTV,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,AV,414769599,223.73.176.176,1.0,98:2f:3c:8a:f8:1c,98:2F:3C:90:B7:0A,0a460443ba39b1d8e67a32893fe5a5edc691d934,CH-ZLM60HiS-DTV-00-MG,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,AV,457711600,36.99.96.144,1.0,18:99:f5:5d:b0:c7,18:99:F5:70:38:DE,c02231a5fe8d29294767f807cf76a82b8a85c5b5,CH-ZLM65HiS2-DTV-00-ICNTV,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,HDMI,407070689,175.5.235.4,1.0,64:88:ff:f7:20:96,64:88:FF:F5:B6:5D,2a971ca0683ec0c9568733316f8d0db6e8d122a8,CH-ZLM50HiS-DTV-3D-MG,null,null


  */
object DataCleanCHLive {

  case class TVWatching(sn: String, date: String, area: String, actions: mutable.MutableList[TVWatchedItem]) {
  }

  case class TVWatchedItem(tv: String, date: String, startTime: String, endTime: String)

  case class TVWatchedResultByMinute(tv: String, date: String, startTime: String, endTime: String,
                                     hour: String, minute: String, duration: Int, cnt: Int)


  def main(args: Array[String]) {

    //    val arr = Array(3, 1, 9, 2, 8, 3, 4, 7, 10, 22)
    //
    //    scala.util.Sorting.stableSort(arr)

    //
    //    for (i <- 0 until arr.length) {
    //      println(arr(i))
    //    }

    //        val conf = new SparkConf()
    //          .setMaster("local[1]")
    //          .setAppName("DataCleanCHLive")
    //        val sc = new SparkContext(conf)
    //        run(sc, "2017-08-28")
    //
    //        sc.stop()

  }

  /**
    * 按照长虹的清洗规则 将HH:mm:ss数组转换成时间段数组
    */
  def getTimeRangArrByTimes(timeArr: Array[String]): Array[(String, String)] = {

    val timeStampArr = timeArr.map(x => TimeUtils.convertDateStr2TimeStamp(x, "HH:mm:ss"))

    //        for (i <- 0 until timeStampArr.length) {
    //          println(timeStampArr(i))
    //        }


    scala.util.Sorting.stableSort(timeStampArr)
    val timeRangArr = new scala.collection.mutable.ArrayBuffer[(String, String)]
    var startTime = "0"
    var endTime = "0"

    //用户行为超过间隔10分钟内的累加
    for (i <- 0 until timeStampArr.length - 1) {
      if (i == 0) {
        startTime = timeStampArr(0).toString
      }
      val diff = (timeStampArr(i + 1) - timeStampArr(i)) / 1000 / 60
      if (diff <= 10) {
        endTime = timeStampArr(i + 1).toString
      } else {
        if (endTime.toLong > startTime.toLong)
          timeRangArr.+=((TimeUtils.convertTimeStamp2DateStr(startTime.toLong, "HH:mm:ss"), TimeUtils.convertTimeStamp2DateStr
          (endTime.toLong, "HH:mm:ss")))
        //间隔超过10分钟 算新记录
        startTime = timeStampArr(i + 1).toString
      }
      if (i == (timeStampArr.length - 2) && endTime.toLong > startTime.toLong) {
        timeRangArr.+=((TimeUtils.convertTimeStamp2DateStr(startTime.toLong, "HH:mm:ss"), TimeUtils.convertTimeStamp2DateStr
        (endTime.toLong, "HH:mm:ss")))
      }
    }
    timeRangArr
      .filter(!_._2.equals("0"))
      .distinct.toArray
  }

  def getTVWatchedResultByMinute(item: TVWatching): mutable.MutableList[TVWatchedResultByMinute] = {

    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    val result = mutable.MutableList[TVWatchedResultByMinute]()

    for (i <- 0 until item.actions.size) {
      val tv = item.actions.get(i).get.tv
      val date = item.actions.get(i).get.date
      val startTime = item.actions.get(i).get.startTime
      val endTime = item.actions.get(i).get.endTime
      val list = DataUtil.durationSplitByMinute(startTime, endTime)
      for (l <- list) {
        var hour = l._1
        var min = l._2._1
        var dura = l._2._2.split(";")(0)
        var cnt = l._2._2.split(";")(1)
        result += new TVWatchedResultByMinute(tv, date, startTime, endTime,
          hour, min, dura.toInt, cnt.toInt)
      }
    }

    result
  }

  def run(sc: SparkContext, analysisDate: String) = {

    val channelBc = sc.broadcast(CHTVMapping.mapping)
    //    println("2017-08-29 00:00:00".length)

    //////////////////////test///////////////////////
    //    val initRDD = sc.textFile("E:\\aowei\\tracker-clean\\doc\\livebroadcast.txt")
    //////////////////////test///////////////////////


    val initRDD = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/livebroadcast.txt")

      //TODO 过滤
      .filter(line => {
      val cols = line.split(",")

      val starTime = cols(0).trim
      val channel = cols(5).trim
      val sn = cols(12).trim

      val startTimeIsRight = starTime.length > 17 && starTime.length <= 20

      val channelIsRight = channelBc.value.keySet.toArray.contains(channel)

//      val snIsRight = (sn.length.toInt == 17)
      val snIsRight = (sn.length.toInt >= 12)

      val brandIsRight = cols(2).trim.equals("ChangHong")

      startTimeIsRight && channelIsRight && snIsRight && brandIsRight

    }).distinct
      //TODO 提取
      .map(line => {
      val cols = line.split(",")
      val starTime = cols(0).trim
      val channel = channelBc.value.get(cols(5).trim).get
      val sn = cols(12).trim

      val date = starTime.substring(0, 10)
      val time = starTime.substring(11)

      (sn + "\t" + date + "\t" + channel, time)
    })


    //TODO 排序 计算时间段
    //过滤 用户行为只有一条 或 新记录只有一条的数据
    val tvRdd = initRDD.reduceByKey((pre, post) => {
      pre + "#" + post
    }).flatMap(line => {
      val res = new scala.collection.mutable.ArrayBuffer[(String, (String, String))]()
      val timeRangArr = getTimeRangArrByTimes(line._2.split("#"))
      for (ele <- timeRangArr) {
        res.+=((line._1, ele))
      }
      res
    }).map(line => {
      val left_cols = line._1.split("\t")
      val sn = left_cols(0)
      val date = left_cols(1)
      val channel = left_cols(2)

      // "2017-08-27 07:08:38"
      val startTime = date + " " + line._2._1
      val endTime = date + " " + line._2._2

      val area = "unknow"

      val xs = new TVWatchedItem(channel, date, startTime, endTime)
      val terminalTV = new TVWatching(sn, date, area, mutable.MutableList(xs))
      //(sn + date, terminalTV)
      terminalTV
    })
    //      .foreach(println(_))


    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimChannelCol = Bytes.toBytes("dim_channel")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val dimMinCol = Bytes.toBytes("dim_min")
    val factCntCol = Bytes.toBytes("fact_cnt")
    val factTimeLenghtCol = Bytes.toBytes("fact_time_length")

    val htableName = "tracker_live_active_fact03"

    println("htableName-->"+htableName)

//    tvRdd.take(1000).foreach(println)

    tvRdd.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
//      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_live_fact"))
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf(htableName))
      //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_active_fact03"))
      //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_active_fact02"))
      //val table = hbaseConn.getTable(TableName.valueOf("tracker_live_active_fact"))


      try {
        items.foreach(item => {

          val sn = item.sn

          getTVWatchedResultByMinute(item).foreach(r => {
            val put = new Put(Bytes.toBytes(sn + r.tv + item.date + r.hour + r.minute + "CH"))
            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
            put.addColumn(dimFamilyCol, dimChannelCol, Bytes.toBytes(r.tv))
            //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
            put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
            put.addColumn(dimFamilyCol, dimMinCol, Bytes.toBytes((r.minute.toInt).toString))
            put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(r.cnt.toString))
            put.addColumn(factFamilyCol, factTimeLenghtCol, Bytes.toBytes(r.duration.toString))

            mutator.mutate(put)
          })
        })
        mutator.flush()
      } finally {
        //file.writer.close()
        mutator.close()
        hbaseConn.close()
      }
    })
  }
}

object CHTVMapping extends Serializable {
  val mapping = mutable.Map(
    ("中央8台" -> "CCTV-8"),
    ("中央12台" -> "CCTV-12"),
    ("中央1台" -> "CCTV-1"),
    ("黑龙江卫视" -> "黑龙江卫视"),
    ("中央9台" -> "CCTV-9"),
    ("中央5台" -> "CCTV-5"),
    ("贵州卫视" -> "贵州卫视"),
    ("中央14台" -> "CCTV-14"),
    ("东方卫视" -> "上海东方卫视"),
    ("上海卫视" -> "上海东方卫视"),
    ("湖北卫视" -> "湖北卫视"),
    ("东南卫视" -> "东南卫视"),
    ("广东卫视" -> "广东卫视"),
    ("中央7台" -> "CCTV-7"),
    ("湖南卫视" -> "湖南卫视"),
    ("山东卫视" -> "山东卫视"),
    ("北京卫视" -> "北京卫视"),
    ("青海卫视" -> "青海卫视"),
    ("中央10台" -> "CCTV-10"),
    ("旅游卫视" -> "旅游卫视"),
    ("甘肃卫视" -> "甘肃卫视"),
    ("重庆卫视" -> "重庆卫视"),
    ("中央3台" -> "CCTV-3"),
    ("新疆卫视" -> "新疆卫视"),
    ("厦门卫视频道" -> "厦门卫视"),
    ("中央13台" -> "CCTV-13"),
    ("辽宁卫视" -> "辽宁卫视"),
    ("山西卫视" -> "山西卫视"),
    ("中央2台" -> "CCTV-2"),
    ("宁夏卫视" -> "宁夏卫视"),
    ("安徽卫视" -> "安徽卫视"),
    ("河北卫视" -> "河北卫视"),
    ("中央6台" -> "CCTV-6"),
    ("浙江卫视" -> "浙江卫视"),
    ("江西卫视" -> "江西卫视"),
    ("河南卫视" -> "河南卫视"),
    ("中央11台" -> "CCTV-11"),
    ("广西卫视" -> "广西卫视"),
    ("江苏卫视" -> "江苏卫视"),
    ("四川卫视" -> "四川卫视"),
    ("中央15台" -> "CCTV-15"),
    ("云南卫视" -> "云南卫视"),
    ("中央4台" -> "CCTV-4"),
    ("内蒙古卫视" -> "内蒙古卫视"),
    ("西藏卫视" -> "西藏卫视"),
    ("深圳卫视" -> "深圳卫视"),
    ("天津卫视" -> "天津卫视"),
    ("陕西卫视" -> "陕西卫视"),
    ("吉林卫视" -> "吉林卫视"),
    ("CCTV-1综合" -> "CCTV-1"),
    ("CCTV-2财经" -> "CCTV-2"),
    ("CCTV-3综艺" -> "CCTV-3"),
    ("CCTV-4中文国际" -> "CCTV-4"),
    ("CCTV-5体育" -> "CCTV-5"),
    ("CCTV-6电影" -> "CCTV-6"),
    ("CCTV-7军事农业" -> "CCTV-7"),
    ("CCTV-8电视剧" -> "CCTV-8"),
    ("CCTV-9纪录" -> "CCTV-9"),
    ("CCTV-10科教" -> "CCTV-10"),
    ("CCTV-11戏曲" -> "CCTV-11"),
    ("CCTV-12社会与法" -> "CCTV-12"),
    ("CCTV-13新闻" -> "CCTV-13"),
    ("CCTV-14少儿" -> "CCTV-14"),
    ("CCTV-15音乐" -> "CCTV-15"),
    ("厦门卫视" -> "厦门卫视"),
    ("CCTV-4中文国际(亚)" -> "CCTV-4")
  )

  def getTVStandName(name: String): String = {
    mapping.get(name).getOrElse("其他")
  }
}
package com.avcdata.vbox.clean.live

import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

import scala.collection.mutable

/*
  *长虹直播的清洗
  */
object DataCleanCHLiveOld {

  case class TVWatching(sn: String, date: String, area: String, actions: mutable.MutableList[TVWatchedItem]) {
  }

  case class TVWatchedItem(tv: String, date: String, startTime: String, endTime: String)

  case class TVWatchedResultByMinute(tv: String, date: String, startTime: String, endTime: String,
                                     hour: String, minute: String, duration: Int, cnt: Int)


  //val file = new FileUtil("live")
  def getTVWatchedResultByMinute(item: TVWatching): mutable.MutableList[TVWatchedResultByMinute] = {

    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    val result = mutable.MutableList[TVWatchedResultByMinute]()

    for (i <- 0 until item.actions.size) {
      val tv = item.actions.get(i).get.tv
      val date = item.actions.get(i).get.date
      val startTime = item.actions.get(i).get.startTime
      val endTime = item.actions.get(i).get.endTime
      val list = DataUtil.durationSplitByMinute(startTime, endTime)
      for (l <- list) {
        var hour = l._1
        var min = l._2._1
        var dura = l._2._2.split(";")(0)
        var cnt = l._2._2.split(";")(1)
        result += new TVWatchedResultByMinute(tv, date, startTime, endTime,
          hour, min, dura.toInt, cnt.toInt)
      }
    }

    result
  }

  def run(sc: SparkContext, analysisDate: String) = {

    //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimChannelCol = Bytes.toBytes("dim_channel")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val dimMinCol = Bytes.toBytes("dim_min")
    val factCntCol = Bytes.toBytes("fact_cnt")
    val factTimeLenghtCol = Bytes.toBytes("fact_time_length")

    //val preDate = DateTime.parse(analysisDate).plus(-1).toString("yyyy-MM-dd")--- -1减1  1不变
    val NextDate = DateTime.parse(analysisDate).plusDays(1).toString("yyyy-MM-dd")
    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
    //println(preDate + ", " + yesBeforeDate)

    //val tvBaseRdd = sc.textFile("F:/avc/docs/changhong/2016-11-14/livebroadcast.txt")
    val tvBaseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/livebroadcast.txt")
      .filter(x => x.split('\t').length == 11)
      .filter(x => {
        var date = x.split('\t')(5).substring(0, 10)
        date == analysisDate || date == NextDate || date == preDate || date == yesBeforeDate
      })
      .filter(x => x.split('\t')(0).length >= 12)

    val tvRdd = tvBaseRdd.mapPartitions(items => {
      items.map(line => {
        val cols = line.split('\t')
        val tv = cols(3) //频道名称
        var sn = cols(0) //sn--mac
        if (sn.length == 12) //aabbccddeeff-->aa:bb:cc:dd:ee:ff
        {
          sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
        }

        val date = cols(5).substring(0, 10) //日期
        val startTime = cols(5) //开始时间
        val endTime = cols(6) //结束时间
        val area = cols(4) //地域省份
        val xs = new TVWatchedItem(tv, date, startTime, endTime)
        val terminalTV = new TVWatching(sn, date, area, mutable.MutableList(xs))
        //(sn + date, terminalTV)
        terminalTV
      })
    })
    /*.reduceByKey((left, right) => {
                left.actions ++= right.actions
                left
            }).mapPartitions(items => {
                items.map(item => {
                    val sortedList = item._2.actions.sortBy(_.date).clone()
                    item._2.actions.clear()
                    item._2.actions ++= sortedList
                    item._2
                })
            })*/

    val count = tvRdd.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_active_fact03"))
      //val table = hbaseConn.getTable(TableName.valueOf("tracker_live_active_fact"))


      try {
        items.foreach(item => {

          val sn = item.sn

          getTVWatchedResultByMinute(item).foreach(r => {
            val put = new Put(Bytes.toBytes(sn + r.tv + item.date + r.hour + r.minute + "CH"))
            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
            put.addColumn(dimFamilyCol, dimChannelCol, Bytes.toBytes(CHTVMapping.getTVStandName(r.tv)))
            //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
            put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
            put.addColumn(dimFamilyCol, dimMinCol, Bytes.toBytes((r.minute.toInt).toString))
            put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(r.cnt.toString))
            put.addColumn(factFamilyCol, factTimeLenghtCol, Bytes.toBytes(r.duration.toString))

            mutator.mutate(put)
          })
        })
        mutator.flush()
      } finally {
        //file.writer.close()
        mutator.close()
        hbaseConn.close()
      }
    })
  }
}

//object CHTVMapping extends Serializable {
//  private val mapping = mutable.Map(
//    ("中央8台" -> "CCTV-8"),
//    ("中央12台" -> "CCTV-12"),
//    ("中央1台" -> "CCTV-1"),
//    ("黑龙江卫视" -> "黑龙江卫视"),
//    ("中央9台" -> "CCTV-9"),
//    ("中央5台" -> "CCTV-5"),
//    ("贵州卫视" -> "贵州卫视"),
//    ("中央14台" -> "CCTV-14"),
//    ("东方卫视" -> "上海东方卫视"),
//    ("上海卫视" -> "上海东方卫视"),
//    ("湖北卫视" -> "湖北卫视"),
//    ("东南卫视" -> "东南卫视"),
//    ("广东卫视" -> "广东卫视"),
//    ("中央7台" -> "CCTV-7"),
//    ("湖南卫视" -> "湖南卫视"),
//    ("山东卫视" -> "山东卫视"),
//    ("北京卫视" -> "北京卫视"),
//    ("青海卫视" -> "青海卫视"),
//    ("中央10台" -> "CCTV-10"),
//    ("旅游卫视" -> "旅游卫视"),
//    ("甘肃卫视" -> "甘肃卫视"),
//    ("重庆卫视" -> "重庆卫视"),
//    ("中央3台" -> "CCTV-3"),
//    ("新疆卫视" -> "新疆卫视"),
//    ("厦门卫视频道" -> "厦门卫视"),
//    ("中央13台" -> "CCTV-13"),
//    ("辽宁卫视" -> "辽宁卫视"),
//    ("山西卫视" -> "山西卫视"),
//    ("中央2台" -> "CCTV-2"),
//    ("宁夏卫视" -> "宁夏卫视"),
//    ("安徽卫视" -> "安徽卫视"),
//    ("河北卫视" -> "河北卫视"),
//    ("中央6台" -> "CCTV-6"),
//    ("浙江卫视" -> "浙江卫视"),
//    ("江西卫视" -> "江西卫视"),
//    ("河南卫视" -> "河南卫视"),
//    ("中央11台" -> "CCTV-11"),
//    ("广西卫视" -> "广西卫视"),
//    ("江苏卫视" -> "江苏卫视"),
//    ("四川卫视" -> "四川卫视"),
//    ("中央15台" -> "CCTV-15"),
//    ("云南卫视" -> "云南卫视"),
//    ("中央4台" -> "CCTV-4"),
//    ("内蒙古卫视" -> "内蒙古卫视"),
//    ("西藏卫视" -> "西藏卫视"),
//    ("深圳卫视" -> "深圳卫视"),
//    ("天津卫视" -> "天津卫视"),
//    ("陕西卫视" -> "陕西卫视"),
//    ("吉林卫视" -> "吉林卫视"),
//    ("CCTV-1综合" -> "CCTV-1"),
//    ("CCTV-2财经" -> "CCTV-2"),
//    ("CCTV-3综艺" -> "CCTV-3"),
//    ("CCTV-4中文国际" -> "CCTV-4"),
//    ("CCTV-5体育" -> "CCTV-5"),
//    ("CCTV-6电影" -> "CCTV-6"),
//    ("CCTV-7军事农业" -> "CCTV-7"),
//    ("CCTV-8电视剧" -> "CCTV-8"),
//    ("CCTV-9纪录" -> "CCTV-9"),
//    ("CCTV-10科教" -> "CCTV-10"),
//    ("CCTV-11戏曲" -> "CCTV-11"),
//    ("CCTV-12社会与法" -> "CCTV-12"),
//    ("CCTV-13新闻" -> "CCTV-13"),
//    ("CCTV-14少儿" -> "CCTV-14"),
//    ("CCTV-15音乐" -> "CCTV-15"),
//    ("厦门卫视" -> "厦门卫视"),
//    ("CCTV-4中文国际(亚)" -> "CCTV-4")
//  )
//
//  def getTVStandName(name: String): String = {
//    mapping.get(name).getOrElse("其他")
//  }
//}
package com.avcdata.vbox.clean.terminal

import com.avcdata.vbox.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/*
  *长虹直播终端信息的清洗
  *
  * 2017-08-29 00:00:00,1000,ChangHong,null,cctv7,CCTV-7军事农业,11,null,HDMI,447544026,124.88.66.130,1.0,18:99:f5:53:6d:59,18:99:F5:36:42:48,89fef0a25391486b00e1a43bcd700127d09c1a3e,CH-ZLM65HiS2-DTV-00-ICNTV,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,HDMI,445544944,123.232.157.191,1.0,00:6c:fd:69:ca:49,00:6C:FD:A0:09:03,af07ae0bd71c9d48f403c95f803eccfc5117c9a3,CH-ZLM65HiS2-DTV-00-ICNTV,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,AV,414769599,223.73.176.176,1.0,98:2f:3c:8a:f8:1c,98:2F:3C:90:B7:0A,0a460443ba39b1d8e67a32893fe5a5edc691d934,CH-ZLM60HiS-DTV-00-MG,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,AV,457711600,36.99.96.144,1.0,18:99:f5:5d:b0:c7,18:99:F5:70:38:DE,c02231a5fe8d29294767f807cf76a82b8a85c5b5,CH-ZLM65HiS2-DTV-00-ICNTV,null,null
2017-08-29 00:00:00,3000,ChangHong,null,unknow,unknow,-11,null,HDMI,407070689,175.5.235.4,1.0,64:88:ff:f7:20:96,64:88:FF:F5:B6:5D,2a971ca0683ec0c9568733316f8d0db6e8d122a8,CH-ZLM50HiS-DTV-3D-MG,null,null


  */
object DataCleanCHLiveTerminal {


  def main(args: Array[String]) {

    //    val arr = Array(3, 1, 9, 2, 8, 3, 4, 7, 10, 22)
    //
    //    scala.util.Sorting.stableSort(arr)

    //
    //    for (i <- 0 until arr.length) {
    //      println(arr(i))
    //    }

    //        val conf = new SparkConf()
    //          .setMaster("local[1]")
    //          .setAppName("DataCleanCHLive")
    //        val sc = new SparkContext(conf)
    //        run(sc, "2017-08-28")
    //
    //        sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {


    //////////////////////test///////////////////////
    //    val initRDD = sc.textFile("E:\\aowei\\tracker-clean\\doc\\livebroadcast.txt")
    //////////////////////test///////////////////////


    val liveRDD = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/livebroadcast.txt")
      .filter(line => {
        val cols = line.split(",")
        val sn = cols(12).trim
        sn.length >= 12
      })
      //TODO 提取
      .map(line => {
      val cols = line.split(",")
      val sn = cols(12).trim
      sn
    }).distinct().map((_, 1))


    val hiveContext = new HiveContext(sc)
    val teRdd = hiveContext.sql("select sn, province, city,license from hr.terminal where brand = 'CH'").map(item => {
      val sn = item(0).toString
      val province = item(1).toString
      val city = item(2).toString
      val license = item(3)

      val area = Codearea.getArea(province) //大区
      (sn, area + "\t" + province + "\t" + city + "\t" + license)
    })



    val resultRDD = liveRDD.join(teRdd).filter(x => !x._2.toString().contains("None"))

    val dimFamilyCol = Bytes.toBytes("terminalProperty")
    val dimSeriesNoCol = Bytes.toBytes("sn")
    val dimBrandCol = Bytes.toBytes("brand")
    val dimLicenseCol = Bytes.toBytes("license")
    val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
    val dimAreaCol = Bytes.toBytes("area")
    val dimProvinceCol = Bytes.toBytes("province")
    val dimCityCol = Bytes.toBytes("city")
    val dimCitylevelCol = Bytes.toBytes("citylevel")
    val dimSizeCol = Bytes.toBytes("size")
    val dimModelCol = Bytes.toBytes("model")

    resultRDD.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_terminal"))

      try {
        items.foreach(item => {
          val sn = item._1
          val brand = "CH"
          val model = ""
          val lastPowerOn = ""
          val size = ""
          val cols = item._2._2.split("\t")
          val area = cols(0)
          val province = cols(1)
          val city = cols(2)
          val license = cols(3)
          var clevel = ""
          if (Codearea.prolist.contains(province)) {
            clevel = Codearea.getCl(city)
          } else {
            clevel = "港澳台及国外"
          }
          println(area + "\t" + province + "\t" + city + "\t" + license)

          val put = new Put(Bytes.toBytes(sn + "CH"))
          put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
          put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes(license))
          put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
          put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
          put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
          put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
          put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
          put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
          put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
          put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))

          mutator.mutate(put)
        })
        mutator.flush()
      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })


  }


}
package com.avcdata.vbox.clean.play

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types.{StringType, StructField, StructType}

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * 长虹到剧的清洗
  */

object DataCleanCHPlay {

    case class Playing(vid: String,awcid:String,categorys:String,title: String, mac: String, date: String, actions: mutable.MutableList[PlayerItem]) {
    }

    case class PlayerItem(name: String, action: String, time: String, title: String, tt: String)

    case class PlayerResultByHour(title: String, part: String, hour: String, duration: Int, vv: Int, start: String, end: String)

    val digitRegex = """^\d+$""".r
    val dateRegex = """^(?:(?!0000)[0-9]{4}(?:(?:0[1-9]|1[0-2])(?:0[1-9]|1[0-9]|2[0-8])|(?:0[13-9]|1[0-2])-(?:29|30)|(?:0[13578]|1[02])-31)|(?:[0-9]{2}(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)-02-29)$""".r
    val dateRegex1 = """^(?:(?!0000)[0-9]{4})$""".r

    /**
      * 获取name和title中相同字符的个数
      *
      * @param str1 name
      * @param str2 title
      * @return
      */
    def getSameLen(str1: String, str2: String): (Int, Int) = {
        var len = 0
        for (i <- 0 until str1.length) {
            //极限挑战之皇家宝藏--极限挑战
            if (i == str2.length) {
                return (len, 1)
            }
            if (str1.charAt(i) == str2.charAt(i)) {
                len += 1
            } else {
                return (len, 0)
            }
        }

        (len, 0)
    }

    /**
      * 根据处理一些特点不明显的title获取part
      *
      * @param title
      * @return
      */
    def getPartByTitle(title: String, name: String): String = {
        var p = ""
        var part = ""

        for (i <- 0 until title.length) {
            if (digitRegex.findFirstMatchIn(title.charAt(i).toString) != None) {
                p = p + title.charAt(i).toString
            } else if (digitRegex.findFirstMatchIn(title.charAt(i).toString) == None && p != ""
              && title.charAt(i) == ' ' || title.charAt(i).toString.equals("集") || title.charAt(i) == '_'
              || title.charAt(i) == '期' || title.charAt(i) == ':' || title.charAt(i) == '：') {

                return part
            } else if (dateRegex1.findFirstMatchIn(part) != None && p != ""){
                //小小智慧树/[SD]小小智慧树2011年11月第2期
                if (title.contains(part + "年")) {
                    part = ""
                    return part
                }
            } else if (p != "" && ((name.length+i) <= (title.length - i)) && title.substring(i, name.length+i).equals(name)) {
                //超级飞侠大百科/24超级飞侠大百科仙人掌为什么会有刺0826
                part = p
                return part
            } else if (name.contains(p)) {
                return ""
            }
        }

        part = p
        if (dateRegex1.findFirstMatchIn(part) != None) {
            if (title.contains(part + "年")) {
                //println("year")
            }
        } else if (part.length > 8) {
            part = ""
        } else if (name.contains(part)) {
            part = ""
        }

        part
    }

    /**
      * 获取集数
      *
      * @param title
      * @param name
      * @return
      */
    def getPart(title: String, name: String): (String, String) = {
        var part = ""
        var ti = name + "/" + title

        if (title.contains("先导集") || title.contains("先导集上") || title.contains("先导集下")) {
            return (ti, "1")
        }

        //"name":"仲夏夜魔法","title":"仲夏夜魔法"
        if (title.equals(name)) {
            part = ""
        } else {
            val same = getSameLen(name, title)
            val sameLen = same._1
            val flag = same._2

            if (sameLen >= 2 && flag == 1) {
                //"name":"极限挑战之皇家宝藏","title":"极限挑战"
                part = ""

            } else {
                if (title.contains("_")) {
                    //"name":"超级育儿师","title":"[SD]超级育儿师20131220_"
                    if (title.split("_").length < 2 || title.contains("集锦") || title.contains("录像") || title.contains("花絮")
                      || title.contains("得分") || title.contains("扣篮") || title.contains("助攻")
                      || title.contains("原声")) {
                        return (ti, "")
                    }
                    //"name":"大仙衙门","title":"大仙衙门_01"
                    //"name":"大仙衙门","title":"大仙衙门_1"
                    else if (digitRegex.findFirstMatchIn(title.split("_")(1).substring(0, title.split("_")(1).length)) != None) {
                        part = title.split("_")(1)
                    } else if (title.contains("集")) {
                        val parts1 = title.substring(title.indexOf("集") - 2, title.indexOf("集"))
                        val parts2 = title.substring(title.indexOf("集") - 1, title.indexOf("集"))
                        //"name":"小马宝莉友谊的魔力第4季","title":"小马宝莉友谊的魔力第4季第10集：云宝落幕"
                        if (digitRegex.findFirstMatchIn(parts1) != None) {
                            part = parts1
                        }
                        //"name":"小马宝莉友谊的魔力第4季","title":"小马宝莉友谊的魔力第4季第4集：无畏天马的真相"
                        else if (digitRegex.findFirstMatchIn(parts2) != None
                          && digitRegex.findFirstMatchIn(parts1) == None) {
                            part = parts2
                        }
                    } else {
                        part = getPartByTitle(title, name)

                    }
                } else if (title.contains("集")) {
                    if (title.split("集")(0).length == 0) {
                        return (ti, "")
                    }

                    val dipart = title.split("集")(0).charAt(title.split("集")(0).length-1).toString
                    if (digitRegex.findFirstMatchIn(dipart) != None) {
                        val parts1 = title.substring(title.indexOf("集") - 2, title.indexOf("集"))
                        val parts2 = title.substring(title.indexOf("集") - 1, title.indexOf("集"))
                        //"name":"小马宝莉友谊的魔力 第4季","title":"小马宝莉友谊的魔力第4季第10集：云宝落幕"
                        if (digitRegex.findFirstMatchIn(parts1) != None) {
                            part = parts1
                        }
                        //"name":"小马宝莉友谊的魔力 第4季","title":"小马宝莉友谊的魔力第4季第4集：无畏天马的真相"
                        else if (digitRegex.findFirstMatchIn(parts2) != None
                          && digitRegex.findFirstMatchIn(parts1) == None) {
                            part = parts2
                        }
                    } else {
                        part = ""
                    }
                } else if (title.contains("：")) {
                    if (title.split("：").length < 2) {
                        return (ti, "")
                    }

                    if (digitRegex.findFirstMatchIn(title.split("：")(1).substring(0, title.split("：")(1).length)) != None) {
                        part = title.split("：")(1)
                    }
                    //拍客纪实/《拍客纪实》第14期：男子月入3000寄回家 穴居山洞对家人称住宾馆
                    //
                    else {
                        part = getPartByTitle(title.split("：")(0), name)
                    }

                } else {
                    //"name":"勇士115-98快船","title":"【集锦】勇士115-98快船 库里19分7抢断格里芬上演惊天隔扣"
                    if (title.contains("集锦") || title.contains("录像") || title.contains("花絮")
                      || title.contains("得分") || title.contains("扣篮") || title.contains("助攻")
                      || title.contains("原声")) {
                        return (ti, "")
                    }

                    part = getPartByTitle(title, name)
                }
            }
        }

        //年份的处理
        if (dateRegex.findFirstMatchIn(part) != None) {
            part = ""
        } else if (dateRegex1.findFirstMatchIn(part) != None) {
            if (title.contains(part + "年"))
                part = ""
        }

        (ti, part)
    }

    /**
      * 中文数据的处理
      *
      * @param title 标题
      * @return
      */
    def numProcess(title : String) : String = {
        var pn = title
        val filterStr = Array("一季","二季","三季","四季","五季","六季","七季","八季","九季")
        val numStr = Array("1季","2季","3季","4季","5季","6季","7季","8季","9季")
        var i = 0
        for (i<- 0 to filterStr.length-1) {
            if (title.contains(filterStr(i))) {
                pn = title.replace(filterStr(i), numStr(i))

                return pn
            }
        }

        pn
    }

    /**
      *第*季的处理
      *
      * @param str name or title
      * @return
      */
    def seasonProcess(str: String) : String = {

        for (i <- 0 until str.length-2) {
            if (str.charAt(i).toString.equals("第") && str.charAt(i+2).toString.equals("季")) {
                return str.charAt(i+1).toString
            }
        }

        null
    }

    /**
      *第*季的处理
      *
      * @param str name or title
      * @return
      */
    def seasonProcess1(str: String) : String = {

        for (i <- 0 until str.length-2) {
            if (str.charAt(i).toString.equals("第") && str.charAt(i+2).toString.equals("季")) {
                return str.substring(0, str.indexOf(str.charAt(i+2).toString)+1)
            }
        }

        str
    }

    /**
      * 过滤掉花絮
      *
      * @param title 剧集的集
      * @return
      */
    def filterProcess(title: String) : Int = {
        val filterStr = Array("剧透","预告","抢先看","片花","片段","花絮","插曲","片尾曲","主题曲",
            "广场舞","图书","独家策划","原创","搞笑","海报","将映","剧照")
        for (fs <- filterStr) {
            if (title.contains(fs)) {
                return 0
            }
        }

        1
    }

    /**
      * 剧名和具体剧集中的季度要对应上
      *
      * @param title 剧集的集
      * @param na 剧集名称
      * @return
      */
    def filterProcess1(title: String, na: String) : Int = {
        var ti = title
        var name = na

        if (ti.contains("一") || ti.contains("二") || ti.contains("三") || ti.contains("四") || ti.contains("五")
          || ti.contains("六") || ti.contains("七") || ti.contains("八") || ti.contains("九")) {
            ti = numProcess(ti)
        }

        if (name.contains("一") || name.contains("二") || name.contains("三") || name.contains("四") || name.contains("五")
          || name.contains("六") || name.contains("七") || name.contains("八") || name.contains("九")) {
            name = numProcess(name)
        }

        var tiNum = seasonProcess(ti)
        var nameNum = seasonProcess(name)

        if(tiNum!=null && nameNum != null) {

            if (tiNum.equals(nameNum)){
                return 1
            } else {
                return 0
            }
        }

        1
    }

    /**
      * 版本处理
      *
      * @param name 名称
      * @return
      */
    def versionProcess(name: String) : String = {
        var pn = name
        val filterStr = Array("未删减版", "[未删减版]", "完整版", "全集",
            "合集", "完全版", "[TV版]", "精华版", "（国语）","乐高版", " 精华版", "精华版",
            "（国语版）", "国语版", "国语中字", "国语", "（英语版）","Q版","（一月）","二月）","（三月）","（四月）","（五月）","（六月）",
            "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "日播版","（七月）","（八月）","（九月）","（十月）",
            "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "粤语", "（日语版）","日语版", "日语","（十一月）","（十二月）",
            "TV中文版", "（中文版）", "中文版", "[韩语版]", "韩语中字","韩语版", "四川话版", "云南话版", "东北话版",
            "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版",
            "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版",
            "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版",
            "（4K）", "VR版", "（VR）", "【3D版】", "（新3D版）", "3D版", "（3D）", "3D", "标清版", "_标清",
            "（蓝光真高清）", "蓝光真高清", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版",
            "纯享版", "精简版", "（加长版）", "加长版", "（加长重映版）", "精编版", "重制版", "双语字幕版", "字幕版",
            "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "（免费版）", " （免费版）","[免费版]", "免费版", "（原声）",
            "原声高清版", "英文原声高清版", "原声", "生肖特别版", "圣诞特别版", "特别版", "完全版", " (环绕声版)", "(环绕声版)", "环绕声版"
        )

        for (fs <- filterStr) {
            if (name.contains(fs)) {
                pn = name.replace(fs, "")

                return pn
            }
        }

        pn
    }

    /**
      * 获取每个小时的数据
      *
      * @param item
      * @return
      */
    def getPlayerResultByHour(item: Playing): mutable.MutableList[PlayerResultByHour] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var startItem: PlayerItem = null
        var startTime: DateTime = null
        var quitItem: PlayerItem = null
        var quitTime: DateTime = null
        var tempItem: PlayerItem = null
        var tempTime: DateTime = null
        var result = mutable.MutableList[PlayerResultByHour]()
        var preResult: PlayerResultByHour = null
        var flag = 0
        var vvSameHour = 0
        var dura = 0

        for (i <- 0 until item.actions.size) {

            val title = item.actions.get(i).get.title
            val name = item.actions.get(i).get.name
            val pt = getPart(title, name)
            if (startItem == null) {
                startItem = item.actions.get(i).get
                startTime = DateTime.parse(startItem.time, format)
            }
            else {
                quitItem = item.actions.get(i).get
                quitTime = DateTime.parse(quitItem.time, format)

                if (startItem.action.equals("Start") && quitItem.action.equals("Quit")
                  && startItem.title.equals(quitItem.title)) {
                    val duration = DataUtil.dateTotimestamp(quitItem.time) - DataUtil.dateTotimestamp(startItem.time)
                    val map = DataUtil.durationSplitByHour(duration.toString, quitItem.time)
                    map.keys.foreach { x =>
                        //当前的记录是否和之前的最后一个记录在同一个小时段,而且是同一个节目
                        if (map.size == 1 && preResult != null) {
                            if (preResult.title.equals(pt._1) && preResult.part.equals(pt._2) &&
                              preResult.hour.equals(x)) {
                                vvSameHour += 1
                                dura = dura + (map(x).toLong / 1000).toString.toInt
                                result += new PlayerResultByHour(pt._1, pt._2, x, dura, vvSameHour, startItem.time, quitItem.time)
                            }
                        } else {
                            if (flag == 0) {
                                if ((map(x).toLong / 1000).toString.toInt != 0) {
                                    result += new PlayerResultByHour(pt._1, pt._2, x, (map(x).toLong / 1000).toString.toInt, 1, startItem.time, quitItem.time)
                                }
                                vvSameHour += 1
                                flag = 1
                            } else {
                                if ((map(x).toLong / 1000).toString.toInt != 0) {
                                    result += new PlayerResultByHour(pt._1, pt._2, x, (map(x).toLong / 1000).toString.toInt, 0, startItem.time, quitItem.time)
                                }
                            }

                            if (map.size == 1 && result.length != 0) {
                                preResult = result.last //记住最后一个的数据
                                dura = preResult.duration
                            } else {
                                vvSameHour = 0
                                dura = 0
                            }
                        }
                    }

                    flag = 0
                    startItem = null
                } else {
                    flag = 0
                    startItem = null
                }
            }
        }

        vvSameHour = 0

        result
    }

    /**
      * 获取title标准名称
      *
      * @param title 标题
      * @return
      */
    def getStartandTitle(title: String, categorys: String) : String = {
        var ti = ""

        if (title.contains("【") && title.contains("】") && categorys.equals("动画片")) {
            ti = title.substring(title.indexOf("【")+1, title.indexOf(("】")))
        }
        //小马宝莉友谊的魔力第6季第2集 --小马宝莉友谊的魔力第6季 第2集
        else if (title.contains("第") && title.contains("集") && title.contains("季")) {
            ti = title.split("第")(0)+"第"+title.split("第")(1)
            if (ti != "" && ti.charAt(ti.length - 1).equals(" ")) {
                ti = title.substring(0, title.length - 2)
            }
        } else if (title.contains("第") && title.contains("集") && !title.contains("季")) {
            //小马宝莉友谊的魔力 第2集
            ti = title.split("第")(0)
            if (ti != "" && ti.charAt(ti.length - 1).equals(" ")) {
                ti = title.substring(0, title.length - 2)
            }
        } else if (title.contains("_")) {
            ti = title.split("_")(0)
            if (ti.contains("[SD]") || ti.contains("[HD]")) {
                ti = ti.substring(ti.indexOf("D]")+2, ti.length)
            }

            if (digitRegex.findFirstMatchIn(ti) != None) {
                ti = title.split("_")(1)
            } else if (ti != "" && ti.charAt(ti.length - 1).equals(" ")) {
                ti = title.substring(0, title.length - 2)
            } else if (digitRegex.findFirstMatchIn(ti) != None) {
                ti = title.split("_")(0)
            }
        } else if (title.contains(" ")) {
            val strs = title.split(" ")
            if(strs.length < 2) {
                ti = title.replaceAll(" ", "")
            } else if (digitRegex.findFirstMatchIn(strs(1)) != None) {
                ti = title.split(" ")(0)
            }
        } else {
            for (i <- 0 until title.length) {
                if (digitRegex.findFirstMatchIn(title.charAt(i).toString) == None) {
                    ti = ti + title.charAt(i).toString
                }
            }
            ti = title
        }

        //星际宝贝：神奇大冒险第二季_10
        if (ti.contains("：")) {
            ti = title.split("：")(0)
        }

        //去除空格和，！· 。 一些不需要的字符
        ti = ti.replaceAll(" ", "")
        ti = ti.replaceAll("，", "")
        ti = ti.replaceAll("！", "")
        ti = ti.replaceAll("·", "")
        ti = ti.replaceAll("。", "")
        ti = ti.replaceAll("\\(", "")
        ti = ti.replaceAll("\\)", "")
        ti = versionProcess(ti)

        if (ti.contains("[SD]") || ti.contains("[HD]")) {
            ti = ti.substring(ti.indexOf("D]")+2, ti.length)
        }

        if (ti.contains("《") && ti.contains("》")) {
            ti = ti.substring(ti.indexOf("《")+1, ti.indexOf(("》")))
        }

        if (ti.contains("一") || ti.contains("二") || ti.contains("三") || ti.contains("四") || ti.contains("五")
          || ti.contains("六") || ti.contains("七") || ti.contains("八") || ti.contains("九")) {
            ti = numProcess(ti)
        }

        /*if (ti.contains("季") && ti.contains("第") && ((ti.lastIndexOf("季") - ti.lastIndexOf("第")) == 2)){
            ti = ti.substring(0, ti.lastIndexOf("第")) + ti.charAt(ti.lastIndexOf("季")-1)
        }*/
        ti = seasonProcess1(ti)

        ti
    }

    /**
      * 获取name标准名称
      *
      * @param name 标题
      * @return
      */
    def getStartandName(name: String) : String = {
        var nm = ""

        nm = name

        if (nm.contains("：")) {
            nm = nm.split("：")(0)
        }

        //去除空格和，！· 。 一些不需要的字符
        nm = nm.replaceAll(" ", "")
        nm = nm.replaceAll("，", "")
        nm = nm.replaceAll("！", "")
        nm = nm.replaceAll("·", "")
        nm = nm.replaceAll("。", "")
        nm = nm.replaceAll("\\(", "")
        nm = nm.replaceAll("\\)", "")
        nm = versionProcess(nm)

        if (nm.contains("《") && nm.contains("》")) {
            nm = nm.substring(nm.indexOf("《")+1, nm.indexOf(("》")))
        }

        if (nm.contains("一") || nm.contains("二") || nm.contains("三") || nm.contains("四") || nm.contains("五")
          || nm.contains("六") || nm.contains("七") || nm.contains("八") || nm.contains("九")) {
            nm = numProcess(nm)
        }

        /*if (nm.contains("季") && nm.contains("第") && ((nm.lastIndexOf("季") - nm.lastIndexOf("第")) == 2)){
            nm = nm.substring(0, nm.lastIndexOf("第"))+nm.charAt(nm.lastIndexOf("季")-1)
        }*/
        nm = seasonProcess1(nm)

        nm
    }

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimNameCol = Bytes.toBytes("dim_name")
        val dimTitleCol = Bytes.toBytes("dim_title")
        val dimAwcidCol = Bytes.toBytes("dim_awcid")
        val dimPartCol = Bytes.toBytes("dim_part")
        val dimYearCol = Bytes.toBytes("dim_year")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val dimModelCol = Bytes.toBytes("dim_model")
        val dimCrowdCol = Bytes.toBytes("dim_crowd")
        val dimRegionCol = Bytes.toBytes("dim_region")
        val factVvCol = Bytes.toBytes("fact_vv")
        val factDurationCol = Bytes.toBytes("fact_duration")


        //val filmTextRdd = sc.textFile("D:/film.txt")
        val filmTextRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/film.txt")
        val filmRdd = filmTextRdd.mapPartitions(items => {
            items.map(line => {
                val strs = line.split("""},""")
                strs
            })
        }).flatMap(x => {
            val mutableArr = ArrayBuffer[(String, String)]()
            for (i <- 0 until x.length) {
                val strs = x(i).split(""","""")
                if (strs.length == 7) {
                    val name = strs(1).split("""":"""")(1).substring(0, strs(1).split("""":"""")(1).indexOf("""""""))
                    val title = strs(2).split("""":"""")(1).substring(0, strs(2).split("""":"""")(1).indexOf("""""""))
                    val vid = strs(3).split("""":"""")(1).substring(0, strs(3).split("""":"""")(1).indexOf("""""""))
                    val categorys = strs(6).split("""":"""")(1).substring(0, strs(6).split("""":"""")(1).indexOf("""""""))
                    mutableArr.append((vid, title + "0x01" + name + "0x01" + categorys))
                }
            }
            val c = mutableArr.map(y => (y._1, y._2))
            c
        }).filter(x => filterProcess(x._2.split("0x01")(0)) == 1)
          .filter(x => filterProcess1(x._2.split("0x01")(0), x._2.split("0x01")(1)) == 1)


        //val tencentRdd = sc.textFile("D:/tencent.txt")
        val tencentRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/tencent.txt")

        val tenRdd = tencentRdd.mapPartitions(items => {
            items.map(line => {
                val cols = line.split("""","""")
                cols
            }).filter(x => x(0).split("""":"""").length > 1).flatMap(x => {
                val mac = x(0).split("""":"""")(1).substring(0, x(0).split("""":"""")(1).length)
                val mutableArr = ArrayBuffer[String]()
                for (i <- 1 until x.length)
                    mutableArr += x(i)
                val c = mutableArr.map(y => (mac, y))
                c
            })
        }).filter(x => x._2.split(";").length == 5)
          .filter(x => x._2.split("time:").length > 1)
          .filter(x => x._2.split("time:")(1).length >= 19)
          .filter(x => x._2.contains("vid"))
          .filter(x => x._2.split("time:")(1).substring(0, 10) == analysisDate)

        val ten = tenRdd.mapPartitions(items => {
            items.map(line => {
                val mac = line._1
                val astrs = line._2.split(";")
                val vid = handlnArr(astrs(2), "=")
                val action = handlnArr(astrs(3), "=") //action
                val time = astrs(4).split("time:")(1).substring(0, 19) //time

                (vid, mac + "," + action + "," + time)
            })
        }).distinct()

        //得到时间 action等
        val djRdd = ten.leftOuterJoin(filmRdd).filter(x => x._2.toString().split("0x01").length == 3)

        //存储Tencent和film没有匹配上的数据
        val unMatchedRdd = ten.leftOuterJoin(filmRdd).filter(x => x._2.toString().split("0x01").length != 3)
        //unMatchedRdd.map(x=>(x._1, 1)).reduceByKey(_+_).saveAsTextFile("hdfs:///user/hdfs/player/tenfiml-" + analysisDate + ".txt")

        val tfRdd = djRdd.mapPartitions(items => {
            items.map(item => {
                val mac = item._2._1.split(",")(0)
                val action = item._2._1.split(",")(1)
                val time = item._2._1.split(",")(2)
                val title = item._2._2.get.split("0x01")(0)
                var name = item._2._2.get.split("0x01")(1)

                var categorys = item._2._2.get.split("0x01")(2)
                if (categorys.equals("动漫")) {
                    categorys = "动画片"
                }

                //处理时间相同的Start和Quit，把Start放在前面
                var ac = ""
                if(action.equals("Start")) {
                    ac = "cStart"
                } else if (action.equals("Quit")) {
                    ac = "Quit"
                }
                val xs = new PlayerItem(name, action, time, title, name+title+time+ac)

                val film = new Playing(item._1, "", categorys, title, mac, time.substring(0, 10), mutable.MutableList(xs))


                (name+"0x01"+categorys, film)
            })
        })


        //内容库
        val sqlc = new HiveContext(sc)
//        val fRdd = sc.textFile("D:/data/ID-20170711.csv")
//        val schemaString = "id,original_name,standard_name,year,model,crowd,region"
//        val schema = StructType (
//            schemaString.split(",").map(fieldName => StructField(fieldName,StringType,true))
//        )
//        val rowRDD = fRdd.map(_.split('|')).map(p => Row(p(0).trim,p(1),p(2),p(3),p(4),p(5),p(6)))
//        val resultDF=sqlc.createDataFrame(rowRDD,schema)
//        resultDF.registerTempTable("film_properties")


        val hiveDataFrame = sqlc.sql("select * from hr.film_properties " +
          " where  id is not null and original_name is not null and  standard_name is not null and  year is not null and  region is not null and crowd is not null and  model is not null").cache()
        val tagRdd = hiveDataFrame.mapPartitions(items =>
        {
            items.map(item => {
                val awcid = item(0).toString
                val originalName = item(1).toString
                val standardName = item(2).toString
                var year = item(3).toString
                val model = item(4).toString
                val crowd = item(5).toString
                var region = item(6).toString

                (originalName+"0x01"+model, awcid + "0x01" + year + "0x01" + model + "0x01" + crowd + "0x01" + region + "0x01" + standardName)
            })
        })

        val ttRdd = tfRdd.leftOuterJoin(tagRdd)

        //1、先匹配name + origin
        val noReRdd = ttRdd.filter(x => !x._2.toString().contains("None"))

        //2、然后匹配name + standard
        val nsBaseRdd = ttRdd.filter(x => x._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  val categorys = item._1.split("0x01")(1)
                  val film = item._2._1
                  var name = item._1.split("0x01")(0)
                  name = getStartandName(name)

                  (name+"0x01"+categorys, item._2._1)
              })
          })

        //内容库的标准名称
        val sdnRdd = hiveDataFrame.mapPartitions(items =>
        {
            items.map(item => {
                val awcid = item(0).toString
                val originalName = item(1).toString
                val standardName = item(2).toString
                val year = item(3).toString
                val model = item(4).toString
                val crowd = item(5).toString
                val region = item(6).toString

                (standardName+"0x01"+model, awcid + "0x01" + year + "0x01" + model + "0x01" + crowd + "0x01" + region + "0x01" + standardName)
            })
        })

        //
        val nsRdd = nsBaseRdd.leftOuterJoin(sdnRdd)
        val nsReRdd = nsRdd.filter(x => !x._2.toString().contains("None"))

        //3、name 匹配不上的就去重新匹配title+originalname
        val titleOriRdd = nsRdd.filter(x => x._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  val categorys = item._1.split("0x01")(1)
                  val film = item._2._1
                  var title = item._2._1.title
                  title = getStartandTitle(title, categorys)

                  (title+"0x01"+categorys, film)
              })
          })
        val toRdd = titleOriRdd.leftOuterJoin(tagRdd)
        val toReRdd = toRdd.filter(x => !x._2.toString().contains("None"))

        //4、name 匹配不上的就去重新匹配title+standardname
        val titleStRdd = toRdd.filter(x => x._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  val categorys = item._1.split("0x01")(1)
                  val film = item._2._1
                  var title = item._2._1.title
                  title = getStartandTitle(title, categorys)

                  (title+"0x01"+categorys, film)
              })
          })
        //title + standardName
        val tsBaseRdd = titleStRdd.leftOuterJoin(sdnRdd)
        val tsReRdd = tsBaseRdd.filter(x => !x._2.toString().contains("None"))

        ///////////////////////////////////////////////////////////////////////////////////////
        //不需要category的匹配方式再来一遍
        //5 name+origin
        val noRdd1 = tsBaseRdd.filter(x => x._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  var name = item._1.split("0x01")(0)
                  name = getStartandName(name)

                  (name, item._2._1)
              })
          })

        val sdnRdd1 = sdnRdd.mapPartitions({items =>
            items.map({item =>
                (item._1.split("0x01")(0), item._2)
            })
        })
        val noBaseRdd1 = noRdd1.leftOuterJoin(sdnRdd1)
        val noReRdd1 = noBaseRdd1.filter(!_._2.toString().contains("None"))

        //6 name + standard
        val nsRdd1 = noBaseRdd1.filter(_._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  var name = item._1.split("0x01")(0)
                  name = getStartandName(name)

                  (name, item._2._1)
              })
          })

        val tagRdd1 = tagRdd.mapPartitions({items =>
            items.map({item =>
                (item._1.split("0x01")(0), item._2)
            })
        })
        val nsBaseRdd1 = nsRdd1.leftOuterJoin(tagRdd1)
        val nsReRdd1 = nsBaseRdd1.filter(!_._2.toString().contains("None"))

        //7 title+origin
        val toRdd1 = nsBaseRdd1.filter(_._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  val film = item._2._1
                  var title = item._2._1.title
                  title = getStartandTitle(title, item._2._1.categorys)

                  (title, film)
              })
          })
        val toBaseRdd1 = toRdd1.leftOuterJoin(tagRdd1)
        val toReRdd1 = toBaseRdd1.filter(!_._2.toString().contains("None"))

        //8 title+standard
        val tsRdd1 = toBaseRdd1.filter(_._2.toString().contains("None"))
          .mapPartitions(items => {
              items.map(item => {
                  val film = item._2._1
                  var title = item._2._1.title
                  title = getStartandTitle(title, item._2._1.categorys)

                  (title, film)
              })
          })
        val tsBaseRdd1 = tsRdd1.leftOuterJoin(sdnRdd1)
        val tsReRdd1 = tsBaseRdd1.filter(!_._2.toString().contains("None"))

        val unMatched1Rdd = tsBaseRdd1.filter(x => x._2.toString().contains("None"))
        //unMatched1Rdd.map(x => (x._1 ,1)).reduceByKey(_+_).saveAsTextFile("hdfs:///user/hdfs/player/tfproperty-" + analysisDate + ".txt")

        val unionRdd = noReRdd.union(nsReRdd).union(toReRdd).union(tsReRdd)
          .union(noReRdd1).union(nsReRdd1).union(toReRdd1).union(tsReRdd1).mapPartitions(items => {
            items.map(item => {
                val play = item._2._1
                val awcid = item._2._2.get
                val p = new Playing(play.vid, awcid, "", play.title, play.mac, play.date, play.actions)
                val vid = play.vid
                (vid + play.mac + awcid.split("0x01")(0) , p)
            })
        })

        val reRdd =unionRdd.repartition(unionRdd.getNumPartitions + 3).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        }).mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.tt).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })
     val rowrdd=reRdd.mapPartitions(items=>{
            items.map(item=>{
                getPlayerResultByHour(item).map(r=>{
                    var part = ""
                    if (r.part != "") {
                        part = r.part.toInt.toString
                    }
                    val eles = item.awcid.split("0x01")
                    item.awcid.split("0x01")(0) + r.title.split("/")(1) + item.mac + r.part + item.date + r.hour + "CH"+"#"+item.mac +"#"+r.title+"#"+eles(5)+"#"+eles(0)+"#"+part+"#"+item.date+"#"+r.hour.toInt.toString+"#"+r.vv.toString+"#"+r.duration.toString+"#"+eles(1)+"#"+eles(2) +"#"+eles(3)+"#"+eles(4)
                })
            })
        }).flatMap(row=>row)
        .map(_.split('#')).map(p=>Row(p(0),p(1),p(2),p(3),p(4),p(5),p(6),p(7),p(8),p(9),p(10),p(11),p(12),p(13))).distinct()

//        val  ch_cc_rdd=  rowrdd.union(DataCleanCCPlay.run(sc,analysisDate))
//          .map(_.split("#")).map(p=>Row(p(0),p(1),p(2),p(3),p(4),p(5),p(6),p(7),p(8),p(9),p(10),p(11),p(12),p(13))).distinct()


        val fact_str="key,dim_sn,dim_name,dim_title,dim_awcid,dim_part,dim_date,dim_hour,fact_vv,fact_duration,dim_year,dim_model,dim_crowd,dim_region"
        val fact_schema = StructType (
            fact_str.split(",").map(fieldName => StructField(fieldName,StringType,true))
        )
        val  chplayDF=sqlc.createDataFrame(rowrdd,fact_schema)
        chplayDF.registerTempTable("tracker_play")

      sqlc.sql(" use hr")
      sqlc.sql(" set hive.exec.dynamic.partition=true")
      sqlc.sql(" set hive.exec.dynamic.partition.mode=nonstrict")
      sqlc.sql(" INSERT OVERWRITE TABLE tracker_player_fact_ch PARTITION(date)   " +
                " select key,dim_sn,dim_name,dim_title,dim_awcid,dim_part,dim_hour,fact_vv,fact_duration,dim_year,dim_model,dim_crowd,dim_region,dim_date  from tracker_play " +
                " where dim_date='"+analysisDate+"' ")

        /*reRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_player_active_fact")) //tracker_player_active_fact

            try {
                items.foreach(item => {
                    getPlayerResultByHour(item).foreach(r => {
                        val uuid = UUID.randomUUID().toString
                        //hbase key 也就是行
                        val put = new Put(Bytes.toBytes(item.awcid.split("0x01")(0) + r.title.split("/")(1) + item.mac + r.part + item.date + r.hour + "CH"))
                        var part = ""
                        if (r.part != "") {
                            part = r.part.toInt.toString
                        }

                        val eles = item.awcid.split("0x01")

                        //'列族', '列名称:', '值'
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(item.mac))
                        put.addColumn(dimFamilyCol, dimNameCol, Bytes.toBytes(r.title))
                        put.addColumn(dimFamilyCol, dimTitleCol, Bytes.toBytes(eles(5)))
                        put.addColumn(dimFamilyCol, dimAwcidCol, Bytes.toBytes(eles(0)))
                        put.addColumn(dimFamilyCol, dimPartCol, Bytes.toBytes(part))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes((r.hour.toInt).toString))
                        put.addColumn(dimFamilyCol, dimYearCol, Bytes.toBytes(eles(1)))
                        put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(eles(2)))
                        put.addColumn(dimFamilyCol, dimCrowdCol, Bytes.toBytes(eles(3)))
                        put.addColumn(dimFamilyCol, dimRegionCol, Bytes.toBytes(eles(4)))
                        put.addColumn(factFamilyCol, factVvCol, Bytes.toBytes(r.vv.toString))
                        put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(r.duration.toString))

                        mutator.mutate(put)
                    })
                    mutator.flush()
                })

            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
        */

    }

    def bf(item: (String, Playing), vu: Array[(String, String)]) : (String, Playing) ={
        var p:Playing = null
        var vid = ""
        var mac = ""
        vu.foreach(ele => {
            if (item._1.equals(ele._1)) {
                val play = item._2
                val awcid = ele._2
                p = new Playing(play.vid, awcid, "", "", play.mac, play.date, play.actions)
                vid = play.vid
                mac = play.mac
            }
        })

        (vid + mac, p)
    }
    /**
      * 处理数组
      *
      * @param str   字符串
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            null
        } else {
            val ele = arr(1)
            ele
        }
    }

    def main(args: Array[String]): Unit = {

        val sc=new SparkContext(new SparkConf())
        var  dateFormat:SimpleDateFormat = new SimpleDateFormat("yyyy-MM-dd")
        var cal:Calendar=Calendar.getInstance()
        cal.add(Calendar.DATE,-1)
        var yesterday=dateFormat.format(cal.getTime())
        run(sc,yesterday)

    }
}
package com.avcdata.vbox.clean.oc

import com.avcdata.vbox.clean.oc.DataCleanCHPowerOn.{HeartItem, Heart}
import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.{HashPartitioner, SparkContext}
import org.joda.time.Seconds

import scala.collection.mutable

/*
**长虹开关机的清洗
 */

/**
  * Created by wxc on 11/07/16.
  */
object DataCleanCHPowerOn {

    case class Heart(sn: String, actions: mutable.MutableList[HeartItem]) {
    }

    case class HeartItem(mac: String, timestamp: String)

    case class HeartResultByMinute(date: String, hour: String, duration: Int, cnt: Int)

    /**
      * 处理数组
      *
      * @param str
      * @param split 分隔符
      * @return
      */
    def handlnArr(str: String, split: String): String = {
        val arr = str.split(split)
        if (arr.length == 1) {
            null
        } else {
            val ele = arr(1)
            ele
        }
    }

    def getSecondsToHourEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.hours).withSecondOfMinute(0).withMinuteOfHour(0)).getSeconds.abs
    }

    def getSecondsToMinuteEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.minutes).withSecondOfMinute(0)).getSeconds.abs
    }

    def getSecondsAbs(from: DateTime, to: DateTime): Int = {
        Seconds.secondsBetween(from, to).getSeconds.abs
    }

    def getHeartResultByMinute(item: Heart): mutable.MutableList[HeartResultByMinute] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var startItem: HeartItem = null
        var startTime: DateTime = null
        var endItem: HeartItem = null
        var endTime: DateTime = null
        var tempItem: HeartItem = null
        var tempTime: DateTime = null
        var flag = 0
        val result = mutable.MutableList[HeartResultByMinute]()
        var firstTime:DateTime = null
        var heartTime:Long = 0

        for (i <- 0 until item.actions.size) {
            if (startItem == null) {
                startItem = item.actions.get(i).get
                startTime = DateTime.parse(DataUtil.timestampToDate(startItem.timestamp), format)
                tempItem = startItem
                tempTime = startTime
            } else {
                endItem = item.actions.get(i).get
                endTime = DateTime.parse(DataUtil.timestampToDate(endItem.timestamp), format)
                heartTime = (endItem.timestamp.toLong - tempItem.timestamp.toLong) / 1000

                //120 是2分钟 时间超过20分钟的间隔也算一次开机
                if (heartTime < 120 * 10) {
                    if (endItem.mac == tempItem.mac) {
                        val startHour = tempTime.getHourOfDay
                        val endHour = endTime.getHourOfDay

                        //当不在同一小时的时候，做对应的处理
                        if (startHour != endHour) {
                            if (flag == 0) {
                                flag = 1
                                if (DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)>=("00:00")
                                    && DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)<=("00:02")) {
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, getSecondsToHourEnd(startTime) / 60, 0)
                                } else {
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, getSecondsToHourEnd(startTime) / 60, 1)
                                }
                            } else {
                                result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, 60, 0)
                            }

                            firstTime = null

                        } else if (startHour == endHour) {
                            if (firstTime == null || flag == 0) {
                                firstTime = tempTime
                            }
                        }

                        tempItem = endItem
                        tempTime = endTime
                    }
                    //日志跳到另外一个Mac中也算是一次开机,需要把时长存储起来
                    else if (endItem.mac != tempItem.mac) {
                        result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), endTime.getHourOfDay.toString, endTime.getMinuteOfHour, 0)
                        flag = 0
                    }
                } else if (heartTime >= 120 * 10) {
                    if (flag == 1) {
                        if (firstTime == null) {
                            // 同时段的处理或者类似18:50:01,52,54,19:02,04-->19:24:01的处理，时长2分钟
                            result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 0)
                        } else {
                            // start为2017-05-01 01:20:24，tmp是2017-05-01 01:26:24，endTime为2017-05-01 01:54:50,时长是6分钟
                            result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, (tempTime.getMinuteOfHour-firstTime.getMinuteOfHour), 0)
                            firstTime = null
                        }
                    } else {
                        if (DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)>=("00:00")
                            && DataUtil.timestampToDate(startItem.timestamp).substring(11, 16)<=("00:02")) {
                            //比如start为2017-05-01 00:01:24，tmp是2017-05-01 00:03:24，再次为endTime为2017-05-01 13:04:50
                            //这个时候开机次数为0，而且开机时长为1分钟
                            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 0)
                        } else {
                            if (tempTime.getHourOfDay == endTime.getHourOfDay && result.length == 0) {
                                if (firstTime != null) {//比如start为2017-05-01 01:20:24，23-->endTime为2017-05-01 01:54:50,时长是2分钟
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, (tempTime.getMinuteOfHour - firstTime.getMinuteOfHour), 1)
                                    firstTime = null
                                } else {//比如start为2017-05-01 01:20:24-->endTime为2017-05-01 01:54:50,时长是2分钟
                                    result += new HeartResultByMinute(DataUtil.timestampToDate(tempItem.timestamp), tempTime.getHourOfDay.toString, 2, 1)
                                }

                            } else {//比如start为为2017-05-01 01:20:24，23-->endTime为2017-05-01 11:34:50,时长是3分钟
                                result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 1)
                            }
                        }
                    }

                    //如果时间间隔大于20分钟，则算一次开关机,这个时候从头开始算
                    startItem = endItem
                    startTime = endTime
                    flag = 0
                    tempItem = endItem
                    tempTime = endTime
                }
            }
        }

        if (flag == 1) {
            //最后一个小时的处理，比如：2017-01-09 18:59:00--2017-01-09 19:01:00,这个时候2017-01-09 19:01:00的1分钟就在这处理
            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), endTime.getHourOfDay.toString, endTime.getMinuteOfHour, 0)
            flag = 0
        } else {
            if (heartTime < 120* 10) {
                if (firstTime != null) {
                    //比如只有23时的数据
                    result += new HeartResultByMinute(DataUtil.timestampToDate(endItem.timestamp), endTime.getHourOfDay.toString, (endTime.getMinuteOfHour - firstTime.getMinuteOfHour), 1)
                    firstTime = null
                }
            }
        }

        /*if(item.actions.size == 1) {
            val startItem = item.actions.get(0).get
            val startTime = DateTime.parse(DataUtil.timestampToDate(startItem.timestamp), format)
            val startHour = tempTime.getHourOfDay
            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, 2, 1)
        }*/

        result
    }

    def run(sc: SparkContext, analysisDate: String) = {

        //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")

        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSeriesNoCol = Bytes.toBytes("sn")
        //val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimPowerONDateCol = Bytes.toBytes("power_on_day")
        val dimPowerONTimeCol = Bytes.toBytes("power_on_time")
        val factPowerLenghtCol = Bytes.toBytes("power_on_length")
        val factCntCol = Bytes.toBytes("cnt")
        val digitRegex = """^\d+$""".r


        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
        val baseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/heartbeats.txt")
        //val baseRdd = sc.textFile("/user/hdfs/rsync/CH/2017-01-09/oc/oc"+analysisDate)
        //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/oc05")
                .filter(x => x.split("""":"""").length >= 4)
                .filter(x => x.contains("Mac") && x.split("""Mac":"""").length == 2 && x.split("""Mac":"""")(1).length >= 12)
                .filter(x => {
                    var timest = x.split("""timestamp":""")(1).substring(1, 14)
                    if (digitRegex.findFirstMatchIn(timest) == None) {
                        timest = x.split("""timestamp":""")(1).substring(0, 13)
                    }

                    var date = ""
                    if (digitRegex.findFirstMatchIn(timest) != None)
                        date = DataUtil.timestampToDate(timest).substring(0, 10)

                    date == analysisDate || date == preDate || date == yesBeforeDate
                })
            .mapPartitions(items => {
                items.map(line => {
                    val macs = line.split("""Mac":"""")
                    var sn = ""
                    if (macs(1).length > 16)
                        sn = macs(1).substring(0, 17)
                    else {
                        sn = macs(1).substring(0, 12)
                        sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
                            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
                    }

                    var timest = line.split("""timestamp":""")(1).substring(1, 14)
                    if (digitRegex.findFirstMatchIn(timest) == None) {
                        timest = line.split("""timestamp":""")(1).substring(0, 13)
                    }
                    val timestamp = timest
                    val xs = new HeartItem(sn, timestamp)
                    val terminalTV = new Heart(sn, mutable.MutableList(xs))
                    (sn, terminalTV)
                })
            })

        val i = 0
        val ter1Rdd = baseRdd.repartition(baseRdd.getNumPartitions + 10).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        })

        val terRdd = ter1Rdd.mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.timestamp).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })

        val count = terRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal_active_fact")) //tracker_terminal_active_fact

            try {
                items.foreach(x => {
                    val sn = x.sn
                    getHeartResultByMinute(x).foreach(r => {
                        val lastPowerOnDate = r.date.substring(0, 10)  //date
                        val lastPowerOnTime = r.hour  //hour
                        val timeLenth = r.duration
                        val cnt = r.cnt

                        val put = new Put(Bytes.toBytes(sn + lastPowerOnDate + lastPowerOnTime + "CH"))
                        put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                        //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimPowerONDateCol, Bytes.toBytes(lastPowerOnDate))
                        put.addColumn(dimFamilyCol, dimPowerONTimeCol, Bytes.toBytes((lastPowerOnTime.toInt).toString))
                        put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(timeLenth.toString))
                        put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(cnt.toString))
                        mutator.mutate(put)
                    })
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

object heartTest1 {
    def main(args: Array[String]): Unit = {
        val x1 = """{"ip":"/1.12.1.150:39450","timestamp":"1483611481078","mac":"{"Mac":"d8:47:10:a8:82:8a"}"}	1"""
        val x2 = """{"ip":"/1.12.1.150:39460","timestamp":1483611599037,"mac":"{"Mac":"d8:47:10:a8:82:8a"}"}	1"""

        println(x1.split("""timestamp":""")(1).substring(1, 14))
        println(x2.split("""timestamp":""")(1).substring(0, 13))

        println(x1.split("""Mac":"""")(1).substring(0, 17))
        println(x2.split("""Mac":"""")(1).substring(0, 17))
        println("2017-04-30 23:00:40".substring(11, 13))

        val item1:Heart = new Heart("d8:47:10:a8:82:8a", mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483974060000")) ++ //01
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483974180000")) ++ //03
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483975440000")) ++  //24
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483975620000")) ++  //27
            mutable.MutableList(new HeartItem("d8:47:10:a8:82:8a", "1483975740000")) //29
        )
//        DataCleanCHPowerOn.getHeartResultByMinute(item1)
    }
}
package com.avcdata.vbox.test

import com.avcdata.vbox.clean.common.OCFactPartition
import com.avcdata.vbox.util.DataUtil
import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.{SparkConf, SparkContext}
import org.joda.time.Seconds

import scala.collection.mutable


object DataCleanCHPowerOn01 {

  def main(args: Array[String]) {

    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("DataCleanCHPowerOnForCH")
    //      .set("spark.driver.allowMultipleContexts", "true");
    val sc = new SparkContext(conf)
    run(sc, "2017-07-02")


    //    val line = """{"ip":"/1.119.129.16:25003","timestamp":1499532139175,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}  1"""
    //    val ip = RegexUtils.getIP(line).get
    //    println(ip.split("\\.")(0) + "." + ip.split("\\.")(1))
    //    println(!RegexUtils.getIP(line).isEmpty && !RegexUtils.getTimeStamp(line).isEmpty && !RegexUtils.getMac(line).isEmpty)

    //    val timestamp = RegexUtils.getTimeStamp(line).get
    //    val mac = RegexUtils.getMac(line).get
    //    println(ip + "," + timestamp + "," + mac)
    //
    sc.stop()


  }

  /*
**长虹开关机的清洗
 */
  case class Heart(sn: String, actions: mutable.MutableList[HeartItem]) {
  }

  case class HeartItem(mac: String, timestamp: String)

  case class HeartResultByMinute(date: String, hour: String, duration: Int, cnt: Int)

  /**
    * 处理数组
    *
    * @param str
    * @param split 分隔符
    * @return
    */
  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      null
    } else {
      val ele = arr(1)
      ele
    }
  }

  def getSecondsToHourEnd(dateTime: DateTime): Int = {
    Seconds.secondsBetween(dateTime, (dateTime + 1.hours).withSecondOfMinute(0).withMinuteOfHour(0)).getSeconds.abs
  }

  def getSecondsToMinuteEnd(dateTime: DateTime): Int = {
    Seconds.secondsBetween(dateTime, (dateTime + 1.minutes).withSecondOfMinute(0)).getSeconds.abs
  }

  def getSecondsAbs(from: DateTime, to: DateTime): Int = {
    Seconds.secondsBetween(from, to).getSeconds.abs
  }

  def getHeartResultByMinute(item: Heart): mutable.MutableList[HeartResultByMinute] = {

    val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
    var startItem: HeartItem = null
    var startTime: DateTime = null
    var endItem: HeartItem = null
    var endTime: DateTime = null
    var tempItem: HeartItem = null
    var tempTime: DateTime = null
    var flag = 0
    val result = mutable.MutableList[HeartResultByMinute]()

    for (i <- 0 until item.actions.size) {
      if (startItem == null) {
        startItem = item.actions.get(i).get
        startTime = DateTime.parse(DataUtil.timestampToDate(startItem.timestamp), format)
        tempItem = startItem
        tempTime = startTime
      }
      else {
        endItem = item.actions.get(i).get
        endTime = DateTime.parse(DataUtil.timestampToDate(endItem.timestamp), format)
        val heartTime = (endItem.timestamp.toLong - tempItem.timestamp.toLong) / 1000

        //120 是2分钟 时间超过20分钟的间隔也算一次开机
        if (heartTime < 120 * 10) {
          if (endItem.mac == tempItem.mac) {
            val startHour = tempTime.getHourOfDay
            val endHour = endTime.getHourOfDay
            if (startHour != endHour) {
              if (flag == 0) {
                flag = 1
                result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, getSecondsToHourEnd(startTime) / 60, 1)
              } else {
                result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, 60, 0)
              }

            }

            tempItem = endItem
            tempTime = endTime
          }
          //日志跳到另外一个Mac中也算是一次开机
          else if (endItem.mac != tempItem.mac) {
            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), endTime.getHourOfDay.toString, endTime.getMinuteOfHour, 0)
            flag = 0
          }
        }
        else if (heartTime >= 120 * 10) {
          if (flag == 1) {
            //
            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), tempTime.getHourOfDay.toString, tempTime.getMinuteOfHour, 0)
          } else {
            //在同时时段内 进行了开关机的动作的处理
            result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), tempTime.getHourOfDay.toString, (tempTime.getMinuteOfHour - startTime.getMinuteOfHour), 1)
          }

          //如果时间间隔大于20分钟，则算一次开关机,这个时候从头开始算
          startItem = endItem
          startTime = endTime
          flag = 0
          tempItem = endItem
          tempTime = endTime
        }
      }
    }

    if (flag == 1) {
      result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), endTime.getHourOfDay.toString, endTime.getMinuteOfHour, 0)
      flag = 0
    }

    //    if (item.actions.size == 1) {
    //      val startItem = item.actions.get(0).get
    //      val startTime = DateTime.parse(DataUtil.timestampToDate(startItem.timestamp), format)
    //      val startHour = tempTime.getHourOfDay
    //      result += new HeartResultByMinute(DataUtil.timestampToDate(startItem.timestamp), startHour.toString, 2, 1)
    //    }


    result
  }

  def run(sc: SparkContext, analysisDate: String) = {

    //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")

    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSeriesNoCol = Bytes.toBytes("sn")
    //val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimPowerONDateCol = Bytes.toBytes("power_on_day")
    val dimPowerONTimeCol = Bytes.toBytes("power_on_time")
    val factPowerLenghtCol = Bytes.toBytes("power_on_length")
    val factCntCol = Bytes.toBytes("cnt")
    val digitRegex = """^\d+$""".r


    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
//        val baseRdd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/heartbeats.txt")
    val baseRdd = sc.textFile("E:\\aowei\\tracker-clean\\doc\\heartbeats.txt").distinct()
      //val baseRdd = sc.textFile("/user/hdfs/rsync/CH/2017-01-09/oc/oc"+analysisDate)
      //val baseRdd = sc.textFile("F:/avc/docs/changhong/data/oc05")
      .filter(x => x.split("""":"""").length >= 4)
      .filter(x => x.contains("Mac") && x.split("""Mac":"""").length == 2 && x.split("""Mac":"""")(1).length >= 12)
      /*.filter(x => {
          var timest = x.split("""timestamp":""")(1).substring(1, 14)
          if (digitRegex.findFirstMatchIn(timest) == None) {
              timest = x.split("""timestamp":""")(1).substring(0, 13)
          }

          var date = ""
          if (digitRegex.findFirstMatchIn(timest) != None)
              date = DataUtil.timestampToDate(timest).substring(0, 10)

          date == analysisDate || date == preDate || date == yesBeforeDate
      })*/
      .mapPartitions(items => {
      items.map(line => {
        val macs = line.split("""Mac":"""")
        var sn = ""
        if (macs(1).length > 16)
          sn = macs(1).substring(0, 17)
        else {
          sn = macs(1).substring(0, 12)
          sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
        }

        var timest = line.split("""timestamp":""")(1).substring(1, 14)
        if (digitRegex.findFirstMatchIn(timest) == None) {
          timest = line.split("""timestamp":""")(1).substring(0, 13)
        }
        val timestamp = timest
        val xs = new HeartItem(sn, timestamp)
        val terminalTV = new Heart(sn, mutable.MutableList(xs))
        (sn, terminalTV)
      })
    })

    val i = 0
    val ter1Rdd = baseRdd.repartition(baseRdd.getNumPartitions + 10).reduceByKey((left, right) => {
      left.actions ++= right.actions
      left
    })

    //ter1Rdd.foreach(println)
    val terRdd = ter1Rdd.mapPartitions(items => {
      items.map(item => {
        val sortedList = item._2.actions.sortBy(_.timestamp).clone()
        item._2.actions.clear()
        item._2.actions ++= sortedList
        (item._1, item._2)

      })
    })

      .flatMap(item => {
        val list = getHeartResultByMinute(item._2)
        val sn = item._1
        val arr = new Array[OCFactPartition](list.length)

        for (i <- 0 until list.length) {

          arr(i) = OCFactPartition(
            "CH": String,
            sn: String,
            list(i).date.substring(0, 10): String,
            list(i).hour: String,
            list(i).duration.toString: String,
            list(i).cnt.toString: String,
            list(i).date.substring(0, 10): String
          )
        }
        arr
      })
      .sortBy(_.sn)
//      .map(line => {
//        (line.sn,line.power_on_time,line.cnt)
//      })
     .foreach(println(_))




    //    val count = terRdd.foreachPartition(items => {
    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf)
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal_active_fact")) //tracker_terminal_active_fact
    //
    //      try {
    //        items.foreach(x => {
    //          val sn = x.sn
    //          getHeartResultByMinute(x).foreach(r => {
    //            val lastPowerOnDate = r.date.substring(0, 10) //date
    //            val lastPowerOnTime = r.hour //hour
    //            val timeLenth = r.duration
    //            val cnt = r.cnt
    //
    //            val put = new Put(Bytes.toBytes(sn + lastPowerOnDate + lastPowerOnTime + "CH"))
    //            put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
    //            //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
    //            put.addColumn(dimFamilyCol, dimPowerONDateCol, Bytes.toBytes(lastPowerOnDate))
    //            put.addColumn(dimFamilyCol, dimPowerONTimeCol, Bytes.toBytes((lastPowerOnTime.toInt).toString))
    //            put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(timeLenth.toString))
    //            put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(cnt.toString))
    //            mutator.mutate(put)
    //          })
    //        })
    //        mutator.flush()
    //      } finally {
    //        mutator.close()
    //        hbaseConn.close()
    //      }
    //    })

    //////////////////////////////////end of method //////////////////////////////////
//    sc.stop()
  }
}

object heartTest {
  def main(args: Array[String]): Unit = {
    val x1 = """{"ip":"/1.12.1.150:39450","timestamp":"1483611481078","mac":"{"Mac":"d8:47:10:a8:82:8a"}"}	1"""
    val x2 = """{"ip":"/1.12.1.150:39460","timestamp":1483611599037,"mac":"{"Mac":"d8:47:10:a8:82:8a"}"}	1"""

    println(x1.split("""timestamp":""")(1).substring(1, 14))
    println(x2.split("""timestamp":""")(1).substring(0, 13))

    println(x1.split("""Mac":"""")(1).substring(0, 17))
    println(x2.split("""Mac":"""")(1).substring(0, 17))
  }
}
package com.avcdata.vbox.tmp

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  *
  * 数据维度：分日、周、月；分地区（全国、各省份)；分智能电视、智能电视开机的终端数、次数、时长、每终端时长、开机率等指标数据，具体指标见下表

  * 日期维度：7月份，2017.07.01-2017.7.31
  *

  * @author zhangyongtian
  * @define 长虹开关机新清洗规则统计
  */
object DataCleanCHPowerOnCnt {

  def main(args: Array[String]) {

    val all_acnt = 100

//    val sql =
//      s"""
//        select
//          ''$all_acnt'' as all_acnt,
//          '智能电视开机' as t_name,
//        	t.province as province,
//        	afc.power_on_day as date,
//        	'daily' as dateType,
//        	count(distinct t.sn) as acnt,
//        	sum(afc.power_on_length)/60 as tcnt,
//        	sum(afc.cnt) as ucnt
//        from hr.sample_terminal_three t
//        join hr.tracker_oc_fact_ch afc
//        on
//        (t.sn=afc.sn)
//        where t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
//        group by t.province,afc.power_on_day
//      """.stripMargin
//
//    println(sql)

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("DataCleanCHPowerOnForCH")
    //    //      .set("spark.driver.allowMultipleContexts", "true");
    //    val sc = new SparkContext(conf)
    //
    //
    //
    //    run(sc, "2017-07-02")
    //
    //
    //    sc.stop()


  }

  def run(sc: SparkContext, analysisDate: String) = {
    val hiveContext = new HiveContext(sc)

    //    val apkArr = Array[String](
    //      "银河·奇异果", "云视听极光", "CIBN环球影视", "芒果TV", "CIBN微视听", "云视听·泰捷", "CIBN聚体育", "CIBN聚精彩", "CIBN悦厅TV"
    //    )

    //用到的表
    //    tracker_oc_fact_ch
    //    	key	sn	power_on_day	power_on_time	power_on_length	cnt
    //
    //    sample_terminal_three
    //    key	sn	brand	last_poweron	area	province	city	citylevel	size	model	license


    val all_acnt_arr = hiveContext.sql("select count(distinct sn) from hr.sample_terminal_three where key like '%CH%'")
     .collect

//    select province,count(distinct sn) from hr.sample_terminal_three where key like '%CH%' group by province

    val all_acnt = all_acnt_arr(0).getLong(0).toString


    val sql =
      s"""
        select
          '$all_acnt' as all_acnt,
          '智能电视开机' as t_name,
        	t.province as province,
        	afc.power_on_day as date,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_ch afc
        on
        (t.sn=afc.sn)
        where t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,afc.power_on_day
      """.stripMargin

    println("Sql:" + sql)

    //TODO 日-分省
    val dailyProvinceDF = hiveContext.sql(
      s"""
        select
          '$all_acnt' as all_acnt,
          '智能电视开机' as t_name,
        	t.province as province,
        	afc.power_on_day as date,
        	'daily' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_ch afc
        on
        (t.sn=afc.sn)
        where t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,afc.power_on_day
      """.stripMargin)

    dailyProvinceDF.registerTempTable("daily_tmp")

    //TODO 日-全国
    val dailyNationDF = hiveContext.sql(
      s"""
         select
           '$all_acnt' as all_acnt,
          '智能电视开机' as t_name,
        	'全国' as province,
        	date,
        	'daily' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM daily_tmp dt
        GROUP BY date

      """.stripMargin)
    /////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 周-分省
    val weeklyProvinceDF = hiveContext.sql(
      s"""
         select
           '$all_acnt' as all_acnt,
          '智能电视开机' as t_name,
        	t.province as province,
        	weekofyear(afc.power_on_day) as date,
        	'weekly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_ch afc
        on (t.sn=afc.sn)
        where t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,weekofyear(afc.power_on_day)
      """.stripMargin)

    weeklyProvinceDF.registerTempTable("weekly_tmp")


    //TODO 周-全国
    val weeklyNationDF = hiveContext.sql(
      s"""
         select
           '$all_acnt' as all_acnt,
         '智能电视开机' as t_name,
        	'全国' as province,
        	date,
        	'weekly' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM weekly_tmp dt
        GROUP BY date

      """.stripMargin)


    ////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 月-分省
    val monthlyProvinceDF = hiveContext.sql(
      s"""
         select
          '$all_acnt' as all_acnt,
         '智能电视开机' as t_name,
        	t.province as province,
        	month(afc.power_on_day) as date,
        	'monthly' as dateType,
        	count(distinct t.sn) as acnt,
        	sum(afc.power_on_length)/60 as tcnt,
        	sum(afc.cnt) as ucnt
        from hr.sample_terminal_three t
        join hr.tracker_oc_fact_ch afc
        on (t.sn=afc.sn)
        where t.province in ('上海市','云南省','内蒙古自治区','北京市','吉林省','四川省','天津市','宁夏回族自治区','安徽省','山东省','山西省','广东省','广西壮族自治区','新疆维吾尔自治区','江苏省','江西省','河北省','河南省','浙江省','海南省','湖北省','湖南省','甘肃省','福建省','西藏自治区','贵州省','辽宁省','重庆市','陕西省','青海省','黑龙江省')
        group by t.province,month(afc.power_on_day)
      """.stripMargin)

    monthlyProvinceDF.registerTempTable("monthly_tmp")


    //TODO 月-全国
    val monthlyNationDF = hiveContext.sql(
      s"""
         select
         '$all_acnt' as all_acnt,
         '智能电视开机' as t_name,
        	'全国' as province,
        	date,
        	'monthly' as dateType,
        	sum(acnt) as acnt,
        	sum(tcnt) as tcnt,
        	sum(ucnt) as ucnt
        FROM monthly_tmp dt
        GROUP BY date

      """.stripMargin)



    val allDF = dailyProvinceDF.unionAll(dailyNationDF).unionAll(weeklyProvinceDF).unionAll(weeklyNationDF).unionAll(monthlyProvinceDF).unionAll(monthlyNationDF)

    //      .rdd.saveAsTextFile("/tmp/DataCleanCHApkCnt")

    JdbcUtils.writeDF2Mysql(sc, allDF, Helper.mysqlConf, "vboxDB", "DataCleanCHPowerOnCnt", true,
      SaveMode.Append)







    //////////////////////////////////end of method ////////////////////////////////////
    //    sc.stop()
  }


}
package com.avcdata.vbox.test

import com.avcdata.vbox.clean.common.OCFactPartition
import com.avcdata.vbox.util.TimeUtils
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

import scala.collection.mutable
import scala.math.BigDecimal

/**
  * 一、开关机数据处理规则：
  * 规则1：先去掉卖场和库存（暂不考虑）
  * 规则2：对一个MAC对应多个IP地址的情况， 开机时长= mac对应的总开机时长/这个mac对应的ip前两段不相同的组数；
  * 规则3：统计开机时长时，根据心跳次数统计，不再根据时间戳进行，时间戳只用于判断开机时间点
  * 请根据以上确定的数据清洗规则，重跑长虹1-2天的开关机和应用数据
  * 开关机：
  * 开机终端数、开机时长 、开机次数、每终端开机时长、每终端开机次数
  * 日期	品牌	开机终端数	开机时长	开机次数
  *
  * @author zhangyongtian
  * @define 长虹开关机日志清洗
  */
object DataCleanCHPowerOnForCH {

  case class Hearts(sn: String, items: mutable.ArrayBuffer[HeartItem], ipGroupCnt: Int) {
  }

  case class HeartItem(mac: String, timestamp: String, ipGroup: String)

  case class TimeStampResult(sn: String, date: String, hour: Int, avgDura: BigDecimal, cnt: Int)

  case class HeartResultByDate(date: String, brand: String, duration: Double, cnt: Int, acnt: Int)


  def main(args: Array[String]) {

    //    val list = List("1498978164758","1498959697641","1498957350778")
    //
    //    val newList =  list.sortWith((a,b)=>a<b)
    //
    //    for(i<- 0 until list.length){
    //      println(list(i))
    //    }


    //    for(i<- 0 until newList.length){
    //      println(newList(i))
    //    }


    //    val iList = List(HeartItem("1", "1498978164758", "0"), HeartItem("2", "1498959697641", "0"), HeartItem("3",
    //      "1498957350778", "0"))
    //
    //
    //    for(i<- 0 until iList.length){
    //      println(iList(i))
    //    }
    //
    //    println("/////////////////")
    //
    //    val newList = iList.sortWith((a, b) => a.timestamp < b.timestamp)
    //
    //        for(i<- 0 until newList.length){
    //          println(newList(i))
    //        }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("DataCleanCHPowerOnForCH")
    //    //      .set("spark.driver.allowMultipleContexts", "true");
    //    val sc = new SparkContext(conf)
    //
    //
    //
    //    run(sc, "2017-07-02")
    //
    //
    //    //    val line = """{"ip":"/1.119.129.16:25003","timestamp":1499532139175,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}  1"""
    //    //    val ip = RegexUtils.getIP(line).get
    //    //    println(ip.split("\\.")(0) + "." + ip.split("\\.")(1))
    //    //    println(!RegexUtils.getIP(line).isEmpty && !RegexUtils.getTimeStamp(line).isEmpty && !RegexUtils.getMac(line).isEmpty)
    //
    //    //    val timestamp = RegexUtils.getTimeStamp(line).get
    //    //    val mac = RegexUtils.getMac(line).get
    //    //    println(ip + "," + timestamp + "," + mac)
    //    //
    //    sc.stop()


  }

  def run(sc: SparkContext, analysisDate: String) = {

//    sc.setCheckpointDir("/tmp/DataCleanCHPowerOnForCH_checkpoint" + System.currentTimeMillis())

    //    val hiveContext = new HiveContext(sc)

    /////////////////////test//////////////////////////
    //        val hiveContext = new SQLContext(sc)
    /////////////////////test///////////////////////

    //TODO 原始文件日志格式
    //    {"ip":"/1.119.129.16:44278","timestamp":1497225599973,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44382","timestamp":1497226327261,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44486","timestamp":1497227064422,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44705","timestamp":1497228014612,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44803","timestamp":1497228200833,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45899","timestamp":1497233811457,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45910","timestamp":1497233931574,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45924","timestamp":1497234051762,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45934","timestamp":1497234171820,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45944","timestamp":1497234291966,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45954","timestamp":1497234412158,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45977","timestamp":1497234532342,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45983","timestamp":1497234652511,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45998","timestamp":1497234772611,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:46030","timestamp":1497234892755,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:46154","timestamp":1497235086741,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:46195","timestamp":1497235241112,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48313","timestamp":1497240740677,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48342","timestamp":1497240856458,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48403","timestamp":1497241058545,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48506","timestamp":1497241435117,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //
    //读取日志文件
    /////////////////test///////////////
    //    val initRDD = sc.textFile("/user/hdfs/rsync/test/heartbeats.txt")
    //    val initRDD = sc.textFile("E:\\aowei\\tracker-clean\\doc\\heartbeats.txt")
    //            val initRDD = sc.textFile("/tmp/heartbeats.txt")
    //////////////////test//////////////////
    val initRDD = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/heartbeats.txt").distinct()

    //        val initRDD = sc.textFile("/tmp/heartbeats.txt").distinct()
    //    println("initRDD:" + initRDD.count)

    //TODO 提取
    //    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    //    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
    val rdd01 = initRDD
      .filter(x => x.split("""":"""").length >= 4)
      .filter(x => x.contains("Mac") && x.split("""Mac":"""").length == 2 && x.split("""Mac":"""")(1).length >= 12)
      /*.filter(x => {
          var timest = x.split("""timestamp":""")(1).substring(1, 14)
          if (digitRegex.findFirstMatchIn(timest) == None) {
              timest = x.split("""timestamp":""")(1).substring(0, 13)
          }

          var date = ""
          if (digitRegex.findFirstMatchIn(timest) != None)
              date = DataUtil.timestampToDate(timest).substring(0, 10)

          date == analysisDate || date == preDate || date == yesBeforeDate
      })*/
      .coalesce(100, shuffle = false)
      .map(line => {
        val macs = line.split("""Mac":"""")
        var sn = ""
        if (macs(1).length > 16)
          sn = macs(1).substring(0, 17)
        else {
          sn = macs(1).substring(0, 12)
          sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
        }
        var timest = line.split("""timestamp":""")(1).substring(1, 14)
        if (
          """^\d+$""".r.findFirstMatchIn(timest) == None) {
          timest = line.split("""timestamp":""")(1).substring(0, 13)
        }
        val timestamp = timest
        val ip = line.split(""":""")(1).substring(2, line.split(""":""")(1).length).replace(".", "&")
        (ip, timestamp, sn)
      })


    //TODO 过滤非当天的
    //      .filter(line => {
    //      val timestamp = line._2.toLong
    //      val date = TimeUtils.convertTimeStamp2DateStr(timestamp, TimeUtils.DAY_DATE_FORMAT_ONE)
    //      date.equals(analysisDate)
    //    })
    // .distinct


    //      .filter(line => {
    //      val testArr = Array[String]("00:14:49:75:85:7f", "00:14:49:ea:3b:89", "00:30:1b:ba:02:db")
    //      testArr.contains(line._3)
    //    })


    ///////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO 转换成Hearts
    val rdd02 = rdd01.map(line => {
      //截取ip前两段
      val ipGroup = line._1.split("&")(0) + "." + line._1.split("&")(1)
      val timestamp = line._2
      val sn = line._3
      (sn, new Hearts(sn, mutable.ArrayBuffer(new HeartItem(sn, timestamp, ipGroup)), 0))
    })
      //TODO 同一个mac的timestamp和ip聚合在一起
      .reduceByKey((left, right) => {
      left.items ++= right.items
      left
    })
      .map(_._2)

      // TODO 按时间戳排序
      .map(line => {
      val sortedArr = line.items.sortWith((pre, post) => {
        pre.timestamp < post.timestamp
      })

      new Hearts(line.sn, sortedArr, 0)
    })

      //////////////////////////////////test//////////////////////////////////////
      //    println(rdd02.count)
      //////////////////////////////////test//////////////////////////////////////

      .map(line => {
      var ipGroupArr = scala.collection.mutable.ArrayBuffer[String]()
      for (i <- 0 until line.items.size) {
        ipGroupArr = ipGroupArr :+ line.items(i).ipGroup
      }
      //TODO sn 心跳记录 ip区段去重个数
      new Hearts(line.sn, line.items, ipGroupArr.distinct.length)
    })

      //TODO 过滤掉只有一条心跳sn的数据
      .filter(line => {
      line.items.size > 1
    })


    //    TODO 转换成中间结果
    val rdd03 = rdd02.flatMap(line => {
      val sn = line.sn
      val ipGroupCnt = line.ipGroupCnt
      val items = line.items

      //sn date timestamp avgDura cnt
      val array = new Array[TimeStampResult](items.length)

      for (i <- 0 until items.length) {
        //        println("array:" + i + ":" + array(i))
        val date = TimeUtils.convertTimeStamp2DateStr(items(i).timestamp.toLong,
          "yyyy-MM-dd")
        if (i == 0) {
          array(i) = TimeStampResult(sn, date, TimeUtils.convertTimeStamp2DateStr(items(i).timestamp.toLong,
            "HH").toInt, BigDecimal(2)./(BigDecimal(ipGroupCnt)), 1)
        }
        //TODO 时间戳间隔大于20分钟的 增加次数
        if ((i + 1) <= (items.length - 1)) {
          val preTimeStamp = items(i).timestamp
          val postTimeStamp = items(i + 1).timestamp
          val interval = (postTimeStamp.toLong - preTimeStamp.toLong) / 1000 / 60
          if (interval > 20) {
            array(i + 1) = TimeStampResult(sn, date, TimeUtils.convertTimeStamp2DateStr(items(i + 1).timestamp.toLong,
              "HH").toInt, BigDecimal(2)./(BigDecimal(ipGroupCnt)), 1)
          } else {
            array(i + 1) = TimeStampResult(sn, date, TimeUtils.convertTimeStamp2DateStr(items(i + 1).timestamp.toLong,
              "HH").toInt, BigDecimal(2)./(BigDecimal(ipGroupCnt)), 0)
          }
        }
        //        println("array2:" + i + ":" + array(i))
      }
      array
    })

      //TODO 分时
      .map(line => {
      ((line.sn, line.date, line.hour), (line.avgDura, line.cnt))
    }).reduceByKey((pre, post) => {
      ((pre._1.+(post._1)), (pre._2 + post._2))
    })


    val resultRDD = rdd03.map(line => {
      val sn = line._1._1
      val power_on_day = line._1._2
      val power_on_time = line._1._3.toString
      val power_on_length = line._2._1.toDouble.toString
      val cnt = line._2._2.toString
      val date = power_on_day
      val key = sn + power_on_day + power_on_time + "CH"

      OCFactPartition(
        key: String,
        sn: String,
        power_on_day: String,
        power_on_time: String,
        power_on_length: String,
        cnt: String,
        date: String
      )
    }).filter(x => x != null)

    //      .sortBy(_.sn)
    //      .map(line => {
    //        (line.sn, line.power_on_time, line.cnt)
    //      }).sortBy(_._1)
    //      .foreach(println(_))
    //
    //    println(resultRDD.distinct.count)
    //
    //      .saveAsTextFile("/tmp/out" + System.currentTimeMillis())
    //
    //      .toDF.registerTempTable("tmp_table")
    //
    //    println(hiveContext.sql("select * from tmp_table").count)
    //
    //    val sql =
    //      """
    //            insert overwrite table hr.tracker_oc_fact_partition_ch partition(date='""" + analysisDate +
    //        """') select key,sn,power_on_day,power_on_time,power_on_length,cnt from tmp_table where power_on_day='""" +
    //        analysisDate +
    //        """'
    //        """.stripMargin
    //
    //    hiveContext.sql(sql)



    //    println(rdd03.count)

    //TODO 写入HBase
    resultRDD.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_oc_fact_ch")) //tracker_terminal_active_fact

      try {
        items.foreach(item => {

          val put = new Put(Bytes.toBytes(item.key))
          put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("sn"), Bytes.toBytes(item.sn))
          put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("power_on_day"), Bytes.toBytes(item.power_on_day))
          put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("power_on_time"), Bytes.toBytes(item.power_on_time))
          put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("power_on_length"), Bytes.toBytes(item.power_on_length))
          put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("cnt"), Bytes.toBytes(item.cnt))
          mutator.mutate(put)
        })
        mutator.flush()
      } finally {
        mutator.close()
      }
    })


    //////////////////////////////////end of method ////////////////////////////////////
    //    sc.stop()
  }


}
package com.avcdata.vbox.test

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable

/**
  * 一、开关机数据处理规则：
  * 规则1：先去掉卖场和库存（暂不考虑）
  * 规则2：对一个MAC对应多个IP地址的情况， 开机时长= mac对应的总开机时长/这个mac对应的ip前两段不相同的组数；
  * 规则3：统计开机时长时，根据心跳次数统计，不再根据时间戳进行，时间戳只用于判断开机时间点
  * 请根据以上确定的数据清洗规则，重跑长虹1-2天的开关机和应用数据
  * 开关机：
  * 开机终端数、开机时长 、开机次数、每终端开机时长、每终端开机次数
  * 日期	品牌	开机终端数	开机时长	开机次数
  *
  * @author zhangyongtian
  * @define 长虹开关机日志清洗
  */
object DataCleanCHPowerOnForCHOfDay {

  case class Hearts(sn: String, items: mutable.MutableList[HeartItem], ipGroupCnt: Int) extends Serializable {
  }

  case class HeartItem(mac: String, timestamp: String, ipGroup: String) extends Serializable

  case class HeartResultByDate(date: String, brand: String, duration: Double, cnt: Int, acnt: Int) extends Serializable


  def main(args: Array[String]) {

    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("DataCleanCHPowerOnForCH")
      .set("spark.driver.allowMultipleContexts", "true");
    val sc = new SparkContext(conf)
    run(sc, "2017-06-12")


    //    val line = """{"ip":"/1.119.129.16:25003","timestamp":1499532139175,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}  1"""
    //    val ip = RegexUtils.getIP(line).get
    //    println(ip.split("\\.")(0) + "." + ip.split("\\.")(1))
    //    println(!RegexUtils.getIP(line).isEmpty && !RegexUtils.getTimeStamp(line).isEmpty && !RegexUtils.getMac(line).isEmpty)

    //    val timestamp = RegexUtils.getTimeStamp(line).get
    //    val mac = RegexUtils.getMac(line).get
    //    println(ip + "," + timestamp + "," + mac)
    //
    sc.stop()


  }

  def run(sc: SparkContext, analysisDate: String) = {

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    /////////////////////test//////////////////////////
    //    val hiveContext = new SQLContext(sc)
    /////////////////////test///////////////////////

    //TODO 原始文件日志格式
    //    {"ip":"/1.119.129.16:44278","timestamp":1497225599973,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44382","timestamp":1497226327261,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44486","timestamp":1497227064422,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44705","timestamp":1497228014612,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:44803","timestamp":1497228200833,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45899","timestamp":1497233811457,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45910","timestamp":1497233931574,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45924","timestamp":1497234051762,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45934","timestamp":1497234171820,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45944","timestamp":1497234291966,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45954","timestamp":1497234412158,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45977","timestamp":1497234532342,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45983","timestamp":1497234652511,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:45998","timestamp":1497234772611,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:46030","timestamp":1497234892755,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:46154","timestamp":1497235086741,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:46195","timestamp":1497235241112,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48313","timestamp":1497240740677,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48342","timestamp":1497240856458,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48403","timestamp":1497241058545,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //    {"ip":"/1.119.129.16:48506","timestamp":1497241435117,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}	1
    //
    //读取日志文件
    /////////////////test///////////////
    //    val initRDD = sc.textFile("/user/hdfs/rsync/test/heartbeats.txt")
    //        val initRDD = sc.textFile("E:\\aowei\\tracker-clean\\doc\\heartbeats.txt")
    //////////////////test//////////////////
    val initRDD = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/heartbeats.txt").distinct()

    //    val initRDD = sc.textFile("/tmp/heartbeats.txt").distinct()
    //    println("initRDD:" + initRDD.count)

    //TODO 提取
    val filteredRDD = initRDD
      .filter(x => x.split("""":"""").length >= 4)
      .filter(x => x.contains("Mac") && x.split("""Mac":"""").length == 2 && x.split("""Mac":"""")(1).length >= 12)
      .map(line => {
        val macs = line.split("""Mac":"""")
        var sn = ""
        if (macs(1).length > 16)
          sn = macs(1).substring(0, 17)
        else {
          sn = macs(1).substring(0, 12)
          sn = sn.substring(0, 2) + ":" + sn.substring(2, 4) + ":" + sn.substring(4, 6) +
            ":" + sn.substring(6, 8) + ":" + sn.substring(8, 10) + ":" + sn.substring(10, 12)
        }

        var timest = line.split("""timestamp":""")(1).substring(1, 14)
        if (
          """^\d+$""".r.findFirstMatchIn(timest) == None) {
          timest = line.split("""timestamp":""")(1).substring(0, 13)
        }
        val timestamp = timest
        val ip = line.split(""":""")(1).substring(2, line.split(""":""")(1).length).replace(".", "&")
        (ip, timestamp, sn)
      })
      //TODO 过滤非当天的数据
      //      .filter(line => {
      //      val timestamp = line._2.toLong
      //      val date = TimeUtils.convertTimeStamp2DateStr(timestamp, TimeUtils.DAY_DATE_FORMAT_ONE)
      //      date.equals(analysisDate)
      //    }).distinct


//      .filter(line => {
//      val testArr = Array[String]("00:14:49:75:85:7f", "00:14:49:ea:3b:89", "00:30:1b:ba:02:db")
//      testArr.contains(line._3)
//    })
      //TODO 转换成Hearts
      .map(line => {
      //截取ip前两段
      val ipGroup = line._1.split("&")(0) + "." + line._1.split("&")(1)
      val timestamp = line._2
      val sn = line._3
      (sn, new Hearts(sn, mutable.MutableList(new HeartItem(sn, timestamp, ipGroup)), 0))
    })



      //TODO 同一个mac的timestamp和ip聚合在一起
      .repartition(initRDD.getNumPartitions + 10)
      .reduceByKey((left, right) => {
        left.items ++= right.items
        left
      })

      //TODO 按时间戳排序
      //      .map(line => {
      //      val sortedList = line._2.items.sortBy(_.timestamp).clone()
      //      line._2.items.clear()
      //      line._2.items ++= sortedList
      //      line._2
      //    })

      .map(_._2)

    //    println("filteredRDD:" + filteredRDD.count)
    //
    val resultDF = filteredRDD.map(line => {
      var ipGroupArr = scala.collection.mutable.ArrayBuffer[String]()

      for (i <- 0 until line.items.size) {
        ipGroupArr = ipGroupArr :+ line.items.get(i).get.ipGroup
      }

      //TODO sn 心跳记录 ip区段去重个数
      new Hearts(line.sn, line.items, ipGroupArr.distinct.length)
    })

      //TODO 过滤掉只有一条心跳sn的数据
      //      .filter(line => {
      //      line.items.size > 1
      //    })

      //    resultDF.map(_.items)
      //      .saveAsTextFile("/tmp/DataCleanCHPowerOnForCH"+System.currentTimeMillis)
      /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
      // TODO 转换成结果数据
      .map(hearts => {
      val sn = hearts.sn
      val date = analysisDate
      // 开机时长= mac心跳次数*2min/这个mac对应的ip前两段不相同的组数
      val duration = (hearts.items.size * 2) / hearts.ipGroupCnt
      val ucnt = 1
      val acnt = 1
      (date, (duration, ucnt, acnt))
    })

      //      .saveAsTextFile("/tmp/DataCleanCHPowerOnForCH" + System.currentTimeMillis())
      .reduceByKey((pre, post) => {
      (pre._1 + post._1, pre._2 + post._2, pre._3 + post._3)
    }).map(line => {
      val date = line._1
      val brand = "CH"
      val duration = line._2._1 / 60
      val ucnt = line._2._2
      val acnt = line._2._3

      HeartResultByDate(date: String, brand: String, duration: Double, ucnt: Int, acnt: Int)
    }).toDF


    //        resultDF.rdd.saveAsTextFile("/tmp/DataCleanCHPowerOnForCH"+System.currentTimeMillis)

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "vboxDB", "DataCleanCHPowerOnForCH", false,
      org.apache.spark.sql.SaveMode.Append)


    //////////////////////////////////end of method //////////////////////////////////
  }


}
package com.avcdata.vbox.clean.terminal

import com.avcdata.vbox.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

/**
  * Created by wxy on 8/29/16.
  * 总的长虹的终端数
  */
object DataCleanCHTerminal {

  val pro = List("北京市", "天津市", "上海市", "重庆市", "香港特别行政区", "河北省",
    "山西省", "内蒙古自治区", "辽宁省", "吉林省", "黑龙江省", "江苏省", "浙江省",
    "安徽省", "福建省", "江西省", "山东省", "河南省", "湖北省", "湖南省",
    "广东省", "广西壮族自治区", "海南省", "四川省", "贵州省", "云南省",
    "西藏自治区", "陕西省", "甘肃省", "青海省", "宁夏回族自治区", "新疆维吾尔自治区", "台湾省"
  )

  /**
    * 处理数组
    *
    * @param str
    * @param split 分隔符
    * @return
    */
  def handlnArr(str: String, split: String): String = {
    val arr = str.split(split)
    if (arr.length == 1) {
      "其他"
    } else {
      var ele = arr(1)
      if (ele.equals("null") || ele.equals("unknown")) {
        ele = "其他"
      }
      ele
    }
  }

  def run(sc: SparkContext, analysisDate: String) = {
    val dimFamilyCol = Bytes.toBytes("terminalProperty")
    val dimSeriesNoCol = Bytes.toBytes("sn")
    val dimBrandCol = Bytes.toBytes("brand")
    val dimLicenseCol = Bytes.toBytes("license")
    val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
    val dimAreaCol = Bytes.toBytes("area")
    val dimProvinceCol = Bytes.toBytes("province")
    val dimCityCol = Bytes.toBytes("city")
    val dimCitylevelCol = Bytes.toBytes("citylevel")
    val dimSizeCol = Bytes.toBytes("size")
    val dimModelCol = Bytes.toBytes("model")


    //val mardd = sc.textFile("F:/avc/docs/changhong/data/deviceInfo")
    val mardd = sc.textFile("/user/hdfs/rsync/CH/" + analysisDate + "/deviceInfo.txt")
      .filter(x => x.split("""Mac":"""")(1).substring(0, 4) != "null")
      .filter(x => x.split(""""Data":""")(1).contains("province"))
    val maSpRdd = mardd.mapPartitions(items => {
      items.map(line => {
        val cols = line.split("""","""") //以","来隔
        val mac = cols(0).substring(8, cols(0).length)
        val mstrs = cols(1).split(';')
        var province = handlnArr(mstrs(5), "=") //省
        var city = "" //市
        var district = "" //区
        if (!pro.contains(province)) {
          province = "其他"
          city = "其他"
          district = "其他"
        }
        else {
          city = handlnArr(mstrs(6), "=") //市
          if (city == null || city.equals("")) {
            city = "其他"
          }
          district = handlnArr(mstrs(7), "=") //区
        }
        //val city = handlnArr(mstrs(6), "=") //市
        //val district = handlnArr(mstrs(7), "=") //区
        val area = Codearea.getArea(province) //大区
        val procductName = handlnArr(mstrs(1), "=") //机型名称
        //val clevel = Codearea.getCl(city)
        var clevel = ""
        if (Codearea.prolist.contains(province)) {
          clevel = Codearea.getCl(city)
        } else {
          clevel = "港澳台及国外"
        }

        (mac, area, province, city, procductName, clevel)
      })
    })
    //maSpRdd.foreach(println)

    maSpRdd.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal"))

      try {
        items.foreach(line => {
          val sn = line._1
          val brand = "CH"
          //val model = ""
          val lastPowerOn = ""
          val size = ""
          val area = line._2
          val province = line._3
          val city = line._4
          val procductName = line._5
          val clevel = line._6
          val put = new Put(Bytes.toBytes(sn + "CH"))
          put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
          put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes("tencent"))
          put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
          put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(procductName))
          put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
          put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
          put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
          put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
          put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
          put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))
          //println(sn + "\t" + area + "\t" + province + "\t" + city)
          mutator.mutate(put)
        })
        mutator.flush()
      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })
  }
}
package com.avcdata.vbox.clean.epg

import com.avcdata.vbox.util.{MapingUtils, HBaseUtils, TimeUtils}
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object DataCleanEpg {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("DataCleanEpg")
    val sc = new SparkContext(conf)
    run(sc, "2017-02-26")
    //    println(getTimeRangeByStartTime("06:23"))

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    //TODO 日期格式转换
    val timeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    val log_date = TimeUtils.convertTimeStamp2DateStr(timeStamp, TimeUtils.DAY_DATE_FORMAT_TWO)


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\2016-11-15_2016-11-21.csv")

    //////////////////test//////////////////////////////////////
    //        val localPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\epg\\epg.log." + analysisDate
    //
    //        val initRDD = sc.textFile(localPath)
    ///////////////////test///////////////////////////////////

    //    val hdfsPath = "/user/hdfs/rsync/KONKA/" + analysisDate + "/epg.log." + analysisDate
    //    val hdfsPath = "/user/hdfs/rsync/epg/epg-all.log"


    /////////////////////////////////test/////////////////////////////////////////////////
    //    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\epg\\epg" + log_date
    /////////////////////////////////test/////////////////////////////////////////////////

    val hdfsPath = "/user/hdfs/rsync/epg/epg" + log_date

//    println(hdfsPath)

    //TODO 原始文件日志格式
    //02月26日	四川卫视	07:42	琅琊榜（33）

    //TODO  读取原始日志文件
    val initRDD = sc.textFile(hdfsPath).distinct

    val dataRDD = initRDD

      //TODO 过滤
      .filter(line => {
      val cols = line.split('\t')
      true

    })
      //TODO 小时数加上0
      //      .map(line => {
      //      val cols = line.split("\t")
      //      val time = cols(2)
      //      val hour = time.split(":")(0)
      //      val min = time.split(":")(1)
      //      val newTime = TimeUtils.addZero(hour) + ":" + min
      //      cols(0) + "," + cols(1) + "," + newTime + "," + cols(3)
      //    })


      //TODO 根据参数设置日期
      .map(line => {
      val cols = line.split("\t")
      //      val time = cols(2)
      //      val hour = time.split(":")(0)
      //      val min = time.split(":")(1)
      //      val newTime = TimeUtils.addZero(hour) + ":" + min
      analysisDate + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3)
    })

      //TODO 按时间排序
      .sortBy(line => {
      val cols = line.split("\t")
      //      println("##########" + cols(0) + "," + cols(2))
      //      "2017-02-22,CCTV-1,0:00,国际艺苑"
      val date = cols(0)
      val channel = cols(1)
      val startTime = cols(2)
      date + "#" + channel + "#" + startTime
    }, true, 1)


    //TODO 配对 添加结束时间
    dataRDD
      .map(line => {
        val cols = line.split("\t")
        val date = cols(0)
        val channel = cols(1)
        val startTime = cols(2)
        val pg = cols(3)

        (date + "\t" + channel, startTime + "\t" + pg)
      })

      .reduceByKey((pre, post) => {
        pre + "#" + post
      })


      .flatMap(line => {
        var resultArr = new ArrayBuffer[String]()

        val dateChannel = line._1.split("\t")
        val date = dateChannel(0)
        val channel = dateChannel(1)
        val startTimePGArr = line._2.split("#")

        for (i <- 0 until startTimePGArr.length) {
          val startTime = startTimePGArr(i).split("\t")(0)
          val pg = startTimePGArr(i).split("\t")(1)
          if (i + 1 < startTimePGArr.length) {
            val endTime = startTimePGArr(i + 1).split("\t")(0)
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + endTime + "\t" + pg)
          } else {
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + "24:00" + "\t" + pg)
          }
        }
        resultArr
      })

      //TODO 写入hbase
      .foreachPartition(items => {

            val mutator = HBaseUtils.getMutator("tracker_epg")
//      val mutator = HBaseUtils.getMutator("epg02")

      try {

        items.flatMap(line => {

          val cols = line.split('\t')

          //TODO 转换频道名称
          val channel = MapingUtils.epgChannel2Stand(cols(0).toString)

          //日期
          val tv_date = cols(1)

          //开始时间
          val start_time = cols(2).toString

          //结束时间
          val end_time = cols(3).toString

          //开始时间-结束时间
          val start_end = cols(2) + "-" + cols(3)

          //TODO 获取时间段
          val time_range = getTimeRangeByStartTime(start_time.toString)

          //节目类型
          val pg_type = ""

          //节目
          val pg = cols(4)

          val otherInfo = channel + "\t" + tv_date + "\t" + start_end + "\t" + time_range + "\t" + pg_type + "\t" + pg + "\t"

          //TODO 分时
          val orderedLines = TimeUtils.splitTimeByMinute(otherInfo, start_time, end_time)
//          println(channel + "\t" + tv_date + "\t" + pg)
          orderedLines

        })
          .foreach(line => {

            //          println(line)
            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_epg(line))
          })

        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  def getTimeRangeByStartTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }

    }


  }
}
package com.avcdata.vbox.clean.epg

import com.avcdata.vbox.util.{MapingUtils, TimeUtils}
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer
import org.apache.spark.sql.SaveMode

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object DataCleanEpg2 {

  val log = Logger.getLogger(getClass.getName)


  case class  Epg(channel:String, tv_date:String, start_end:String, time_range:String, pg_type:String, pg:String, tv_hour:String, tv_min:String)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("DataCleanEpg")
    val sc = new SparkContext(conf)
    println(getTimeRangeByStartTime("06:23"))
    sc.stop()
  }


  def run(sc: SparkContext, analysisDate: String, originalDF: DataFrame) = {
    //    val hiveContext = new SQLContext(sc)
    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._


    //TODO 日期格式转换
//    val timeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)
//    val log_date = TimeUtils.convertTimeStamp2DateStr(timeStamp, TimeUtils.DAY_DATE_FORMAT_TWO)
//    val c = new SQLContext(sc)
//    val url = "jdbc:mysql://192.168.20.29:3306/vboxDB?useUnicode=true&characterEncoding=utf-8&useSSL=false"
//    val reader = new SQLContext(sc).read.format("jdbc")
//    var options = Map[String, String]()
//    options += ("url" -> url)
//    options += ("driver" -> "com.mysql.jdbc.Driver")
//    options += ("user" -> "root")
//    options += ("password" -> "new.1234")
//    options += ("dbtable" -> "epg")
//    reader.options(options)
//    var originalDF = reader.load()


    //TODO 小时数加上0
   val rdd01 = originalDF.rdd.map(line => {
      val date = line.get(0)
      val channel = line.get(1)
      val startTime = line.get(2)
      val pg = line.get(3)

      date + "\t" + channel + "\t" + startTime + "\t" + pg
    })
      //TODO 按时间排序
      .sortBy(line => {
      val cols = line.split("\t")
      //      println("##########" + cols(0) + "," + cols(2))
      //      "2017-02-22,CCTV-1,0:00,国际艺苑"
      val date = cols(0)
      val channel = cols(1)
      val startTime = cols(2)
      date + "#" + channel + "#" + startTime
    }, true, 1)
      //TODO 配对 添加结束时间
      .map(line => {
      val cols = line.split("\t")
      val date = cols(0)
      val channel = cols(1)
      val startTime = cols(2)
      val pg = cols(3)
      (date + "\t" + channel, startTime + "\t" + pg)
    })
      .reduceByKey((pre, post) => {
        pre + "#" + post
      })
      .flatMap(line => {
        var resultArr = new ArrayBuffer[String]()
        val dateChannel = line._1.split("\t")
        val date = dateChannel(0)
        val channel = dateChannel(1)
        val startTimePGArr = line._2.split("#")
        for (i <- 0 until startTimePGArr.length) {
          val startTime = startTimePGArr(i).split("\t")(0)
          val pg = startTimePGArr(i).split("\t")(1)
          if (i + 1 < startTimePGArr.length) {
            val endTime = startTimePGArr(i + 1).split("\t")(0)
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + endTime + "\t" + pg)
          } else {
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + "24:00" + "\t" + pg)
          }
        }
        resultArr
      })
      //TODO 写入Hive
    val resultDF = rdd01.flatMap(line => {
      val cols = line.split('\t')
      //TODO 转换频道名称
      val channel = MapingUtils.epgChannel2Stand(cols(0).toString)
      //日期
      val tv_date = cols(1)
      //开始时间
      val start_time = cols(2).toString
      //结束时间
      val end_time = cols(3).toString
      //开始时间-结束时间
      val start_end = cols(2) + "-" + cols(3)
      //TODO 获取时间段
      val time_range = getTimeRangeByStartTime(start_time.toString)
      //节目类型
      val pg_type = ""
      //节目
      val pg = cols(4)
      val otherInfo = channel + "\t" + tv_date + "\t" + start_end + "\t" + time_range + "\t" + pg_type + "\t" + pg + "\t"
      //TODO 分时
      val orderedLines = TimeUtils.splitTimeByMinute(otherInfo, start_time, end_time)
      //          println(channel + "\t" + tv_date + "\t" + pg)

      orderedLines

    }).map(line => {
      val cols = line.split('\t')

      val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

      var i = 0
      val channel = cols(i)

      i = i + 1
      val tv_date = cols(i)

      i = i + 1
      val start_end = cols(i)

      i = i + 1
      val time_range = cols(i)

      i = i + 1
      val pg_type = cols(i)

      i = i + 1
      val pg = cols(i)

      i = i + 1
      val tv_hour = cols(i)

      i = i + 1
      val tv_min = cols(i)


      Epg(channel, tv_date, start_end, time_range, pg_type, pg, tv_hour, tv_min)
    }).toDF


    resultDF.write.mode(SaveMode.Append).saveAsTable("hr.epg02")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  def getTimeRangeByStartTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }
    }
  }
}
package com.avcdata.vbox.clean.apk

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

/**
  * Created by wxy on 8/29/16.
  * Konka apk的清洗
  */
object DataCleanKOApk {

  def run(sc: SparkContext, analysisDate: String) = {
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimApkCol = Bytes.toBytes("dim_apk")
    //val dimAreaCol = Bytes.toBytes("dim_area")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimDateCol = Bytes.toBytes("dim_date")
    val dimHourCol = Bytes.toBytes("dim_hour")
    val factCountCol = Bytes.toBytes("fact_cnt")
    val factDurationCol = Bytes.toBytes("fact_duration")

    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //val rdd = sc.textFile("F:/avc/docs/konka/activity.log.2016-08-16")
    val rdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/activity.log." + analysisDate)
      //val rdd = sc.textFile("/user/hdfs/rsync/KONKA/history/activity.log.since_10-01")
      .filter(x => {
      val date = x.split('|')(8)
      date == analysisDate || date == preDate || date == yesBeforeDate
    }).filter(x => {
      val dura = x.split('|')(11)
      dura.toInt > 0
    })

    rdd.foreachPartition(items => {

      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_apk_active_fact"))

      //tracker_apk_active_fact
      //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_apk_active_fact"))

      try {

        items.foreach(line => {
          val cols = line.split('|')
          val sn = cols(0)
          val area = cols(5).substring(0, 3)
          val apk = cols(6)
          val date = cols(8)
          val hour = cols(9)
          val cnt = cols(10)
          val duration = cols(11)

          val put = new Put(Bytes.toBytes(apk + date + hour + sn + "KO"))
          put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
          put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apk))
          //put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
          //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
          put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))
          put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(hour))
          put.addColumn(factFamilyCol, factCountCol, Bytes.toBytes(cnt))
          put.addColumn(factFamilyCol, factDurationCol, Bytes.toBytes(duration))

          mutator.mutate(put)
        })

        mutator.flush()
      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })

    //val frdd = rdd.map(x=> (x.split('|')(6), 1)).reduceByKey(_+_)
    //file.writer.close()
    //frdd.saveAsTextFile("hdfs:///user/hdfs/apk/KO-" + analysisDate + ".txt")
  }
}
package com.avcdata.vbox.clean.live

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.joda.time.Seconds

import scala.collection.mutable

/*
**Konka的直播处理
 */

object DataCleanKOLive {

    case class TVWatching(sn: String, date: String, actions: mutable.MutableList[TVWatchedItem]) {
    }

    case class TVWatchedItem(tv: String, date: String)

    case class TVWatchedResult(tv: String, date: String, hour: String, duration: Int)

    case class TVWatchedResultByMinute(tv: String, date: String, hour: String, minute: String, duration: Int, cnt: Int)

    case class Live(name:String,col1:String,col2:String,col3:String,col4:String,col5:String,col6:String,col7:String)



    def getSecondsToHourEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.hours).withSecondOfMinute(0).withMinuteOfHour(0)).getSeconds.abs
    }

    def getSecondsToMinuteEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.minutes).withSecondOfMinute(0)).getSeconds.abs
    }

    def getSecondsFromHourBegin(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime.withSecondOfMinute(0).withMinuteOfHour(0), dateTime).getSeconds.abs
    }

    def getSecondsAbs(from: DateTime, to: DateTime): Int = {
        Seconds.secondsBetween(from, to).getSeconds.abs
    }


    def getSeconds(from: DateTime, to: DateTime): Int = {
        Seconds.secondsBetween(from, to).getSeconds
    }

    def getTVWatchedResultByMinute(item: TVWatching): mutable.MutableList[TVWatchedResultByMinute] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var startItem: TVWatchedItem = null
        var startTime: DateTime = null
        var endItem: TVWatchedItem = null
        var endTime: DateTime = null
        var tempItem: TVWatchedItem = null
        var tempTime: DateTime = null
        val result = mutable.MutableList[TVWatchedResultByMinute]()

        for (i <- 0 until item.actions.size) {


            if (startItem == null) {
                startItem = item.actions.get(i).get
                startTime = DateTime.parse(startItem.date, format)
            }
            else {
                endItem = item.actions.get(i).get
                endTime = DateTime.parse(endItem.date, format)

                if (endItem.tv != startItem.tv || getSecondsAbs(tempTime, endTime) > 630) {
                    if (getSecondsAbs(tempTime, endTime) > 630)
                        endTime = DateTime.parse(tempItem.date, format) + 5.minutes

                    for (gap <- 0 to org.joda.time.Minutes.minutesBetween(startTime, endTime).getMinutes.abs) {
                        val time: DateTime = startTime + gap.minutes
                        if (time.getDayOfMonth == startTime.getDayOfMonth) {
                            if (gap == 0)
                                result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, getSecondsToMinuteEnd(startTime), 1)
                            else {
                                if (getSeconds(startTime + gap.minutes, endTime) < 60 && getSeconds(startTime + gap.minutes, endTime) > -60)
                                    result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, endTime.getSecondOfMinute, 0)
                                else
                                    result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, 60, 0)
                            }
                        }

                    }
                    startItem = endItem
                    startTime = DateTime.parse(startItem.date, format)
                    endItem = null
                    endTime = null
                }
            }


            tempItem = item.actions.get(i).get
            tempTime = DateTime.parse(tempItem.date, format)

        }

        if (startItem != null) {
            if (endItem == null)
                endTime = startTime + 5.minutes
            for (gap <- 0 to org.joda.time.Minutes.minutesBetween(startTime, endTime).getMinutes.abs) {
                val time: DateTime = startTime + gap.minutes
                if (time.getDayOfMonth == startTime.getDayOfMonth) {
                    if (gap == 0)
                        result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, getSecondsToMinuteEnd(startTime), 1)
                    else {
                        if (getSeconds(startTime + gap.minutes, endTime) < 60 && getSeconds(startTime + gap.minutes, endTime) > -60)
                            result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, endTime.getSecondOfMinute, 0)
                        else
                            result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, 60, 0)
                    }
                }


            }
        }



        result
    }


    def getTVWatchedResult(item: TVWatching): mutable.MutableList[TVWatchedResult] = {

        var tempItem = item.actions.get(0).get
        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var accumulator = 0

        var tempTime = DateTime.parse(tempItem.date, format)

        val result = mutable.MutableList[TVWatchedResult]()

        if (item.actions.size == 1) {
            if (tempTime.getMinuteOfHour > 55) {
                result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, getSecondsToHourEnd(tempTime))
                if (tempTime.getHourOfDay < 23)
                    result += new TVWatchedResult(tempItem.tv, item.date, (tempTime + 1.hours).getHourOfDay.toString, getSecondsFromHourBegin(tempTime + 5.minutes))
            }
            else
                result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, 300)
        }

        for (i <- 1 until item.actions.size) {

            val currItem = item.actions.get(i).get
            val currTime = DateTime.parse(currItem.date, format)

            if (currItem.tv == tempItem.tv) {

                if (getSecondsAbs(tempTime, currTime) < 630) {
                    //未换台
                    if (currTime.getHourOfDay == tempTime.getHourOfDay) {
                        //未跨小时
                        accumulator += getSecondsAbs(tempTime, currTime)
                    }
                    else {
                        //跨小时 记录上时段 重置计数器
                        result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, (accumulator + getSecondsToHourEnd(tempTime)))
                        accumulator = getSecondsFromHourBegin(currTime)
                    }
                }
                else {
                    if (currTime.getHourOfDay == tempTime.getHourOfDay) {
                        accumulator += 300
                    }
                    else if (currTime.getHourOfDay - tempTime.getHourOfDay > 1) {
                        if (tempTime.getMinuteOfHour > 55) {
                            result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, accumulator + getSecondsToHourEnd(tempTime))
                            if (tempTime.getHourOfDay < 23)
                                result += new TVWatchedResult(tempItem.tv, item.date, (tempTime + 1.hours).getHourOfDay.toString, getSecondsFromHourBegin(tempTime + 5.minutes))
                            accumulator = 0
                        }
                    }
                    else {
                        //跨1小时 记录上时段 重置计数器
                        result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, accumulator + getSecondsToHourEnd(tempTime))
                        accumulator = getSecondsFromHourBegin(currTime)
                    }
                }
            }
            else {
                if (getSecondsAbs(tempTime, currTime) < 630) {
                    result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, accumulator + getSecondsAbs(tempTime, currTime))
                }
                else {
                    if (tempTime.getMinuteOfHour > 55) {
                        result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, accumulator + getSecondsToHourEnd(tempTime))
                        if (tempTime.getHourOfDay < 23)
                            result += new TVWatchedResult(tempItem.tv, item.date, (tempTime + 1.hours).getHourOfDay.toString, getSecondsFromHourBegin(tempTime + 5.minutes))
                    }
                    else
                        result += new TVWatchedResult(tempItem.tv, item.date, tempTime.getHourOfDay.toString, accumulator + 300)
                }
                accumulator = 0
            }

            if (i == item.actions.size - 1) {

                if (currTime.getMinuteOfHour > 55 && currTime.getHourOfDay - tempTime.getHourOfDay > 1) {
                    result += new TVWatchedResult(currItem.tv, item.date, currTime.getHourOfDay.toString, accumulator + getSecondsToHourEnd(currTime))
                    if (currTime.getHourOfDay < 23)
                        result += new TVWatchedResult(currItem.tv, item.date, (currTime + 1.hours).getHourOfDay.toString, getSecondsFromHourBegin(currTime + 5.minutes))
                }
                else
                    result += new TVWatchedResult(currItem.tv, item.date, currTime.getHourOfDay.toString, accumulator + 300)

            }

            tempItem = currItem
            tempTime = currTime

        }
        result
    }

    def run(sc: SparkContext, analysisDate: String) = {

        //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")

        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimChannelCol = Bytes.toBytes("dim_channel")
        val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val dimMinCol = Bytes.toBytes("dim_min")
        val factCntCol = Bytes.toBytes("fact_cnt")
        val factTimeLenghtCol = Bytes.toBytes("fact_time_length")

        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

        //val count = sc.textFile("F:/avc/docs/konka/tv_logo.log.2016-08-22")
        val baseRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/tv_logo.log." + analysisDate)
        //val count = sc.textFile("/user/hdfs/rsync/KONKA/history/tv/tv" + analysisDate)
                .filter(x =>{
                    val cols = x.split('|')
                    cols.length == 7 && cols(6) != "" && cols(6).length == 19
                })
                .filter(x => {
                    val date = x.split('|')(6).substring(0, 10)
                    date == analysisDate || date == preDate || date == yesBeforeDate
                })
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split('|')
                    val tv = cols(5)
                    val sn = cols(0)
                    val date = cols(6)
                    val xs = new TVWatchedItem(tv, date)
                    val terminalTV = new TVWatching(sn, date.substring(0, 10), mutable.MutableList(xs))
                    (sn + date.substring(0, 10), terminalTV)
                })
            }).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        }).mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.date).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })

        baseRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_active_fact03")) //tracker_live_active_fact
            //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_live_active_fact"))

            try {
                items.foreach(item => {

                    val sn = item.sn

                    getTVWatchedResultByMinute(item).foreach(r => {
                        val put = new Put(Bytes.toBytes(sn + r.tv + item.date + r.hour + r.minute + "KO"))
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimChannelCol, Bytes.toBytes(KONKATVMapping.getTVStandName(r.tv)))
                        //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(r.hour))
                        put.addColumn(dimFamilyCol, dimMinCol, Bytes.toBytes(r.minute))
                        put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(r.cnt.toString))
                        put.addColumn(factFamilyCol, factTimeLenghtCol, Bytes.toBytes(r.duration.toString))
                        //println(item.sn + "\t" + item.date + "\t" + r.tv + "\t" + r.hour + "\t" + r.minute + "\t" + r.duration.toString + "\t" + r.cnt)
                        mutator.mutate(put)
                    })
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

object KONKATVMapping extends Serializable {
    private val mapping = mutable.Map(
        ("CCTV-8" -> "CCTV-8"),
        ("CCTV-12" -> "CCTV-12"),
        ("CCTV-1" -> "CCTV-1"),
        ("黑龙江卫视" -> "黑龙江卫视"),
        ("CCTV-9" -> "CCTV-9"),
        ("CCTV-5" -> "CCTV-5"),
        ("贵州卫视" -> "贵州卫视"),
        ("CCTV-14" -> "CCTV-14"),
        ("东方卫视" -> "上海东方卫视"),
        ("湖北卫视" -> "湖北卫视"),
        ("东南卫视" -> "东南卫视"),
        ("广东卫视" -> "广东卫视"),
        ("CCTV-7" -> "CCTV-7"),
        ("湖南卫视" -> "湖南卫视"),
        ("山东卫视" -> "山东卫视"),
        ("BTV北京卫视" -> "北京卫视"),
        ("青海卫视" -> "青海卫视"),
        ("CCTV-10" -> "CCTV-10"),
        ("旅游卫视" -> "旅游卫视"),
        ("甘肃卫视" -> "甘肃卫视"),
        ("重庆卫视" -> "重庆卫视"),
        ("CCTV-3" -> "CCTV-3"),
        ("新疆卫视" -> "新疆卫视"),
        ("厦门卫视" -> "厦门卫视"),
        ("CCTV-13" -> "CCTV-13"),
        ("辽宁卫视" -> "辽宁卫视"),
        ("山西卫视" -> "山西卫视"),
        ("CCTV-2" -> "CCTV-2"),
        ("宁夏卫视" -> "宁夏卫视"),
        ("安徽卫视" -> "安徽卫视"),
        ("河北卫视" -> "河北卫视"),
        ("CCTV-6" -> "CCTV-6"),
        ("浙江卫视" -> "浙江卫视"),
        ("江西卫视" -> "江西卫视"),
        ("河南卫视" -> "河南卫视"),
        ("CCTV-11" -> "CCTV-11"),
        ("广西卫视" -> "广西卫视"),
        ("江苏卫视" -> "江苏卫视"),
        ("四川卫视" -> "四川卫视"),
        ("CCTV-15" -> "CCTV-15"),
        ("云南卫视" -> "云南卫视"),
        ("CCTV-4" -> "CCTV-4"),
        ("内蒙古卫视" -> "内蒙古卫视"),
        ("西藏卫视" -> "西藏卫视"),
        ("深圳卫视" -> "深圳卫视"),
        ("天津卫视" -> "天津卫视"),
        ("陕西卫视" -> "陕西卫视"),
        ("吉林卫视" -> "吉林卫视"),
        ("CCTV-1综合" -> "CCTV-1"),
        ("CCTV-2财经" -> "CCTV-2"),
        ("CCTV-3综艺" -> "CCTV-3"),
        ("CCTV-4中文国际" -> "CCTV-4"),
        ("CCTV-5体育" -> "CCTV-5"),
        ("CCTV-6电影" -> "CCTV-6"),
        ("CCTV-7军事农业" -> "CCTV-7"),
        ("CCTV-8电视剧" -> "CCTV-8"),
        ("CCTV-9纪录" -> "CCTV-9"),
        ("CCTV-10科教" -> "CCTV-10"),
        ("CCTV-11戏曲" -> "CCTV-11"),
        ("CCTV-12社会与法" -> "CCTV-12"),
        ("CCTV-13新闻" -> "CCTV-13"),
        ("CCTV-14少儿" -> "CCTV-14"),
        ("CCTV-15音乐" -> "CCTV-15"),
        ("厦门卫视" -> "厦门卫视")
    )

    def getTVStandName(name: String): String = {
        mapping.get(name).getOrElse("其他")
    }

    def main(args: Array[String]): Unit = {
        println(DateTime.parse("2016-12-14 11:11:11".substring(0, 10)).weekOfWeekyear().get())
        println(DateTime.parse("2016-12-14 11:11:11".substring(0, 10)).monthOfYear().get())
        println(DateTime.parse("2016-12-14".substring(0, 10)).dayOfYear().get())
        println(DateTime.parse("2016-12-14".substring(0, 10)).dayOfMonth().get())

        println(DateTime.parse("2017-03-01").plusDays(-2).toString("yyyy-MM-dd").substring(5,7))
    }
}
package com.avcdata.vbox.clean.oc

import java.sql.Timestamp
import java.text.SimpleDateFormat

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

import scala.collection.SortedMap

/**
  * Created by wxy on 8/29/16.
  * 开关机的处理,根据startTime过滤数据
  */
object DataCleanKOPowerOn {

  def run(sc: SparkContext, analysisDate: String) = {
    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSeriesNoCol = Bytes.toBytes("sn")
    val dimLogtimeCol = Bytes.toBytes("logtime")
    val dimPowerONDateCol = Bytes.toBytes("power_on_day")
    val dimPowerONTimeCol = Bytes.toBytes("power_on_time")
    val factPowerLenghtCol = Bytes.toBytes("power_on_length")
    val factCntCol = Bytes.toBytes("cnt")


    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    //    LWR1415YD0019292UU51 串号
    //      116.207.89.124  IP
    //      LED42K70U_1BOM  机型
    //      rtd2995d 平台
    //      42 尺寸
    //      421000 地域代码
    //      2016-07-12 23:05:42 开机时间
    //      65 开机时长（单位：分钟）
    //    2016-07-15 04:22:31  记录创建时间
//    MKL1612D5012328XXXX1|39.128.61.72|A55U_1BOM|hisiv500|55|110000|2017-09-16 21:12:17|115|2017-09-18 03:00:36

    //val terRdd = sc.textFile("F:/avc/docs/konka/duration.log.2016-08-22")
    val terRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/duration.log." + analysisDate)
      //val terRdd = sc.textFile("/user/hdfs/rsync/KONKA/history/duration.log.since_10-01")
      .filter(x => x.split('|').length > 7)
      .filter(x => {
        val date = x.split('|')(6).substring(0, 10)
        date == analysisDate || date == preDate || date == yesBeforeDate
      })
      .mapPartitions(items => {
        items.map(line => {
          val cols = line.split('|')
          val sn = cols(0)
          val lastPowerOnDate = cols(6).substring(0, 10)
          val lastPowerOnTime = cols(6).substring(11, 13)
          val timeLenth = cols(7)
          val endTime = getEndTime(timeLenth, cols(6))
          (sn, cols(6), timeLenth, endTime)
        })
      }).repartition(100)

    terRdd.foreachPartition(items => {
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
      val hbaseConn = ConnectionFactory.createConnection(myConf)
      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal_active_fact")) //tracker_terminal_active_fact
      //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_terminal_active_fact"))

      try {
        items.foreach(line => {
          val sn = line._1
          val startTime = line._2
          val date = line._2.substring(0, 10)
          val timeLenth = line._3
          val endTime = line._4
          val map = durationSplitByHour(timeLenth, endTime, startTime)
          for (m <- map) {
            val hour = m._1.toInt
            val duraPerHour = m._2.split(";")(0)
            val cnt = m._2.split(";")(1)
            val put = new Put(Bytes.toBytes(sn + startTime + hour + "KO"))
            put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
            //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
            put.addColumn(dimFamilyCol, dimPowerONDateCol, Bytes.toBytes(date))
            put.addColumn(dimFamilyCol, dimPowerONTimeCol, Bytes.toBytes(hour.toString))
            //put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(timeLenth))
            put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(duraPerHour.toString))
            put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(cnt))
            //println(sn + "\t" + date + "\t" + startTime + "\t" + endTime
            //    + "\t" + timeLenth + "\t" + hour + "\t" + duraPerHour + "\t" + cnt)
            mutator.mutate(put)
          }
        })
        mutator.flush()
      } finally {
        mutator.close()
        hbaseConn.close()
      }
    })
  }


  /**
    * 时间戳转为时间
    *
    * @param timestamp 时间戳
    * @return
    */
  def timestampToDate(timestamp: String): String = {
    val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val d = format.format(timestamp.toLong)
    d
  }

  /**
    * 时间转为时间戳,单位是毫秒
    * @param date 时间
    * @return
    */
  def dateTotimestamp(date: String): Long = {
    val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val d = format.parse(date)
    val time = new Timestamp(d.getTime())
    (time.getTime)
  }

  /**
    * 获取关机的时间，供后面的时长和次数使用
    * @param duration  时长，分钟
    * @param startTime 开始时间，格式是2016-11-02 14:59:10
    * @return
    */
  def getEndTime(duration: String, startTime: String): String = {
    val start = dateTotimestamp(startTime)
    val endtTime = timestampToDate((start + (duration).toLong * 60 * 1000).toString)

    endtTime
  }

  /**
    * 直播拆分为小时单位
    * @param duration  时长(分钟)
    * @param eTime     结束时间
    * @param startTime 开始时间
    * @return
    */
  def durationSplitByHour(duration: String, eTime: String, startTime: String): SortedMap[String, String] = {
    var endTime = eTime
    var startHour = startTime.substring(11, 13) //获取小时
    val endHour = endTime.substring(11, 13)
    var startDay = startTime.substring(8, 10) //获取天
    val endDay = endTime.substring(8, 10)
    var hours = 0

    /*if (endDay == startDay) {
        //同一天
        hours = endHour.toInt - startHour.toInt
    } else if (endDay > startDay) {
        //跨天
        startTime = endTime.substring(0, 10) + " 00:00:00"
        hours = endHour.toInt
    }*/
    if (endTime.substring(5, 7) == startTime.substring(5, 7)) {
      if (endDay == startDay) {
        //同一天
        hours = endHour.toInt - startHour.toInt
      } else if (endDay > startDay) {
        //跨天
        endTime = startTime.substring(0, 10) + " 23:60:00"
        hours = 23 - startHour.toInt
      }
    } else if (endTime.substring(5, 7) > startTime.substring(5, 7)) {
      //跨月
      endTime = startTime.substring(0, 10) + " 23:60:00"
      hours = 23 - startHour.toInt
    }

    val map = handleTime(duration, startTime, endTime, hours)
    map
  }

  /**
    * 处理开关机时间
    * @param duration  时长(分钟)
    * @param startTime 开始时间
    * @param endTime   结束时间
    * @param hours     开始到结束的小时数
    * @return
    */
  def handleTime(duration: String, startTime: String, endTime: String, hours: Int): SortedMap[String, String] = {
    var currTime = startTime
    var map: SortedMap[String, String] = SortedMap()
    var cntFlag = 0

    if (hours == 0) {
      //map += (startTime.substring(11, 13) -> (duration + ";1"))
      //println("================ hours : " + hours)
      var dura = duration.toLong * 60000
      //当出现endtime为2016-11-14 00:51:37,但时长又是夸了天的，但是这个时候把startTime置为2016-11-14 00:00:00了
      //需要对时长duration做一个处理
      if (startTime.substring(11, 13).equals("00") && endTime.substring(11, 13).equals("00"))
        dura = dateTotimestamp(endTime) - dateTotimestamp(startTime)
      map += (startTime.substring(11, 13) -> ((dura / 60000).toString + ";1"))
      map
    }
    else {
      for (i <- 0 to hours) {
        var hour = currTime.substring(11, 13).toInt //获取当前的小时时间
        var nextTime = ""
        if (hour + 1 < 10) {
          nextTime = currTime.substring(0, 10) + " 0" + (hour + 1) + ":00:00" //个位数补0
        } else {
          nextTime = currTime.substring(0, 10) + " " + (hour + 1) + ":00:00" //得到下一小时的时间
        }
        var dura = 0l

        //println("nextTime : " + nextTime)
        //println("hour : " + hour + ", currtime : " + currTime)
        //最后一个小时的
        if (hour == endTime.substring(11, 13).toInt) {
          //  毫秒/60000-->分钟
          dura = dateTotimestamp(endTime) - dateTotimestamp(currTime)
          map += (endTime.substring(11, 13) -> ((dura / 60000).toString + ";0"))
          cntFlag = 0
          //println("=======2========= hours : " + hours)
        } else {
          //  毫秒/60000-->分钟
          dura = dateTotimestamp(nextTime) - dateTotimestamp(currTime)
          if (cntFlag == 0) {
            map += (currTime.substring(11, 13) -> ((dura / 60000).toString + ";1"))
            cntFlag = 1
          } else {
            map += (currTime.substring(11, 13) -> ((dura / 60000).toString + ";0"))
          }

        }

        currTime = nextTime
      }
      map
    }

  }


}
package com.avcdata.vbox.clean.coocaa

import com.avcdata.vbox.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by wxy on 8/29/16.
  * 终端处理
  */
object DataCleanKOTerminal {
    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")

        val sqlc = new HiveContext(sc)
        val snBrand = sqlc.sql("select brand,model,license from hr.ko_mp_brand").mapPartitions(items => {
            items.map(line => {
                (line(0).toString+"0x01"+line(1).toString, line(2).toString)
            })
        })

        //sc.textFile("F:/avc/docs/konka/duration.log.2017-03-29")
        val koRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/duration.log." + analysisDate)
            .filter(x => x.split('|').length > 7).mapPartitions(items =>{
            items.map(item => {
                val cols = item.split('|')
                val sn = cols(0)
                val brand = "KO"
                val model = cols(2)
                val lastPowerOn = cols(6)
                val size = cols(4)
                //val area = cols(5).substring(0, 3)
                val area = Codearea.getPc(cols(5)).split(",")(0)
                val province = Codearea.getPc(cols(5)).split(",")(1)
                val city = Codearea.getPc(cols(5)).split(",")(2)
                var clevel = ""
                if(Codearea.prolist.contains(province)) {
                    clevel = Codearea.getCl(city)
                } else {
                    clevel = "港澳台及国外"
                }

                (brand+"0x01"+model, item)
            })
        })

        val reRdd = koRdd.leftOuterJoin(snBrand).filter(x => !x._2.toString.contains("None")).mapPartitions(items => {
            items.map(item => {
                (item._2._2.get, item._2._1)
            })
        })
        //sc.textFile("/user/hdfs/rsync/KONKA/history/duration.log.since_10-01")
        reRdd.foreachPartition(items => {
                val myConf = HBaseConfiguration.create()
                myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
                myConf.set("hbase.zookeeper.property.clientPort", "2181")
                val hbaseConn = ConnectionFactory.createConnection(myConf)
                val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal"))
                //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_terminal"))

                try {
                    items.foreach(line => {
                        val cols = line._2.split('|')
                        val sn = cols(0)
                        val brand = "KO"
                        val model = cols(2)
                        val lastPowerOn = cols(6)
                        val size = cols(4)
                        //val area = cols(5).substring(0, 3)
                        val area = Codearea.getPc(cols(5)).split(",")(0)
                        val province = Codearea.getPc(cols(5)).split(",")(1)
                        val city = Codearea.getPc(cols(5)).split(",")(2)
                        var clevel = ""
                        if(Codearea.prolist.contains(province)) {
                            clevel = Codearea.getCl(city)
                        } else {
                            clevel = "港澳台及国外"
                        }
                        val license = line._1

                        val put = new Put(Bytes.toBytes(sn + "KO"))
                        put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                        put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes(license))
                        put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                        put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                        put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                        put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                        put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                        put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                        put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))
                        //println(cols(5) + "\t" + area + "\t" + province + "\t" + city)
                        mutator.mutate(put)
                    })
                    mutator.flush()
                } finally {
                    mutator.close()
                    hbaseConn.close()
                }
            })
    }
}
package com.avcdata.spark.job.playcrawler

import com.avcdata.vbox.util.HBaseUtils
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 到剧搜索指数数据清洗
  */
object DataCleanPlaySearchIndex {
  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SearchIndexDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-01")
    sc.stop()
  }

  def run(sc: SparkContext, analysisDate: String) = {

    //////////////test/////////////////////////////////
    //    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\爬虫\\11月\\"+ analysisDate + "_index_360.csv"
    //////////////test/////////////////////////////////

    val hdfsPath360 = "/user/hdfs/rsync/playcrawler/" + analysisDate + "/" + analysisDate + "_index_360.csv"
    val hdfsPathBaidu = "/user/hdfs/rsync/playcrawler/" + analysisDate + "/" + analysisDate + "_index_baidu.csv"

    val result360RDD = sc.textFile(hdfsPath360).distinct().filter(line => {
      !line.contains("category")
    })

      .map(line => {
        val cols = line.split(",")
        val category = cols(0)
        val index = cols(1)
        val name = cols(2)
        val time = cols(3)
        val platform = hdfsPath360.substring(hdfsPath360.lastIndexOf("_") + 1, hdfsPath360.lastIndexOf("."))
        platform + "\t" + category + "\t" + index + "\t" + name + "\t" + time
      })

    val resultBaiduRDD = sc.textFile(hdfsPathBaidu).distinct().filter(line => {
      !line.contains("category")
    })
      .map(line => {
        val cols = line.split(",")
        val category = cols(0)
        val index = cols(1)
        val name = cols(2)
        val time = cols(3)
        val platform = hdfsPathBaidu.substring(hdfsPathBaidu.lastIndexOf("_") + 1, hdfsPathBaidu.lastIndexOf("."))
        platform + "\t" + category + "\t" + index + "\t" + name + "\t" + time
      })

    val resultRDD = result360RDD.union(resultBaiduRDD)
    resultRDD
      //    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_plays_active_index")

      try {

        lines.foreach(line => {
          var i = 0

          val cols = line.split("\t")

          val platform = cols(i)
          i = i + 1

          val category = cols(i)
          i = i + 1

          val index = cols(i)
          i = i + 1

          val name = cols(i)
          i = i + 1

          val time = cols(i)

          val sortedLine = platform + "\t" + category + "\t" + index + "\t" + name + "\t" + time

          //          println(sortedLine)

          mutator.mutate(HBaseUtils.getPut_Plays_SearchIndex(sortedLine))


        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

  }

}
package com.avcdata.vbox.clean.apk

import com.avcdata.vbox.clean.common.ApkFactPartition
import com.avcdata.vbox.util.{TimeUtils, ValidateUtils}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @define TCL开关机数据清洗
  */
object DataCleanTCLApk {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {


    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////


    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._


    val twoDaysAgo = TimeUtils.dateStrAddDays(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)
    val oneDaysAgo = TimeUtils.dateStrAddDays(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE, -1)


    val hdfsPathArr = Array[String](
      "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log",
      "/user/hdfs/rsync/TCL/" + oneDaysAgo + "/userBehaviourLogClean." + oneDaysAgo + ".log",
      "/user/hdfs/rsync/TCL/" + twoDaysAgo + "/userBehaviourLogClean." + twoDaysAgo + ".log"
    )

    val initRDD =
      sc.textFile(hdfsPathArr(0))
        .union(sc.textFile(hdfsPathArr(1)))
        .union(sc.textFile(hdfsPathArr(2)))

    val daysArr = Array[String](
      twoDaysAgo
    )


    //TODO apk
    val resultRDD = initRDD
      .map(line => {
        line.replaceAll("com\\.tcl\\.TVBasicBehavior", "#").replaceAll("com\\.tcl\\.tv", "#")
      })
      .flatMap(line => {
        val arr = new ArrayBuffer[(String, String, String, Long, Long)]()
        var sn = "#"
        var date = analysisDate
        val cols = line.split(";")

        for (i <- 0 until cols.length) {

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

          if (!cols(i).contains("IPAddress") && cols(i).split('.').length > 1) {

            val startTime = cols(i).substring(cols(i).indexOf("1=") + 2, cols(i).indexOf(",2="))

            if (startTime.length == 19 && !startTime.contains(",") && !ValidateUtils.isContainsLetter(startTime)) {
              date = startTime.substring(0, 10)

              val endTime = cols(i).substring(cols(i).indexOf("2=") + 2, cols(i).indexOf("}"))

              if (endTime.length == 19) {
                arr.+=((sn, date, cols(i).substring(0, cols(i).indexOf(",{")), TimeUtils.convertDateStr2TimeStamp(startTime, "yyyy-MM-dd HH:mm:ss"), TimeUtils.convertDateStr2TimeStamp(endTime, "yyyy-MM-dd HH:mm:ss")))
              }
            }
          }
        }
        arr
      })
      //只取日期近四天的数
      .filter(line => {
      val sn = line._1
      val date = line._2
      daysArr.contains(date)
    })

      //TODO 分时
      .flatMap(line => {
      val sn = line._1
      val date = line._2
      val dim_apk = line._3
      TimeUtils.splitTimeByHour(sn + "\t" + date + "\t" + dim_apk + "\t", line._4, line._5)
    })
      //f8b94128207616a56920e7e44094c896	2017-08-29	com.audiocn.kalaok.tv	12	1
      .map(line => {
      val cols = line.split("\t")
      val dim_sn = cols(0)
      val dim_date = cols(1)
      val dim_apk = cols(2)
      val dim_hour = cols(3)
      val fact_cnt = cols(4)
      val fact_duration = cols(5)

      val key = dim_sn + dim_apk + dim_date.substring(0, 10) + dim_hour + "CH"

      ApkFactPartition(key: String,
        dim_sn: String,
        dim_apk: String,
        dim_date: String,
        dim_hour: String,
        fact_cnt: String,
        fact_duration: String,
        dim_date: String)
    })

    ///////////////////////test///////////////////////
    //    resultRDD.take(100).foreach(println)
    //////////////////////test////////////////////////

    resultRDD.toDF.registerTempTable("tmp_table")

    println(resultRDD.count)

    val sql =
      """
          insert overwrite table hr.tracker_apk_fact_partition_tcl partition(date='""" + twoDaysAgo +
        """')  select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from tmp_table where dim_date='""" + twoDaysAgo +
        """' and fact_duration>'0'
        """.stripMargin

    println(sql)

    hiveContext.sql(sql)

  }


}
package com.avcdata.vbox.clean.apk

import com.avcdata.vbox.util.{HBaseUtils, TimeUtils, ValidateUtils}
import com.github.nscala_time.time.Imports._
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @define TCL开关机数据清洗
  */
object DataCleanTCLApk01 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {


    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val fst_Day = DateTime.parse(analysisDate).plusDays(-3).toString("yyyy-MM-dd")
    val sec_Day = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
    val td_Day = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")

    val daysArr = Array[String](
      fst_Day,
      sec_Day,
      td_Day,
      analysisDate
    )

    val initRDD = sc.textFile(hdfsPath)

    //TODO apk
    initRDD
      .map(line => {
        line.replaceAll("com\\.tcl\\.TVBasicBehavior", "#").replaceAll("com\\.tcl\\.tv", "#")
      })
      .flatMap(line => {
        val arr = new ArrayBuffer[(String, String, String, Long, Long)]()
        var sn = "#"
        var date = analysisDate
        val cols = line.split(";")

        for (i <- 0 until cols.length) {

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

          if (!cols(i).contains("IPAddress") && cols(i).split('.').length > 1) {

            val startTime = cols(i).substring(cols(i).indexOf("1=") + 2, cols(i).indexOf(",2="))

            if (startTime.length == 19 && !startTime.contains(",") && !ValidateUtils.isContainsLetter(startTime)) {
              date = startTime.substring(0, 10)

              val endTime = cols(i).substring(cols(i).indexOf("2=") + 2, cols(i).indexOf("}"))

              if (endTime.length == 19) {
                arr.+=((sn, date, cols(i).substring(0, cols(i).indexOf(",{")), TimeUtils.convertDateStr2TimeStamp(startTime, "yyyy-MM-dd HH:mm:ss"), TimeUtils.convertDateStr2TimeStamp(endTime, "yyyy-MM-dd HH:mm:ss")))
              }
            }
          }
        }
        arr
      })
      //只取日期近四天的数
      .filter(line => {
      val sn = line._1
      val date = line._2
      daysArr.contains(date)
    })

      //TODO 分时
      .flatMap(line => {
      val sn = line._1
      val date = line._2
      val dim_apk = line._3
      TimeUtils.splitTimeByHour(sn + "\t" + date + "\t" + dim_apk + "\t", line._4, line._5)
    })
      //f8b94128207616a56920e7e44094c896	2017-08-29	com.audiocn.kalaok.tv	12	1
      .map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val date = cols(1)
      val dim_apk = cols(2)
      val hour = cols(3)
      val cnt = cols(4)
      val duration = cols(5)

      (sn, date, dim_apk, hour, cnt, duration)
    })

      //      .foreach(println(_))

      //TODO  写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_apk_fact_tcl")
      //      val mutator = HBaseUtils.getMutator("tracker_apk_active_fact")

      try {

        lines.foreach(line => {

          val sn = line._1

          val date = line._2

          val apkPackage = line._3

          val hour = line._4

          val launchCnt = line._5

          val duration = line._6

          val sortedLine = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration

          //          println(sortedLine)


          val brand = "TCL"

          mutator.mutate(HBaseUtils.getPut_apk(sortedLine, brand))

        })
        mutator.flush()

      } finally {
        mutator.close()
        HBaseUtils.getHbaseConn().close()
      }
    }
    )
  }


}
package com.avcdata.vbox.clean.oc

import com.avcdata.vbox.clean.common.OCFactPartition
import com.avcdata.vbox.util.{TimeUtils, ValidateUtils}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @define TCL开关机数据清洗
  */
object DataCleanTCLPowerOn {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {


    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val twoDaysAgo = TimeUtils.dateStrAddDays(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)
    val oneDaysAgo = TimeUtils.dateStrAddDays(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE, -1)


    val hdfsPathArr = Array[String](
      "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log",
      "/user/hdfs/rsync/TCL/" + oneDaysAgo + "/userBehaviourLogClean." + oneDaysAgo + ".log",
      "/user/hdfs/rsync/TCL/" + twoDaysAgo + "/userBehaviourLogClean." + twoDaysAgo + ".log"
    )

    val initRDD =
      sc.textFile(hdfsPathArr(0))
        .union(sc.textFile(hdfsPathArr(1)))
        .union(sc.textFile(hdfsPathArr(2)))


    val daysArr = Array[String](
      twoDaysAgo
    )

    //    val hiveContext = new SQLContext(sc)
    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    //TODO 提取
    val resultRDD = initRDD
      .filter(_.contains(",2="))
      .flatMap(line => {
        val arr = new ArrayBuffer[(String, String, Long, Long)]()
        var sn = "#"
        var date = analysisDate
        val cols = line.split(";")

        for (i <- 0 until cols.length) {

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

          if (cols(i).contains("1=") && cols(i).contains(",2=") && (cols(i).contains("com.tcl.TVBasicBehavior") || cols(i).contains("com.tcl.tv"))) {

            val startTime = cols(i).substring(cols(i).indexOf("1=") + 2, cols(i).indexOf(",2="))


            if (startTime.length == 19 && !startTime.contains(",") && !ValidateUtils.isContainsLetter(startTime)) {
              date = startTime.substring(0, 10)

              val endTime = cols(i).substring(cols(i).indexOf("2=") + 2, cols(i).indexOf("}"))
              if (endTime.length == 19) {
                arr.+=((sn, date, TimeUtils.convertDateStr2TimeStamp(startTime, "yyyy-MM-dd HH:mm:ss"), TimeUtils.convertDateStr2TimeStamp(endTime, "yyyy-MM-dd HH:mm:ss")))
              }
            }

          }

        }

        arr
      })

      .filter(line => {
        val sn = line._1
        val date = line._2
        daysArr.contains(date)
      })

      //(f8b94128207616a56920e7e44094c896,2017-08-29 ,2017-08-29 12:54:17,2017-08-29 22:15:02)
      //TODO 分时
      .flatMap(line => {
      val sn = line._1
      val date = line._2
      TimeUtils.splitTimeByHour(sn + "\t" + date + "\t", line._3, line._4)
    })
      //去重
      .distinct

      //f8b94128207616a56920e7e44094c896	2017-08-29	11	1	1832
      .map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val power_on_day = cols(1)
      val power_on_time = cols(2)
      val cnt = cols(3)
      val power_on_length = cols(4)

      val key = sn + power_on_day + power_on_time + "TCL"

      OCFactPartition(
        key: String,
        sn: String,
        power_on_day: String,
        power_on_time: String,
        Math.round(power_on_length.toDouble / 60).toString: String,
        cnt: String,
        power_on_day: String
      )
    })

    resultRDD.toDF.registerTempTable("tmp_table")

    println(resultRDD.count)

    val sql =
      """
        insert overwrite table hr.tracker_oc_fact_partition_tcl partition(date='""" + twoDaysAgo +
        """') select key,sn,power_on_day,power_on_time,power_on_length,cnt from tmp_table where power_on_day='""" +
        twoDaysAgo +
        """'""".stripMargin

    println(sql)

    hiveContext.sql(sql)

  }


}
package com.avcdata.vbox.clean.oc

import com.avcdata.vbox.util.{HBaseUtils, TimeUtils, ValidateUtils}
import com.github.nscala_time.time.Imports._
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @define TCL开关机数据清洗
  */
object DataCleanTCLPowerOn20170918 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {


    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)


    val fst_Day = DateTime.parse(analysisDate).plusDays(-3).toString("yyyy-MM-dd")
    val sec_Day = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
    val td_Day = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")

    val daysArr = Array[String](
      fst_Day,
      sec_Day,
      td_Day,
      analysisDate
    )




    //TODO 开关机


    //TODO 提取
    initRDD
      .filter(_.contains(",2="))
      .flatMap(line => {
        val arr = new ArrayBuffer[(String, String, Long, Long)]()
        var sn = "#"
        var date = analysisDate
        val cols = line.split(";")

        for (i <- 0 until cols.length) {

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

          if (cols(i).contains("1=") && cols(i).contains(",2=") && (cols(i).contains("com.tcl.TVBasicBehavior") || cols(i).contains("com.tcl.tv"))) {

            val startTime = cols(i).substring(cols(i).indexOf("1=") + 2, cols(i).indexOf(",2="))


            if (startTime.length == 19 && !startTime.contains(",")&& !ValidateUtils.isContainsLetter(startTime)) {
              date = startTime.substring(0, 10)

              val endTime = cols(i).substring(cols(i).indexOf("2=") + 2, cols(i).indexOf("}"))
              if(endTime.length==19){
                arr.+=((sn, date, TimeUtils.convertDateStr2TimeStamp(startTime, "yyyy-MM-dd HH:mm:ss"), TimeUtils.convertDateStr2TimeStamp(endTime, "yyyy-MM-dd HH:mm:ss")))
              }
            }

          }

        }

        arr
      })

      //只取日期近四天的数
      .filter(line => {
      val sn = line._1
      val date = line._2
      daysArr.contains(date)
    })

      //(f8b94128207616a56920e7e44094c896,2017-08-29 ,2017-08-29 12:54:17,2017-08-29 22:15:02)
      //TODO 分时
      .flatMap(line => {
      val sn = line._1
      val date = line._2
      TimeUtils.splitTimeByHour(sn + "\t" + date + "\t", line._3, line._4)
    })

      //f8b94128207616a56920e7e44094c896	2017-08-29	11	1	1832
      .map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val date = cols(1)
      val hour = cols(2)
      val cnt = cols(3)
      val duration = cols(4)

      (sn, date, hour, cnt, duration)
    })

      .foreachPartition(lines => {

        val mutator = HBaseUtils.getMutator("tracker_oc_fact_tcl")

        try {

          lines.foreach(line => {

            val sn = line._1

            val date = line._2

            val hour = line._3

            val cnt = line._4

            //单位 分钟
            val duration = Math.round(line._5.toDouble / 60)


            val brand = "TCL"

            val sortedLine = sn + "\t" + date + "\t" + hour + "\t" + cnt + "\t" + duration


            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_oc(sortedLine, brand))
          })
          mutator.flush()

        } finally {
          mutator.close()
        }
      }

      )

  }


}
package com.avcdata.vbox.clean.terminal

import com.avcdata.vbox.common.Codearea
import com.avcdata.vbox.util.{HBaseUtils, UDFUtils, ValidateUtils}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal_tcl"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct

    //    rdd01.cache()

    //TODO 和IP库匹配

    //TODO 将终端信息放入广播
    val terminalBr = sc.broadcast(rdd01.collect())

    //    "[{""w"":""-33.867900"",""j"":""151.207000"",""p"":"""",""c"":"""",""d"":""""}]"	""
    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo/aiwen_free_district_v2_0_4.txt")
      .filter(line => {
        val cols = line.split("\t")
        val minip = cols(1).replaceAll("\"", "")
        val maxip = cols(2).replaceAll("\"", "")
        val multiarea = cols(6).replaceAll("\"", "")

        var flag = false

        val multiareaCols = multiarea.split(",")

        flag = multiareaCols.length > 3

        if (flag) {
          val provinceTmp = multiareaCols(2)
          val province = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

          val cityTmp = multiareaCols(3)
          val city = cityTmp.substring(cityTmp.indexOf(":") + 1)

          flag = !province.isEmpty && !city.isEmpty

        }

        flag && line.contains("中国") && !line.contains("香港") && multiarea.length > 80
      })

      .map(line => {
        val cols = line.split("\t")
        val minip = cols(1).replaceAll("\"", "")
        val maxip = cols(2).replaceAll("\"", "")
        val multiarea = cols(6).replaceAll("\"", "")

        val provinceTmp = multiarea.split(",")(2)
        val province = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

        val cityTmp = multiarea.split(",")(3)
        val city = cityTmp.substring(cityTmp.indexOf(":") + 1)

        (province + "\t" + city, minip + "\t" + maxip)
      }).reduceByKey((pre, post) => {
      pre + "#" + post
    })

    //    ipGeoRDD.cache()

    val resultRDD = ipGeoRDD.flatMap(line => {
      val rightCols01 = line._1.split("\t")
      val province = rightCols01(0)
      val city = rightCols01(1)

      val minMaxIpArr = line._2.split("#")

      val terminalArr = terminalBr.value

      val resultArr = new scala.collection.mutable.ArrayBuffer[String]

      var flag = false

      for (terminal <- terminalArr) {

        val cols = terminal.split("\t")

        val ip = cols(0)
        val sn = cols(1)
        val license = cols(2)

        val ipNum = UDFUtils.inet_aton(ip)

        import scala.util.control.Breaks._

        breakable(
          for (i <- 0 until minMaxIpArr.length) {
            val cols = minMaxIpArr(i).split("\t")
            val minip = cols(0).toLong
            val maxip = cols(1).toLong
            flag = (ipNum <= maxip && ipNum >= minip)
            if (ValidateUtils.isContainsCN(province) && ValidateUtils.isContainsCN(city))
              resultArr.+=(sn + "\t" + license + "\t" + province + "\t" + city)
            break
          })
      }
      resultArr
    })


    //      .saveAsTextFile("/tmp/tclTerminalBR/" + System.currentTimeMillis())


    //    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    val pccArrBroadcast = sc.broadcast(pccArr)


    //    //TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator(htableName)

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = cols(1)

          var province = cols(2)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""

          //型号
          val model = ""
          val size = ""
          val city = cols(3)
          val sn = cols(0)

          //大区
          val area = Codearea.getArea(province)

          //城市级别
          //          val citylevel = Codearea.getCl(city)

          //TODO 根据映射表获取城市级别
          var citylevel = "港澳台及国外"
          val pccArrValue = pccArrBroadcast.value

          import scala.util.control.Breaks._
          breakable(
            for (i <- 0 until pccArrValue.length) {
              if (city.equals(pccArrValue(i).getString(1))) {
                citylevel = pccArrValue(i).getString(2)
                break
              }
            }
          )
          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal_tcl("TCL", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }


}
package com.avcdata.vbox.clean.terminal

import java.sql.ResultSet

import com.avcdata.vbox.util.MDBManager
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext};

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal02 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)

    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    //
    //    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    //    val pccArrBroadcast = sc.broadcast(pccArr)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct

    //TODO 和IP库匹配
    /////////////从hdfs文件读取////////////////////////////////////////
    //    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo").filter(!_.contains("multiarea")).filter(_.contains("中国"))
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //
    //      val province = cols(0)
    //      val city = cols(0)
    //
    //      (ip, (province + "\t" + city))
    //    })

    ///////////////从mysql读取////////////////////////////////////

    println("@@@@@@@@@@@@@@@@@@@" + rdd01.partitions)

    rdd01.mapPartitions(lines => {
      val config: Config = ConfigFactory.load()
      val dbc = "jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false"
      classOf[com.mysql.jdbc.Driver]
      var conn = MDBManager.getMDBManager(true).getConnection
      try {
        lines.map(line => {
          val cols = line.split("\t")
          val ip = cols(0)
          val province = cols(1)
          val city = cols(2)
          if (conn.isClosed) {
            conn = MDBManager.getMDBManager(true).getConnection
          }
          val statement = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)
          val rs = statement.executeQuery("SELECT multiarea FROM ip_geo WHERE INET_ATON('" + ip + "') BETWEEN minip AND maxip AND country = '中国' LIMIT 1 ")
          var areaInfo = "#"
          while (rs.next) {
            areaInfo = rs.getString("multiarea")
          }
          //[{"w":"40.026203","j":"116.227029","p":"北京市","c":"北京市","d":"海淀区"}, {"w":"39.645216","j":"116.412542","p":"北京市","c":"北京市","d":"大兴区"}]

          //          val cols = areaInfo.split(",")

          (areaInfo, province, city)
        }
        )

      } finally {
        conn.close
      }
    }
    ).saveAsTextFile("/tmp/tclterminal" + System.currentTimeMillis())

    //写入hbase
    //    rdd01.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator(htableName)
    //
    //      try {
    //
    //        items.foreach(line => {
    //          val cols = line.split('\t')
    //
    //          //牌照
    //          val license = cols(6)
    //
    //          var province = cols(1)
    //
    //
    //          if (province.equals("未匹配")) {
    //            province = "其他"
    //          }
    //
    //          //激活时间
    //          val last_poweron = ""
    //
    //          //型号
    //          val model = cols(3) + "_" + cols(4)
    //          val size = cols(5)
    //          val city = cols(2)
    //          val sn = cols(0)
    //
    //          //大区
    //          val area = Codearea.getArea(province)
    //
    //          //城市级别
    //          //          val citylevel = Codearea.getCl(city)
    //
    //
    //          //TODO 根据映射表获取城市级别
    //          var citylevel = "港澳台及国外"
    //          val pccArrValue = pccArrBroadcast.value
    //          breakable(
    //            for (i <- 0 until pccArrValue.length) {
    //              if (city.equals(pccArrValue(i).getString(1))) {
    //                citylevel = pccArrValue(i).getString(2)
    //                break
    //              }
    //            }
    //          )
    //          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel
    //
    //          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
    //          mutator.mutate(HBaseUtils.getPut_terminal("TCL", orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //      }
    //    }
    //
    //    )

  }


}
package com.avcdata.vbox.clean.terminal

import java.sql.ResultSet

import org.apache.spark.{SparkConf, SparkContext};

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal03 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)

    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    //
    //    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    //    val pccArrBroadcast = sc.broadcast(pccArr)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct

    //TODO 和IP库匹配
    /////////////从hdfs文件读取////////////////////////////////////////
    //    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo").filter(!_.contains("multiarea")).filter(_.contains("中国"))
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //
    //      val province = cols(0)
    //      val city = cols(0)
    //
    //      (ip, (province + "\t" + city))
    //    })

    ///////////////从mysql读取////////////////////////////////////

    println("@@@@@@@@@@@@@@@@@@@" + rdd01.partitions)

    rdd01.mapPartitions(data2MySQL).saveAsTextFile("/tmp/tclterminal" + System.currentTimeMillis())

    //写入hbase
    //    rdd01.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator(htableName)
    //
    //      try {
    //
    //        items.foreach(line => {
    //          val cols = line.split('\t')
    //
    //          //牌照
    //          val license = cols(6)
    //
    //          var province = cols(1)
    //
    //
    //          if (province.equals("未匹配")) {
    //            province = "其他"
    //          }
    //
    //          //激活时间
    //          val last_poweron = ""
    //
    //          //型号
    //          val model = cols(3) + "_" + cols(4)
    //          val size = cols(5)
    //          val city = cols(2)
    //          val sn = cols(0)
    //
    //          //大区
    //          val area = Codearea.getArea(province)
    //
    //          //城市级别
    //          //          val citylevel = Codearea.getCl(city)
    //
    //
    //          //TODO 根据映射表获取城市级别
    //          var citylevel = "港澳台及国外"
    //          val pccArrValue = pccArrBroadcast.value
    //          breakable(
    //            for (i <- 0 until pccArrValue.length) {
    //              if (city.equals(pccArrValue(i).getString(1))) {
    //                citylevel = pccArrValue(i).getString(2)
    //                break
    //              }
    //            }
    //          )
    //          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel
    //
    //          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
    //          mutator.mutate(HBaseUtils.getPut_terminal("TCL", orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //      }
    //    }
    //
    //    )

  }


  val data2MySQL = (iterator: Iterator[String]) => {
    var conn: java.sql.Connection = null
    try {
      conn = java.sql.DriverManager.getConnection("jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false",
        "root","new.1234")
      iterator.map(line => {
        val cols = line.split("\t")
        val ip = cols(0)
        val province = cols(1)
        val city = cols(2)

        val statement = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)
        val rs = statement.executeQuery("SELECT multiarea FROM ip_geo WHERE INET_ATON('" + ip + "') BETWEEN minip AND maxip AND country = '中国' LIMIT 1 ")
        var areaInfo = "#"
        while (rs.next) {
          areaInfo = rs.getString("multiarea")
        }
        (areaInfo, province, city)
      })
    } finally {
      if (conn != null)
        conn.close()
    }
  }


}
package com.avcdata.vbox.clean.terminal

import java.sql.ResultSet

import com.avcdata.vbox.util.MDBManager
import org.apache.spark.{SparkConf, SparkContext};

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal04 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)

    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    //
    //    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    //    val pccArrBroadcast = sc.broadcast(pccArr)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct

    //TODO 和IP库匹配
    /////////////从hdfs文件读取////////////////////////////////////////
    //    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo").filter(!_.contains("multiarea")).filter(_.contains("中国"))
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //
    //      val province = cols(0)
    //      val city = cols(0)
    //
    //      (ip, (province + "\t" + city))
    //    })

    ///////////////从mysql读取////////////////////////////////////

    println("@@@@@@@@@@@@@@@@@@@" + rdd01.partitions)

    rdd01.mapPartitions(data2MySQL).saveAsTextFile("/tmp/tclterminal" + System.currentTimeMillis())

    //写入hbase
    //    rdd01.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator(htableName)
    //
    //      try {
    //
    //        items.foreach(line => {
    //          val cols = line.split('\t')
    //
    //          //牌照
    //          val license = cols(6)
    //
    //          var province = cols(1)
    //
    //
    //          if (province.equals("未匹配")) {
    //            province = "其他"
    //          }
    //
    //          //激活时间
    //          val last_poweron = ""
    //
    //          //型号
    //          val model = cols(3) + "_" + cols(4)
    //          val size = cols(5)
    //          val city = cols(2)
    //          val sn = cols(0)
    //
    //          //大区
    //          val area = Codearea.getArea(province)
    //
    //          //城市级别
    //          //          val citylevel = Codearea.getCl(city)
    //
    //
    //          //TODO 根据映射表获取城市级别
    //          var citylevel = "港澳台及国外"
    //          val pccArrValue = pccArrBroadcast.value
    //          breakable(
    //            for (i <- 0 until pccArrValue.length) {
    //              if (city.equals(pccArrValue(i).getString(1))) {
    //                citylevel = pccArrValue(i).getString(2)
    //                break
    //              }
    //            }
    //          )
    //          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel
    //
    //          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
    //          mutator.mutate(HBaseUtils.getPut_terminal("TCL", orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //      }
    //    }
    //
    //    )

  }


  val data2MySQL = (iterator: Iterator[String]) => {
    var conn: java.sql.Connection = null
    try {
      conn = MDBManager.getMDBManager(true).getConnection
      iterator.map(line => {
        val cols = line.split("\t")
        val ip = cols(0)
        val province = cols(1)
        val city = cols(2)
        if (conn.isClosed) {
          conn = MDBManager.getMDBManager(true).getConnection
        }
        val statement = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)
        val rs = statement.executeQuery("SELECT multiarea FROM ip_geo WHERE INET_ATON('" + ip + "') BETWEEN minip AND maxip AND country = '中国' LIMIT 1 ")
        var areaInfo = "#"
        while (rs.next) {
          areaInfo = rs.getString("multiarea")
        }
        (areaInfo, province, city)
      })
    } finally {
      if (conn != null)
        conn.close()
    }
  }


}
package com.avcdata.vbox.clean.terminal

import java.sql.ResultSet

import com.avcdata.vbox.util.MDBManager
import org.apache.spark.{SparkConf, SparkContext};

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal05 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)

    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    //
    //    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    //    val pccArrBroadcast = sc.broadcast(pccArr)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct

    //TODO 和IP库匹配
    /////////////从hdfs文件读取////////////////////////////////////////
    //    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo").filter(!_.contains("multiarea")).filter(_.contains("中国"))
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //
    //      val province = cols(0)
    //      val city = cols(0)
    //
    //      (ip, (province + "\t" + city))
    //    })

    ///////////////从mysql读取////////////////////////////////////

    println("@@@@@@@@@@@@@@@@@@@" + rdd01.partitions)

    rdd01.mapPartitions(data2MySQL).saveAsTextFile("/tmp/tclterminal" + System.currentTimeMillis())

    //写入hbase
    //    rdd01.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator(htableName)
    //
    //      try {
    //
    //        items.foreach(line => {
    //          val cols = line.split('\t')
    //
    //          //牌照
    //          val license = cols(6)
    //
    //          var province = cols(1)
    //
    //
    //          if (province.equals("未匹配")) {
    //            province = "其他"
    //          }
    //
    //          //激活时间
    //          val last_poweron = ""
    //
    //          //型号
    //          val model = cols(3) + "_" + cols(4)
    //          val size = cols(5)
    //          val city = cols(2)
    //          val sn = cols(0)
    //
    //          //大区
    //          val area = Codearea.getArea(province)
    //
    //          //城市级别
    //          //          val citylevel = Codearea.getCl(city)
    //
    //
    //          //TODO 根据映射表获取城市级别
    //          var citylevel = "港澳台及国外"
    //          val pccArrValue = pccArrBroadcast.value
    //          breakable(
    //            for (i <- 0 until pccArrValue.length) {
    //              if (city.equals(pccArrValue(i).getString(1))) {
    //                citylevel = pccArrValue(i).getString(2)
    //                break
    //              }
    //            }
    //          )
    //          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel
    //
    //          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
    //          mutator.mutate(HBaseUtils.getPut_terminal("TCL", orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //      }
    //    }
    //
    //    )

  }


  val data2MySQL = (iterator: Iterator[String]) => {
    var conn: java.sql.Connection = null
    try {
      conn = MDBManager.getMDBManager(true).getConnection
      iterator.map(line => {
        val cols = line.split("\t")
        val ip = cols(0)
        val province = cols(1)
        val city = cols(2)
        if (conn.isClosed) {
          conn = MDBManager.getMDBManager(true).getConnection
        }
        val statement = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)
        val rs = statement.executeQuery("SELECT multiarea FROM ip_geo WHERE INET_ATON('" + ip + "') BETWEEN minip AND maxip AND country = '中国' LIMIT 1 ")
        var areaInfo = "#"
        while (rs.next) {
          areaInfo = rs.getString("multiarea")
        }
        (areaInfo, province, city)
      })
    } finally {
      if (conn != null)
        conn.close()
    }
  }


}
package com.avcdata.vbox.clean.terminal

import com.avcdata.vbox.util.UDFUtils
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal06 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)

    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    //
    //    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    //    val pccArrBroadcast = sc.broadcast(pccArr)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct





    //    rdd01.cache()

    //TODO 和IP库匹配
    /////////////从hdfs文件读取////////////////////////////////////////
    //    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo").filter(!_.contains("multiarea")).filter(_.contains("中国"))
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //
    //      val province = cols(0)
    //      val city = cols(0)
    //
    //      (ip, (province + "\t" + city))
    //    })

    ///////////////从mysql读取////////////////////////////////////

    //    val tableQuery = "(SELECT maxip,minip,multiarea FROM ip_geo  WHERE country = '中国' ) tmp"

    //    SELECT geo.`maxip`,geo.`minip`,geo.`multiarea`  FROM `ip_geo` geo
    //      WHERE
    //    INET_ATON('222.85.85.85') BETWEEN minip AND maxip AND country = '中国'  LIMIT 1; ”
    //    classOf[com.mysql.jdbc.Driver]
    //    val ipGeoRDD = hiveContext.read.format("jdbc").options(
    //      Map("url" -> "jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false&user=root&password=new.1234",
    //        "dbtable" -> tableQuery,
    //        "fetchSize" -> "100000000",
    //        "numPartitions" -> "300"
    //      )).load()
    //      .map(row => {
    //        val maxip = row.getLong(0)
    //        val minip = row.getLong(1)
    //        val multiarea = row.getString(2)
    //        maxip + "\t" + minip + "\t" + multiarea
    //      })


    //    val connection = () => {
    ////      Class.forName("com.mysql.jdbc.Driver").newInstance()
    ////      DriverManager.getConnection("jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false", "root", "new.1234")
    //      MDBManager.getMDBManager(true).getConnection
    //    }
    //    val ipGeoRDD = new JdbcRDD(
    //      sc,
    //      connection,
    //      "SELECT maxip,minip,multiarea FROM ip_geo  WHERE country = '中国' and id >= ? AND id <= ?",
    //      1, 30000000, 1000,
    //      r => {
    //        val maxip = r.getInt(1)
    //        val minip = r.getInt(2)
    //        val multiarea = r.getString(3)
    //        maxip + "\t" + minip + "\t" + multiarea
    //      }
    //    )

    //    val config: Config = ConfigFactory.load()
    //    val prop = new java.util.Properties()
    //    prop.put("user", config.getString("mysql.user"))
    //    prop.put("password", config.getString("mysql.password"))
    //    prop.put("driver", "com.mysql.jdbc.Driver")
    //    val ipGeoRDD = JdbcUtils.readMysql2DF(sc, config.getString("mysql.host"), prop, "vboxDB", "ip_geo_zh")
    //      .map(row => {
    //        val maxip = row.getAs("maxip").toString
    //        val minip = row.getAs("minip").toString
    //        val multiarea = row.getAs("multiarea").toString
    //        maxip + "\t" + minip + "\t" + multiarea
    //      })

    //    "[{""w"":""-33.867900"",""j"":""151.207000"",""p"":"""",""c"":"""",""d"":""""}]"	""
    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo/aiwen_free_district_v2_0_4.txt")
      .filter(line => {
        val cols = line.split("\t")
        val multiarea = cols(6)
        line.contains("中国") && multiarea.length > 80
      })

      .map(line => {
        val cols = line.split("\t")
        val minip = cols(1).replaceAll("\"", "")
        val maxip = cols(2).replaceAll("\"", "")
        val multiarea = cols(6).replaceAll("\"", "")

        val provinceTmp = multiarea.split(",")(2)
        val province = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

        val cityTmp = multiarea.split(",")(3)
        val city = provinceTmp.substring(provinceTmp.indexOf(":") + 1)


        (province + "\t" + city, minip + "\t" + maxip)
      }).reduceByKey((pre, post) => {
      pre + "#" + post
    })

    ipGeoRDD.cache()


    var splitRDDs = rdd01.randomSplit(Array(
      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0))


    val rddArr = scala.collection.mutable.ArrayBuffer[org.apache.spark.rdd.RDD[String]]()

    for (i <- 0 until splitRDDs.length) {

      val rddi = splitRDDs(i).cartesian(ipGeoRDD).filter(line => {

        val leftCols = line._1.split("\t")
        val ip = leftCols(0)
        val sn = leftCols(1)
        val license = leftCols(2)

        val rightCols01 = line._2._1.split("\t")
        val province = rightCols01(0)
        val city = rightCols01(1)

        val minMaxIpArr = line._2._2.split("#")

        val ipNum = UDFUtils.inet_aton(ip)

        var flag = false

        import scala.util.control.Breaks._

        breakable(
          for (i <- 0 until minMaxIpArr.length) {
            val cols = minMaxIpArr(i).split("\t")
            val minip = cols(0).toLong
            val maxip = cols(1).toLong
            flag = (ipNum <= maxip && ipNum >= minip)
            break
          })

        flag

      })
        .map(line => {
          val leftCols = line._1.split("\t")
          val ip = leftCols(0)
          val sn = leftCols(1)
          val license = leftCols(2)

          val rightCols01 = line._2._1.split("\t")
          val province = rightCols01(0)
          val city = rightCols01(1)

          sn + "\t" + license + "\t" + province + "\t" + city

        })

      rddArr.+=((rddi))

      //       .saveAsTextFile("/tmp/tclterminal/" + i + "#" + System.currentTimeMillis())

    }


    var new_initRDD = sc.makeRDD(Array("#"))

    for (i <- 0 until rddArr.length) {
      new_initRDD = new_initRDD.union(new_initRDD)
    }

    new_initRDD.filter(!_.equals("#")).saveAsTextFile("/tmp/new_initRDD/" + System.currentTimeMillis())



    //    val ipGeoBc = sc.broadcast(jdbcDFtmp)
    //
    //    rdd01.foreach(line=>{
    //      ipGeoBc.value.foreach(row=>{
    //        println(row)
    //      })
    //    })


    //    jdbcDFtmp.registerTempTable("ip_geo_tmp")

    //TODO 注册UDF函数
    //    UDFUtils.registerUDF(hiveContext, "inet_aton")
    //
    //    rdd01.map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //      val province = hiveContext.sql("select multiarea from ip_geo_tmp where INET_ATON('" + ip + "') BETWEEN minip " +
    //        "AND maxip AND LIMIT 1")
    //
    //
    //    })

    //    hiveContext.sql("SELECT maxip,minip,multiarea FROM ip_geo  from ip_geo_tmp WHERE INET_ATON('222.85.85.85') BETWEEN minip AND maxip")


    //SELECT geo.`maxip`,geo.`minip`,geo.`multiarea`  FROM `ip_geo` geo
    //WHERE INET_ATON('222.85.85.85') BETWEEN minip AND maxip LIMIT 1;


    //rdd01.foreach(println(_))

    //写入hbase
    //    rdd01.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator(htableName)
    //
    //      try {
    //
    //        items.foreach(line => {
    //          val cols = line.split('\t')
    //
    //          //牌照
    //          val license = cols(6)
    //
    //          var province = cols(1)
    //
    //
    //          if (province.equals("未匹配")) {
    //            province = "其他"
    //          }
    //
    //          //激活时间
    //          val last_poweron = ""
    //
    //          //型号
    //          val model = cols(3) + "_" + cols(4)
    //          val size = cols(5)
    //          val city = cols(2)
    //          val sn = cols(0)
    //
    //          //大区
    //          val area = Codearea.getArea(province)
    //
    //          //城市级别
    //          //          val citylevel = Codearea.getCl(city)
    //
    //
    //          //TODO 根据映射表获取城市级别
    //          var citylevel = "港澳台及国外"
    //          val pccArrValue = pccArrBroadcast.value
    //          breakable(
    //            for (i <- 0 until pccArrValue.length) {
    //              if (city.equals(pccArrValue(i).getString(1))) {
    //                citylevel = pccArrValue(i).getString(2)
    //                break
    //              }
    //            }
    //          )
    //          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel
    //
    //          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
    //          mutator.mutate(HBaseUtils.getPut_terminal("TCL", orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //      }
    //    }
    //
    //    )

  }


}
package com.avcdata.vbox.clean.terminal

import com.avcdata.vbox.common.Codearea
import com.avcdata.vbox.util.{HBaseUtils, UDFUtils, ValidateUtils}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminal20170920 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal_tcl"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    //    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        ip + "\t" + sn + "\t" + license

      }).distinct

    //    rdd01.cache()

    //TODO 和IP库匹配

    //TODO 将终端信息放入广播
    val terminalBr = sc.broadcast(rdd01.collect())

    //    "[{""w"":""-33.867900"",""j"":""151.207000"",""p"":"""",""c"":"""",""d"":""""}]"	""
    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo/aiwen_free_district_v2_0_4.txt")
      .filter(line => {
        val cols = line.split("\t")
        val minip = cols(1).replaceAll("\"", "")
        val maxip = cols(2).replaceAll("\"", "")
        val multiarea = cols(6).replaceAll("\"", "")

        var flag = false

        val multiareaCols = multiarea.split(",")

        flag = multiareaCols.length > 3

        if (flag) {
          val provinceTmp = multiareaCols(2)
          val province = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

          val cityTmp = multiareaCols(3)
          val city = cityTmp.substring(cityTmp.indexOf(":") + 1)

          flag = !province.isEmpty && !city.isEmpty

        }

        flag && line.contains("中国") && !line.contains("香港") && multiarea.length > 80
      })

      .map(line => {
        val cols = line.split("\t")
        val minip = cols(1).replaceAll("\"", "")
        val maxip = cols(2).replaceAll("\"", "")
        val multiarea = cols(6).replaceAll("\"", "")

        val provinceTmp = multiarea.split(",")(2)
        val province = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

        val cityTmp = multiarea.split(",")(3)
        val city = cityTmp.substring(cityTmp.indexOf(":") + 1)

        val districtTmp = multiarea.split(",")(4)
        val district = districtTmp.substring(districtTmp.indexOf(":") + 1)

        (province + "\t" + city + "\t" + district, minip + "\t" + maxip)
      }).reduceByKey((pre, post) => {
      pre + "#" + post
    })

    //    ipGeoRDD.cache()

    val resultRDD = ipGeoRDD.flatMap(line => {
      val rightCols01 = line._1.split("\t")
      val province = rightCols01(0)
      val city = rightCols01(1)
      val district = rightCols01(2)

      val minMaxIpArr = line._2.split("#")

      val terminalArr = terminalBr.value

      val resultArr = new scala.collection.mutable.ArrayBuffer[String]

      var flag = false

      for (terminal <- terminalArr) {

        val cols = terminal.split("\t")

        val ip = cols(0)
        val sn = cols(1)
        val license = cols(2)

        val ipNum = UDFUtils.inet_aton(ip)

        import scala.util.control.Breaks._

        breakable(
          for (i <- 0 until minMaxIpArr.length) {
            val cols = minMaxIpArr(i).split("\t")
            val minip = cols(0).toLong
            val maxip = cols(1).toLong
            flag = (ipNum <= maxip && ipNum >= minip)
            if (ValidateUtils.isContainsCN(province) && ValidateUtils.isContainsCN(city) && ValidateUtils.isContainsCN(district))
              resultArr.+=(sn + "\t" + license + "\t" + province + "\t" + city + "\t" + district)
            break
          })
      }
      resultArr
    })


    //      .saveAsTextFile("/tmp/tclTerminalBR/" + System.currentTimeMillis())


    //    //TODO 城市级别映射表
    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    val pccArrBroadcast = sc.broadcast(pccArr)


    //    //TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator(htableName)

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = cols(1)

          var province = cols(2)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          val district = cols(4)

          //激活时间
          val last_poweron = ""

          //型号
          val model = ""
          val size = ""
          val city = cols(3)
          val sn = cols(0)

          //大区
          val area = Codearea.getArea(province)

          //城市级别
          //          val citylevel = Codearea.getCl(city)

          //TODO 根据映射表获取城市级别
          var citylevel = "港澳台及国外"
          val pccArrValue = pccArrBroadcast.value

          import scala.util.control.Breaks._
          breakable(
            for (i <- 0 until pccArrValue.length) {
              if (city.equals(pccArrValue(i).getString(1))) {
                citylevel = pccArrValue(i).getString(2)
                break
              }
            }
          )
          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel + "\t" + district

          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal_tcl("TCL", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

  }


}
package com.avcdata.vbox.clean.terminal

import com.avcdata.vbox.util.UDFUtils
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @define TCL开关机终端信息清洗
  */
object DataCleanTCLTerminalLocal {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName(this.getClass.getSimpleName.filter(!_.equals('$')))
    val sc = new SparkContext(conf)
    run(sc, "2016-11-20")

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    val htableName = "tracker_terminal"

    //TODO 日志格式
    //    IPAddress=119.122.13.168;Clienttype=ddad28e7eb6c0d06d0e61a6c86fc73ed;dnum=5bd9ff60139bf60875b130b6135333b8;macline=f8b94128207616a56920e7e44094c896;macwifi=d06278320496d2c0bd8fb83dc161f994;license=tencent;com.tcl.TVBasicBehavior,{1=2017-08-29 11:29:28,2=2017-08-29 12:52:53};com.tcl.TVBasicBehavior,{1=2017-08-29 12:54:17,2=2017-08-29 22:15:02};com.tcl.tv,{1=2017-08-29 22:08:03,2=2017-08-29 22:08:08};com.audiocn.kalaok.tv,{1=2017-08-29 12:54:24,2=2017-08-29 17:33:24};com.ktcp.video,{1=2017-08-29 17:33:48,2=2017-08-29 18:12:25};com.ktcp.video,{1=2017-08-29 18:12:45,2=2017-08-29 18:13:43};com.ktcp.video,{1=2017-08-29 22:11:08,2=2017-08-29 22:15:02}

    //////////////////////test//////////////////
    val hdfsPath = "E:\\aowei\\tracker-clean\\doc\\userBehaviourLogClean.2017-08-30.log"
    ///////////////////test///////////////

    //    val hdfsPath = "/user/hdfs/rsync/TCL/" + analysisDate + "/userBehaviourLogClean." + analysisDate + ".log"

    val initRDD = sc.textFile(hdfsPath)
//      .sample(false, 0.1, 0)

    //TODO 城市级别映射表
//    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

    /////////////////////////test//////////////////////
    val hiveContext = new SQLContext(sc)
    /////////////////////////test//////////////////////
    //
    //    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    //    val pccArrBroadcast = sc.broadcast(pccArr)


    //TODO 提取

    val rdd01 = initRDD
      .map(line => {

        var ip = "#"
        var sn = "#"
        //        var date = analysisDate
        val cols = line.split(";")
        var license = "#"

        for (i <- 0 until cols.length) {

          if (cols(i).contains("IPAddress=")) {
            ip = cols(i).substring(10)
          }

          if (cols(i).contains("license=")) {
            license = cols(i).substring(8)
          }

          if (cols(i).contains("macline=")) {
            sn = cols(i).replaceAll("macline=", "")
          }

          if (cols(i).contains("macwifi=")) {
            if (sn.equals("#") || sn.isEmpty) {
              sn = cols(i).replaceAll("macwifi=", "")
            }
          }

        }

        println(ip + "\t" + sn + "\t" + license)

        ip + "\t" + sn + "\t" + license

      }).distinct





    //    rdd01.cache()

    //TODO 和IP库匹配
    /////////////从hdfs文件读取////////////////////////////////////////
    //    val ipGeoRDD = sc.textFile("/user/hdfs/rsync/ip_geo").filter(!_.contains("multiarea")).filter(_.contains("中国"))
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //
    //      val province = cols(0)
    //      val city = cols(0)
    //
    //      (ip, (province + "\t" + city))
    //    })

    ///////////////从mysql读取////////////////////////////////////

    //    val tableQuery = "(SELECT maxip,minip,multiarea FROM ip_geo  WHERE country = '中国' ) tmp"

    //    SELECT geo.`maxip`,geo.`minip`,geo.`multiarea`  FROM `ip_geo` geo
    //      WHERE
    //    INET_ATON('222.85.85.85') BETWEEN minip AND maxip AND country = '中国'  LIMIT 1; ”
    //    classOf[com.mysql.jdbc.Driver]
    //    val ipGeoRDD = hiveContext.read.format("jdbc").options(
    //      Map("url" -> "jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false&user=root&password=new.1234",
    //        "dbtable" -> tableQuery,
    //        "fetchSize" -> "100000000",
    //        "numPartitions" -> "300"
    //      )).load()
    //      .map(row => {
    //        val maxip = row.getLong(0)
    //        val minip = row.getLong(1)
    //        val multiarea = row.getString(2)
    //        maxip + "\t" + minip + "\t" + multiarea
    //      })


    //    val connection = () => {
    ////      Class.forName("com.mysql.jdbc.Driver").newInstance()
    ////      DriverManager.getConnection("jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false", "root", "new.1234")
    //      MDBManager.getMDBManager(true).getConnection
    //    }
    //    val ipGeoRDD = new JdbcRDD(
    //      sc,
    //      connection,
    //      "SELECT maxip,minip,multiarea FROM ip_geo  WHERE country = '中国' and id >= ? AND id <= ?",
    //      1, 30000000, 1000,
    //      r => {
    //        val maxip = r.getInt(1)
    //        val minip = r.getInt(2)
    //        val multiarea = r.getString(3)
    //        maxip + "\t" + minip + "\t" + multiarea
    //      }
    //    )

    //    val config: Config = ConfigFactory.load()
    //    val prop = new java.util.Properties()
    //    prop.put("user", config.getString("mysql.user"))
    //    prop.put("password", config.getString("mysql.password"))
    //    prop.put("driver", "com.mysql.jdbc.Driver")
    //    val ipGeoRDD = JdbcUtils.readMysql2DF(sc, config.getString("mysql.host"), prop, "vboxDB", "ip_geo_zh")
    //      .map(row => {
    //        val maxip = row.getAs("maxip").toString
    //        val minip = row.getAs("minip").toString
    //        val multiarea = row.getAs("multiarea").toString
    //        maxip + "\t" + minip + "\t" + multiarea
    //      })

    //    "[{""w"":""-33.867900"",""j"":""151.207000"",""p"":"""",""c"":"""",""d"":""""}]"	""

    //////////////////////test//////////////////
    val ipGeoFile = "E:\\aowei\\tracker-clean\\doc\\aiwen_free_district_v2_0_4.txt"
    ///////////////////test///////////////
    //    val ipGeoFile = "/user/hdfs/rsync/ip_geo/aiwen_free_district_v2_0_4.txt"

    val ipGeoRDD = sc.textFile(ipGeoFile)
      .filter(line => {
        val cols = line.split("\t")
        val multiarea = cols(6)
        line.contains("中国") && multiarea.length > 80 && !line.contains("香港")
      })

      .map(line => {
        val cols = line.split("\t")
        val minip = cols(1).replaceAll("\"", "")
        val maxip = cols(2).replaceAll("\"", "")
        val multiarea = cols(6).replaceAll("\"", "")

        val provinceTmp = multiarea.split(",")(2)
        val province = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

        val cityTmp = multiarea.split(",")(3)
        val city = provinceTmp.substring(provinceTmp.indexOf(":") + 1)

        println((province + "\t" + city, minip + "\t" + maxip))

        (province + "\t" + city, minip + "\t" + maxip)
      }).reduceByKey((pre, post) => {
      pre + "#" + post
    })

    //    ipGeoRDD.cache

    rdd01.cartesian(ipGeoRDD).filter(line => {

      val leftCols = line._1.split("\t")
      val ip = leftCols(0)
      val sn = leftCols(1)
      val license = leftCols(2)

      val rightCols01 = line._2._1.split("\t")
      val province = rightCols01(0)
      val city = rightCols01(1)

      val minMaxIpArr = line._2._2.split("#")

      val ipNum = UDFUtils.inet_aton(ip)

      var flag = false

      import scala.util.control.Breaks._

      breakable(
        for (i <- 0 until minMaxIpArr.length) {
          val cols = minMaxIpArr(i).split("\t")
          val minip = cols(0).toLong
          val maxip = cols(1).toLong
          flag = (ipNum <= maxip && ipNum >= minip)
          break
        })

      flag

    })
      .map(line => {
        val leftCols = line._1.split("\t")
        val ip = leftCols(0)
        val sn = leftCols(1)
        val license = leftCols(2)

        val rightCols01 = line._2._1.split("\t")
        val province = rightCols01(0)
        val city = rightCols01(1)

        println(sn + "\t" + license + "\t" + province + "\t" + city)

      }).count

//      .saveAsTextFile("/tmp/tclterminal" + "#" + System.currentTimeMillis())



    var splitRDDs = rdd01.randomSplit(Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0))

    //    for (i <- 0 until splitRDDs.length) {
    //
    //      splitRDDs(i).cartesian(ipGeoRDD).filter(line => {
    //
    //        val leftCols = line._1.split("\t")
    //        val ip = leftCols(0)
    //        val sn = leftCols(1)
    //        val license = leftCols(2)
    //
    //        val rightCols01 = line._2._1.split("\t")
    //        val province = rightCols01(0)
    //        val city = rightCols01(1)
    //
    //        val minMaxIpArr = line._2._2.split("#")
    //
    //        val ipNum = UDFUtils.inet_aton(ip)
    //
    //        var flag = false
    //
    //        import scala.util.control.Breaks._
    //
    //        breakable(
    //          for (i <- 0 until minMaxIpArr.length) {
    //            val cols = minMaxIpArr(i).split("\t")
    //            val minip = cols(0).toLong
    //            val maxip = cols(1).toLong
    //            flag = (ipNum <= maxip && ipNum >= minip)
    //            break
    //          })
    //
    //        flag
    //
    //      })
    //        .map(line => {
    //          val leftCols = line._1.split("\t")
    //          val ip = leftCols(0)
    //          val sn = leftCols(1)
    //          val license = leftCols(2)
    //
    //          val rightCols01 = line._2._1.split("\t")
    //          val province = rightCols01(0)
    //          val city = rightCols01(1)
    //
    //          sn + "\t" + license + "\t" + province + "\t" + city
    //
    //        }).saveAsTextFile("/tmp/tclterminal" + i + "#" + System.currentTimeMillis())
    //
    //
    //    }


    //    val ipGeoBc = sc.broadcast(jdbcDFtmp)
    //
    //    rdd01.foreach(line=>{
    //      ipGeoBc.value.foreach(row=>{
    //        println(row)
    //      })
    //    })


    //    jdbcDFtmp.registerTempTable("ip_geo_tmp")

    //TODO 注册UDF函数
    //    UDFUtils.registerUDF(hiveContext, "inet_aton")
    //
    //    rdd01.map(line => {
    //      val cols = line.split("\t")
    //      val ip = cols(0)
    //      val province = hiveContext.sql("select multiarea from ip_geo_tmp where INET_ATON('" + ip + "') BETWEEN minip " +
    //        "AND maxip AND LIMIT 1")
    //
    //
    //    })

    //    hiveContext.sql("SELECT maxip,minip,multiarea FROM ip_geo  from ip_geo_tmp WHERE INET_ATON('222.85.85.85') BETWEEN minip AND maxip")


    //SELECT geo.`maxip`,geo.`minip`,geo.`multiarea`  FROM `ip_geo` geo
    //WHERE INET_ATON('222.85.85.85') BETWEEN minip AND maxip LIMIT 1;


    //rdd01.foreach(println(_))

    //写入hbase
    //    rdd01.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator(htableName)
    //
    //      try {
    //
    //        items.foreach(line => {
    //          val cols = line.split('\t')
    //
    //          //牌照
    //          val license = cols(6)
    //
    //          var province = cols(1)
    //
    //
    //          if (province.equals("未匹配")) {
    //            province = "其他"
    //          }
    //
    //          //激活时间
    //          val last_poweron = ""
    //
    //          //型号
    //          val model = cols(3) + "_" + cols(4)
    //          val size = cols(5)
    //          val city = cols(2)
    //          val sn = cols(0)
    //
    //          //大区
    //          val area = Codearea.getArea(province)
    //
    //          //城市级别
    //          //          val citylevel = Codearea.getCl(city)
    //
    //
    //          //TODO 根据映射表获取城市级别
    //          var citylevel = "港澳台及国外"
    //          val pccArrValue = pccArrBroadcast.value
    //          breakable(
    //            for (i <- 0 until pccArrValue.length) {
    //              if (city.equals(pccArrValue(i).getString(1))) {
    //                citylevel = pccArrValue(i).getString(2)
    //                break
    //              }
    //            }
    //          )
    //          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel
    //
    //          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
    //          mutator.mutate(HBaseUtils.getPut_terminal("TCL", orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //      }
    //    }
    //
    //    )

  }


}
package com.avcdata.bean;

import org.springframework.boot.autoconfigure.jdbc.DataSourceBuilder;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Component;

import javax.sql.DataSource;

@Configuration
@Component
public class DataSourceConfig {

    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.mysql_ds")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean
    @Primary
    public JdbcTemplate jdbcTemplate() {
        return new JdbcTemplate(dataSource());
    }



    @Bean
    @ConfigurationProperties(prefix = "spring.hive_ds")
    public DataSource hiveDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean(name="hive")
    public JdbcTemplate hiveJdbcTemplate() {
        return new JdbcTemplate(hiveDataSource());
    }


    @Bean
    @ConfigurationProperties(prefix = "spring.ds_impala")
    public DataSource impalaDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean(name="impala")
    public JdbcTemplate impalaJdbcTemplate() {
        return new JdbcTemplate(impalaDataSource());
    }



}

package com.avcdata.spark.job.changhong

import java.sql.Timestamp
import java.text.SimpleDateFormat

import com.github.nscala_time.time.Imports.DateTime

import scala.collection.SortedMap

/**
  * Created by avc on 2016/11/2.
  */
object DataUtil {
    /**
      * 直播拆分为小时单位
      * @param duration 时长 毫秒
      * @param endTime 结束时间
      * @return
      */
    def durationSplitByHour(duration: String, endTime: String): SortedMap[String, String] = {
        var startTime = getStartTime(duration, endTime)
        var startHour = startTime.substring(11, 13) //获取小时
        val endHour = endTime.substring(11, 13)
        var startDay = startTime.substring(8, 10) //获取天
        val endDay = endTime.substring(8, 10)
        var hours = 0

        //println(startHour.toInt + ", " + endHour.toInt)
        /*if (startHour.toInt > endHour.toInt) {
            startTime = endTime.substring(0, 10) + " 00:00:00"
            hours = endHour.toInt
        } else {
            hours = endHour.toInt - startHour.toInt
        }*/
        //println(endTime.substring(5, 7) + ", " + startTime.substring(5, 7))
        if (endTime.substring(5, 7) == startTime.substring(5, 7)) {
            if (endDay == startDay) {//同一天
                hours = endHour.toInt - startHour.toInt
                //println(hours)
            } else if (endDay > startDay) {//跨天
                startTime = endTime.substring(0, 10) + " 00:00:00"
                hours = endHour.toInt
            }
        } else if (endTime.substring(5, 7) > startTime.substring(5, 7)
        || endTime.substring(0, 4) > startTime.substring(0, 4)){ //跨月
            startTime = endTime.substring(0, 10) + " 00:00:00"
            hours = endHour.toInt
        }

        val map = handleTime(duration, startTime, endTime, hours)
        map
    }

    /**
      *处理apk时间
      * @param duration 时长
      * @param startTime 开始时间
      * @param endTime 结束时间
      * @param hours 开始到结束的小时数
      * @return
      */
    def handleTime(duration: String, startTime: String, endTime: String, hours: Int): SortedMap[String, String] = {
        var currTime = startTime
        var map: SortedMap[String, String] = SortedMap()

        if (hours == 0) {
            var dura = duration.toLong
            //当出现endtime为2016-11-14 00:51:37,但时长有是夸了天的，但是这个时候把startTime置为2016-11-14 00:00:00了
            //需要对时长duration做一个处理
            if (startTime.substring(11, 13).equals("00") && endTime.substring(11, 13).equals("00"))
                dura = dateTotimestamp(endTime) - dateTotimestamp(startTime)
            map += (startTime.substring(11, 13) -> dura.toString)
            map
        }
        else {
            for (i <- 0 to hours) {
                var hour = currTime.substring(11, 13).toInt //获取当前的小时时间
                var nextTime = ""
                if (hour + 1 < 10) {
                    nextTime = currTime.substring(0, 10) + " 0" + (hour + 1) + ":00:00" //个位数补0
                } else {
                    nextTime = currTime.substring(0, 10) + " " + (hour + 1) + ":00:00" //得到下一小时的时间
                }
                var dura = 0l

                //println("nextTime : " + nextTime)
                //println("hour : " + hour + ", currtime : " + currTime)
                //最后一个小时的
                if (hour == endTime.substring(11, 13).toInt) {
                    dura = dateTotimestamp(endTime) - dateTotimestamp(currTime)
                    map += (endTime.substring(11, 13) -> dura.toString)
                    //println("=======2========= hours : " + hours)
                } else {
                    dura = dateTotimestamp(nextTime) - dateTotimestamp(currTime)
                    map += (currTime.substring(11, 13) -> dura.toString)
                }

                //println(currTime + ", " +dura)

                currTime = nextTime
            }
            map
        }

    }

    /**
      * 直播拆分为分钟
      * @param startTime 开始时间
      * @param endTime 结束时间
      * @return
      */
    def durationSplitByMinute(startTime: String, endTime: String): List[(String, (String, String))] = {
        var currTime = startTime
        var etime = endTime
        var startHour = startTime.substring(11, 13) //获取小时
        var endHour = endTime.substring(11, 13)
        var hours = 0
        var mins = 0
        var list: List[(String, (String, String))] = List()

        //day相同，小时相同
        if (startHour == endHour && startTime.substring(8 ,10).equals(endTime.substring(8 ,10))) {
            var startMinSec = startTime.substring(14, 19) //获取分钟和秒
            var endMinSec = endTime.substring(14, 19)
            if (endMinSec < startMinSec) {
                //println("jiao huan==" + endMinSec + ", " + startMinSec)
                val tmp = endMinSec
                endMinSec = startMinSec
                startMinSec = tmp
            }
            var minList = handleTimeByMinute(startMinSec, endMinSec, 0)
            for (l <- minList) {
                list = list :+ (startHour, l)
            }
            list

        } else {
            if (etime.substring(8, 10).toInt > currTime.substring(8, 10).toInt) {
                etime = endTime.substring(0, 10) + " 23:59:60" //跨天设置
            }

            endHour = etime.substring(11, 13)
            //println("endHour : " + startHour.toInt + ", " + endHour.toInt)
            for (hour <- startHour.toInt to endHour.toInt) {
                var nextTime = ""
                if (hour + 1 < 10) {
                    nextTime = currTime.substring(0, 10) + " 0" + (hour + 1) + ":00:00" //个位数补0
                } else {
                    nextTime = currTime.substring(0, 10) + " " + (hour + 1) + ":00:00" //得到下一小时的时间
                }

                if (hour == etime.substring(11, 13).toInt) {
                    var startMinSec = currTime.substring(14, 19) //获取分钟和秒
                    var endMinSec = etime.substring(14, 19)
                    //10:00:00--11:00:00
                    if (startMinSec.equals(endMinSec) && endMinSec.equals("00:00")) {
                        endMinSec = "59:60"
                    }
                    var minList = handleTimeByMinute(startMinSec, endMinSec, 1)

                    for (l <- minList) {
                        list = list :+ (currTime.substring(11, 13), l)
                    }
                } else {
                    var startMinSec = currTime.substring(14, 19) //获取分钟和秒
                    var endMinSec = nextTime.substring(14, 19)
                    if (startMinSec.equals(endMinSec) && endMinSec.equals("00:00")) {
                        endMinSec = "59:60"
                    }
                    var minList = handleTimeByMinute(startMinSec, endMinSec, 0)

                    for (l <- minList) {
                        list = list :+ (currTime.substring(11, 13), l)
                    }
                }

                currTime = nextTime
            }
            list
        }
    }

    /**
      *计算某一个小时每分钟的时长
      * @param startMinSec 同一小时的开始时间
      * @param endMinSec 同一小时的结束时间
      * @param flag 1 最后一个小时 0：不是最后一个小时
      * @return
      */
    def handleTimeByMinute(startMinSec: String, endMinSec: String, flag: Int): List[(String, String)] = {
        var currMinSec = startMinSec
        var startSec = startMinSec.substring(3, 5) //获取秒
        var endSec = endMinSec.substring(3, 5)
        var startMin = startMinSec.substring(0, 2) //获取分钟
        var endMin = endMinSec.substring(0, 2)
        var list: List[(String, String)] = List()
        var mins = 0
        var cntFlag = 0 //次数标志

        if (endMin.equals("00") && endSec.equals("00")) {
            if (flag == 1)//时间参考:11:00:00--11:00:00
            {
                mins = 0
            } else {//时间参考:10:02:11--11:00:00
                mins = 60 - startMin.toInt
            }

        }
        else {
            mins = endMin.toInt - startMin.toInt
        }

        if (mins == 0) {
            //1 是 cnt 次数
            /*var tmpEndSec = 0
            if (endSec.equals("00")) {
                tmpEndSec = 60
            } else {
                tmpEndSec = endSec.toInt
            }*/

            var ms = (endMin, (endSec.toInt - startSec.toInt).toString + ";1")
            list = list :+ ms
            list
        }
        else {
            for (i <- 0 to mins) {
                var min = currMinSec.substring(0, 2).toInt //获取当前的分钟时间
                var nextMin = ""
                if (min + 1 < 10) {
                    nextMin = "0" + (min + 1) + ":00" //个位分钟补0
                } else {
                    nextMin = (min + 1) + ":00" //得到下一分钟的时间
                }
                var dura = 0

                //不加min != 0当endMinSec为00的话导致第0分钟的时长为0
                if (min == endMinSec.substring(0, 2).toInt && min != 0) {
                    //dura = dateTotimestamp(endTime) - dateTotimestamp(currTime)
                    dura = endMinSec.substring(3, 5).toInt
                    list = list :+ (min.toString, dura.toString+";0")
                    cntFlag = 0
                } else {
                    dura = 60 - currMinSec.substring(3, 5).toInt
                    if (cntFlag == 0){
                        list = list :+ (min.toString, dura.toString + ";1")
                        cntFlag = 1
                    } else {
                        list = list :+ (min.toString, dura.toString + ";0")
                    }
                }

                currMinSec = nextMin
            }
            list
        }
    }

    /**
      * 获取apk打开开始的时间，供后面的时长和次数使用
      *
      * @param duration 时长，毫秒
      * @param endTime  结束时间，格式是2016-11-02 14:59:10
      * @return
      */
    def getStartTime(duration: String, endTime: String): String = {
        val end = dateTotimestamp(endTime)
        var startTime = timestampToDate((end - duration.toLong).toString)
        val preDate = DateTime.parse(endTime.substring(0 ,10)).plusDays(-1).toString("yyyy-MM-dd")
        if (preDate.equals(startTime.substring(0, 10))) {
            startTime = endTime.substring(0, 10) + " 00:00:00"
        }

        startTime
    }

    /**
      * 时间戳转为时间
      *
      * @param timestamp 时间戳
      * @return
      */
    def timestampToDate(timestamp: String): String = {
        val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
        val d = format.format(timestamp.toLong)
        d
    }

    /**
      * 时间转为时间戳,单位是毫秒
      *
      * @param apkDate 时间
      * @return
      */
    def dateTotimestamp(apkDate: String): Long = {
        val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
        val d = format.parse(apkDate)
        val time = new Timestamp(d.getTime())
        (time.getTime)
    }

    /*def intToStr(time : Int) : String = {
        var strTime = ""
        if (time < 10)
            strTime = "0" + time.toString
        else
            strTime = time.toString
        strTime
    }*/
}

object dataTest {
    def main(args: Array[String]): Unit = {
        val list = DataUtil.durationSplitByMinute("2017-04-01 08:49:00", "2017-04-01 08:53:00")
        for(l<-list) {
            //if (l._2._2.split(";")(0).equals("0"))
                println(l._2._1, l._2._2.split(";")(0))
        }
    }
}
package com.avcdata.vbox.util

import java.sql.Timestamp
import java.text.SimpleDateFormat

import com.github.nscala_time.time.Imports.DateTime

import scala.collection.SortedMap

/**
  * Created by avc on 2016/11/2.
  */
object DataUtil {
  /**
    * 直播拆分为小时单位
    *
    * @param duration 时长 毫秒
    * @param endTime  结束时间
    * @return
    */
  def durationSplitByHour(duration: String, endTime: String): SortedMap[String, String] = {
    var startTime = getStartTime(duration, endTime)
    var startHour = startTime.substring(11, 13) //获取小时
    val endHour = endTime.substring(11, 13)
    var startDay = startTime.substring(8, 10) //获取天
    val endDay = endTime.substring(8, 10)
    var hours = 0

    //println(startHour.toInt + ", " + endHour.toInt)
    /*if (startHour.toInt > endHour.toInt) {
        startTime = endTime.substring(0, 10) + " 00:00:00"
        hours = endHour.toInt
    } else {
        hours = endHour.toInt - startHour.toInt
    }*/
    //println(endTime.substring(5, 7) + ", " + startTime.substring(5, 7))
    if (endTime.substring(5, 7) == startTime.substring(5, 7)) {
      if (endDay == startDay) {
        //同一天
        hours = endHour.toInt - startHour.toInt
        //println(hours)
      } else if (endDay > startDay) {
        //跨天
        startTime = endTime.substring(0, 10) + " 00:00:00"
        hours = endHour.toInt
      }
    } else if (endTime.substring(5, 7) > startTime.substring(5, 7)
      || endTime.substring(0, 4) > startTime.substring(0, 4)) {
      //跨月
      startTime = endTime.substring(0, 10) + " 00:00:00"
      hours = endHour.toInt
    }

    handleTime(duration, startTime, endTime, hours)
  }

  /**
    * 处理apk时间
    *
    * @param duration  时长
    * @param startTime 开始时间
    * @param endTime   结束时间
    * @param hours     开始到结束的小时数
    * @return
    */
  def handleTime(duration: String, startTime: String, endTime: String, hours: Int): SortedMap[String, String] = {
    var currTime = startTime
    var map: SortedMap[String, String] = SortedMap()

    if (hours == 0) {
      var dura = duration.toLong
      //当出现endtime为2016-11-14 00:51:37,但时长有是夸了天的，但是这个时候把startTime置为2016-11-14 00:00:00了
      //需要对时长duration做一个处理
      if (startTime.substring(11, 13).equals("00") && endTime.substring(11, 13).equals("00"))
        dura = dateTotimestamp(endTime) - dateTotimestamp(startTime)
      map += (startTime.substring(11, 13) -> dura.toString)
      map
    }
    else {
      for (i <- 0 to hours) {
        var hour = currTime.substring(11, 13).toInt //获取当前的小时时间
        var nextTime = ""
        if (hour + 1 < 10) {
          nextTime = currTime.substring(0, 10) + " 0" + (hour + 1) + ":00:00" //个位数补0
        } else {
          nextTime = currTime.substring(0, 10) + " " + (hour + 1) + ":00:00" //得到下一小时的时间
        }
        var dura = 0l

        //println("nextTime : " + nextTime)
        //println("hour : " + hour + ", currtime : " + currTime)
        //最后一个小时的
        if (hour == endTime.substring(11, 13).toInt) {
          dura = dateTotimestamp(endTime) - dateTotimestamp(currTime)
          map += (endTime.substring(11, 13) -> dura.toString)
          //println("=======2========= hours : " + hours)
        } else {
          dura = dateTotimestamp(nextTime) - dateTotimestamp(currTime)
          map += (currTime.substring(11, 13) -> dura.toString)
        }

        //println(currTime + ", " +dura)

        currTime = nextTime
      }
      map
    }

  }

  /**
    * 直播拆分为分钟
    *
    * @param startTime 开始时间
    * @param endTime   结束时间
    * @return
    */
  def durationSplitByMinute(startTime: String, endTime: String): List[(String, (String, String))] = {
    var currTime = startTime
    var etime = endTime
    var startHour = startTime.substring(11, 13) //获取小时
    var endHour = endTime.substring(11, 13)
    var hours = 0
    var mins = 0
    var list: List[(String, (String, String))] = List()

    //day相同，小时相同
    if (startHour == endHour && startTime.substring(8, 10).equals(endTime.substring(8, 10))) {
      var startMinSec = startTime.substring(14, 19) //获取分钟和秒
      var endMinSec = endTime.substring(14, 19)
      if (endMinSec < startMinSec) {
        //println("jiao huan==" + endMinSec + ", " + startMinSec)
        val tmp = endMinSec
        endMinSec = startMinSec
        startMinSec = tmp
      }
      var minList = handleTimeByMinute(startMinSec, endMinSec, 0)
      for (l <- minList) {
        list = list :+(startHour, l)
      }
      list

    } else {
      if (etime.substring(8, 10).toInt > currTime.substring(8, 10).toInt) {
        etime = endTime.substring(0, 10) + " 23:59:60" //跨天设置
      }

      endHour = etime.substring(11, 13)
      //println("endHour : " + startHour.toInt + ", " + endHour.toInt)
      for (hour <- startHour.toInt to endHour.toInt) {
        var nextTime = ""
        if (hour + 1 < 10) {
          nextTime = currTime.substring(0, 10) + " 0" + (hour + 1) + ":00:00" //个位数补0
        } else {
          nextTime = currTime.substring(0, 10) + " " + (hour + 1) + ":00:00" //得到下一小时的时间
        }

        if (hour == etime.substring(11, 13).toInt) {
          var startMinSec = currTime.substring(14, 19) //获取分钟和秒
          var endMinSec = etime.substring(14, 19)
          //10:00:00--11:00:00
          if (startMinSec.equals(endMinSec) && endMinSec.equals("00:00")) {
            endMinSec = "59:60"
          }
          val minList = handleTimeByMinute(startMinSec, endMinSec, 1)

          for (l <- minList) {
            list = list :+(currTime.substring(11, 13), l)
          }
        } else {
          val startMinSec = currTime.substring(14, 19) //获取分钟和秒
          var endMinSec = nextTime.substring(14, 19)
          if (startMinSec.equals(endMinSec) && endMinSec.equals("00:00")) {
            endMinSec = "59:60"
          }
          val minList = handleTimeByMinute(startMinSec, endMinSec, 0)

          for (l <- minList) {
            list = list :+(currTime.substring(11, 13), l)
          }
        }

        currTime = nextTime
      }
      list
    }
  }

  /**
    * 计算某一个小时每分钟的时长
    *
    * @param startMinSec 同一小时的开始时间
    * @param endMinSec   同一小时的结束时间
    * @param flag        1 最后一个小时 0：不是最后一个小时
    * @return
    */
  def handleTimeByMinute(startMinSec: String, endMinSec: String, flag: Int): List[(String, String)] = {
    var currMinSec = startMinSec
    var startSec = startMinSec.substring(3, 5) //获取秒
    var endSec = endMinSec.substring(3, 5)
    var startMin = startMinSec.substring(0, 2) //获取分钟
    var endMin = endMinSec.substring(0, 2)
    var list: List[(String, String)] = List()
    var mins = 0
    var cntFlag = 0 //次数标志

    if (endMin.equals("00") && endSec.equals("00")) {
      if (flag == 1) //时间参考:11:00:00--11:00:00
      {
        mins = 0
      } else {
        //时间参考:10:02:11--11:00:00
        mins = 59 - startMin.toInt
      }

    }
    else {
      mins = endMin.toInt - startMin.toInt
    }

    if (mins == 0) {
      //1 是 cnt 次数
      /*var tmpEndSec = 0
      if (endSec.equals("00")) {
          tmpEndSec = 60
      } else {
          tmpEndSec = endSec.toInt
      }*/

      var ms = (endMin, (endSec.toInt - startSec.toInt).toString + ";1")
      list = list :+ ms
      list
    }
    else {
      for (i <- 0 to mins) {
        var min = currMinSec.substring(0, 2).toInt //获取当前的分钟时间
        var nextMin = ""
        if (min + 1 < 10) {
          nextMin = "0" + (min + 1) + ":00" //个位分钟补0
        } else {
          nextMin = (min + 1) + ":00" //得到下一分钟的时间
        }
        var dura = 0

        //不加min != 0当endMinSec为00的话导致第0分钟的时长为0
        if (min == endMinSec.substring(0, 2).toInt && min != 0) {
          //dura = dateTotimestamp(endTime) - dateTotimestamp(currTime)
          dura = endMinSec.substring(3, 5).toInt
          list = list :+(min.toString, dura.toString + ";0")
          cntFlag = 0
        } else {
          dura = 60 - currMinSec.substring(3, 5).toInt
          if (cntFlag == 0) {
            list = list :+(min.toString, dura.toString + ";1")
            cntFlag = 1
          } else {
            list = list :+(min.toString, dura.toString + ";0")
          }
        }

        currMinSec = nextMin
      }
      list
    }
  }

  /**
    * 获取apk打开开始的时间，供后面的时长和次数使用
    *
    * @param duration 时长，毫秒
    * @param endTime  结束时间，格式是2016-11-02 14:59:10
    * @return
    */
  def getStartTime(duration: String, endTime: String): String = {
    val end = dateTotimestamp(endTime)
    var startTime = timestampToDate((end - duration.toLong).toString)
    val preDate = DateTime.parse(endTime.substring(0, 10)).plusDays(-1).toString("yyyy-MM-dd")
    if (preDate.equals(startTime.substring(0, 10))) {
      startTime = endTime.substring(0, 10) + " 00:00:00"
    }

    startTime
  }

  /**
    * 时间戳转为时间
    *
    * @param timestamp 时间戳
    * @return
    */
  def timestampToDate(timestamp: String): String = {
    val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val d = format.format(timestamp.toLong)
    d
  }

  /**
    * 时间转为时间戳,单位是毫秒
    *
    * @param apkDate 时间
    * @return
    */
  def dateTotimestamp(apkDate: String): Long = {
    val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val d = format.parse(apkDate)
    val time = new Timestamp(d.getTime())
    (time.getTime)
  }

  /*def intToStr(time : Int) : String = {
      var strTime = ""
      if (time < 10)
          strTime = "0" + time.toString
      else
          strTime = time.toString
      strTime
  }*/
}

object dataTest {
  def main(args: Array[String]): Unit = {
    val list = DataUtil.durationSplitByMinute("2017-04-01 08:49:00", "2017-04-01 08:53:00")
    for (l <- list) {
      //if (l._2._2.split(";")(0).equals("0"))
      println(l._2._1, l._2._2.split(";")(0))
    }
  }
}
package com.avcdata.spark.job.common

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2016/12/13.
  */
object DateInfo {
    /**
      * 获取某天对应的周月
      * @param sc sparkcontext
      * @param sql sql
      * @param flag 是否包含*
      * @return
      */
    def getWeek(sc: SparkContext, sql: String, flag: Boolean): (String, String, String, String) = {
        val hiveContext = new HiveContext(sc)

        println(sql)
        var dm = "" //dayformonth
        var w = "" //week
        var dw = "" //dayforweek
        var mon = "" //month

        if (flag == true) {
            return (w, dw, mon, dm)
        }
        val hiveDataFrame = hiveContext.sql(sql)

        //hiveDataFrame.rdd.collect().foreach(println)

        w = hiveDataFrame.first()(2).toString
        dw = hiveDataFrame.first()(3).toString
        mon = hiveDataFrame.first()(4).toString
        dm = hiveDataFrame.first()(1).toString

        (w, dw, mon, dm)
    }

    /**
      * 获取每周的第一天
      * @param sc sparkcontext
      * @param sql sql
      * @return
      */
    def getFirstOfWeek(sc: SparkContext, sql: String): String = {
        val hiveContext = new HiveContext(sc)

        println(sql)
        val hiveDataFrame = hiveContext.sql(sql)
        val firstOfWeek = hiveDataFrame.first()(0).toString() // first day of week

        firstOfWeek
    }
}
package com.avcdata.vbox.common

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2016/12/13.
  */
object DateInfo {
    /**
      * 获取某天对应的周月
      * @param sc sparkcontext
      * @param sql sql
      * @param flag 是否包含*
      * @return
      */
    def getWeek(sc: SparkContext, sql: String, flag: Boolean): (String, String, String, String) = {
        val hiveContext = new HiveContext(sc)

        println(sql)
        var dm = "" //dayformonth
        var w = "" //week
        var dw = "" //dayforweek
        var mon = "" //month

        if (flag == true) {
            return (w, dw, mon, dm)
        }
        val hiveDataFrame = hiveContext.sql(sql)

        //hiveDataFrame.rdd.collect().foreach(println)

        w = hiveDataFrame.first()(2).toString
        dw = hiveDataFrame.first()(3).toString
        mon = hiveDataFrame.first()(4).toString
        dm = hiveDataFrame.first()(1).toString

        (w, dw, mon, dm)
    }

    /**
      * 获取每周的第一天
      * @param sc sparkcontext
      * @param sql sql
      * @return
      */
    def getFirstOfWeek(sc: SparkContext, sql: String): String = {
        val hiveContext = new HiveContext(sc)

        println(sql)
        val hiveDataFrame = hiveContext.sql(sql)
        val firstOfWeek = hiveDataFrame.first()(0).toString() // first day of week

        firstOfWeek
    }
}
package com.avcdata.etl.common.udf

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.etl.common.util.DateTimeUtil

/**
  * 时间日期相关的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/5/31 下午1:49
  */
object DateTimeUDF
{
  /**
    * 将指定时间转换为一年中的第几周
    */
  val timeToWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    val week = ca.get(Calendar.WEEK_OF_YEAR)
    if (week < 10) "0" + week else "" + week
  }

  /**
    * 将指定时间转换为YearWWeek格式,例如16W21
    */
  val timeToYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W21
    */
  val timeToLastYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为本年开始YearWWeek格式,例如16W01
    */
  val timeToBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W01
    */
  val timeToLastBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  val dateToYearWeek = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, beginYearWeek: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    val offsetTime = sdf.format(ca.getTime)

    if ("true".equalsIgnoreCase(beginYearWeek) || "y".equalsIgnoreCase(beginYearWeek))
    {
      timeToBeginYearWeek(offsetTime, timePattern)
    }
    else
    {
      timeToYearWeek(offsetTime, timePattern, 0)
    }

  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val timeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val fullTimeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int
                        , hourOffset: Int, minuteOffset: Int, secondOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)
    ca.add(Calendar.HOUR_OF_DAY, hourOffset)
    ca.add(Calendar.MINUTE, minuteOffset)
    ca.add(Calendar.SECOND, secondOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 周偏移计算
    */
  val yearWeek = (srcYearWeek: String, yearOffset: Int, weekOffset: Int) =>
  {
    //将年W周格式分解
    val yearWeek = srcYearWeek.split("[Ww]")

    val afterOffsetYear = ("20" + yearWeek(0)).toInt + yearOffset

    //获取偏移年后的最大周数
    val maxOffsetWeekNum = DateTimeUtil.getMaxWeekNumOfYear(afterOffsetYear)
    val effectiveWeek = if (yearWeek(1).toInt > maxOffsetWeekNum) maxOffsetWeekNum else yearWeek(1).toInt

    //增加年偏移量后所在周的开始时间
    val offsetCurrentDate = DateTimeUtil.getCurrentWeekDay(afterOffsetYear, effectiveWeek)

    val ca = Calendar.getInstance()
    ca.setTime(offsetCurrentDate)
    ca.add(Calendar.DAY_OF_YEAR, weekOffset * 7)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周开始时间,使用指定时间格式化
    */
  val timeToWeekStart = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周结束时间,使用指定时间格式化
    */
  val timeToWeekEnd = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY)
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月开始时间,使用指定时间格式化
    */
  val timeToMonthStart = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月结束时间,使用指定时间格式化
    */
  val timeToMonthEnd = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.DAY_OF_MONTH, ca.getActualMaximum(Calendar.DAY_OF_MONTH))
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }
}
package com.avcdata.etl.common.udf

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.etl.common.util.DateTimeUtil

/**
  * 时间日期相关的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/5/31 下午1:49
  */
object DateTimeUDF
{
  /**
    * 将指定时间转换为一年中的第几周
    */
  val timeToWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    val week = ca.get(Calendar.WEEK_OF_YEAR)
    if (week < 10) "0" + week else "" + week
  }

  /**
    * 将指定时间转换为YearWWeek格式,例如16W21
    */
  val timeToYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W21
    */
  val timeToLastYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为本年开始YearWWeek格式,例如16W01
    */
  val timeToBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W01
    */
  val timeToLastBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  val dateToYearWeek = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, beginYearWeek: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    val offsetTime = sdf.format(ca.getTime)

    if ("true".equalsIgnoreCase(beginYearWeek) || "y".equalsIgnoreCase(beginYearWeek))
    {
      timeToBeginYearWeek(offsetTime, timePattern)
    }
    else
    {
      timeToYearWeek(offsetTime, timePattern, 0)
    }

  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val timeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val fullTimeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int
                        , hourOffset: Int, minuteOffset: Int, secondOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)
    ca.add(Calendar.HOUR_OF_DAY, hourOffset)
    ca.add(Calendar.MINUTE, minuteOffset)
    ca.add(Calendar.SECOND, secondOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 周偏移计算
    */
  val yearWeek = (srcYearWeek: String, yearOffset: Int, weekOffset: Int) =>
  {
    //将年W周格式分解
    val yearWeek = srcYearWeek.split("[Ww]")

    val afterOffsetYear = ("20" + yearWeek(0)).toInt + yearOffset

    //获取偏移年后的最大周数
    val maxOffsetWeekNum = DateTimeUtil.getMaxWeekNumOfYear(afterOffsetYear)
    val effectiveWeek = if (yearWeek(1).toInt > maxOffsetWeekNum) maxOffsetWeekNum else yearWeek(1).toInt

    //增加年偏移量后所在周的开始时间
    val offsetCurrentDate = DateTimeUtil.getCurrentWeekDay(afterOffsetYear, effectiveWeek)

    val ca = Calendar.getInstance()
    ca.setTime(offsetCurrentDate)
    ca.add(Calendar.DAY_OF_YEAR, weekOffset * 7)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周开始时间,使用指定时间格式化
    */
  val timeToWeekStart = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周结束时间,使用指定时间格式化
    */
  val timeToWeekEnd = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY)
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月开始时间,使用指定时间格式化
    */
  val timeToMonthStart = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月结束时间,使用指定时间格式化
    */
  val timeToMonthEnd = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.DAY_OF_MONTH, ca.getActualMaximum(Calendar.DAY_OF_MONTH))
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }
}
package com.avcdata.etl.common.udf

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.etl.common.util.DateTimeUtil

/**
  * 时间日期相关的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/5/31 下午1:49
  */
object DateTimeUDF
{
  /**
    * 将指定时间转换为一年中的第几周
    */
  val timeToWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    val week = ca.get(Calendar.WEEK_OF_YEAR)
    if (week < 10) "0" + week else "" + week
  }

  /**
    * 将指定时间转换为YearWWeek格式,例如16W21
    */
  val timeToYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W21
    */
  val timeToLastYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为本年开始YearWWeek格式,例如16W01
    */
  val timeToBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W01
    */
  val timeToLastBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  val dateToYearWeek = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, beginYearWeek: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    val offsetTime = sdf.format(ca.getTime)

    if ("true".equalsIgnoreCase(beginYearWeek) || "y".equalsIgnoreCase(beginYearWeek))
    {
      timeToBeginYearWeek(offsetTime, timePattern)
    }
    else
    {
      timeToYearWeek(offsetTime, timePattern, 0)
    }

  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val timeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val fullTimeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int
                        , hourOffset: Int, minuteOffset: Int, secondOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)
    ca.add(Calendar.HOUR_OF_DAY, hourOffset)
    ca.add(Calendar.MINUTE, minuteOffset)
    ca.add(Calendar.SECOND, secondOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 周偏移计算
    */
  val yearWeek = (srcYearWeek: String, yearOffset: Int, weekOffset: Int) =>
  {
    //将年W周格式分解
    val yearWeek = srcYearWeek.split("[Ww]")

    val afterOffsetYear = ("20" + yearWeek(0)).toInt + yearOffset

    //获取偏移年后的最大周数
    val maxOffsetWeekNum = DateTimeUtil.getMaxWeekNumOfYear(afterOffsetYear)
    val effectiveWeek = if (yearWeek(1).toInt > maxOffsetWeekNum) maxOffsetWeekNum else yearWeek(1).toInt

    //增加年偏移量后所在周的开始时间
    val offsetCurrentDate = DateTimeUtil.getCurrentWeekDay(afterOffsetYear, effectiveWeek)

    val ca = Calendar.getInstance()
    ca.setTime(offsetCurrentDate)
    ca.add(Calendar.DAY_OF_YEAR, weekOffset * 7)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周开始时间,使用指定时间格式化
    */
  val timeToWeekStart = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周结束时间,使用指定时间格式化
    */
  val timeToWeekEnd = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY)
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月开始时间,使用指定时间格式化
    */
  val timeToMonthStart = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月结束时间,使用指定时间格式化
    */
  val timeToMonthEnd = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.DAY_OF_MONTH, ca.getActualMaximum(Calendar.DAY_OF_MONTH))
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }
}
package com.avcdata.etl.common.udf

import java.text.SimpleDateFormat
import java.util.Calendar

import com.avcdata.etl.common.util.DateTimeUtil

/**
  * 时间日期相关的UDF
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/5/31 下午1:49
  */
object DateTimeUDF
{
  /**
    * 将指定时间转换为一年中的第几周
    */
  val timeToWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    val week = ca.get(Calendar.WEEK_OF_YEAR)
    if (week < 10) "0" + week else "" + week
  }

  /**
    * 将指定时间转换为YearWWeek格式,例如16W21
    */
  val timeToYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W21
    */
  val timeToLastYearWeek = (sourceTime: String, timePattern: String, weekOffset: Int) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)
    ca.add(Calendar.DAY_OF_YEAR, 7 * weekOffset)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 将指定时间转换为本年开始YearWWeek格式,例如16W01
    */
  val timeToBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间转换为去年YearWWeek格式,例如15W01
    */
  val timeToLastBeginYearWeek = (sourceTime: String, timePattern: String) =>
  {
    val parsedTime = new SimpleDateFormat(timePattern).parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.add(Calendar.YEAR, -1)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime).toString.substring(0, 2) + "W" + "01"
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  val dateToYearWeek = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, beginYearWeek: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    val offsetTime = sdf.format(ca.getTime)

    if ("true".equalsIgnoreCase(beginYearWeek) || "y".equalsIgnoreCase(beginYearWeek))
    {
      timeToBeginYearWeek(offsetTime, timePattern)
    }
    else
    {
      timeToYearWeek(offsetTime, timePattern, 0)
    }

  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val timeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 将指定时间计算时间偏移后转换为指定格式
    */
  val fullTimeFormat = (sourceTime: String, timePattern: String, yearOffset: Int, monthOffset: Int, dayOffset: Int
                        , hourOffset: Int, minuteOffset: Int, secondOffset: Int, destTimePattern: String) =>
  {
    val sdf = new SimpleDateFormat(timePattern)

    val parsedTime = sdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)

    ca.add(Calendar.YEAR, yearOffset)
    ca.add(Calendar.MONTH, monthOffset)
    ca.add(Calendar.DAY_OF_MONTH, dayOffset)
    ca.add(Calendar.HOUR_OF_DAY, hourOffset)
    ca.add(Calendar.MINUTE, minuteOffset)
    ca.add(Calendar.SECOND, secondOffset)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 周偏移计算
    */
  val yearWeek = (srcYearWeek: String, yearOffset: Int, weekOffset: Int) =>
  {
    //将年W周格式分解
    val yearWeek = srcYearWeek.split("[Ww]")

    val afterOffsetYear = ("20" + yearWeek(0)).toInt + yearOffset

    //获取偏移年后的最大周数
    val maxOffsetWeekNum = DateTimeUtil.getMaxWeekNumOfYear(afterOffsetYear)
    val effectiveWeek = if (yearWeek(1).toInt > maxOffsetWeekNum) maxOffsetWeekNum else yearWeek(1).toInt

    //增加年偏移量后所在周的开始时间
    val offsetCurrentDate = DateTimeUtil.getCurrentWeekDay(afterOffsetYear, effectiveWeek)

    val ca = Calendar.getInstance()
    ca.setTime(offsetCurrentDate)
    ca.add(Calendar.DAY_OF_YEAR, weekOffset * 7)

    DateTimeUtil.dateToHalfYearWeek(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周开始时间,使用指定时间格式化
    */
  val timeToWeekStart = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前周结束时间,使用指定时间格式化
    */
  val timeToWeekEnd = (sourceTime: String, timePattern: String, dayOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.DAY_OF_MONTH, dayOffset)

    ca.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY)
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月开始时间,使用指定时间格式化
    */
  val timeToMonthStart = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.HOUR_OF_DAY, 0)
    ca.set(Calendar.MINUTE, 0)
    ca.set(Calendar.SECOND, 0)
    ca.set(Calendar.MILLISECOND, 0)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }

  /**
    * 获取指定时间偏移量的当前月结束时间,使用指定时间格式化
    */
  val timeToMonthEnd = (sourceTime: String, timePattern: String, monthOffset: Int, destTimePattern: String) =>
  {
    val srcSdf = new SimpleDateFormat(timePattern)

    val parsedTime = srcSdf.parse(sourceTime)

    val ca = Calendar.getInstance()
    ca.setTime(parsedTime)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    ca.add(Calendar.MONTH, monthOffset)

    ca.set(Calendar.DAY_OF_MONTH, 1)
    ca.set(Calendar.DAY_OF_MONTH, ca.getActualMaximum(Calendar.DAY_OF_MONTH))
    ca.set(Calendar.HOUR_OF_DAY, 23)
    ca.set(Calendar.MINUTE, 59)
    ca.set(Calendar.SECOND, 59)
    ca.set(Calendar.MILLISECOND, 999)

    new SimpleDateFormat(destTimePattern).format(ca.getTime)
  }
}
package com.avcdata.etl.common.util

import java.text.SimpleDateFormat
import java.util.{Calendar, Date}

import scala.util.Try

/**
  * TODO 类说明
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/27 18:33
  */
object DateTimeUtil
{
  val collectTimeFormat = "yyyy-MM-dd HH:mm:ss"

  val timeWithZoneFormat = "yyyy-MM-dd'T'HH:mm:ssX"

  def parse(dt: String): Date =
  {
    Try
    {
      new SimpleDateFormat(collectTimeFormat).parse(dt)
    }.getOrElse(null)
  }

  def format(date: Date): String =
  {
    new SimpleDateFormat(collectTimeFormat).format(date)
  }

  def parseWithZone(dt: String): String =
  {
    Try
    {
      new SimpleDateFormat(timeWithZoneFormat).format(new SimpleDateFormat(collectTimeFormat).parse(dt))
    }.getOrElse("")
  }

  /** 获取当前时间所在年的周数
    *
    * @param date 当前时间
    * @return 当前时间的周数
    */
  def getWeekOfYear(date: Date): Int =
  {
    val ca = Calendar.getInstance
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(date)

    ca.get(Calendar.WEEK_OF_YEAR)
  }

  /**
    * 获取当前时间所在年的最大周数
    *
    * @param year 所在年
    * @return 所在年的最大周
    */
  def getMaxWeekNumOfYear(year: Int): Int =
  {
    val ca = Calendar.getInstance()
    ca.set(year, Calendar.DECEMBER, 31, 23, 59, 59)

    val calcWeekNum = getWeekOfYear(ca.getTime)
    if (calcWeekNum == 1)
    {
      ca.add(Calendar.DAY_OF_YEAR, -7)

      getWeekOfYear(ca.getTime)
    }
    else
    {
      calcWeekNum
    }
  }


  /**
    * 获取某年的第几周的开始日期
    *
    * @param year 指定的年
    * @param week 指定的周
    * @return 指定年指定周的开始日期
    */
  def getFirstDayOfWeek(year: Int, week: Int): Date =
  {
    getFirstDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前年当前周的当前时间
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的当前时间
    */
  def getCurrentWeekDay(year: Int, week: Int): Date =
  {
    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    //将当前年份设置为当前年
    ca.set(Calendar.YEAR, year)

    //获取到当前的年W周
    val currentYearWeek = dateToHalfYearWeek(ca.getTime)
    //当前年最大周度
    val maxWeek = getMaxWeekNumOfYear(year)

    //获取周数
    val currentWeek = ca.get(Calendar.WEEK_OF_YEAR)

    //若跨年,则加上最大周度
    val currentHalfYear = currentYearWeek.substring(0, 2).toInt
    val calcHalfYear = String.valueOf(year).substring(2, 4).toInt
    val actualWeek = if (currentHalfYear != calcHalfYear) maxWeek + currentWeek else currentWeek

    //获取指定周度的时间
    ca.add(Calendar.DAY_OF_MONTH, (week - actualWeek) * 7)

    ca.getTime
  }

  /**
    * 获取某年的第几周的结束日期
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的结束日期
    */
  def getLastDayOfWeek(year: Int, week: Int): Date =
  {
    getLastDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前时间所在周的开始日期
    *
    * @param date 当前时间
    * @return 当前时间的周开始日期
    */
  def getFirstDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)

    c.getTime
  }

  /**
    * 获取当前时间所在周的结束日期
    *
    * @param date 当前日期
    * @return 所在周的结束日期
    */
  def getLastDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY) // Sunday

    c.getTime
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  def dateToHalfYearWeek(date: Date): String =
  {
    val ca = Calendar.getInstance()
    ca.setTime(date)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    val year = ca.get(Calendar.YEAR)
    val week = ca.get(Calendar.WEEK_OF_YEAR)

    ca.add(Calendar.DAY_OF_MONTH, -7)
    val beforeYear = ca.get(Calendar.YEAR)

    ca.add(Calendar.DAY_OF_MONTH, 14)
    val afterYear = ca.get(Calendar.YEAR)
    val afterWeek = ca.get(Calendar.WEEK_OF_YEAR)

    val calcYear = if (year == beforeYear && year == afterYear) year
    else if (year != afterYear)
    {
      if (afterWeek - week == 1) afterYear else year
    }
    else
    {
      year
    }

    calcYear.toString.substring(2) + "W" + (if (week < 10) "0" + week else "" + week)
  }
}
package com.avcdata.etl.common.util

import java.text.SimpleDateFormat
import java.util.{Calendar, Date}

import scala.util.Try

/**
  * TODO 类说明
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/27 18:33
  */
object DateTimeUtil
{
  val collectTimeFormat = "yyyy-MM-dd HH:mm:ss"

  val timeWithZoneFormat = "yyyy-MM-dd'T'HH:mm:ssX"

  def parse(dt: String): Date =
  {
    Try
    {
      new SimpleDateFormat(collectTimeFormat).parse(dt)
    }.getOrElse(null)
  }

  def format(date: Date): String =
  {
    new SimpleDateFormat(collectTimeFormat).format(date)
  }

  def parseWithZone(dt: String): String =
  {
    Try
    {
      new SimpleDateFormat(timeWithZoneFormat).format(new SimpleDateFormat(collectTimeFormat).parse(dt))
    }.getOrElse("")
  }

  /** 获取当前时间所在年的周数
    *
    * @param date 当前时间
    * @return 当前时间的周数
    */
  def getWeekOfYear(date: Date): Int =
  {
    val ca = Calendar.getInstance
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(date)

    ca.get(Calendar.WEEK_OF_YEAR)
  }

  /**
    * 获取当前时间所在年的最大周数
    *
    * @param year 所在年
    * @return 所在年的最大周
    */
  def getMaxWeekNumOfYear(year: Int): Int =
  {
    val ca = Calendar.getInstance()
    ca.set(year, Calendar.DECEMBER, 31, 23, 59, 59)

    val calcWeekNum = getWeekOfYear(ca.getTime)
    if (calcWeekNum == 1)
    {
      ca.add(Calendar.DAY_OF_YEAR, -7)

      getWeekOfYear(ca.getTime)
    }
    else
    {
      calcWeekNum
    }
  }


  /**
    * 获取某年的第几周的开始日期
    *
    * @param year 指定的年
    * @param week 指定的周
    * @return 指定年指定周的开始日期
    */
  def getFirstDayOfWeek(year: Int, week: Int): Date =
  {
    getFirstDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前年当前周的当前时间
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的当前时间
    */
  def getCurrentWeekDay(year: Int, week: Int): Date =
  {
    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    //将当前年份设置为当前年
    ca.set(Calendar.YEAR, year)

    //获取到当前的年W周
    val currentYearWeek = dateToHalfYearWeek(ca.getTime)
    //当前年最大周度
    val maxWeek = getMaxWeekNumOfYear(year)

    //获取周数
    val currentWeek = ca.get(Calendar.WEEK_OF_YEAR)

    //若跨年,则加上最大周度
    val currentHalfYear = currentYearWeek.substring(0, 2).toInt
    val calcHalfYear = String.valueOf(year).substring(2, 4).toInt
    val actualWeek = if (currentHalfYear != calcHalfYear) maxWeek + currentWeek else currentWeek

    //获取指定周度的时间
    ca.add(Calendar.DAY_OF_MONTH, (week - actualWeek) * 7)

    ca.getTime
  }

  /**
    * 获取某年的第几周的结束日期
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的结束日期
    */
  def getLastDayOfWeek(year: Int, week: Int): Date =
  {
    getLastDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前时间所在周的开始日期
    *
    * @param date 当前时间
    * @return 当前时间的周开始日期
    */
  def getFirstDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)

    c.getTime
  }

  /**
    * 获取当前时间所在周的结束日期
    *
    * @param date 当前日期
    * @return 所在周的结束日期
    */
  def getLastDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY) // Sunday

    c.getTime
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  def dateToHalfYearWeek(date: Date): String =
  {
    val ca = Calendar.getInstance()
    ca.setTime(date)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    val year = ca.get(Calendar.YEAR)
    val week = ca.get(Calendar.WEEK_OF_YEAR)

    ca.add(Calendar.DAY_OF_MONTH, -7)
    val beforeYear = ca.get(Calendar.YEAR)

    ca.add(Calendar.DAY_OF_MONTH, 14)
    val afterYear = ca.get(Calendar.YEAR)
    val afterWeek = ca.get(Calendar.WEEK_OF_YEAR)

    val calcYear = if (year == beforeYear && year == afterYear) year
    else if (year != afterYear)
    {
      if (afterWeek - week == 1) afterYear else year
    }
    else
    {
      year
    }

    calcYear.toString.substring(2) + "W" + (if (week < 10) "0" + week else "" + week)
  }
}
package com.avcdata.etl.common.util

import java.text.SimpleDateFormat
import java.util.{Calendar, Date}

import scala.util.Try

/**
  * TODO 类说明
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/27 18:33
  */
object DateTimeUtil
{
  val collectTimeFormat = "yyyy-MM-dd HH:mm:ss"

  val timeWithZoneFormat = "yyyy-MM-dd'T'HH:mm:ssX"

  def parse(dt: String): Date =
  {
    Try
    {
      new SimpleDateFormat(collectTimeFormat).parse(dt)
    }.getOrElse(null)
  }

  def format(date: Date): String =
  {
    new SimpleDateFormat(collectTimeFormat).format(date)
  }

  def parseWithZone(dt: String): String =
  {
    Try
    {
      new SimpleDateFormat(timeWithZoneFormat).format(new SimpleDateFormat(collectTimeFormat).parse(dt))
    }.getOrElse("")
  }

  /** 获取当前时间所在年的周数
    *
    * @param date 当前时间
    * @return 当前时间的周数
    */
  def getWeekOfYear(date: Date): Int =
  {
    val ca = Calendar.getInstance
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(date)

    ca.get(Calendar.WEEK_OF_YEAR)
  }

  /**
    * 获取当前时间所在年的最大周数
    *
    * @param year 所在年
    * @return 所在年的最大周
    */
  def getMaxWeekNumOfYear(year: Int): Int =
  {
    val ca = Calendar.getInstance()
    ca.set(year, Calendar.DECEMBER, 31, 23, 59, 59)

    val calcWeekNum = getWeekOfYear(ca.getTime)
    if (calcWeekNum == 1)
    {
      ca.add(Calendar.DAY_OF_YEAR, -7)

      getWeekOfYear(ca.getTime)
    }
    else
    {
      calcWeekNum
    }
  }


  /**
    * 获取某年的第几周的开始日期
    *
    * @param year 指定的年
    * @param week 指定的周
    * @return 指定年指定周的开始日期
    */
  def getFirstDayOfWeek(year: Int, week: Int): Date =
  {
    getFirstDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前年当前周的当前时间
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的当前时间
    */
  def getCurrentWeekDay(year: Int, week: Int): Date =
  {
    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    //将当前年份设置为当前年
    ca.set(Calendar.YEAR, year)

    //获取到当前的年W周
    val currentYearWeek = dateToHalfYearWeek(ca.getTime)
    //当前年最大周度
    val maxWeek = getMaxWeekNumOfYear(year)

    //获取周数
    val currentWeek = ca.get(Calendar.WEEK_OF_YEAR)

    //若跨年,则加上最大周度
    val currentHalfYear = currentYearWeek.substring(0, 2).toInt
    val calcHalfYear = String.valueOf(year).substring(2, 4).toInt
    val actualWeek = if (currentHalfYear != calcHalfYear) maxWeek + currentWeek else currentWeek

    //获取指定周度的时间
    ca.add(Calendar.DAY_OF_MONTH, (week - actualWeek) * 7)

    ca.getTime
  }

  /**
    * 获取某年的第几周的结束日期
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的结束日期
    */
  def getLastDayOfWeek(year: Int, week: Int): Date =
  {
    getLastDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前时间所在周的开始日期
    *
    * @param date 当前时间
    * @return 当前时间的周开始日期
    */
  def getFirstDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)

    c.getTime
  }

  /**
    * 获取当前时间所在周的结束日期
    *
    * @param date 当前日期
    * @return 所在周的结束日期
    */
  def getLastDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY) // Sunday

    c.getTime
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  def dateToHalfYearWeek(date: Date): String =
  {
    val ca = Calendar.getInstance()
    ca.setTime(date)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    val year = ca.get(Calendar.YEAR)
    val week = ca.get(Calendar.WEEK_OF_YEAR)

    ca.add(Calendar.DAY_OF_MONTH, -7)
    val beforeYear = ca.get(Calendar.YEAR)

    ca.add(Calendar.DAY_OF_MONTH, 14)
    val afterYear = ca.get(Calendar.YEAR)
    val afterWeek = ca.get(Calendar.WEEK_OF_YEAR)

    val calcYear = if (year == beforeYear && year == afterYear) year
    else if (year != afterYear)
    {
      if (afterWeek - week == 1) afterYear else year
    }
    else
    {
      year
    }

    calcYear.toString.substring(2) + "W" + (if (week < 10) "0" + week else "" + week)
  }
}
package com.avcdata.etl.common.util

import java.text.SimpleDateFormat
import java.util.{Calendar, Date}

import scala.util.Try

/**
  * TODO 类说明
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/27 18:33
  */
object DateTimeUtil
{
  val collectTimeFormat = "yyyy-MM-dd HH:mm:ss"

  val timeWithZoneFormat = "yyyy-MM-dd'T'HH:mm:ssX"

  def parse(dt: String): Date =
  {
    Try
    {
      new SimpleDateFormat(collectTimeFormat).parse(dt)
    }.getOrElse(null)
  }

  def format(date: Date): String =
  {
    new SimpleDateFormat(collectTimeFormat).format(date)
  }

  def parseWithZone(dt: String): String =
  {
    Try
    {
      new SimpleDateFormat(timeWithZoneFormat).format(new SimpleDateFormat(collectTimeFormat).parse(dt))
    }.getOrElse("")
  }

  /** 获取当前时间所在年的周数
    *
    * @param date 当前时间
    * @return 当前时间的周数
    */
  def getWeekOfYear(date: Date): Int =
  {
    val ca = Calendar.getInstance
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(date)

    ca.get(Calendar.WEEK_OF_YEAR)
  }

  /**
    * 获取当前时间所在年的最大周数
    *
    * @param year 所在年
    * @return 所在年的最大周
    */
  def getMaxWeekNumOfYear(year: Int): Int =
  {
    val ca = Calendar.getInstance()
    ca.set(year, Calendar.DECEMBER, 31, 23, 59, 59)

    val calcWeekNum = getWeekOfYear(ca.getTime)
    if (calcWeekNum == 1)
    {
      ca.add(Calendar.DAY_OF_YEAR, -7)

      getWeekOfYear(ca.getTime)
    }
    else
    {
      calcWeekNum
    }
  }


  /**
    * 获取某年的第几周的开始日期
    *
    * @param year 指定的年
    * @param week 指定的周
    * @return 指定年指定周的开始日期
    */
  def getFirstDayOfWeek(year: Int, week: Int): Date =
  {
    getFirstDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前年当前周的当前时间
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的当前时间
    */
  def getCurrentWeekDay(year: Int, week: Int): Date =
  {
    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    //将当前年份设置为当前年
    ca.set(Calendar.YEAR, year)

    //获取到当前的年W周
    val currentYearWeek = dateToHalfYearWeek(ca.getTime)
    //当前年最大周度
    val maxWeek = getMaxWeekNumOfYear(year)

    //获取周数
    val currentWeek = ca.get(Calendar.WEEK_OF_YEAR)

    //若跨年,则加上最大周度
    val currentHalfYear = currentYearWeek.substring(0, 2).toInt
    val calcHalfYear = String.valueOf(year).substring(2, 4).toInt
    val actualWeek = if (currentHalfYear != calcHalfYear) maxWeek + currentWeek else currentWeek

    //获取指定周度的时间
    ca.add(Calendar.DAY_OF_MONTH, (week - actualWeek) * 7)

    ca.getTime
  }

  /**
    * 获取某年的第几周的结束日期
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的结束日期
    */
  def getLastDayOfWeek(year: Int, week: Int): Date =
  {
    getLastDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前时间所在周的开始日期
    *
    * @param date 当前时间
    * @return 当前时间的周开始日期
    */
  def getFirstDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)

    c.getTime
  }

  /**
    * 获取当前时间所在周的结束日期
    *
    * @param date 当前日期
    * @return 所在周的结束日期
    */
  def getLastDayOfWeek(date: Date): Date =
  {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY) // Sunday

    c.getTime
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  def dateToHalfYearWeek(date: Date): String =
  {
    val ca = Calendar.getInstance()
    ca.setTime(date)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    val year = ca.get(Calendar.YEAR)
    val week = ca.get(Calendar.WEEK_OF_YEAR)

    ca.add(Calendar.DAY_OF_MONTH, -7)
    val beforeYear = ca.get(Calendar.YEAR)

    ca.add(Calendar.DAY_OF_MONTH, 14)
    val afterYear = ca.get(Calendar.YEAR)
    val afterWeek = ca.get(Calendar.WEEK_OF_YEAR)

    val calcYear = if (year == beforeYear && year == afterYear) year
    else if (year != afterYear)
    {
      if (afterWeek - week == 1) afterYear else year
    }
    else
    {
      year
    }

    calcYear.toString.substring(2) + "W" + (if (week < 10) "0" + week else "" + week)
  }
}
package utils

import java.sql.Connection
import javax.sql.DataSource
import com.mchange.v2.c3p0.ComboPooledDataSource
import org.slf4j.LoggerFactory

/**
  * Created by guxiaoyang on 2017/6/13.
  */
object DBHelper
{
  val driver = "com.mysql.jdbc.Driver"
  val url = "jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf-8&useSSL=false"
  val username = "root"
  val password = "new.1234"

  def insertException(exception: String, connection: Connection): Unit =
  {

    val prep = connection.prepareStatement("INSERT INTO sdk_exception (date, exception) VALUES (now(), ?) ")

    prep.setString(1, exception)

    try
    {
      prep.executeUpdate()
    }
    catch
    {
      case e: Exception => println(e.toString)
    }
  }
}

object JDBCConnectionPool
{
  private val logger = LoggerFactory.getLogger(JDBCConnectionPool.getClass)

  //数据源连接池
  private var dataSourcePool: Map[(String, String, String), ComboPooledDataSource] = Map()

  sys.addShutdownHook
  {
    dataSourcePool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(connectURI: String, user: String, password: String): Connection =
  {
    dataSourcePool.getOrElse((connectURI, user, password),
      {
        JDBCConnectionPool.synchronized[DataSource](
          dataSourcePool.getOrElse((connectURI, user, password),
            {
              logger.info("Initialize c3p0 database connection pool.")

              //创建数据源
              val dataSource = new ComboPooledDataSource
              dataSource.setDriverClass(JDBC.getDriverClassName(connectURI))
              dataSource.setJdbcUrl(connectURI)
              dataSource.setUser(user)
              dataSource.setPassword(password)

              //初始化时获取一个连接，取值应在minPoolSize与maxPoolSize之间。Default: 3
              dataSource.setInitialPoolSize(1)
              //最大空闲时间,60秒内未使用则连接被丢弃。若为0则永不丢弃。Default: 0
              dataSource.setMaxIdleTime(60)
              //连接池中保留的最大连接数。Default: 15
              dataSource.setMaxPoolSize(10)
              //初始化时获取三个连接，取值应在minPoolSize与maxPoolSize之间。Default: 3
              dataSource.setMinPoolSize(1)

              //当连接池中的连接耗尽的时候c3p0一次同时获取的连接数。Default: 3
              dataSource.setAcquireIncrement(3)
              //定义所有连接测试都执行的测试语句。
              dataSource.setPreferredTestQuery("SELECT SYSDATE()")
              //每60秒检查所有连接池中的空闲连接。Default: 0
              dataSource.setIdleConnectionTestPeriod(60)

              dataSourcePool += (connectURI, user, password) -> dataSource

              dataSource
            }
          )
        )
      }
    ).getConnection
  }
}

object JDBC
{
  val MYSQL_DBP_NAME = "MySQL"

  private val MYSQL_JDBC_URI_PREFIX = "jdbc:mysql://"

  val MYSQL_DRIVER_NAME = "com.mysql.jdbc.Driver"

  val GREENPLUM_DBP_NAME = "Greenplum"

  private val GREENPLUM_JDBC_URI_PREFIX = "jdbc:pivotal:greenplum://"

  val GREENPLUM_DRIVER_NAME = "com.pivotal.jdbc.GreenplumDriver"

  private val JDBC_URI_PREFIX_TO_DRIVER_NAME = Map(
    MYSQL_JDBC_URI_PREFIX -> MYSQL_DRIVER_NAME
    , GREENPLUM_JDBC_URI_PREFIX -> GREENPLUM_DRIVER_NAME
  )

  /**
    * 获取数据库驱动名称
    *
    * @param connectUri 连接URI
    */
  def getDriverClassName(connectUri: String): String =
  {
    val driverName = JDBC_URI_PREFIX_TO_DRIVER_NAME.find(e =>
    {
      val (uriPrefix, _) = e
      connectUri.startsWith(uriPrefix)
    }
    )

    driverName match
    {
      case Some((_, driverClass)) => driverClass
      case None => throw new IllegalArgumentException(s"Can not find `$connectUri`'s driver.")
    }
  }
}
package com.avcdata.etl.util.db

import com.avcdata.etl.common.util.{FunctionCompiler, HdfsFileUtil, ScriptFormatter, VariableSubstitution}
import org.apache.commons.cli._
import org.slf4j.LoggerFactory

import scala.collection.JavaConversions._

/**
  * 执行数据相关文件操作
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/28 15:14
  */
object DBOperationExecutor
{
  private val logger = LoggerFactory.getLogger(DBOperationExecutor.getClass)

  def main(args: Array[String]): Unit =
  {
    logger.info(s"The original params are (${args.mkString(" ")}).")

    val cl = parse(args)
    if (cl.hasOption("help"))
    {
      printHelp()
    }
    else
    {
      val params = mergedParams(cl)

      logger.info(s"The merged params are $params")

      //处理执行的脚本内容
      val executableScripts = if (cl.hasOption("script")) parseScripts(Seq(cl.getOptionValue("script")), params)
      else if (cl.hasOption("script-file")) parseScripts(HdfsFileUtil.read(cl.getOptionValue("script-file")), params)
      else throw new IllegalArgumentException("You must specify the --script or --script-file param.")

      Operation(cl.getOptionValue("db-type"), cl.getOptionValue("op-type")) match
      {
        case Operation("jdbc", _) => JDBCExecutor.execute(executableScripts, parseJDBCConfig(params))
        case Operation(dbType, opType) => throw new IllegalArgumentException(s"Unsupported db-type($dbType) & op-type($opType).")
      }
    }
  }

  /**
    * 解析JDBC的配置信息
    *
    * @param params 命令行参数信息
    * @return JDBC配置信息
    */
  def parseJDBCConfig(params: Map[String, String]): JDBCConfig =
  {
    val connectUri = params("connect-uri")
    val username = params.getOrElse("username", null)
    val password = params.getOrElse("password", null)
    val transactional = params.getOrElse("need-transaction", "false").toBoolean

    JDBCConfig(connectUri, username, password, transactional)
  }

  /**
    * 解析脚本内容
    *
    * @param scripts 脚本内容
    * @return 解析后的脚本内容
    */
  private def parseScripts(scripts: Seq[String], mergedParams: Map[String, String]): Seq[String] =
  {
    logger.info(s"The original script content is -> ${scripts.mkString("\n")}")

    //变量替换
    val varSubstitution = new VariableSubstitution(mergedParams)
    val parsedScripts = scripts.map(varSubstitution.substitute).map(FunctionCompiler.execute)

    logger.info("Parsed script content is -> " + parsedScripts.mkString("\n"))

    //格式化Script
    val formatedScripts = ScriptFormatter.groupingScripts(parsedScripts)

    if (formatedScripts.isEmpty) throw new IllegalArgumentException(s"There is no executable lines in $scripts file.")

    logger.info("Formated script content is -> " + formatedScripts.mkString("\n"))

    formatedScripts
  }

  /**
    * 从命令行获取param参数,合并(覆盖)配置文件中的参数
    *
    * @param cl 命令行配置对象
    * @return 合并后的参数
    */
  private def mergedParams(cl: CommandLine): Map[String, String] =
  {
    val fileConfigs = if (cl.hasOption("config-file"))
    {
      HdfsFileUtil.readPropertiesToMap(cl.getOptionValue("config-file"))
    }
    else
    {
      Map[String, String]()
    }

    fileConfigs ++ cl.getOptionProperties("param").toMap
  }

  /**
    * 解析命令行参数
    *
    * @param args 命令行参数
    * @return 命令行对象
    */
  private def parse(args: Array[String]): CommandLine =
  {
    val parser = new PosixParser()

    parser.parse(options(), args)
  }

  /**
    * 打印命令行帮助
    */
  def printHelp() =
  {
    val hf = new HelpFormatter()

    hf.printHelp("DBOperationExecutor", options())
  }

  /**
    * 获取参数配置选项
    *
    * @return 配置选项
    */
  private def options(): Options =
  {
    // 创建 Options 对象
    val options = new Options()

    val typeOption = new Option(null, "db-type", true, "The target db type(jdbc, mongo) of operate.")
    typeOption.setArgName("db-type")

    val opOption = new Option(null, "op-type", true, "The target operation type(c, r, u, d) of operate.")
    opOption.setArgName("op-type")

    val filenameOption = new Option("f", "script-file", true, "The execute file from hdfs.")
    filenameOption.setArgName("script-file")

    val scriptOption = new Option("s", "script", true, "The execute script.")
    scriptOption.setArgName("script")

    val configFileOption = new Option("c", "config-file", true, "Operation config file from hdfs.")
    configFileOption.setArgName("filename")

    val paramsOption = new Option("p", "param", true, "The params for execute.")
    paramsOption.setArgName("name=value")
    paramsOption.setValueSeparator('=')
    paramsOption.setArgs(2)

    options.addOption("h", "help", false, "Lists short help.")
    options.addOption(typeOption)
    options.addOption(opOption)
    options.addOption(filenameOption)
    options.addOption(scriptOption)
    options.addOption(configFileOption)
    options.addOption(paramsOption)
  }
}
package com.avcdata.etl.common.util

/**
  * 序列/反序列化
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/9/27 14:09
  */
object DeSerializationUtil
{
  //数据反序列化
  def deserialize[A](json: String)(implicit mf: scala.reflect.Manifest[A]): A =
  {
    import net.liftweb.json._
    implicit val formats = DefaultFormats

    //解析对象
    parse(json).extract[A]
  }

  def serialize(obj: AnyRef): String =
  {
    import net.liftweb.json.Serialization.write
    import net.liftweb.json._

    implicit val formats = Serialization.formats(NoTypeHints)

    write(obj)
  }
}
package com.avcdata.etl.common.util

/**
  * 序列/反序列化
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/9/27 14:09
  */
object DeSerializationUtil
{
  //数据反序列化
  def deserialize[A](json: String)(implicit mf: scala.reflect.Manifest[A]): A =
  {
    import net.liftweb.json._
    implicit val formats = DefaultFormats

    //解析对象
    parse(json).extract[A]
  }

  def serialize(obj: AnyRef): String =
  {
    import net.liftweb.json.Serialization.write
    import net.liftweb.json._

    implicit val formats = Serialization.formats(NoTypeHints)

    write(obj)
  }
}
package com.avcdata.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;

/**
 * Created by dev on 15-12-19.
 */
@Data
public class DisPriceInfo {

    private String collectiontime;
    private String platform;
    private String model;
    private String guideprice;
    private String realprice;
    private String lowpricebalance;
    private String lowprice;
    private String balance;
    private String url;

    public String toDataString() {
        return collectiontime+""+
                platform+","+
                model+","+
                guideprice+","+
                realprice+","+
                lowpricebalance+","+
                lowprice+","+
                balance+","+
                url;
    }


}
package com.avcdata.spark.job.konka

import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

/**
 * Created by wxy on 8/29/16.
  * 开机时长
 */
object DurationDataLoadJob {
  def run(sc: SparkContext, analysisDate: String) = {


    //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")

    val dimFamilyCol = Bytes.toBytes("dim")
    val factFamilyCol = Bytes.toBytes("fact")

    val dimSnCol = Bytes.toBytes("dim_sn")
    val dimAreaCol = Bytes.toBytes("dim_area")
    val dimDateCol = Bytes.toBytes("dim_date")
    val factDurationCol = Bytes.toBytes("fact_duration")

      //sc.textFile("/Users/wxy/Desktop/OTT/data/KONKA/" + analysisDate + "/duration.log." + analysisDate)
    val count = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/duration.log." + analysisDate)
      .foreachPartition(items => {

        val myConf = HBaseConfiguration.create()
        myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
        myConf.set("hbase.zookeeper.property.clientPort", "2181")
        val hbaseConn = ConnectionFactory.createConnection(myConf);
        val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_duration_active_fact"))
        try {

          items.foreach(line => {
            val cols = line.split('|')
            val sn = cols(0)
            val area = cols(5).substring(0, 3)
            val dateTime = cols(6)
            val duration = cols(7)

            val put = new Put(Bytes.toBytes(dateTime + "-" + sn + "-KONKA"))
            put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
            put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
            put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(dateTime.substring(0, 10)))
            put.addColumn(dimFamilyCol, factDurationCol, Bytes.toBytes(duration))

            mutator.mutate(put)
          })
          mutator.flush()
        } finally {
          mutator.close()
          hbaseConn.close()
        }
      })


  }
}
package com.avcdata.spark.job.util

import java.security.MessageDigest

object EncryptUtils {

  def main(args: Array[String]) {
    val text = "hahaha"
    println(toMd5One(text))
    println(toMd5Two(text))
    println(md5Hash(text))

  }

  def toMd5One(s: String) = {
    val m = java.security.MessageDigest.getInstance("MD5")
    val b = s.getBytes("UTF-8")
    m.update(b, 0, b.length)
    new java.math.BigInteger(1, m.digest()).toString(16)
  }


  def toMd5Two(text: String): String = {
    val digest = MessageDigest.getInstance("MD5")
    digest.digest(text.getBytes).map("%02x".format(_)).mkString

  }

//  def toMd5Three(text: String): String = {
//    val digest = MessageDigest.getInstance("MD5")
//    //    digest.update("MD5 ".getBytes())
//    //    digest.update("this ".getBytes())
//    //    digest.update("text!".getBytes())
//    digest.digest().map(0xFF & _).map("%02x".format(_)).mkString
//  }

  def md5Hash(text: String): String =
    java.security.MessageDigest.getInstance("MD5").digest(text.getBytes()).map(0xFF & _).map {
      "%02x".format(_)
    }.foldLeft("") {
      _ + _
    }
}
package com.avcdata.spark.job.util

import java.security.MessageDigest

object EncryptUtils {

  def main(args: Array[String]) {
    val text = "hahaha"
    println(toMd5One(text))
    println(toMd5Two(text))
    println(md5Hash(text))

  }

  def toMd5One(s: String) = {
    val m = java.security.MessageDigest.getInstance("MD5")
    val b = s.getBytes("UTF-8")
    m.update(b, 0, b.length)
    new java.math.BigInteger(1, m.digest()).toString(16)
  }


  def toMd5Two(text: String): String = {
    val digest = MessageDigest.getInstance("MD5")
    digest.digest(text.getBytes).map("%02x".format(_)).mkString

  }

//  def toMd5Three(text: String): String = {
//    val digest = MessageDigest.getInstance("MD5")
//    //    digest.update("MD5 ".getBytes())
//    //    digest.update("this ".getBytes())
//    //    digest.update("text!".getBytes())
//    digest.digest().map(0xFF & _).map("%02x".format(_)).mkString
//  }

  def md5Hash(text: String): String =
    java.security.MessageDigest.getInstance("MD5").digest(text.getBytes()).map(0xFF & _).map {
      "%02x".format(_)
    }.foldLeft("") {
      _ + _
    }
}
package com.avcdata.vbox.util

import java.security.MessageDigest

object EncryptUtils {

  def main(args: Array[String]) {
    val text = "hahaha"
    println(toMd5One(text))
    println(toMd5Two(text))
    println(md5Hash(text))

  }

  def toMd5One(s: String) = {
    val m = java.security.MessageDigest.getInstance("MD5")
    val b = s.getBytes("UTF-8")
    m.update(b, 0, b.length)
    new java.math.BigInteger(1, m.digest()).toString(16)
  }


  def toMd5Two(text: String): String = {
    val digest = MessageDigest.getInstance("MD5")
    digest.digest(text.getBytes).map("%02x".format(_)).mkString

  }

//  def toMd5Three(text: String): String = {
//    val digest = MessageDigest.getInstance("MD5")
//    //    digest.update("MD5 ".getBytes())
//    //    digest.update("this ".getBytes())
//    //    digest.update("text!".getBytes())
//    digest.digest().map(0xFF & _).map("%02x".format(_)).mkString
//  }

  def md5Hash(text: String): String =
    java.security.MessageDigest.getInstance("MD5").digest(text.getBytes()).map(0xFF & _).map {
      "%02x".format(_)
    }.foldLeft("") {
      _ + _
    }
}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.until.{MapingUtils, TimeUtils}
import com.avcdata.spark.job.util.HBaseDao
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object EpgDataDao {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("EpgDataLoadJob")
    //    val sc = new SparkContext(conf)

    //    sc.stop()

  }

  def delete(sc: SparkContext, fromDay: String, toDay: String) = {
    val sqlContext = new HiveContext(sc)
    val sql = "select key from hr.epg where tv_date between '" + fromDay + "' and  '" + toDay + "'"
    println(sql)
    val keyRDD = sqlContext.sql(sql).rdd
      .map(line => line.toString)

    println(keyRDD.count)

    HBaseDao.deleteData(keyRDD, "tracker_epg")

    sc.stop()
  }


}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object EpgDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("EpgDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2017-02-26")
    //    println(getTimeRangeByStartTime("06:23"))

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    //TODO 日期格式转换
    val timeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    val log_date = TimeUtils.convertTimeStamp2DateStr(timeStamp, TimeUtils.DAY_DATE_FORMAT_TWO)


    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\2016-11-15_2016-11-21.csv")

    //////////////////test//////////////////////////////////////
    //        val localPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\epg\\epg.log." + analysisDate
    //
    //        val initRDD = sc.textFile(localPath)
    ///////////////////test///////////////////////////////////

    //    val hdfsPath = "/user/hdfs/rsync/KONKA/" + analysisDate + "/epg.log." + analysisDate
    //    val hdfsPath = "/user/hdfs/rsync/epg/epg-all.log"


    /////////////////////////////////test/////////////////////////////////////////////////
//    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\epg\\epg" + log_date
    /////////////////////////////////test/////////////////////////////////////////////////

    val hdfsPath = "/user/hdfs/rsync/epg/epg" + log_date

    println(hdfsPath)

    //TODO 原始文件日志格式
    //02月26日	四川卫视	07:42	琅琊榜（33）

    //TODO  读取原始日志文件
    val initRDD = sc.textFile(hdfsPath).distinct

    val dataRDD = initRDD

      //TODO 过滤
      .filter(line => {
      val cols = line.split('\t')
      true

    })
      //TODO 小时数加上0
      //      .map(line => {
      //      val cols = line.split("\t")
      //      val time = cols(2)
      //      val hour = time.split(":")(0)
      //      val min = time.split(":")(1)
      //      val newTime = TimeUtils.addZero(hour) + ":" + min
      //      cols(0) + "," + cols(1) + "," + newTime + "," + cols(3)
      //    })


      //TODO 根据参数设置日期
      .map(line => {
      val cols = line.split("\t")
//      val time = cols(2)
//      val hour = time.split(":")(0)
//      val min = time.split(":")(1)
//      val newTime = TimeUtils.addZero(hour) + ":" + min
      analysisDate + "\t" + cols(1) + "\t" + cols(2) + "\t" + cols(3)
    })

      //TODO 按时间排序
      .sortBy(line => {
      val cols = line.split("\t")
      //      println("##########" + cols(0) + "," + cols(2))
      //      "2017-02-22,CCTV-1,0:00,国际艺苑"
      val date = cols(0)
      val channel = cols(1)
      val startTime = cols(2)
      date + "#" + channel + "#" + startTime
    }, true, 1)


    //TODO 配对 添加结束时间
    dataRDD
      .map(line => {
        val cols = line.split("\t")
        val date = cols(0)
        val channel = cols(1)
        val startTime = cols(2)
        val pg = cols(3)

        (date + "\t" + channel, startTime + "\t" + pg)
      })

      .reduceByKey((pre, post) => {
        pre + "#" + post
      })


      .flatMap(line => {
        var resultArr = new ArrayBuffer[String]()

        val dateChannel = line._1.split("\t")
        val date = dateChannel(0)
        val channel = dateChannel(1)
        val startTimePGArr = line._2.split("#")

        for (i <- 0 until startTimePGArr.length) {
          val startTime = startTimePGArr(i).split("\t")(0)
          val pg = startTimePGArr(i).split("\t")(1)
          if (i + 1 < startTimePGArr.length) {
            val endTime = startTimePGArr(i + 1).split("\t")(0)
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + endTime + "\t" + pg)
          } else {
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + "24:00" + "\t" + pg)
          }
        }
        resultArr
      })

      //TODO 写入hbase
      .foreachPartition(items => {

//      val mutator = HBaseUtils.getMutator("tracker_epg")
      val mutator = HBaseUtils.getMutator("epg02")

      try {

        items.flatMap(line => {

          val cols = line.split('\t')

          //TODO 转换频道名称
          val channel = MapingUtils.epgChannel2Stand(cols(0).toString)

          //日期
          val tv_date = cols(1)

          //开始时间
          val start_time = cols(2).toString

          //结束时间
          val end_time = cols(3).toString

          //开始时间-结束时间
          val start_end = cols(2) + "-" + cols(3)

          //TODO 获取时间段
          val time_range = getTimeRangeByStartTime(start_time.toString)

          //节目类型
          val pg_type = ""

          //节目
          val pg = cols(4)

          val otherInfo = channel + "\t" + tv_date + "\t" + start_end + "\t" + time_range + "\t" + pg_type + "\t" + pg + "\t"

          //TODO 分时
          val orderedLines = TimeUtils.splitTimeByMinute(otherInfo, start_time, end_time)
          println(channel + "\t" + tv_date + "\t" + pg)
          orderedLines

        })
          .foreach(line => {

            //          println(line)
            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_epg(line))
          })

        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  def getTimeRangeByStartTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }

    }


  }
}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object EpgDataLoadJob00 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("EpgDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2017-02-26")
    //    println(getTimeRangeByStartTime("06:23"))

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

//    val sqlContext = new HiveContext(sc)
//    sqlContext.sql("select * from hr.epg ").rdd.saveAsTextFile("/tmp/out")

    //TODO 日期格式转换
    val timeStamp = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    val log_date = TimeUtils.convertTimeStamp2DateStr(timeStamp, TimeUtils.DAY_DATE_FORMAT_TWO)

    //[2017-02-15-00:00-00:01-湖北卫视-0-0,湖北卫视,2017-02-15,0,0,00:00-00:01,00:00-06:00,,大揭秘（20170214）（首播）]
    sc.textFile("/tmp/out").map(line=>{
      val noBracketLine =  line.substring(line.indexOf("[")+1,line.indexOf("]"))
      val cols = noBracketLine.split(",")
      val key = cols(0)
      val channel = cols(1)
      val tv_date = cols(2)
      val tv_hour = cols(3)
      val tv_min = cols(4)
      val start_end = cols(5)
      val time_range = cols(6)
      val pg_type = cols(7)
      val pg = cols(8)

      channel+"\t"+tv_date+"\t"+start_end+"\t"+time_range+"\t"+pg_type+"\t"+pg+"\t"+tv_hour+"\t"+tv_min
    }).filter(line=>{
      val cols = line.split("\t")
      val tv_date = cols(1)
      var dateIsRight = true

      val filterDateArr = Array[String](
        "2017-03-31"
//        "2017-04-01",
//        "2017-04-02",
//        "2017-04-03",
//        "2017-04-04",
//        "2017-04-05",
//        "2017-04-06",
//        "2017-04-07",
//        "2017-04-08",
//        "2017-04-09",
//        "2017-04-10",
//        "2017-04-11",
//        "2017-04-12",
//        "2017-04-13",
//        "2017-04-14",
//        "2017-04-15",
//        "2017-04-16"
      )

      for(ele<-filterDateArr){
        if(ele.equals(tv_date))
          dateIsRight = false
      }

      dateIsRight
    })



      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_epg")
      try {

        items
          .foreach(line => {
            //          println(line)
            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_epg(line))
          })

        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00
//14:30
  def getTimeRangeByTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }

    }


  }
}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object EpgDataLoadJob01 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("EpgDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")
    //    println(getTimeRangeByStartTime("06:23"))

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\2016-11-15_2016-11-21.csv")

    //    val initRDD = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/epg.log."+analysisDate+)
    val initRDD = sc.textFile("/user/hdfs/rsync/epg/2016-11-15_2016-11-26.csv")

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    //过滤
    initRDD.filter(line => {

      val cols = line.split(',')

      //      TimeUtils.convertDateStr2TimeStamp(cols(2).toString, TimeUtils.DAY_DATE_FORMAT_ONE) <= checkTime

      true

    })


      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_epg")

      try {

        items.flatMap(line => {

          val cols = line.split(',')

          //频道
          val channel = cols(1)

          //日期
          val tv_date = cols(2)


          //开始时间
          val start_time = cols(3).toString

          //结束时间
          val end_time = cols(4).toString



          //开始时间-结束时间
          val start_end = cols(3) + "-" + cols(4)

          //时间段
          val time_range = getTimeRangeByStartTime(start_time.toString)

          //节目类型
          val pg_type = ""

          //节目
          val pg = cols(6)

          val otherInfo = channel + "\t" + tv_date + "\t" + start_end + "\t" + time_range + "\t" + pg_type + "\t" + pg + "\t"

          val orderedLines = TimeUtils.splitTimeByMinute(otherInfo, start_time, end_time)


          orderedLines


        }).foreach(line => {

          //          println(line)
          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_epg(line))
        })

        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  def getTimeRangeByStartTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }

    }


  }
}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define KONKAEPG数据清洗
  */
object EpgDataLoadJob02 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("EpgDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    //    println(getTimeRangeByStartTime("06:23"))

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\2016-11-15_2016-11-21.csv")

    //////////////////test//////////////////////////////////////
    //        val localPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\epg\\epg.log." + analysisDate
    //
    //        val initRDD = sc.textFile(localPath)
    ///////////////////test///////////////////////////////////

    val hdfsPath = "/user/hdfs/rsync/KONKA/" + analysisDate + "/epg.log." + analysisDate
    //    val hdfsPath = "/user/hdfs/rsync/epg/epg-all.log"

    println(hdfsPath)

    //TODO 原始文件日志格式
    //CCTV-6|2016-11-25|17:40|17:54|电影全解码 48|cctv6

    //TODO  读取原始日志文件
    val initRDD = sc.textFile(hdfsPath)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    //TODO 过滤
    initRDD.filter(line => {

      val cols = line.split('|')

      //      TimeUtils.convertDateStr2TimeStamp(cols(2).toString, TimeUtils.DAY_DATE_FORMAT_ONE) <= checkTime

      true

    })


      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_epg")

      try {

        items.flatMap(line => {

          val cols = line.split('|')

          //TODO 转换频道名称
          val channel = MapingUtils.epgChannel2Stand(cols(0).toString)

          //日期
          val tv_date = cols(1)

          //开始时间
          val start_time = cols(2).toString

          //结束时间
          val end_time = cols(3).toString

          //开始时间-结束时间
          val start_end = cols(2) + "-" + cols(3)

          //TODO 获取时间段
          val time_range = getTimeRangeByStartTime(start_time.toString)

          //节目类型
          val pg_type = ""

          //节目
          val pg = cols(4)

          val otherInfo = channel + "\t" + tv_date + "\t" + start_end + "\t" + time_range + "\t" + pg_type + "\t" + pg + "\t"

          //TODO 分时
          val orderedLines = TimeUtils.splitTimeByMinute(otherInfo, start_time, end_time)
          println(orderedLines)
          orderedLines

        }).foreach(line => {

          //          println(line)
          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_epg(line))
        })

        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  def getTimeRangeByStartTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }

    }


  }
}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.until.{MapingUtils, TimeUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define EPG数据清洗
  */
object EpgKonkaDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("EpgDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    //    println(getTimeRangeByStartTime("06:23"))

    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String) = {

    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\2016-11-15_2016-11-21.csv")

    //////////////////test//////////////////////////////////////
    //        val localPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\epg\\epg.log." + analysisDate
    //
    //        val initRDD = sc.textFile(localPath)
    ///////////////////test///////////////////////////////////

    //    val hdfsPath = "/user/hdfs/rsync/KONKA/" + analysisDate + "/epg.log." + analysisDate
    //    val hdfsPath = "/user/hdfs/rsync/epg/epg-all.log"

    /////////////////////////////////test/////////////////////////////////////////////////
    //    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\epg\\epg-2017-02-22-2.csv"
    /////////////////////////////////test/////////////////////////////////////////////////

    /////////////////////////////////test/////////////////////////////////////////////////
    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\epg\\epg-2017-01-15-2017-02-22.csv"
    /////////////////////////////////test/////////////////////////////////////////////////

    //    val hdfsPath = "/user/hdfs/rsync/epg/epg-2017-01-15-2017-02-22.csv"

    println(hdfsPath)

    //TODO 原始文件日志格式
    //2017-02-22,CCTV-1,0:00,国际艺苑

    //KONKA数据格式：西藏卫视|2016-11-17|03:41|04:11|中央台新闻联播|feccf21eb7e50753355efdab2d54d9e8

    //TODO  读取原始日志文件
    val initRDD = sc.textFile(hdfsPath)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_ONE)


    val dataRDD = initRDD

      //TODO 过滤
      .filter(line => {
      val cols = line.split(',')
      true

    })
      //TODO 小时数加上0
      //      .map(line => {
      //      val cols = line.split(",")
      //      val time = cols(2)
      //      val hour = time.split(":")(0)
      //      val min = time.split(":")(1)
      //      val newTime = TimeUtils.addZero(hour) + ":" + min
      //      cols(0) + "," + cols(1) + "," + newTime + "," + cols(3)
      //    })


      //TODO 按时间排序
      .sortBy(line => {
      val cols = line.split(",")
      //      println("##########" + cols(0) + "," + cols(2))
      //      "2017-02-22,CCTV-1,0:00,国际艺苑"
      val date = cols(0)
      val channel = cols(1)
      val startTime = cols(2)
      date + "#" + channel + "#" + startTime
    }, true, 1)


    //TODO 配对 添加结束时间
    dataRDD
      .map(line => {
        val cols = line.split(",")
        val date = cols(0)
        val channel = cols(1)
        val startTime = cols(2)
        val pg = cols(3)

        (date + "," + channel, startTime + "," + pg)
      })

      .reduceByKey((pre, post) => {
        pre + "\t" + post
      })
      .flatMap(line => {
        var resultArr = new ArrayBuffer[String]()

        val dateChannel = line._1.split(",")
        val date = dateChannel(0)
        val channel = dateChannel(1)
        val startTimePGArr = line._2.split("\t")

        for (i <- 0 until startTimePGArr.length) {
          val startTime = startTimePGArr(i).split(",")(0)
          val pg = startTimePGArr(i).split(",")(1)
          if (i + 1 < startTimePGArr.length) {
            val endTime = startTimePGArr(i + 1).split(",")(0)

            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + endTime + "\t" + pg)
          } else {
            resultArr += (channel + "\t" + date + "\t" + startTime + "\t" + "24:00" + "\t" + pg)
          }
        }
        resultArr
      })

      //      .foreach(println(_))
      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_epg")

      try {

        items.flatMap(line => {

          val cols = line.split('\t')

          //TODO 转换频道名称
          val channel = MapingUtils.epgChannel2Stand(cols(0).toString)

          //日期
          val tv_date = cols(1)

          //开始时间
          val start_time = cols(2).toString

          //结束时间
          val end_time = cols(3).toString

          //开始时间-结束时间
          val start_end = cols(2) + "-" + cols(3)

          //TODO 获取时间段
          val time_range = getTimeRangeByStartTime(start_time.toString)

          //节目类型
          val pg_type = ""

          //节目
          val pg = cols(4)

          val otherInfo = channel + "\t" + tv_date + "\t" + start_end + "\t" + time_range + "\t" + pg_type + "\t" + pg + "\t"

          //TODO 分时
          val orderedLines = TimeUtils.splitTimeByMinute(otherInfo, start_time, end_time)
                    println(channel + "\t" + tv_date+ "\t" + pg)
          orderedLines

        })
          .foreach(line => {

            //          println(line)
            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_epg(line))
          })

        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  def getTimeRangeByStartTime(start_time: String): String = {
    var n = 0
    val flag = start_time.charAt(0).toString.toShort
    if (flag == 0) {
      n = start_time.charAt(1).toString.toShort
      if (n < 6) {
        "00:00-06:00"
      }
      else if (n >= 6 && n < 8) {
        "06:00-08:00"
      }
      else if (n >= 8 && n < 12) {
        "08:00-12:00"
      } else {
        "error"
      }
    } else {
      n = start_time.substring(0, 2).toShort
      if (n >= 8 && n < 12) {
        "08:00-12:00"
      }
      else if (n >= 12 && n < 14) {
        "12:00-14:00"
      }
      else if (n >= 14 && n < 17) {
        "14:00-17:00"
      }
      else if (n >= 17 && n < 19) {
        "17:00-19:00"
      }
      else if (n >= 19 && n < 22) {
        "19:00-22:00"
      }
      else if (n >= 22 && n < 24) {
        "22:00-24:00"
      } else {
        "error"
      }

    }


  }
}
package com.avc.spark.mllib

import java.sql.DriverManager

import com.avc.util.{Helper, mysqlDB}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkContext}


/**
  * Created by Administrator on 2017/4/20.
  */
object evaluateIndexs {

  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getLogger("org.apache.eclipse.jetty.server").setLevel(Level.OFF)


  def main(args: Array[String]) {

    val sc = Helper.sparkContext

    run(sc, "2017-03-15", "15")

    sc.stop()

  }
  def run(sc:SparkContext,analysisDate: String, recentDaysNum: String): Unit ={
    algorithm(sc, analysisDate, recentDaysNum, 4)
    algorithm(sc, analysisDate, recentDaysNum, 8)
    algorithm(sc, analysisDate, recentDaysNum, 12)
    algorithm(sc, analysisDate, recentDaysNum, 16)
  }

  //TODO 算法过程
  def algorithm(sc: SparkContext,analysisDate: String, recentDaysNum: String,k:Int) = {
    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-UserVectorAllETL")

    //data training points
    val vec= initRDD.map(line=>{

      val cols = line.split("\t")
      //TODO 合并向量字符串
      val sb = new StringBuilder
      for (i <- 3 until cols.length) {
        if (i < (cols.length - 1)) {
          sb.append(cols(i) + ",")
        } else {
          sb.append(cols(i))
        }
      }

      //TODO 转换成大向量
      val vector = Vectors.dense(sb.toString.split(",")
        .map(_.toDouble))

      vector
    })


      val label_feature_vec = initRDD.map(line=>{

      val cols = line.split("\t")

      var i = 0
      val sn = cols(i)
      i = i + 1
      val stat_date = cols(i)
      i = i + 1
      val period = cols(i)
      i = i + 1

      val brand = cols(i)
      i = i + 1
      val province = cols(i)
      i = i + 1
      val price = cols(i)
      i = i + 1
      val size = cols(i)
      i = i + 1

      val workday_oc_dist = cols(i)
      i = i + 1
      val restday_oc_dist = cols(i)
      i = i + 1
      val workday_channel_dist = cols(i)
      i = i + 1
      val restday_channel_dist = cols(i)
      i = i + 1
      val pg_subject_dist = cols(i)
      i = i + 1
      val pg_year_dist = cols(i)
      i = i + 1
      val pg_region_dist = cols(i)


      //TODO 合并向量字符串
      val sb = new StringBuilder
      for (i <- 3 until cols.length) {
        if (i < (cols.length - 1)) {
          sb.append(cols(i) + ",")
        } else {
          sb.append(cols(i))
        }
      }
      val vectorStr = sb.toString

      (sn,stat_date,period,brand,province,price,size,workday_oc_dist,restday_oc_dist,workday_channel_dist,restday_channel_dist,pg_subject_dist,pg_year_dist,pg_region_dist,vectorStr)

    })


    /////////////////////////////////////////////////////////////////////////////////////////
    //TODO 模型训练
    // 设置最大迭代次数
    val dataModelTrainTimes = 30
    // 运行3次选出最优解
    val runs = 3
    // 初始聚类中心的选取为k-means++
    val initMode = "k-means||"

    //TODO 生成模型
    val model_k:KMeansModel = KMeans.train(vec,k,dataModelTrainTimes,runs,initMode)
    // 指标一：计算集合内方差和，数值越小说明聚类效果越好
    val ssd = model_k.computeCost(vec)
//    //指标一评估结果存入数据库
    mysqlDB.index_resultTODB(k,ssd)
//
//    // 聚类中心点打印
    val centers = model_k.clusterCenters
    centers.foreach(println(_))
//
//    // 保存预测结果（所有信息+类别ID)
    val clu_label_vec = label_feature_vec.map(x=>{
      val v = Vectors.dense(x._15.split(",")
        .map(_.toDouble))

      //预测 生成类别ID
      val cluster_id = model_k.predict(v)

      x._1 + "\t" + x._2 + "\t" + x._3 + "\t" + x._4 + "\t" + x._5 + "\t" + x._6 + "\t" + x._7 + "\t" + x._8 + "\t" + x._9 + "\t" + x._10 + "\t" + x._11 + "\t" + x._12 + "\t" + x._13 + "\t" + x._14 + "\t" + cluster_id

    })

      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + k)

    //TODO 指标二
    val samples =
      sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-FamilyVectorDataExport")
        .map(line => {
          val arr = line.split("\t")
          val sn = arr(0)

          val member_num = arr(12)
          val has_child = arr(14)
          val has_old = arr(15)

          var sample_vectorStr = ""
          for (i <- 1 until arr.length - 4)
            if(i>1){
              sample_vectorStr = sample_vectorStr + "," +arr(i)
            }else{
              sample_vectorStr = arr(i)
            }

          val sample_vector = Vectors.dense(sample_vectorStr.split(",")
            .map(_.toDouble))
          //带有sn号和向量的字段，样本向量，样本中的家庭构成
          (sn, sample_vector, member_num, has_child, has_old)
        })

    //样本数据结果
    val sample_label_vec = samples.map(x => {
      //TODO 预测 生成类别ID
      val cluster_id = model_k.predict(x._2)
      (x._1, cluster_id, x._3, x._4, x._5)
    })
    sample_label_vec.foreach(println(_))

    //TODO 样本数据聚类结果及家庭组成存入数据库
    sample_label_vec.foreachPartition(items => {
      val conn = DriverManager.getConnection(mysqlDB.url, "root", "new.1234")
      val sql = "insert into sample_allTags_cluster_result_k" + k + "(sn,cluster_id,family_compose, has_child,has_old) values (?,?,?,?,?)"
      val ps = conn.prepareStatement(sql)
      items.foreach(item => {
        ps.setString(1, item._1)
        ps.setInt(2, item._2)
        ps.setString(3, item._3)
        ps.setString(4, item._4)
        ps.setString(5, item._5)
        ps.executeUpdate()
      })
      conn.close()

    })

  }

}

package com.avcdata.etl.util.redis.evaluationbutler

import java.net.URI

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import com.avcdata.etl.common.util.DeSerializationUtil
import org.slf4j.LoggerFactory
import redis.clients.jedis.JedisPool

/**
  * 评价管家URL推送
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/30 09:26
  */
object EvaluationButlerUrlPush
{
  private val logger = LoggerFactory.getLogger(EvaluationButlerUrlPush.getClass)

  val selectSql =
    """SELECT url, platform, brand, category, model FROM collect_url
      |WHERE url NOT IN (SELECT url FROM invalid_collect_url)""".stripMargin

  def main(args: Array[String])
  {
    if (args.length < 5)
    {
      throw new IllegalArgumentException("There have not enough params to use." +
        "\nUsage:EvaluationButlerUrlPush db-connectUri db-user db-password redis-uri redis-key")
    }

    // 生成连接池配置信息
    val redisPool = new JedisPool(new URI(args(3)))
    val redisKey = args(4)

    LoanPattern.using(JDBCConnectionPool(args(0), args(1), args(2)))
    { dbConn =>

      LoanPattern.using(dbConn.prepareStatement(selectSql))
      { ps =>

        LoanPattern.using(ps.executeQuery())
        { rs =>

          LoanPattern.using(redisPool.getResource)
          { jedis =>

            //推送前先删除旧KEY
            jedis.del(redisKey)

            logger.info(s"Begin to push collect urls.")

            var pushCount = 0
            while (rs.next())
            {
              val url = rs.getString("url")
              val attr = CollectUrlAttr(rs.getString("platform"), rs.getString("brand").toUpperCase(), rs.getString("category"), rs.getString("model").toUpperCase())
              val collectUrl = CollectUrl(url, attr)

              jedis.rpush(redisKey, DeSerializationUtil.serialize(collectUrl))

              pushCount += 1
              logger.info(s"Has already pushed <$pushCount> collect urls.")
            }
          }
        }
      }

      //推送完成后删除数据库记录
      LoanPattern.using(dbConn.prepareStatement("DELETE FROM collect_url"))
      { ps =>

        ps.execute()
      }
    }

    logger.info(s"Completed push collect urls.")

    sys.addShutdownHook(redisPool.destroy())
  }
}

case class CollectUrlAttr(urlweb: String, brand: String, urlleibie: String, model: String)

case class CollectUrl(url: String, attr: CollectUrlAttr)package com.avc.launcher

import com.avc.spark.mllib.KMeans_V2
import com.avc.util.Helper


object Excutor {

  def main(args: Array[String]): Unit = {
    val sc = Helper.sparkContext
//    KMeans_V2.run(sc, args(0), args(1))

    KMeans_V2.run(sc, "2017-03-15", "15")

//    KMeans_V2.run(sc, "2017-03-15", "15")


    sc.stop()
  }

}
package com.avcdata.spark.job.epg

import com.avcdata.spark.job.coocaa.Helper
import org.apache.log4j.Logger

/**
  * @author zhangyongtian
  * @define 执行入口
  */
object Executor {
  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0")

    //clean table
    println("start job for " + analysisDate)

    val sc = Helper.sparkContext


    if (executePart.charAt(5) == '1') {
      println("tracker_epg start")
      EpgDataLoadJob.run(sc, analysisDate);
      println("tracker_epg end")
    }



    sc.stop()
  }

}
package com.avcdata.spark.job.total

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.total.time.{TerminalPowerTimeTotalJob, LiveTimeTotalJob, ApkTimeTotalJob}
import com.avcdata.spark.job.view.{HiveTableViewJob, LogViewJob}
import org.apache.log4j.Logger

/**
  * @author zhangyongtian
  * @define 酷开数据清洗执行入口
  */
object Executor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //////////////////////////////次数和时长推总////////////////////////////////////////

    //开关机推总
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@TerminalPowerTimeTotalJob start ... ")
      TerminalPowerTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@TerminalPowerTimeTotalJob end ... ")
    }


    //直播推总
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@LiveTimeTotalJob start ... ")
      LiveTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveTimeTotalJob end ... ")
    }

    //APK推总
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@ApkTimeTotalJob start ... ")
      ApkTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkTimeTotalJob end ... ")
    }


    sc.stop()
  }

}
package com.avcdata.spark.job.tag.launcher

import com.avcdata.spark.job.tag.evaluate2._

/**
  * 执行入口类
  */
object Executor {

  def parseOptions(args: Array[String], index: Int, defaultValue: String): String = if (args.length > index) args(index) else defaultValue

  def main(args: Array[String]): Unit = {

    val analysisDate = parseOptions(args, 0, "")

    val executePart = parseOptions(args, 1, "000000000000000000000000000000000")

    //TODO 近n天
    val recentDaysNum = parseOptions(args, 2, "30")



    //TODO 标签库-家庭结构-算法统计2
    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@TagRuler1P1 start...")
    //      TagRuler1P1.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler1P1 end....")
    //    }

    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@TagRuler1P2 start...")
    //      TagRuler1P2.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler1P2 end....")
    //    }

    //    if (executePart.charAt(2) == '1') {
    //      println(analysisDate + "@TagRuler1P3 start...")
    //      TagRuler1P3.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler1P3 end....")
    //    }

    //    if (executePart.charAt(3) == '1') {
    //      println(analysisDate + "@TagRuler1P4 start...")
    //      TagRuler1P4.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler1P4 end....")
    //    }

    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@TagRuler2P1 start...")
    //      TagRuler2P1.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler2P1 end....")
    //    }
    //


    //////////////全部sn不符合次规则////////////////////
    //    if (executePart.charAt(5) == '1') {
    //      println(analysisDate + "@TagRuler2P2 start...")
    //      TagRuler2P2.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler2P2 end....")
    //    }


    //    if (executePart.charAt(6) == '1') {
    //      println(analysisDate + "@TagRuler2P3 start...")
    //      TagRuler2P3.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler2P3 end....")
    //    }
    //    if (executePart.charAt(7) == '1') {
    //      println(analysisDate + "@TagRuler2P4 start...")
    //      TagRuler2P4.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler2P4 end....")
    //    }
    //
    //    if (executePart.charAt(8) == '1') {
    //      println(analysisDate + "@TagRuler3P1 start...")
    //      TagRuler3P1.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler3P1 end....")
    //    }
    //    if (executePart.charAt(9) == '1') {
    //      println(analysisDate + "@TagRuler3P2 start...")
    //      TagRuler3P2.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler3P2 end....")
    //    }

    //    if (executePart.charAt(10) == '1') {
    //      println(analysisDate + "@TagRuler4P1 start...")
    //      TagRuler4P1.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler4P1 end....")
    //    }
    //    if (executePart.charAt(11) == '1') {
    //      println(analysisDate + "@TagRuler4P2 start...")
    //      TagRuler4P2.run(analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagRuler4P2 end....")
    //    }

//    if (executePart.charAt(12) == '1') {
//      println(analysisDate + "@TagRuler4P3 start...")
//      TagRuler4P3.run(analysisDate, recentDaysNum);
//      println(analysisDate + "@TagRuler4P3 end....")
//    }

//    if (executePart.charAt(13) == '1') {
//      println(analysisDate + "@TagRuler4P4 start...")
//      TagRuler4P4.run(analysisDate, recentDaysNum);
//      println(analysisDate + "@TagRuler4P4 end....")
//    }
//
//    if (executePart.charAt(14) == '1') {
//      println(analysisDate + "@TagRuler5P1 start...")
//      TagRuler5P1.run(analysisDate, recentDaysNum);
//      println(analysisDate + "@TagRuler5P1 end....")
//    }

//    if (executePart.charAt(15) == '1') {
//      println(analysisDate + "@TagRuler5P2 start...")
//      TagRuler5P2.run(analysisDate, recentDaysNum);
//      println(analysisDate + "@TagRuler5P2 end....")
//    }


    if (executePart.charAt(16) == '1') {
      println(analysisDate + "@TagRuler6P1 start...")
      TagRuler6P1.run(analysisDate, recentDaysNum);
      println(analysisDate + "@TagRuler6P1 end....")
    }

//    if (executePart.charAt(17) == '1') {
//      println(analysisDate + "@TagRulerExportAll start...")
//      TagRulerExportAll.run(analysisDate, recentDaysNum);
//      println(analysisDate + "@TagRulerExportAll end....")
//    }
//




    /////////////////////////////////////////////////////////////////////////////////////////////////////

    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@TagFamilyComposeTwoPerson start...")
    //      TagFamilyComposeCouple.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagFamilyComposeTwoPerson end....")
    //    }
    //    if (executePart.charAt(2) == '1') {
    //      println(analysisDate + "@TagFamilyComposeCoupleAndChild start...")
    //      TagFamilyComposeCoupleAndChild.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagFamilyComposeCoupleAndChild end....")
    //    }
    //    if (executePart.charAt(3) == '1') {
    //      println(analysisDate + "@TagFamilyComposeCoupleAndOld start...")
    //      TagFamilyComposeCoupleAndOld.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagFamilyComposeCoupleAndOld end....")
    //    }
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@TagFamilyComposeCoupleAndChildAndOld start...")
    //      TagFamilyComposeCoupleAndChildAndOld.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@TagFamilyComposeCoupleAndChildAndOld end....")
    //    }

  }
}
package com.avcdata.spark.job.etl.stat.launcher

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.stat.cross._
import com.avcdata.spark.job.etl.stat.family._
import com.avcdata.spark.job.etl.stat.user._

/**
  * 执行入口类
  */
object Executor {

  def main(args: Array[String]): Unit = {


    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    //TODO 近n天
    val recentDaysNum = Helper.parseOptions(args, 2, "30")

    val sc = Helper.sparkContext

    ///////////////////////////////////////统计/////////////////////////////////////////////
    //TODO 统计设备特征-品牌分布-分类数量（频次和)和分类占比
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@TerminalBrandCnt start...")
      TerminalBrandCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@TerminalBrandCnt end....")
    }


    //TODO 统计设备特征-省份分布-分类数量（频次和)和分类占比
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@TerminalProvinceCnt start...")
      TerminalProvinceCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@TerminalProvinceCnt end....")
    }

    //TODO 统计设备特征-价格分布-分类数量（频次和)和分类占比
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@TerminalPriceCnt start...")
      TerminalPriceCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@TerminalPriceCnt end....")
    }


    //TODO 统计设备特征-尺寸分布-分类数量（频次和)和分类占比
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@TerminalSizeDistCnt start...")
      TerminalSizeCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@TerminalSizeDistCnt end....")
    }


    //TODO 统计-工作日开机时间分布-分类数量（频次和)和分类占比
    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@BhWorkdayOcTimeDistCnt start...")
      BhWorkdayOcTimeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@BhWorkdayOcTimeDistCnt end....")
    }

    //TODO 统计-休息日开机时间分布-分类数量（频次和)和分类占比
    if (executePart.charAt(5) == '1') {
      println(analysisDate + "@BhRestdayOcTimeDistCnt start...")
      BhRestdayOcTimeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@BhRestdayOcTimeDistCnt end....")
    }

    //TODO 统计-工作日频道分布-分类数量（频次和)和分类占比
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@BhWorkdayChannelDistCnt start...")
      BhWorkdayChannelDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@BhWorkdayChannelDistCnt end....")
    }

    //TODO 统计-休息日频道分布-分类数量（频次和)和分类占比
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@BhRestdayChannelDistCnt start...")
      BhRestdayChannelDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@BhRestdayChannelDistCnt end....")
    }

    //TODO 统计-到剧题材分布-分类数量（频次和)和分类占比
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@PlayPgSubjectDistCnt start...")
      PlayPgSubjectDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@PlayPgSubjectDistCnt end....")
    }

    //TODO 统计-到剧年份分布-分类数量（频次和)和分类占比
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@PlayPgYearDistCnt start...")
      PlayPgYearDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@PlayPgYearDistCnt end....")
    }


    //TODO 统计-到剧地区分布-分类数量（频次和)和分类占比
    if (executePart.charAt(10) == '1') {
      println(analysisDate + "@PlayPgRegionDistCnt start...")
      PlayPgRegionDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@PlayPgRegionDistCnt end....")
    }

    //////////////////////////家庭构成统计////////////////////////
    //TODO 家庭构成统计-设备特征-品牌分布-分类数量（频次和)和分类占比
    if (executePart.charAt(11) == '1') {
      println(analysisDate + "@FamilyTerminalBrandDistCnt start...")
      FamilyTerminalBrandDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyTerminalBrandDistCnt end....")
    }


    //TODO 家庭构成统计-设备特征-省份分布-分类数量（频次和)和分类占比
    if (executePart.charAt(12) == '1') {
      println(analysisDate + "@FamilyTerminalProvinceDistCnt start...")
      FamilyTerminalProvinceDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyTerminalProvinceDistCnt end....")
    }

    //TODO 家庭构成统计-设备特征-价格分布-分类数量（频次和)和分类占比
    if (executePart.charAt(13) == '1') {
      println(analysisDate + "@FamilyTerminalPriceDistCnt start...")
      FamilyTerminalPriceDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyTerminalPriceDistCnt end....")
    }


    //TODO 家庭构成统计-设备特征-尺寸分布-分类数量（频次和)和分类占比
    if (executePart.charAt(14) == '1') {
      println(analysisDate + "@FamilyTerminalSizeDistCnt start...")
      FamilyTerminalSizeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyTerminalSizeDistCnt end....")
    }


    //TODO 家庭构成统计-工作日开机时间分布-分类数量（频次和)和分类占比
    if (executePart.charAt(15) == '1') {
      println(analysisDate + "@FamilyWorkdayOcTimeDistCnt start...")
      FamilyWorkdayOcTimeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyWorkdayOcTimeDistCnt end....")
    }

    //TODO 家庭构成统计-休息日开机时间分布-分类数量（频次和)和分类占比
    if (executePart.charAt(16) == '1') {
      println(analysisDate + "@FamilyRestdayOcTimeDistCnt start...")
      FamilyRestdayOcTimeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyRestdayOcTimeDistCnt end....")
    }

    //TODO 家庭构成统计-工作日频道分布-分类数量（频次和)和分类占比
    if (executePart.charAt(17) == '1') {
      println(analysisDate + "@FamilyWorkdayChannelTimeDistCnt start...")
      FamilyWorkdayChannelTimeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyWorkdayChannelTimeDistCnt end....")
    }

    //TODO 家庭构成统计-休息日频道分布-分类数量（频次和)和分类占比
    if (executePart.charAt(18) == '1') {
      println(analysisDate + "@FamilyRestdayChannelTimeDistCnt start...")
      FamilyRestdayChannelTimeDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyRestdayChannelTimeDistCnt end....")
    }

    //TODO 家庭构成统计-到剧题材分布-分类数量（频次和)和分类占比
    if (executePart.charAt(19) == '1') {
      println(analysisDate + "@FamilyPlayPgSubjectDistCnt start...")
      FamilyPlayPgSubjectDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyPlayPgSubjectDistCnt end....")
    }

    //TODO 家庭构成统计-到剧年份分布-分类数量（频次和)和分类占比
    if (executePart.charAt(20) == '1') {
      println(analysisDate + "@FamilyPlayPgYearDistCnt start...")
      FamilyPlayPgYearDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyPlayPgYearDistCnt end....")
    }


    //TODO 家庭构成统计-到剧地区分布-分类数量（频次和)和分类占比
    if (executePart.charAt(21) == '1') {
      println(analysisDate + "@FamilyPlayPgRegionDistCnt start...")
      FamilyPlayPgRegionDistCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyPlayPgRegionDistCnt end....")
    }

/////////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 标注样本做强规律梳理--需求1
    if (executePart.charAt(22) == '1') {
      println(analysisDate + "@FamilySampleLiveAndApkTimeCnt start...")
      FamilySampleLiveAndApkTimeCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilySampleLiveAndApkTimeCnt end....")
    }


//    if (executePart.charAt(22) == '1') {
//      println(analysisDate + "@FamilySampleLiveAndApkTerminal start...")
//      FamilySampleLiveAndApkTerminal.run(sc, analysisDate, recentDaysNum);
//      println(analysisDate + "@FamilySampleLiveAndApkTerminal end....")
//    }


//    //TODO 标注样本做强规律梳理--需求3
    if (executePart.charAt(23) == '1') {
      println(analysisDate + "@FamilySampleLiveAndPlayPlotTimeCnt start...")
      FamilySampleLiveAndPlayPlotTimeCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilySampleLiveAndPlayPlotTimeCnt end....")
    }
//    if (executePart.charAt(23) == '1') {
//      println(analysisDate + "@FamilySampleLiveAndPlayTerminal start...")
//      FamilySampleLiveAndPlayTerminal.run(sc, analysisDate, recentDaysNum);
//      println(analysisDate + "@FamilySampleLiveAndPlayTerminal end....")
//    }

    //TODO 标注样本做强规律梳理--需求2
//    if (executePart.charAt(24) == '1') {
//      println(analysisDate + "@FamilySampleLivePlotTimeCnt start...")
//      FamilySampleLivePlotTimeCnt.run(sc, analysisDate, recentDaysNum);
//      println(analysisDate + "@FamilySampleLivePlotTimeCnt end....")
//    }


    if (executePart.charAt(24) == '1') {
      println(analysisDate + "@FamilySampleLiveTerminal start...")
      FamilySampleLiveTerminal.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilySampleLiveTerminal end....")
    }

    //TODO 标注样本做强规律梳理--需求4
    if (executePart.charAt(25) == '1') {
      println(analysisDate + "@FamilySampleLiveRegionTimeCnt start...")
      FamilySampleLiveRegionTimeCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilySampleLiveRegionTimeCnt end....")
    }

    //TODO 标注样本做强规律梳理--需求5、6
    if (executePart.charAt(26) == '1') {
      println(analysisDate + "@FamilySampleOcTimeCnt start...")
      FamilySampleOcTimeCnt.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilySampleOcTimeCnt end....")
    }

//    if (executePart.charAt(26) == '1') {
//      println(analysisDate + "@FamilySampleOcTerminal start...")
//      FamilySampleOcTerminal.run(sc, analysisDate, recentDaysNum);
//      println(analysisDate + "@FamilySampleOcTerminal end....")
//    }


  }
}
package com.avcdata.spark.job.mllib

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.clean.TestAllTerminalETL
import com.avcdata.spark.job.mock.GenerateUserTerminalData

/**
  * 聚类相关-执行入口类
  */
object Executor {
  def main(args: Array[String]): Unit = {

//    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    //TODO 近n天
    val recentDaysNum = Helper.parseOptions(args, 2, "30")

    val sc = Helper.sparkContext

    ///////////////////////数据导出///////////////////////////////////
    //    //TODO 家庭构成类别关联大向量数据导出
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@FamilyVectorDataExport start...")
      FamilyVectorDataExport.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@FamilyVectorDataExport end....")
    }

    //TODO 聚类-评估指标1-聚合度
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@ClusterEvaluateIndex1 start...")
      ClusterEvaluateIndex1.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@ClusterEvaluateIndex1 end....")
    }




    //    //TODO 聚类-评估指标2-类别ID与家庭构成类别ID的差异
    //    if (executePart.charAt(2) == '1') {
    //      println(analysisDate + "@ClusterEvaluateIndex2 start...")
    //      ClusterEvaluateIndex2.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@ClusterEvaluateIndex2 end....")
    //    }


    //TODO  导入模拟数据=>最终表
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@GenerateUserTerminalData start...")
      GenerateUserTerminalData.run(sc)
      println(analysisDate + "@GenerateUserTerminalData end....")
    }

    //TODO  allterminal
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@TestAllTerminalETL start...")
      TestAllTerminalETL.run(sc)
      println(analysisDate + "@TestAllTerminalETL end....")
    }


  }

}
package com.avcdata.spark.job.etl.clean.launcher

import com.avcdata.spark.job.clean.{UserVectorPlayETL, UserVectorBehaviorETL, UserVectorTerminalETL, UserVectorAllETL}
import com.avcdata.spark.job.clean.cross.UserVectorHourCrossETL
import com.avcdata.spark.job.common.Helper

/**
  * 执行入口类
  */
object Executor {

  def main(args: Array[String]): Unit = {


    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    //TODO 近n天
    val recentDaysNum = Helper.parseOptions(args, 2, "30")

    val sc = Helper.sparkContext

    ////////////////////////////////特征向量数据清洗/////////////////////////////////////
    //TODO 终端设备信息数据清洗
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@UserVectorTerminalETL start...")
      UserVectorTerminalETL.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@UserVectorTerminalETL end....")
    }

    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@SizePriceNoneETL start...")
    //      SizePriceNoneETL.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@SizePriceNoneETL end....")
    //    }

    //TODO 用户行为特征（与节目无关）数据清洗
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@UserVectorBehaviorETL start...")
      UserVectorBehaviorETL.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@UserVectorBehaviorETL end....")
    }


    //TODO 到剧行为特征（与节目相关）数据清洗
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@UserVectorPlayETL start...")
      UserVectorPlayETL.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@UserVectorPlayETL end....")
    }

    //TODO 合并成大向量
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@UserVectorAllETL start...")
      UserVectorAllETL.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@UserVectorAllETL end....")
    }

    //TODO 家庭构成标签
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@FamilyComposeETL start...")
    //      FamilyComposeETL.run(sc, analysisDate, recentDaysNum);
    //      println(analysisDate + "@FamilyComposeETL end....")
    //    }


    ///////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO 小时和工作日休息日频道、节目题材、节目年份的交叉向量清洗
    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@UserVectorHourCrossETL start...")
      UserVectorHourCrossETL.run(sc, analysisDate, recentDaysNum);
      println(analysisDate + "@UserVectorHourCrossETL end....")
    }

  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.epg.EpgDataLoadJob
import com.avcdata.spark.job.playcrawler.SearchIndexDataLoadJob
import com.avcdata.spark.job.stat.{SilentTerminal, SilentTerminalTotal}
import com.avcdata.spark.job.total.time._

//import com.avcdata.spark.job.view.{HiveTableViewJob, LogViewJob}
import org.apache.log4j.Logger

/**
  * @author zhangyongtian
  * @define 酷开数据清洗执行入口
  */
object Executor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    val sc = Helper.sparkContext


    ////////////////////////////////EPG数据清洗/////////////////////////////////////
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@EpgDataLoadJob start...")
      EpgDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@EpgDataLoadJob end....")
    }

    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@EpgDataClean start...")
    //      EpgDataClean.run(sc, analysisDate)
    //      println(analysisDate + "@EpgDataClean end....")
    //    }


    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@EpgDataDao start...")
    //      if (args.length < 4) {
    //        println(" Not enough arguments")
    //        System.exit(0)
    //      }
    //      EpgDataDao.delete(sc, args(2), args(3))
    //      println(analysisDate + "@EpgDataDao end....")
    //    }


    //////////////////////////////COOCAA数据清洗////////////////////////////////////////

    //终端
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@COOCAA-TerminalDataLoadJob start....")
      TerminalDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@COOCAA-TerminalDataLoadJob end....")
    }


    //开关机
    //    if (executePart.charAt(2) == '1') {
    //      println(analysisDate + "@TerminalPowerOnDataLoadJob start....")
    //      TerminalPowerOnDataLoadJob.run(sc);
    //      println(analysisDate + "@TerminalPowerOnDataLoadJob end....")
    //    }


    //直播
    //    if (executePart.charAt(3) == '1') {
    //      println(analysisDate + "@LiveDataLoadJob start....")
    //      LiveDataLoadJob.run(sc, analysisDate)
    //      println(analysisDate + "@LiveDataLoadJob end....")
    //    }


    //apk
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
    //      ApkDataLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    //    }
    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
      ApkDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    }

    //////////////////////////到剧数据清洗////////////////////////

    //到剧
    if (executePart.charAt(5) == '1') {
      println(analysisDate + "@COOCAA-PlaysDataLoadJob start....")
      PlaysDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-PlaysDataLoadJob end....")
      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob start....")
      //      PlaysUnpassDataLoadJob.run(sc, analysisDate);
      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob end....")
    }



    //搜索指数
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob start....")
      SearchIndexDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob end....")
    }


    //////////////////////////////推总////////////////////////////////////////

    //开关机推总
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@TerminalPowerTimeTotalJob start ... ")
      TerminalPowerTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@TerminalPowerTimeTotalJob end ... ")
    }


    //直播推总
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@LiveTimeTotalJob start ... ")
      LiveTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveTimeTotalJob end ... ")
    }

    //APK推总
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@ApkTimeTotalJob start ... ")
      ApkTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkTimeTotalJob end ... ")
    }

    //////////////////////////统计日志信息///////////////////////////
    //    if (executePart.charAt(10) == '1') {
    //      println(analysisDate + "@LogViewJob start ... ")
    //      LogViewJob.run(sc, analysisDate);
    //      println(analysisDate + "@LogViewJob end ... ")
    //    }


    //////////////////////////统计Hive表信息///////////////////////////
    //    if (executePart.charAt(11) == '1') {
    //      println(analysisDate + "@HiveTableViewJob start ... ")
    //      HiveTableViewJob.run(sc, analysisDate);
    //      println(analysisDate + "@HiveTableViewJob end ... ")
    //    }

    ////////////////////////teset 龚琴推总////////////////////////

    //    if (executePart.charAt(12) == '1') {
    //      println(analysisDate + "@ApkDataLoadJobGQ start ... ")
    //      ApkDataLoadJobGQ.run(sc, analysisDate);
    //      println(analysisDate + "@ApkDataLoadJobGQ end ... ")
    //    }
    //
    //    //terminal 样本库
    //    if (executePart.charAt(13) == '1') {
    //      println(analysisDate + "@COOCAA-Apk2SampleTerminalLoadJob start ... ")
    //      Apk2SampleTerminalLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-Apk2SampleTerminalLoadJob end ... ")
    //    }
    //
    //    //terminal 样本库2
    //    if (executePart.charAt(14) == '1') {
    //      println(analysisDate + "@COOCAA-TerminalPartition2SampleTerminalTwoLoadJob start ... ")
    //      TerminalPartition2SampleTerminalTwoLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-TerminalPartition2SampleTerminalTwoLoadJob end ... ")
    //    }

    //TODO TerminalDataLoad=>SampleTerminalTwo
    //    if (executePart.charAt(14) == '1') {
    //      println(analysisDate + "@COOCAA-TerminalDataLoad2SampleTerminalTwo start ... ")
    //      TerminalDataLoad2SampleTerminalTwo.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-TerminalDataLoad2SampleTerminalTwo end ... ")
    //    }


    //TODO 将apk清洗数据放入分区表
    if (executePart.charAt(15) == '1') {
      println(analysisDate + "@COOCAA-ApkData2Partition start ... ")
      ApkData2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-ApkData2Partition end ... ")
    }

    //TODO 开关机次数和时长推总数据=>分区表
    if (executePart.charAt(16) == '1') {
      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition start ... ")
      TerminalPowerTimeTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition end ... ")
    }

    //TODO Live次数和时长推总数据=>分区表
    if (executePart.charAt(17) == '1') {
      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition start ... ")
      LiveTimeTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition end ... ")
    }


    //TODO APK次数和时长推总数据=>分区表
    if (executePart.charAt(18) == '1') {
      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition start ... ")
      ApkTimeTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition end ... ")
    }


    //TODO 到剧终端信息清洗
    if (executePart.charAt(19) == '1') {
      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob start ... ")
      PlayerTerminalLoadJob.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob end ... ")
    }


    //TODO 到剧清洗=>分区表
    if (executePart.charAt(20) == '1') {
      println(analysisDate + "@COOCAA-PlayData2Partition start....")
      PlayData2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-PlayData2Partition end....")
    }

    //TODO 计算月沉默终端数
    if (executePart.charAt(21) == '1') {
      println(analysisDate + "@SilentTerminal start....")
      SilentTerminal.run(sc, analysisDate);
      println(analysisDate + "@SilentTerminal end....")
    }

    //TODO 月沉默终端数推总
    if (executePart.charAt(22) == '1') {
      println(analysisDate + "@SilentTerminalTotal start....")
      SilentTerminalTotal.run(sc, analysisDate);
      println(analysisDate + "@SilentTerminalTotal end....")
    }

    //TODO 过滤掉2017.04直播多余省份（31个 省市之外的）的数据
    if (executePart.charAt(23) == '1') {
      println(analysisDate + "@LiveTimeTotalProvinceFilter start....")
      LiveTimeTotalProvinceFilter.run(sc, analysisDate);
      println(analysisDate + "@LiveTimeTotalProvinceFilter end....")
    }




    //////////////////////////////////////////////////////////////////


    ///////////////////////////HiveSQL TO Mysql////////////////////

    //地区分布


    sc.stop()
  }

}
package com.avcdata.vbox.launcher

import com.avcdata.vbox.clean.apk._
import com.avcdata.vbox.clean.coocaa.{DataCleanCCTerminal, DataCleanKOTerminal}
import com.avcdata.vbox.clean.epg.DataCleanEpg
import com.avcdata.vbox.clean.live.{DataCleanCHLive, DataCleanKOLive, LiveData2Partition}
import com.avcdata.vbox.clean.oc._
import com.avcdata.vbox.clean.play.DataCleanCHPlay
import com.avcdata.vbox.clean.terminal.DataCleanCHTerminal
import com.avcdata.vbox.common.Helper
import org.apache.log4j.Logger

object Executor01 {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    val sc = Helper.sparkContext

    println("===============?????????????????????=======================")

    ///////////////////////////////////epg////////////////////////////
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@DataCleanEpg start...")
      DataCleanEpg.run(sc, analysisDate)
      println(analysisDate + "@DataCleanEpg end....")
    }

    //////////////////////////////////终端数////////////////////////////
    //TODO 酷开终端信息清洗
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@DataCleanCCTerminal start....")
      DataCleanCCTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCCTerminal end....")
    }

    //TODO 长虹终端信息清洗
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@DataCleanCHTerminal start....")
      DataCleanCHTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCHTerminal end....")
    }

    //TODO 康佳终端信息清洗
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@DataCleanKOTerminal start....")
      DataCleanKOTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanKOTerminal end....")
    }

    //TODO TCL终端信息清洗
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@DataCleanTCLTerminal start....")
    //      DataCleanTCLTerminal.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanTCLTerminal end....")
    //    }


    ////////////////////////搜索指数/////////////////////////////////////////
    //    //TODO 爬虫搜索指数清洗
    //    if (executePart.charAt(5) == '1') {
    //      println(analysisDate + "@DataCleanPlaySearchIndex start....")
    //      DataCleanPlaySearchIndex.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanPlaySearchIndex end....")
    //    }
    //
    //
    //    //////////////////////////开关机////////////////////////////////////
    //


    // TODO 康佳开关机清洗
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@DataCleanKOPowerOn start....")
      DataCleanKOPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOPowerOn end....")
    }

    //TODO TCL开关机日志清洗
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@DataCleanTCLPowerOn start....")
      DataCleanTCLPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLPowerOn end....")
    }


    // TODO 长虹开关机清洗
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@DataCleanCHPowerOn start....")
      DataCleanCHPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPowerOn end....")
    }
    //
    //
    //
    //
    //    ///////////////////////////直播///////////////////////////////////

    // TODO 康佳直播清洗
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@DataCleanKOLive start....")
      DataCleanKOLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOLive end....")
    }

    // TODO 长虹直播清洗-8月28日
    //    if (executePart.charAt(10) == '1') {
    //      println(analysisDate + "@DataCleanCHLiveOld start....")
    //      DataCleanCHLiveOld.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHLiveOld end....")
    //    }
    //8月28日-今
    if (executePart.charAt(10) == '1') {
      println(analysisDate + "@DataCleanCHLive start....")
      DataCleanCHLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHLive end....")
    }

    //
    //    ///////////////////////////apk///////////////////////////////////////

    //TODO 康佳apk清洗
    if (executePart.charAt(11) == '1') {
      println(analysisDate + "@DataCleanKOApk start....")
      DataCleanKOApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOApk end....")
    }

    // TODO 长虹apk清洗
    if (executePart.charAt(12) == '1') {
      println(analysisDate + "@DataCleanCHApk start....")
      DataCleanCHApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHApk end....")
    }


    //TODO TCL APk日志清洗
    if (executePart.charAt(13) == '1') {
      println(analysisDate + "@DataCleanTCLApk start....")
      DataCleanTCLApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLApk end....")
    }


    // TODO 酷开apk清洗
    if (executePart.charAt(14) == '1') {
      println(analysisDate + "@DataCleanCCApk start....")
      DataCleanCCApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCCApk end....")
    }


    ////////////////////////导入分区表////////////////////////////////////

    if (1 == 2) {
      //TODO 开关机（非TCL)导入分区表
      if (executePart.charAt(15) == '1') {
        println(analysisDate + "@OCData2Partition start....")
        OCData2Partition.run(sc, analysisDate);
        println(analysisDate + "@OCData2Partition end....")
      }

      //TODO 开关机（TCL)导入分区表
      if (executePart.charAt(16) == '1') {
        println(analysisDate + "@OCData2PartitionTCL start....")
        OCData2PartitionTCL.run(sc, analysisDate);
        println(analysisDate + "@OCData2PartitionTCL end....")
      }
      //TODO apk（非TCL)导入分区表
      if (executePart.charAt(17) == '1') {
        println(analysisDate + "@ApkData2Partition start....")
        ApkData2Partition.run(sc, analysisDate);
        println(analysisDate + "@ApkData2Partition end....")
      }
      //TODO apk（TCL)导入分区表
      if (executePart.charAt(18) == '1') {
        println(analysisDate + "@ApkData2PartitionTCL start....")
        ApkData2PartitionTCL.run(sc, analysisDate);
        println(analysisDate + "@ApkData2PartitionTCL end....")
      }

      //TODO 直播（非TCL)导入分区表
      if (executePart.charAt(19) == '1') {
        println(analysisDate + "@ApkData2Partition start....")
        LiveData2Partition.run(sc, analysisDate);
        println(analysisDate + "@ApkData2Partition end....")
      }
    }


    //////////////////////////剧集//////////////////////

    //TODO 长虹到剧清洗
    if (executePart.charAt(20) == '1') {
      println(analysisDate + "@DataCleanCHPlay start....")
      DataCleanCHPlay.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPlay end....")
    }


    //TODO 酷开到剧清洗
    //    if (executePart.charAt(21) == '1') {
    //      println(analysisDate + "@DataCleanCCPlay start....")
    //      DataCleanCCPlay.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCCPlay end....")
    //  }


    ////////////////////////////其他///////////////////////////////////////////


    //    // 长虹对数
    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@GetChannelOfLive start....")
    //      GetChannelOfLive.run(sc, analysisDate);
    //      println(analysisDate + "@GetChannelOfLive end....")
    //    }


    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@DataCleanCHPowerOn01 start....")
    //      DataCleanCHPowerOn01.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHPowerOn01 end....")
    //    }


    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@DataCleanCHPowerOnForCH start....")
    //      DataCleanCHPowerOnForCH.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHPowerOnForCH end....")
    //    }
    //
    //
    //
    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@DataCleanCHApkForCH start....")
    //      DataCleanCHApkForCH.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHApkForCH end....")
    //    }
    //
    //
    //    if (executePart.charAt(2) == '1') {
    //      println(analysisDate + "@DataCleanCHPowerOnCnt start....")
    //      DataCleanCHPowerOnCnt.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHPowerOnCnt end....")
    //    }
    //
    //    if (executePart.charAt(3) == '1') {
    //      println(analysisDate + "@DataCleanCHApkCnt start....")
    //      DataCleanCHApkCnt.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHApkCnt end....")
    //    }
    //
    //
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@DataCleanCHApkOTTCnt start....")
    //      DataCleanCHApkOTTCnt.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHApkOTTCnt end....")
    //    }


    //


    //////////////////测试数据支持//////////////////////
    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanAllPowerOnCnt start....")
    //          DataCleanAllPowerOnCnt.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanAllPowerOnCnt end....")
    //        }

    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanAllApkOTTCnt start....")
    //          DataCleanAllApkOTTCnt.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanAllApkOTTCnt end....")
    //        }

    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanAll9ApkCnt start....")
    //          DataCleanAll9ApkCnt.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanAll9ApkCnt end....")
    //        }

    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@GetCCPgOfPlay start....")
    //          GetCCPgOfPlay.run(sc, analysisDate);
    //          println(analysisDate + "@GetCCPgOfPlay end....")
    //        }

    //
    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanCCPlayTest start....")
    //          DataCleanCCPlayTest.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanCCPlayTest end....")
    //        }


    ////////////////////////////////EPG数据清洗/////////////////////////////////////
    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@DataCleanEpg start...")
    //      DataCleanEpg.run(sc, analysisDate)
    //      println(analysisDate + "@DataCleanEpg end....")
    //    }
    //
    //    //    if (executePart.charAt(0) == '1') {
    //    //      println(analysisDate + "@EpgDataClean start...")
    //    //      EpgDataClean.run(sc, analysisDate)
    //    //      println(analysisDate + "@EpgDataClean end....")
    //    //    }
    //
    //
    //    //    if (executePart.charAt(0) == '1') {
    //    //      println(analysisDate + "@EpgDataDao start...")
    //    //      if (args.length < 4) {
    //    //        println(" Not enough arguments")
    //    //        System.exit(0)
    //    //      }
    //    //      EpgDataDao.delete(sc, args(2), args(3))
    //    //      println(analysisDate + "@EpgDataDao end....")
    //    //    }
    //
    //
    //    //////////////////////////////COOCAA数据清洗////////////////////////////////////////
    //
    //    //终端
    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@COOCAA-TerminalDataLoadJob start....")
    //      TerminalDataLoadJob.run(sc, analysisDate)
    //      println(analysisDate + "@COOCAA-TerminalDataLoadJob end....")
    //    }
    //
    //
    //    //开关机
    //    //    if (executePart.charAt(2) == '1') {
    //    //      println(analysisDate + "@TerminalPowerOnDataLoadJob start....")
    //    //      TerminalPowerOnDataLoadJob.run(sc);
    //    //      println(analysisDate + "@TerminalPowerOnDataLoadJob end....")
    //    //    }
    //
    //
    //    //直播
    //    //    if (executePart.charAt(3) == '1') {
    //    //      println(analysisDate + "@LiveDataLoadJob start....")
    //    //      LiveDataLoadJob.run(sc, analysisDate)
    //    //      println(analysisDate + "@LiveDataLoadJob end....")
    //    //    }
    //
    //
    //    //apk
    //    //    if (executePart.charAt(4) == '1') {
    //    //      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
    //    //      ApkDataLoadJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    //    //    }
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
    //      ApkDataLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    //    }
    //
    //    //////////////////////////到剧数据清洗////////////////////////
    //
    //    //到剧
    //    if (executePart.charAt(5) == '1') {
    //      println(analysisDate + "@COOCAA-DataCleanCCPlay start....")
    //      DataCleanCCPlay.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-DataCleanCCPlay end....")
    //      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob start....")
    //      //      PlaysUnpassDataLoadJob.run(sc, analysisDate);
    //      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob end....")
    //    }
    //
    //
    //
    //    //搜索指数
    //    if (executePart.charAt(6) == '1') {
    //      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob start....")
    //      SearchIndexDataLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob end....")
    //    }
    //
    //
    //    //////////////////////////////推总////////////////////////////////////////
    //
    //    //开关机推总
    //    if (executePart.charAt(7) == '1') {
    //      println(analysisDate + "@TerminalPowerTimeTotalJob start ... ")
    //      TerminalPowerTimeTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@TerminalPowerTimeTotalJob end ... ")
    //    }
    //
    //
    //    //直播推总
    //    if (executePart.charAt(8) == '1') {
    //      println(analysisDate + "@LiveTimeTotalJob start ... ")
    //      LiveTimeTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@LiveTimeTotalJob end ... ")
    //    }
    //
    //    //APK推总
    //    if (executePart.charAt(9) == '1') {
    //      println(analysisDate + "@ApkTimeTotalJob start ... ")
    //      ApkTimeTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@ApkTimeTotalJob end ... ")
    //    }
    //
    //    //////////////////////////统计日志信息///////////////////////////
    //    //    if (executePart.charAt(10) == '1') {
    //    //      println(analysisDate + "@LogViewJob start ... ")
    //    //      LogViewJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@LogViewJob end ... ")
    //    //    }
    //
    //
    //    //////////////////////////统计Hive表信息///////////////////////////
    //    //    if (executePart.charAt(11) == '1') {
    //    //      println(analysisDate + "@HiveTableViewJob start ... ")
    //    //      HiveTableViewJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@HiveTableViewJob end ... ")
    //    //    }
    //
    //    ////////////////////////teset 龚琴推总////////////////////////
    //
    //    //    if (executePart.charAt(12) == '1') {
    //    //      println(analysisDate + "@ApkDataLoadJobGQ start ... ")
    //    //      ApkDataLoadJobGQ.run(sc, analysisDate);
    //    //      println(analysisDate + "@ApkDataLoadJobGQ end ... ")
    //    //    }
    //    //
    //    //    //terminal 样本库
    //    //    if (executePart.charAt(13) == '1') {
    //    //      println(analysisDate + "@COOCAA-Apk2SampleTerminalLoadJob start ... ")
    //    //      Apk2SampleTerminalLoadJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-Apk2SampleTerminalLoadJob end ... ")
    //    //    }
    //    //
    //    //    //terminal 样本库2
    //    //    if (executePart.charAt(14) == '1') {
    //    //      println(analysisDate + "@COOCAA-TerminalPartition2SampleTerminalTwoLoadJob start ... ")
    //    //      TerminalPartition2SampleTerminalTwoLoadJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-TerminalPartition2SampleTerminalTwoLoadJob end ... ")
    //    //    }
    //
    //    //TODO TerminalDataLoad=>SampleTerminalTwo
    //    //    if (executePart.charAt(14) == '1') {
    //    //      println(analysisDate + "@COOCAA-TerminalDataLoad2SampleTerminalTwo start ... ")
    //    //      TerminalDataLoad2SampleTerminalTwo.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-TerminalDataLoad2SampleTerminalTwo end ... ")
    //    //    }
    //
    //
    //    //TODO 将apk清洗数据放入分区表
    //    if (executePart.charAt(15) == '1') {
    //      println(analysisDate + "@COOCAA-ApkData2Partition start ... ")
    //      ApkData2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkData2Partition end ... ")
    //    }
    //
    //    //TODO 开关机次数和时长推总数据=>分区表
    //    if (executePart.charAt(16) == '1') {
    //      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition start ... ")
    //      TerminalPowerTimeTotal2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition end ... ")
    //    }
    //
    //    //TODO Live次数和时长推总数据=>分区表
    //    if (executePart.charAt(17) == '1') {
    //      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition start ... ")
    //      LiveTimeTotal2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition end ... ")
    //    }
    //
    //
    //    //TODO APK次数和时长推总数据=>分区表
    //    if (executePart.charAt(18) == '1') {
    //      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition start ... ")
    //      ApkTimeTotal2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition end ... ")
    //    }
    //
    //
    //    //TODO 到剧终端信息清洗
    //    if (executePart.charAt(19) == '1') {
    //      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob start ... ")
    //      PlayerTerminalLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob end ... ")
    //    }
    //
    //
    //    //TODO 到剧清洗=>分区表
    //    if (executePart.charAt(20) == '1') {
    //      println(analysisDate + "@COOCAA-PlayData2Partition start....")
    //      PlayData2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-PlayData2Partition end....")
    //    }
    //
    //    //TODO 计算月沉默终端数
    //    if (executePart.charAt(21) == '1') {
    //      println(analysisDate + "@SilentTerminal start....")
    //      SilentTerminal.run(sc, analysisDate);
    //      println(analysisDate + "@SilentTerminal end....")
    //    }
    //
    //    //TODO 月沉默终端数推总
    //    if (executePart.charAt(22) == '1') {
    //      println(analysisDate + "@SilentTerminalTotal start....")
    //      SilentTerminalTotal.run(sc, analysisDate);
    //      println(analysisDate + "@SilentTerminalTotal end....")
    //    }
    //
    //    //TODO 过滤掉2017.04直播多余省份（31个 省市之外的）的数据
    //    if (executePart.charAt(23) == '1') {
    //      println(analysisDate + "@LiveTimeTotalProvinceFilter start....")
    //      LiveTimeTotalProvinceFilter.run(sc, analysisDate);
    //      println(analysisDate + "@LiveTimeTotalProvinceFilter end....")
    //    }
    //
    //
    //
    //
    //    //////////////////////////////////////////////////////////////////
    //
    //
    //    ///////////////////////////HiveSQL TO Mysql////////////////////
    //
    //    //地区分布


    sc.stop()
  }

}
package com.avcdata.etl.export.config

/**
  * 导出CSV命令行选项命令名称
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/1 上午9:37
  */
object ExportCsvConfigOption
{
  val SAVE_PATH = (null, "csv-save-path", true, "Csv file save path.")

  val HEADER = (null, "csv-include-header", true, "If the csv file includes header line.")

  val DELIMITER = (null, "csv-field-delimiter", true, "Csv field delimiter.")
}
package com.avcdata.etl.export

import java.sql.Connection

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import com.avcdata.etl.export.config.ExportJDBCConfig
import org.apache.commons.lang.StringUtils
import org.apache.spark.sql.DataFrame
import org.slf4j.LoggerFactory
import org.apache.spark.sql.functions._

/**
  * 数据持久化帮助类
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 15:32
  */
object ExportDBHelper
{
  val logger = LoggerFactory.getLogger(ExportDBHelper.getClass)

  /**
    * 将数据保存至指定的数据库中
    *
    * @param dfResult     数据集
    * @param exportConfig 数据库配置
    */
  def export(dfResult: DataFrame, exportConfig: ExportJDBCConfig): Unit =
  {
    //缓存执行结果
    dfResult.cache()

    //若需要详细数据,则打印
    if (exportConfig.verbose) dfResult.foreach(row => println(row.mkString(", ")))

    val tblName = exportConfig.table
    if (StringUtils.isBlank(tblName) && StringUtils.isBlank(exportConfig.insertSQL))
    {
      throw new IllegalArgumentException("Must specify the table name or insert sql value.")
    }

    val recordCount = dfResult.count()
    if (recordCount <= 0)
    {
      logger.warn("There is no data to insert.")

      return
    }
    else
    {
      logger.info(s"Prepare to insert data size is $recordCount.")
    }

    val deleteKeyCols = obtainDeleteKeyCols(exportConfig)
    if (exportConfig.transactional)
    {
      //使用事务插入数据
      LoanPattern.using(JDBCConnectionPool(exportConfig.connectUri, exportConfig.username, exportConfig.password))
      { conn =>

        val autoCommit = conn.getAutoCommit
        try
        {
          conn.setAutoCommit(false)
          conn.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)

          //删除已经存在的数据
          deleteKeyCols.foreach(keyCols =>
          {
            val delRecords = waitForDeleteRecords(dfResult, keyCols).map(_.toSeq).collect().toSeq
            deleteRecords(conn, delRecords, tblName, keyCols)
          })

          //插入记录
          val datas = dfResult.map(_.toSeq).collect()
          insertRecords(conn, datas, exportConfig)

          conn.commit()
        }
        catch
        {
          case ex: Throwable => conn.rollback(); throw ex
        }
        finally
        {
          conn.setAutoCommit(autoCommit)
        }
      }
    }
    else
    {
      //无事务,并行写
      val executorInstances = dfResult.rdd.sparkContext.getConf.get("spark.executor.instances", "0").toInt
      val parallelWrites = if (executorInstances > 0) executorInstances else 6

      logger.info(s"The parallel write num is $parallelWrites.")

      //删除已经存在的数据
      deleteKeyCols.foreach(keyCols =>
      {
        waitForDeleteRecords(dfResult, keyCols).map(_.toSeq).coalesce(parallelWrites, shuffle = true).foreachPartition(it =>
        {
          LoanPattern.using(JDBCConnectionPool(exportConfig.connectUri, exportConfig.username, exportConfig.password))
          { conn => deleteRecords(conn, it.toSeq, tblName, keyCols) }
        })
      })

      //插入记录
      dfResult.map(_.toSeq).coalesce(parallelWrites, shuffle = true).foreachPartition(it =>
      {
        LoanPattern.using(JDBCConnectionPool(exportConfig.connectUri, exportConfig.username, exportConfig.password))
        { conn => insertRecords(conn, it.toSeq, exportConfig) }
      })
    }

    //清除缓存
    dfResult.unpersist()
  }

  /**
    * 获取删除列信息
    *
    * @param exportConfig 配置信息
    * @return 删除列信息
    */
  private def obtainDeleteKeyCols(exportConfig: ExportJDBCConfig): Option[Seq[String]] =
  {
    val deleteKeys = if (StringUtils.isNotBlank(exportConfig.deleteKey)) exportConfig.deleteKey else exportConfig.updateKey
    if (StringUtils.isNotBlank(deleteKeys))
    {
      //获取删除键
      val keyCols = deleteKeys.split(",").map(_.trim).filter(_ != "").toSeq
      logger.info("The delete keys columns are " + keyCols.mkString(","))

      Some(keyCols)
    }
    else
    {
      None
    }
  }

  /**
    * 根据删除键过滤出需要删除的数据
    *
    * @param dfResult      结果集
    * @param deleteKeyCols 删除键
    * @return 需要删除的数据
    */
  private def waitForDeleteRecords(dfResult: DataFrame, deleteKeyCols: Seq[String]): DataFrame =
  {
    val deleteGroupColNames = dfResult.schema.slice(0, deleteKeyCols.size).map(_.name)
    dfResult.select(deleteGroupColNames.map(col(_)): _*).distinct()
  }

  /**
    * 组装删除SQL语句
    *
    * @param tblName       表名
    * @param deleteKeyCols 删除键
    * @return 删除的SQL语句
    */
  private def assembleDeleteSQL(tblName: String, deleteKeyCols: Seq[String]): String =
  {
    val deleteSqlBuilder = StringBuilder.newBuilder.append("DELETE FROM ").append(tblName).append(" WHERE ")
    val deleteSql = deleteSqlBuilder.append(deleteKeyCols.map(updateKeyCol => s"$updateKeyCol = ?").mkString(" AND ")).toString()

    logger.info(s"The delete sql is $deleteSql")

    deleteSql
  }

  /**
    * 删除记录
    *
    * @param conn          连接对象
    * @param datas         需要删除的数据
    * @param tblName       表名
    * @param deleteKeyCols 删除键
    */
  private def deleteRecords(conn: Connection, datas: Seq[Seq[Any]], tblName: String, deleteKeyCols: Seq[String]): Unit =
  {
    LoanPattern.using(conn.prepareStatement(assembleDeleteSQL(tblName, deleteKeyCols)))
    { ps =>

      datas.foreach(row =>
      {
        deleteKeyCols.zipWithIndex.foreach
        {
          case (_, index) => ps.setObject(index + 1, row(index))
        }

        ps.executeUpdate()
      })
    }
  }

  /**
    * 组装插入SQL
    *
    * @param colCount     插入列的个数
    * @param exportConfig 导出配置
    * @return 插入SQL
    */
  private def assembleInsertSQL(colCount: Int, exportConfig: ExportJDBCConfig): String =
  {
    //获取插入语句
    val preInsertSQL = exportConfig.insertSQL
    val insertSQL = if (StringUtils.isNotBlank(preInsertSQL)) preInsertSQL
    else
    {
      val insertSqlBuilder = StringBuilder.newBuilder.append("INSERT INTO ").append(exportConfig.table)
      val insertColValue = exportConfig.columns
      if (StringUtils.isNotBlank(insertColValue))
      {
        insertSqlBuilder.append("(").append(insertColValue).append(")")
      }

      insertSqlBuilder.append(" VALUES(").append(("?" * colCount).mkString(", ")).append(")")

      insertSqlBuilder.toString()
    }

    logger.info(s"The insert sql is $insertSQL")

    insertSQL
  }

  /**
    * 插入数据
    *
    * @param conn         连接对象
    * @param datas        需要插入的数据
    * @param exportConfig 导出配置
    */
  private def insertRecords(conn: Connection, datas: Seq[Seq[Any]], exportConfig: ExportJDBCConfig): Unit =
  {
    if (null != datas && datas.nonEmpty)
    {
      //插入需要更新的记录
      LoanPattern.using(conn.prepareStatement(assembleInsertSQL(datas.head.length, exportConfig)))
      { insertPrepareStatement =>

        if (exportConfig.batch)
        {
          datas.zipWithIndex.foreach
          {
            case (row, index) =>
              row.zipWithIndex.foreach
              {
                case (colValue, idx) => insertPrepareStatement.setObject(idx + 1, colValue)
              }

              insertPrepareStatement.addBatch()

              if ((index + 1) % exportConfig.batchSize == 0)
              {
                insertPrepareStatement.executeBatch()

                logger.info(s"Already insert ${index + 1} items.")
              }
          }

          if (datas.length % exportConfig.batchSize > 0)
          {
            insertPrepareStatement.executeBatch()

            logger.info(s"Already insert ${datas.length} items.")
          }

          logger.info("Complete insert data for batch mode.")
        }
        else
        {
          datas.zipWithIndex.foreach
          { case (row, rowIdx) =>

            row.zipWithIndex.foreach
            { case (colValue, colIndex) => insertPrepareStatement.setObject(colIndex + 1, colValue) }

            insertPrepareStatement.execute()

            logger.info(s"Already insert ${rowIdx + 1} items.")
          }

          logger.info("Complete insert data for single mode.")
        }
      }
    }
  }
}
package com.avcdata.etl.export

import org.apache.spark.sql.DataFrame

/**
  * 导出
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/22 10:15
  */
trait Exporter
{
  def parallelWriteNum(df: DataFrame, defaultNum: Int = 6): Int =
  {
    val executorInstances = df.rdd.sparkContext.getConf.get("spark.executor.instances", "0").toInt
    if (executorInstances > 0) executorInstances else defaultNum
  }
}
package com.avcdata.etl.export.config

/**
  * 数据库数据导入
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 14:59
  */
case class ExportJDBCConfig(connectUri: String, username: String, password: String
                            , table: String, updateKey: String, deleteKey: String, columns: String
                            , insertSQL: String, transactional: Boolean
                            , batch: Boolean, batchSize: Int, verbose: Boolean) extends Serializablepackage com.avcdata.etl.export.config

/**
  * 导出JDBC命令行选项命令名称
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/1 上午9:37
  */
object ExportJDBCConfigOption
{
  val BATCH = ("b", "batch", true, "Use batch mode for underlying statement execution.")

  val BATCH_SIZE = ("s", "batch-size", true, "SQL batch size for table DML.")

  val COLUMNS = ("c", "columns", true, "Columns to export to table.")

  val CONNECT_URI = ("C", "connect-uri", true, "Specify JDBC connect string.")

  val UPDATE_KEY = ("k", "update-key", true, "Anchor column to use for updates. Use a comma separated list of columns if there are more than one column.")

  val DELETE_KEY = (null, "delete-key", true, "Anchor column to use for deletes. Default to `update-key` Use a comma separated list of columns if there are more than one column.")

  val INSERT_SQL = (null, "insert-sql", true, "The self-define SQL for insert operation.")

  val PASSWORD = ("P", "password", true, "Set authentication password.")

  val ENCRYPT_PASSWORD = (null, "encrypt-password", true, "Set authentication encrypt password.")

  val TABLE = ("t", "table", true, "Table to populate.")

  val USERNAME = ("u", "username", true, "Set authentication username.")

  val VERBOSE = ("v", "verbose", true, "Print more information while working.")

}
package com.avcdata.etl.export.config

/**
  * Kafka至Mongo
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/22 09:21
  */
case class ExportK2MConfig(mongoClientURI: String, collectionName: String, deleteKeys: Seq[String]
                           , zookeeperList: String, topic: String
                           , commonConnectURI: String, commonUsername: String, commonPassword: String) extends Serializablepackage com.avcdata.etl.export

import java.sql.Connection

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import com.avcdata.etl.export.config.ExportK2MConfig
import com.mongodb.MongoClientURI
import org.apache.spark.sql.DataFrame
import org.slf4j.LoggerFactory


object ExportK2MHelper extends Exporter with MongoOperation with KafkaOperation
{
  private val logger = LoggerFactory.getLogger(ExportK2MHelper.getClass)

  val collectNameFile = "_collectionName"

  /**
    * 将数据导入至Kafka
    *
    * @param df     数据集
    * @param config 配置
    */
  def export(df: DataFrame, config: ExportK2MConfig): Unit =
  {
    //缓存原始数
    df.cache()

    //1、将collectName信息插入数据库中
    insertCollectionInfo(config)

    //并行写的数量
    val parallelWrites = parallelWriteNum(df)

    //2、删除Mongo中已经存在的数据
    deleteDataByKeys(df, config.mongoClientURI, config.collectionName, config.deleteKeys, parallelWrites)

    //3、将数据插入Kafka中
    //UDF
    import org.apache.spark.sql.functions._
    writeData2Kafka(df.withColumn(collectNameFile, lit(config.collectionName)), config.zookeeperList, config.topic, parallelWrites)

    //清除缓存
    df.unpersist()
  }

  /**
    * 将collection信息插入至数据库中
    *
    * @param config 配置信息
    */
  private def insertCollectionInfo(config: ExportK2MConfig): Unit =
  {
    val insertSql = "INSERT IGNORE INTO `k2m_collection_info` (`collection_name`, `db_name`) VALUES (?, ?)"

    LoanPattern.using(JDBCConnectionPool(config.commonConnectURI, config.commonUsername, config.commonPassword))
    { conn =>

      val autoCommit = conn.getAutoCommit
      try
      {
        conn.setAutoCommit(false)
        conn.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)

        LoanPattern.using(conn.prepareStatement(insertSql))
        { insertPs =>

          val mongoClientURI = new MongoClientURI(config.mongoClientURI)
          insertPs.setString(1, config.collectionName)
          insertPs.setString(2, mongoClientURI.getDatabase)

          insertPs.execute()
        }

        conn.commit()
      }
      catch
      {
        case ex: Throwable => logger.info(s"Execute sql <$insertSql> error.", ex); conn.rollback(); throw ex
      }
      finally
      {
        conn.setAutoCommit(autoCommit)
      }
    }
  }
}
package com.avcdata.etl.export.config

/**
  * Kafka导出配置
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/21 14:07
  */
case class ExportKafkaConfig(zookeeperList: String, topic: String) extends Serializablepackage com.avcdata.etl.export

import com.avcdata.etl.export.config.ExportKafkaConfig
import org.apache.spark.sql.DataFrame

object ExportKafkaHelper extends Exporter with KafkaOperation
{
  /**
    * 将数据导入至Kafka
    *
    * @param df     数据集
    * @param config 配置
    */
  def export(df: DataFrame, config: ExportKafkaConfig): Unit =
  {
    //并行写的数量
    val parallelWrites = parallelWriteNum(df)

    //插入的数据
    df.cache()

    writeData2Kafka(df, config.zookeeperList, config.topic, parallelWrites)
  }
}
package com.avcdata.etl.export.config

/**
  * 导出至MONGO的配置
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/11/21 17:00
  */
case class ExportMongoConfig(clientURI: String, collectionName: String, deleteKeys: Seq[String]) extends Serializable
package com.avcdata.etl.export

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.export.config.ExportMongoConfig
import com.avcdata.etl.export.pool.mongo.MongoClientPool
import com.mongodb.MongoClientURI
import org.apache.spark.sql.DataFrame
import org.slf4j.LoggerFactory
import org.bson.Document


object ExportMongoHelper extends MongoOperation
{
  val logger = LoggerFactory.getLogger(ExportMongoHelper.getClass)

  /**
    * 将数据导入至MONGO
    *
    * @param df     数据集
    * @param config 配置
    */
  def export(df: DataFrame, config: ExportMongoConfig): Unit =
  {
    //缓存原始数
    df.cache()

    //并行写的数量
    val executorInstances = df.rdd.sparkContext.getConf.get("spark.executor.instances", "0").toInt
    val parallelWrites = if (executorInstances > 0) executorInstances else 6

    logger.info(s"The parallel write num is $parallelWrites.")

    //删除已经存在的数据
    deleteDataByKeys(df, config.clientURI, config.collectionName, config.deleteKeys, parallelWrites)

    //插入的数据
    logger.info(s"Prepared to insert data to mongo, the data size is ${df.count()}.")

    df.toJSON.map(Document.parse).coalesce(parallelWrites, shuffle = true).foreachPartition(docIt =>
    {
      LoanPattern.using(MongoClientPool(config.clientURI))
      { clientWrapper =>

        //获取数据集名称
        val docCollection = clientWrapper.client.getDatabase(new MongoClientURI(config.clientURI).getDatabase).getCollection(config.collectionName)

        //插入数据
        //val bulkData = docIt.map(new InsertOneModel(_)).toSeq
        //if (bulkData.nonEmpty) docCollection.bulkWrite(bulkData)
        docIt.foreach(docCollection.insertOne)
      }
    })

    logger.info(s"End to insert data to mongo.")

    //清除缓存
    df.unpersist()
  }
}
package com.avcdata.etl.launcher.config

/**
  * 导出类型定义
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/25 11:52
  */
object ExportType
{
  val NONE = "none"

  val JDBC = "jdbc"

  val MONGO = "mongo"

  val CSV = "csv"

  val HIVE = "hive"

  val KAFKA = "kafka"

  val K2M = "k2m"
}
package com.avcdata.spark.job.clean

import org.apache.spark.SparkContext

object FamilyComposeETL {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilyComposeETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "30")
    //    sc.stop()

//    println(changeLetterOfStrtoLowerCase("AbcDEFG123123"))


  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    //    load data inpath '/user/hdfs/rsync/userdata/family_compose.txt' into table hr.user_family_compose ;

    val initRDD = sc.textFile("E:\\aowei\\tracker-user\\doc\\user_family_compose2.txt")

    initRDD.map(line => {
      val cols = line.split("\t")

      val sn = cols(0)
      val member_num = cols(1)
      val has_child = cols(2)
      val has_old = cols(3)

      addMaoHao(changeLetterOfStrtoLowerCase(sn)) + "\t" + member_num + "\t" + has_child + "\t" + has_old

    }).filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      sn.equals("0")
    }).foreach(println(_))


  }

  //TODO 没关联上的有多少条


  def addMaoHao(mac: String): String = {
    if (mac.contains(":")) {
      return mac
    }

    val sb = new StringBuilder

    val charArr = mac.toCharArray
    for (i <- 0 until charArr.length) {
      if (i % 2 == 0 && i != 0) {
        sb.append(
          ":" + charArr(i)
        )
      } else {
        sb.append(charArr(i))
      }
    }
    sb.toString
  }

  /**
    * 把一个字符串中的大写转为小写，小写转换为大写
    *
    * @param str
    * @return
    */
  def changeLetterOfStrtoLowerCase(str: String): String = {
    val sb = new StringBuffer()
    if (str != null) {
      for (i <- 0 until str.length()) {
        val c = str.charAt(i)
        if (Character.isUpperCase(c)) {
          sb.append(Character.toLowerCase(c))
        } else {
          sb.append(c)
        }
      }
    }

    sb.toString()

  }

}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.PlayPgRegionDistCnt.PlayPgRegionDistCnt
import com.avcdata.spark.job.etl.stat.user.PlayPgYearDistCnt.PlayPgYearDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyPlayPgRegionDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyPlayPgRegionDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_region_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_region_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_REGION_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_region_dist_map)

    })

    val playPgRegionDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_region_dist_map = line._2._2

      val resultArr = new Array[PlayPgRegionDistCnt](pg_region_dist_map.size)

      val region_arr = pg_region_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgRegionDistCnt(
          sn,
          stat_date,
          period,
          region_arr(i),
          pg_region_dist_map.get(region_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgRegionDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgRegionDistCntDF.registerTempTable("playPgRegionDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.region,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,region,sum(cnt) as cnt  from playPgRegionDistCnt group by stat_date,period,cluster_id,region
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgRegionDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_play_pg_region_dist_cnt2", false,
      SaveMode.Append)



  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.PlayPgRegionDistCnt.PlayPgRegionDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyPlayPgRegionDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyPlayPgRegionDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_region_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_region_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_REGION_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_region_dist_map)

    })

    val playPgRegionDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_region_dist_map = line._2._2

      val resultArr = new Array[PlayPgRegionDistCnt](pg_region_dist_map.size)

      val region_arr = pg_region_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgRegionDistCnt(
          sn,
          stat_date,
          period,
          region_arr(i),
          pg_region_dist_map.get(region_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgRegionDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgRegionDistCntDF.registerTempTable("playPgRegionDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.region,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,region,sum(cnt) as cnt  from playPgRegionDistCnt group by stat_date,period,cluster_id,region
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgRegionDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_play_pg_region_dist_cnt", false,
      SaveMode.Append)



  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhWorkdayChannelDistCnt.WorkdayChannelDistCnt
import com.avcdata.spark.job.etl.stat.user.PlayPgSubjectDistCnt.PlayPgSubjectDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyPlayPgSubjectDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyPlayPgSubjectDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_subject_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_subject_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_SUBJECT_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_subject_dist_map)

    })

    val playPgSubjectDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_subject_dist_map = line._2._2

      val resultArr = new Array[PlayPgSubjectDistCnt](pg_subject_dist_map.size)

      val subject_arr = pg_subject_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgSubjectDistCnt(
          sn,
          stat_date,
          period,
          subject_arr(i),
          pg_subject_dist_map.get(subject_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgSubjectDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgSubjectDistCntDF.registerTempTable("playPgSubjectDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.subject,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,subject,sum(cnt) as cnt  from playPgSubjectDistCnt group by stat_date,period,cluster_id,subject
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgSubjectDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_play_pg_subject_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.PlayPgSubjectDistCnt.PlayPgSubjectDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyPlayPgSubjectDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyPlayPgSubjectDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_subject_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_subject_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_SUBJECT_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_subject_dist_map)

    })

    val playPgSubjectDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_subject_dist_map = line._2._2

      val resultArr = new Array[PlayPgSubjectDistCnt](pg_subject_dist_map.size)

      val subject_arr = pg_subject_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgSubjectDistCnt(
          sn,
          stat_date,
          period,
          subject_arr(i),
          pg_subject_dist_map.get(subject_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgSubjectDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgSubjectDistCntDF.registerTempTable("playPgSubjectDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.subject,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,subject,sum(cnt) as cnt  from playPgSubjectDistCnt group by stat_date,period,cluster_id,subject
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgSubjectDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_play_pg_subject_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.PlayPgSubjectDistCnt.PlayPgSubjectDistCnt
import com.avcdata.spark.job.etl.stat.user.PlayPgYearDistCnt.PlayPgYearDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyPlayPgYearDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyPlayPgYearDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_year_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_year_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_YEAR_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_year_dist_map)

    })

    val playPgYearDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_year_dist_map = line._2._2

      val resultArr = new Array[PlayPgYearDistCnt](pg_year_dist_map.size)

      val year_arr = pg_year_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgYearDistCnt(
          sn,
          stat_date,
          period,
          year_arr(i),
          pg_year_dist_map.get(year_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgYearDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgYearDistCntDF.registerTempTable("playPgYearDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.year,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,year,sum(cnt) as cnt  from playPgYearDistCnt group by stat_date,period,cluster_id,year
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgYearDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_play_pg_year_dist_cnt2", false,
      SaveMode.Append)



  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.PlayPgYearDistCnt.PlayPgYearDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyPlayPgYearDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyPlayPgYearDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_year_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_year_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_YEAR_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_year_dist_map)

    })

    val playPgYearDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_year_dist_map = line._2._2

      val resultArr = new Array[PlayPgYearDistCnt](pg_year_dist_map.size)

      val year_arr = pg_year_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgYearDistCnt(
          sn,
          stat_date,
          period,
          year_arr(i),
          pg_year_dist_map.get(year_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgYearDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgYearDistCntDF.registerTempTable("playPgYearDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.year,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,year,sum(cnt) as cnt  from playPgYearDistCnt group by stat_date,period,cluster_id,year
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgYearDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_play_pg_year_dist_cnt", false,
      SaveMode.Append)



  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhRestdayChannelDistCnt.RestdayChannelDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyRestdayChannelTimeDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyRestdayChannelTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snRestdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,restday_channel_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val restday_channel_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_CHANNEL_ARR)

      (sn + "\t" + stat_date + "\t" + period, restday_channel_dist_map)

    })

    val restdayChannelDistCntDF = snCidRDD.join(snRestdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val restday_channel_dist_map = line._2._2

      val resultArr = new Array[RestdayChannelDistCnt](restday_channel_dist_map.size)

      val channel_arr = restday_channel_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = RestdayChannelDistCnt(
          sn,
          stat_date,
          period,
          channel_arr(i),
          restday_channel_dist_map.get(channel_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    restdayChannelDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    restdayChannelDistCntDF.registerTempTable("restdayChannelDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.channel,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,channel,sum(cnt) as cnt  from restdayChannelDistCnt group by stat_date,period,cluster_id,channel
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from restdayChannelDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_restday_channel_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhRestdayChannelDistCnt.RestdayChannelDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyRestdayChannelTimeDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyRestdayChannelTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snRestdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,restday_channel_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val restday_channel_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_CHANNEL_ARR)

      (sn + "\t" + stat_date + "\t" + period, restday_channel_dist_map)

    })

    val restdayChannelDistCntDF = snCidRDD.join(snRestdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val restday_channel_dist_map = line._2._2

      val resultArr = new Array[RestdayChannelDistCnt](restday_channel_dist_map.size)

      val channel_arr = restday_channel_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = RestdayChannelDistCnt(
          sn,
          stat_date,
          period,
          channel_arr(i),
          restday_channel_dist_map.get(channel_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    restdayChannelDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    restdayChannelDistCntDF.registerTempTable("restdayChannelDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.channel,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,channel,sum(cnt) as cnt  from restdayChannelDistCnt group by stat_date,period,cluster_id,channel
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from restdayChannelDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_restday_channel_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhRestdayOcTimeDistCnt.RestdayOcTimeDistCnt
import com.avcdata.spark.job.etl.stat.user.BhWorkdayOcTimeDistCnt.WorkdayOcTimeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  * @define 标注样本休息日开关机次数分布统计
  */
object FamilyRestdayOcTimeDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyRestdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()
  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snRestdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,restday_oc_dist from hr.user_vector_all").rdd
      .map(line => {
        val sn = line(0)
        val stat_date = line(1)
        val period = line(2)
        val Restday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
          .BH_OC_HOUR_ARR)

        (sn + "\t" + stat_date + "\t" + period, Restday_oc_dist_map)

      })

    val RestdayOcTimeDistCntDF = snCidRDD.join(snRestdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val Restday_oc_dist_map = line._2._2

      val resultArr = new Array[RestdayOcTimeDistCnt](Restday_oc_dist_map.size)

      val hour_arr = Restday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = RestdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          Restday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF


    //    //TODO 注册临时表
    RestdayOcTimeDistCntDF.registerTempTable("RestdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from RestdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from RestdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_restday_oc_time_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhRestdayOcTimeDistCnt.RestdayOcTimeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyRestdayOcTimeDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyRestdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snRestdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,restday_oc_dist from hr.user_vector_all").rdd
      .map(line => {
        val sn = line(0)
        val stat_date = line(1)
        val period = line(2)
        val Restday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
          .BH_OC_HOUR_ARR)

        (sn + "\t" + stat_date + "\t" + period, Restday_oc_dist_map)

      })

    val RestdayOcTimeDistCntDF = snCidRDD.join(snRestdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val Restday_oc_dist_map = line._2._2

      val resultArr = new Array[RestdayOcTimeDistCnt](Restday_oc_dist_map.size)

      val hour_arr = Restday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = RestdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          Restday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF


    //    //TODO 注册临时表
    RestdayOcTimeDistCntDF.registerTempTable("RestdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from RestdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from RestdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_restday_oc_time_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * 对标注样本做强规律梳理
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：标注样本、直播、应用都有的sn
  * 结果维度：家庭结构、城市级别 终端数
  */
object FamilySampleLiveAndApkTerminal {

  case class FamilySampleLiveApkCnt2(
                                          terminal_num: String,
                                          cluster_name: String,
                                          citylevel: String
                                        )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilySampleLiveApkTimeCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-31", "31")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO 获取 标注样本、直播、应用都有的sn

    //TODO 标注样本  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("family_terminal")

    //TODO 直播&&标注样本
    val liveDF = sqlContext.sql(
      """
        select ft.*
        from
        (
        select distinct(dim_sn) as dim_sn from hr.tracker_live_fact_partition
        where
        date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate + """'
        ) lp
        join family_terminal ft
        on lp.dim_sn = ft.sn
        """.stripMargin).registerTempTable("family_live")

    //TODO apk&&标注样本
    val apkDF = sqlContext.sql(
      """
        select ft.*
        from
        (
        select distinct(dim_sn) as dim_sn from hr.tracker_apk_fact_partition
        where
        date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
        ) lp
        join family_terminal ft
        on lp.dim_sn = ft.sn
        """.stripMargin).registerTempTable("family_apk")

    //TODO 直播&&apk&&标注样本
    val resultDF = sqlContext.sql(
      "select count(distinct fl.sn) as terminal_num, fl.cluster_id,fl.citylevel from family_live fl join family_apk fa on fl.sn=fa.sn" +
        " group by fl.cluster_id,fl.citylevel  "
    ).map(line => {
      val terminal_num = line(0).toString
      val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
      val citylevel = line(2).toString

      FamilySampleLiveApkCnt2(
        terminal_num,
        cluster_name,
        citylevel
      )

    }).toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLiveApkTimeCnt2", false,
      SaveMode.Overwrite)


  }

}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 对标注样本做强规律梳理
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：标注样本、直播、应用都有的sn
  */
object FamilySampleLiveAndApkTimeCnt {

  case class FamilySampleLiveApkDuration(
                                          sn: String,
                                          cluster_name: String,
                                          citylevel: String,
                                          workOrRest: String,
                                          dim_hour: String,
                                          cctv_channel_durtaion: Double,
                                          province_channel_duration: Double,
                                          other_channel_duration: Double,
                                          launcher_duration: Double,
                                          norm_apk_duration: Double,
                                          abnorm_apk_duration: Double
                                        )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilySampleLiveApkTimeCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-31", "31")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO 获取 标注样本、直播、应用都有的sn


    //TODO 获取  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("family_terminal")

    //TODO 获取直播workOrRest dim_hour cctv_channel_durtaion  province_channel_duration  other_channel_duration 信息
    val liveRDD = sqlContext.sql(
      """
        select ft.sn,ft.cluster_id,ft.citylevel,tlt.dim_date,tlt.dim_hour,tlt.fact_duration,tlt.dim_channel,ci.channeltype from
         family_terminal ft
        join
        (select dim_sn, date as dim_date,dim_hour,dim_channel,sum(fact_time_length) as fact_duration
        from hr.tracker_live_fact_partition where
        date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
        group by dim_sn,date,dim_hour,dim_channel
        ) tlt
        on
        ft.sn = tlt.dim_sn
        join
        hr.channelinfo ci
        on
        tlt.dim_channel = ci.channel
        """.stripMargin).map(line => {

      val sn = line(0).toString
      val cluster_id = line(1).toString
      val citylevel = line(2).toString

      val dim_date = line(3).toString

      val dim_hour = line(4).toString

      val fact_duration = line(5).toString

      val dim_channel = line(6).toString

      val channeltype = line(7).toString


      var cctv_channel_durtaion = "0"
      var province_channel_duration = "0"
      var other_channel_duration = "0"


      channeltype match {
        case "央视频道" => cctv_channel_durtaion = fact_duration
        case "省级卫视" => province_channel_duration = fact_duration
        case "其他" => other_channel_duration = fact_duration
      }

      (sn + "\t" + dim_date + "\t" + dim_hour, (cluster_id, citylevel, cctv_channel_durtaion, province_channel_duration, other_channel_duration))

    })


    //TODO 获取apk launcher_duration norm_apk_duration  abnorm_apk_duration
    val apkRDD = sqlContext.sql(
      """
        select  ft.sn,tlt.date,tlt.dim_hour,fact_duration,tlt.dim_apk,ai.appname from
        family_terminal ft
        join
        (select * from hr.tracker_apk_fact_partition where date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'  ) tlt
        on
        ft.sn = tlt.dim_sn
        join
        (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai
        on
        tlt.dim_apk = ai.packagename
        """.stripMargin)
      .map(line => {
        val sn = line(0).toString
        val dim_date = line(1).toString

        val dim_hour = line(2).toString
        val fact_duration = line(3).toString
        val dim_apk = line(4).toString
        val appname = line(5).toString



        var launcher_duration = "0"
        var norm_apk_duration = "0"
        var abnorm_apk_duration = "0"

        //合规app名称
        val norm_appname_arr = Array[String](
          "银河·奇异果", "腾讯视频TV端", "CIBN环球影视", "CIBN聚精彩", "云视听·泰捷", "芒果TV", "PPTV聚体育", "CIBN悦厅TV", "CIBN微视听"
        )

        if (dim_apk.equals("中国互联网电视")) {
          launcher_duration = fact_duration
        } else if (norm_appname_arr.contains(appname)) {
          norm_apk_duration = fact_duration
        } else {
          abnorm_apk_duration = fact_duration
        }


        (sn + "\t" + dim_date + "\t" + dim_hour, (launcher_duration, norm_apk_duration, abnorm_apk_duration))

      })

    //TODO 合并
    liveRDD.join(apkRDD).map(line => {
      val leftCols = line._1.split("\t")

      val sn = leftCols(0)
      val dim_date = leftCols(1)
      var workOrRest = "工作日"
      if (TimeUtils.isRestday(dim_date)) {
        workOrRest = "周末"
      }
      val dim_hour = leftCols(2)

      val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line._2._1._1.toString.toInt).getOrElse("其他")
      val citylevel = line._2._1._2
      val cctv_channel_durtaion = line._2._1._3.toString.toDouble
      val province_channel_duration = line._2._1._4.toString.toDouble
      val other_channel_duration = line._2._1._5.toString.toDouble


      val launcher_duration = line._2._2._1.toString.toDouble
      val norm_apk_duration = line._2._2._2.toString.toDouble
      val abnorm_apk_duration = line._2._2._3.toString.toDouble

      FamilySampleLiveApkDuration(
        sn,
        cluster_name,
        citylevel,
        workOrRest,
        dim_hour,
        cctv_channel_durtaion,
        province_channel_duration,
        other_channel_duration,
        launcher_duration,
        norm_apk_duration,
        abnorm_apk_duration
      )
    }).toDF.registerTempTable("fslad")

    sqlContext.sql("select count(distinct sn) from hr.fslad").show()

    val resultDF = sqlContext.sql(
      """
        select
          count(distinct sn) as terminal_num,
          cluster_name,
          citylevel,
          workOrRest,
          dim_hour,
          sum(cctv_channel_durtaion)/3600 as cctv_channel_durtaion,
          sum(province_channel_duration)/3600 as province_channel_duration,
          sum(other_channel_duration)/3600 as other_channel_duration,
          sum(launcher_duration)/3600 as launcher_duration,
          sum(norm_apk_duration)/3600 as norm_apk_duration,
          sum(abnorm_apk_duration)/3600 as abnorm_apk_duration
       from fslad
          group by cluster_name, citylevel, workOrRest, dim_hour
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLiveApkTimeCnt", false,
      SaveMode.Overwrite)


  }

}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求描述
  * 1、根据标注样本的5类人群做剧情类型的使用行为特征分析，细分维度如下：
  * 分城市级别、分工作日周末、分时间段、分直播点播，不同维度可交叉
  * 剧情类型共92个，具体见表 剧情类型标签库中的“新标签”一列
  * 2、sn选取规则：标注样本，直播和到剧（点播）都有的sn
  * 3、数据期间：2017年3月
  * 详细需求说明见附件：标注样本强规律梳理-需求3
  * 此外，提供两个附件供使用：表“add1-带剧情”是第一次整理的带有剧情类型的到剧ID库。表“剧情类型标签库”中包含有新标签。
  *
  */
object FamilySampleLiveAndPlayPlotTimeCnt {

  case class FamilySampleLivePlayDuration(
     terminal_num: String,
     cluster_name: String,
     citylevel: String,
     workOrRest: String,
     dim_hour: String,
     dim_source: String,
     tag: String,
     durtaion: Double)

  case class FilmPlot(
                       id: String,
                       tag: String
                     )

  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")

    //    //TODO 获取 标注样本、直播和到剧(点播)都有的sn

    //TODO 标注样本 && 直播&& 到剧(点播)   cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
       (SELECT distinct dim_sn
        FROM hr.tracker_live_fact_partition
        WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
       ) lp
        on (uf.sn = lp.dim_sn)
       join
       (SELECT distinct dim_sn
        FROM hr.tracker_player_fact_partition
        WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
       ) pp
        on (uf.sn = pp.dim_sn)
       join
        hr.terminal t
        on uf.sn = t.sn
        """
    ).registerTempTable("cross_terminal")

    sqlContext.sql("select * from cross_terminal").show


    //TODO 获取原始的Plot
    //绝命毒师第5季绝命毒师518136电视剧2012非少儿美剧罪案剧/悬疑剧/
    val film_RDD = sc.textFile("/user/hdfs/rsync/add_data/02.28-03.05.csv")
      .union(sc.textFile("/user/hdfs/rsync/add_data/1.30-2.27.csv"))
      .union(sc.textFile("/user/hdfs/rsync/add_data/20170329-two week1.csv"))
      .union(sc.textFile("/user/hdfs/rsync/add_data/20170414-two week.csv"))
      .union(sc.textFile("/user/hdfs/rsync/add_data/to01.17.txt"))
      .filter(!_.contains("原始名称"))
      .distinct
      .flatMap(line => {
        // ﻿原始名称标准名称ID模块年份人群产地剧情类型
        val cols = line.split("\\|")
        val original_name = cols(0).trim
        val standard_name = cols(1).trim
        val id = cols(2).trim
        val model = cols(3).trim
        val year = cols(4).trim
        val crowd = cols(5).trim
        val region = cols(6).trim
        val plots = cols(7).split("/")

        val arr = new Array[(String, String, String)](plots.length)

        for (i <- 0 until plots.length) {
          arr(i) = (id, model, plots(i))
        }

        arr

      }).map(line => {

      val id = line._1.trim
      val model = line._2.trim
      val plot = line._3.trim

      (plot + "\t" + model, id)
    })

    println("film_RDD:" + film_RDD.count)

    //TODO 获取自定义标签
    //    原始标签,模块,新标签
    //罪案剧,电视剧,P25-45
    val tag_RDD = sc.textFile("/user/hdfs/rsync/userdata/plot_type2tag.csv")
      .filter(!_.contains("原始标签"))
      .distinct
      .map(line => {
        val cols = line.split(",")
        val orgin_plot = cols(0).trim
        val model = cols(1).trim
        val tag = cols(2).trim
        (orgin_plot + "\t" + model, tag)
      })

    //TODO 注册film_plot表 id, tag
    film_RDD.join(tag_RDD).distinct.map(line => {
      val id = line._2._1
      val tag = line._2._2

      FilmPlot(id, tag)
    }).toDF.registerTempTable("film_plot")

    //TODO 空 查不出来？？？？？？？？？？？？？？
    sqlContext.sql("select * from film_plot").show()

    // TODO 获取直播workOrRest dim_hour dim_source,plot,durtaion 信息
    val liveRDD = sqlContext.sql(
      """
        SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag, sum(lp.fact_duration)
        FROM (SELECT tp.dim_sn, if(dayOfWeek(tp.date) in (6,7),'周末','工作日') AS workOrRest, tp.dim_hour,epg.pg,SUM(tp
        .fact_time_length) AS  fact_duration
        	FROM (SELECT *
        		FROM hr.tracker_live_fact_partition
        		WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                AND date <= '""" + analysisDate +
        """' ) tp
        JOIN hr.epg epg ON tp.date = epg.tv_date AND tp.dim_channel = epg.channel AND tp.dim_hour = epg.tv_hour AND tp.dim_min = epg.tv_min
        GROUP BY tp.dim_sn, tp.date, tp.dim_hour, epg.pg ) lp
        JOIN hr.epg_film ef ON lp.pg = ef.pg
        JOIN film_plot fp ON ef.id = fp.id
        JOIN cross_terminal ft ON lp.dim_sn = ft.sn
        GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag

        """.stripMargin)
      .map(line => {

        val terminal_num = line(0).toString

        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")

        val citylevel = line(2).toString

        val workOrRest = line(3).toString

        val dim_hour = line(4).toString

        val plot = line(5).toString

        val durtaion = line(6).toString.toDouble / 3600


        val dim_source = "直播"

        FamilySampleLivePlayDuration(
          terminal_num,
          cluster_name,
          citylevel,
          workOrRest,
          dim_hour,
          dim_source,
          plot,
          durtaion)


      })
    //
    //
    println(liveRDD.count)

    //    //
    //    //    //TODO 获取到剧点播 workOrRest dim_hour dim_source,plot,durtaion 信息
    val apkRDD = sqlContext.sql(
      """
        SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag, sum(lp
        .fact_duration)
        FROM (
        	SELECT dim_sn, if(dayOfWeek(date) in (6,7),'周末','工作日') AS workOrRest, dim_hour,dim_awcid,fact_duration
        		FROM hr.tracker_player_fact_partition
        		WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) AND date <= '""" + analysisDate +
        """'
        ) lp
        JOIN film_plot fp ON lp.dim_awcid = fp.id
        JOIN cross_terminal ft ON lp.dim_sn = ft.sn
        GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag
        """.stripMargin)
      .map(line => {

        val terminal_num = line(0).toString
        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
        val citylevel = line(2).toString

        val workOrRest = line(3).toString

        val dim_hour = line(4).toString
        val tag = line(5).toString
        val durtaion = line(6).toString.toDouble / 3600


        val dim_source = "点播"


        FamilySampleLivePlayDuration(
          terminal_num,
          cluster_name,
          citylevel,
          workOrRest,
          dim_hour,
          dim_source,
          tag,
          durtaion)

      })

    println(apkRDD.count)

    val resultDF = liveRDD.union(apkRDD).toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLivePlayTimeCnt", false,
      SaveMode.Overwrite)

  }
}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * 对标注样本做强规律梳理
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：标注样本、直播、应用都有的sn
  * 结果维度：家庭结构、城市级别 终端数
  */
object FamilySampleLiveAndPlayTerminal {

  case class FamilySampleLivePlayCnt2(
                                          terminal_num: String,
                                          cluster_name: String,
                                          citylevel: String
                                        )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilySampleLivePlayTimeCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-31", "31")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO 获取 标注样本、直播、到剧点播都有的sn

    //TODO 标注样本  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("family_terminal")

    //TODO 直播&&标注样本
    sqlContext.sql(
      """
        select ft.*
        from
        (
        select distinct(dim_sn) as dim_sn from hr.tracker_live_fact_partition
        where
        date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate + """'
        ) lp
        join family_terminal ft
        on lp.dim_sn = ft.sn
        """.stripMargin).registerTempTable("family_live")

    println(sqlContext.sql("select * from family_live").count())

    //TODO 到剧&&标注样本
   sqlContext.sql(
      """
        select ft.*
        from
        (
        select distinct(dim_sn) as dim_sn from hr.tracker_player_fact_partition
        where
        date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
        ) lp
        join family_terminal ft
        on lp.dim_sn = ft.sn
        """.stripMargin).registerTempTable("family_play")

    println(sqlContext.sql("select * from family_play").count())

    //TODO 直播&&到剧&&标注样本
    val resultDF = sqlContext.sql(
      "select count(distinct fl.sn) as terminal_num, fl.cluster_id,fl.citylevel from family_live fl join family_play fa on fl.sn=fa.sn group by fl.cluster_id,fl.citylevel  "
    ).map(line => {
      val terminal_num = line(0).toString
      val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
      val citylevel = line(2).toString

      FamilySampleLivePlayCnt2(
        terminal_num,
        cluster_name,
        citylevel
      )

    }).toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLivePlayTimeCnt2", false,
      SaveMode.Overwrite)

  }

}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求描述
  * 1、根据标注样本的5类人群做剧情类型的使用行为特征分析，细分维度如下：
  * 分城市级别、分工作日周末、分时间段、分直播点播，不同维度可交叉
  * 剧情类型共92个，具体见表 剧情类型标签库中的“新标签”一列
  * 2、sn选取规则：标注样本，（直播或者到剧（点播））有的sn
  * 3、数据期间：2017年3月
  * 详细需求说明见附件：标注样本强规律梳理-需求3
  * 此外，提供两个附件供使用：表“add1-带剧情”是第一次整理的带有剧情类型的到剧ID库。表“剧情类型标签库”中包含有新标签。
  *
  *
  * of：静态工厂方法。
    parse：静态工厂方法，关注于解析。
    get：获取某些东西的值。
    is：检查某些东西的是否是true。
    with：不可变的setter等价物。
    plus：加一些量到某个对象。
    minus：从某个对象减去一些量。
    to：转换到另一个类型。
    at：把这个对象与另一个对象组合起来，例如：date.atTime(time)
  *
  */
object FamilySampleLivePlotTimeCnt {

  case class FamilySampleLiveOrPlayDuration(
                                           terminal_num: String,
                                           cluster_name: String,
                                           citylevel: String,
                                           workOrRest: String,
                                           dim_hour: String,
                                           dim_source: String,
                                           tag: String,
                                           durtaion: Double,
                                           model: String)

  case class FilmPlot(
                       id: String,
                       tag: String,
                       model: String
                     )

  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")

    //    //TODO 获取 标注样本、直播和到剧(点播)都有的sn

    //TODO 标注样本 && 直播&& 到剧(点播)   cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("cross_terminal")

    sqlContext.sql("select * from cross_terminal").show


    //TODO 获取原始的Plot
    //绝命毒师第5季绝命毒师518136电视剧2012非少儿美剧罪案剧/悬疑剧/
    val film_RDD = sc.textFile("/user/hdfs/rsync/add_data/02.28-03.05.csv")
      .union(sc.textFile("/user/hdfs/rsync/add_data/1.30-2.27.csv"))
      .union(sc.textFile("/user/hdfs/rsync/add_data/20170329-two week1.csv"))
      .union(sc.textFile("/user/hdfs/rsync/add_data/20170414-two week.csv"))
      .union(sc.textFile("/user/hdfs/rsync/add_data/to01.17.txt"))
      .filter(!_.contains("原始名称"))
      .distinct
      .flatMap(line => {
        // ﻿原始名称标准名称ID模块年份人群产地剧情类型
        val cols = line.split("\\|")
        val original_name = cols(0).trim
        val standard_name = cols(1).trim
        val id = cols(2).trim
        val model = cols(3).trim
        val year = cols(4).trim
        val crowd = cols(5).trim
        val region = cols(6).trim
        val plots = cols(7).split("/")

        val arr = new Array[(String, String, String)](plots.length)

        for (i <- 0 until plots.length) {
          arr(i) = (id, model, plots(i))
        }

        arr

      }).map(line => {

      val id = line._1.trim
      val model = line._2.trim
      val plot = line._3.trim

      (plot + "\t" + model, id)
    })

    println("film_RDD:" + film_RDD.count)

    //TODO 获取自定义标签
    //    原始标签,模块,新标签
    //罪案剧,电视剧,P25-45
    val tag_RDD = sc.textFile("/user/hdfs/rsync/userdata/plot_type2tag.csv")
      .filter(!_.contains("原始标签"))
      .distinct
      .map(line => {
        val cols = line.split(",")
        val orgin_plot = cols(0).trim
        val model = cols(1).trim
        val tag = cols(2).trim
        (orgin_plot + "\t" + model, tag)
      })

    //TODO 注册film_plot表 id, tag
    film_RDD.join(tag_RDD).distinct.map(line => {
      val id = line._2._1
      val tag = line._2._2

      val model = line._1.split("\t")(1)

      FilmPlot(id, tag, model)
    }).toDF.registerTempTable("film_plot")

    sqlContext.sql("select * from film_plot").show()

    // TODO 获取直播workOrRest dim_hour dim_source,plot,durtaion 信息
    val liveRDD = sqlContext.sql(
      """
        SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag, sum(lp.fact_duration),fp.model
        FROM (SELECT tp.dim_sn, if(dayOfWeek(tp.date) in (6,7),'周末','工作日') AS workOrRest, tp.dim_hour,epg.pg,SUM(tp
        .fact_time_length) AS  fact_duration
        	FROM (SELECT *
        		FROM hr.tracker_live_fact_partition
        		WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +""") AND date <= '""" + analysisDate + """' ) tp
        JOIN hr.epg epg ON tp.date = epg.tv_date AND tp.dim_channel = epg.channel AND tp.dim_hour = epg.tv_hour AND tp.dim_min = epg.tv_min
        GROUP BY tp.dim_sn, tp.date, tp.dim_hour, epg.pg ) lp
        JOIN hr.epg_film ef ON lp.pg = ef.pg
        JOIN film_plot fp ON ef.id = fp.id
        JOIN cross_terminal ft ON lp.dim_sn = ft.sn
        GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag,fp.model

        """.stripMargin)
      .map(line => {

        val terminal_num = line(0).toString

        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")

        val citylevel = line(2).toString

        val workOrRest = line(3).toString

        val dim_hour = line(4).toString

        val plot = line(5).toString

        val durtaion = line(6).toString.toDouble / 3600

        val model = line(7).toString

        val dim_source = "直播"

        FamilySampleLiveOrPlayDuration(
          terminal_num,
          cluster_name,
          citylevel,
          workOrRest,
          dim_hour,
          dim_source,
          plot,
          durtaion, model)


      })
    //
    //
    println(liveRDD.count)

    //    //
    //    //    //TODO 获取到剧点播 workOrRest dim_hour dim_source,plot,durtaion 信息
//    val playRDD = sqlContext.sql(
//      """
//        SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag, sum(lp
//        .fact_duration),fp.model
//        FROM (
//        	SELECT dim_sn, if(dayOfWeek(date) in (6,7),'周末','工作日') AS workOrRest, dim_hour,dim_awcid,fact_duration
//        		FROM hr.tracker_player_fact_partition
//        		WHERE date > date_sub('""" + analysisDate +
//        """',""" + recentDaysNum +
//        """) AND date <= '""" + analysisDate +
//        """'
//        ) lp
//        JOIN film_plot fp ON lp.dim_awcid = fp.id
//        JOIN cross_terminal ft ON lp.dim_sn = ft.sn
//        GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.tag,fp.model
//        """.stripMargin)
//      .map(line => {
//
//        val terminal_num = line(0).toString
//        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
//        val citylevel = line(2).toString
//
//        val workOrRest = line(3).toString
//
//        val dim_hour = line(4).toString
//        val tag = line(5).toString
//        val durtaion = line(6).toString.toDouble / 3600
//
//        val model = line(7).toString
//
//        val dim_source = "点播"
//
//        FamilySampleLiveOrPlayDuration(
//          terminal_num,
//          cluster_name,
//          citylevel,
//          workOrRest,
//          dim_hour,
//          dim_source,
//          tag,
//          durtaion, model)
//
//      })
//
//    println(playRDD.count)

//    val resultDF = liveRDD.union(playRDD).toDF
    val resultDF = liveRDD.toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLivePlotTimeCnt", false,
      SaveMode.Overwrite)

  }
}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext


/**
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：标注样本、(直播|到剧(点播))的sn
  * 需求4
  * 家庭结构	城市级别	工作日/周末	时段	产地	使用时长
  *
  * 根据标注样本的5类人群做产地分布的特征分析，细分维度如下：
  * 分城市级别、分工作日周末、分时间段和产地分布，不同维度可交叉
  * 产地共有21类，具体见附件：产地分布
  * sn选取规则：标注样本，直播和到剧（点播）任意一个有的sn
  * 数据期间：2017年3月
  * 另需给出5类人群和城市级别交叉的终端数量
  */
object FamilySampleLiveRegionTimeCnt {

  case class FamilySampleLiveRegionDuration(
     terminal_num: String,
     cluster_name: String,
     citylevel: String,
     workOrRest: String,
     dim_hour: String,
     model:String,
     region: String,
     durtaion: Double)

  case class FilmPlot(
                       id: String,
                       tag: String
                     )

  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")

    //TODO 获取 标注样本sn

    //TODO 标注样本  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
        """
    ).registerTempTable("cross_terminal")

    sqlContext.sql("select * from cross_terminal").show


    // TODO 获取直播workOrRest dim_hour dim_source,plot,durtaion 信息
    val liveRDD = sqlContext.sql(
      """
        SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.model, fp.region, sum(lp
        .fact_duration)
        FROM (SELECT tp.dim_sn, if(dayOfWeek(tp.date) in (6,7),'周末','工作日') AS workOrRest, tp.dim_hour,epg.pg,SUM(tp
        .fact_time_length) AS  fact_duration
        	FROM (SELECT *
        		FROM hr.tracker_live_fact_partition
        		WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                AND date <= '""" + analysisDate +
        """' ) tp
        JOIN hr.epg epg ON tp.date = epg.tv_date AND tp.dim_channel = epg.channel AND tp.dim_hour = epg.tv_hour AND tp.dim_min = epg.tv_min
        GROUP BY tp.dim_sn, tp.date, tp.dim_hour, epg.pg ) lp
        JOIN hr.epg_film ef ON lp.pg = ef.pg
        JOIN hr.film_properties fp ON ef.id = fp.id
        JOIN cross_terminal ft ON lp.dim_sn = ft.sn
        GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, fp.model, fp.region

        """.stripMargin)
      .map(line => {

        val terminal_num = line(0).toString

        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")

        val citylevel = line(2).toString

        val workOrRest = line(3).toString

        val dim_hour = line(4).toString

        val model = line(5).toString

        val region = line(6).toString

        val durtaion = line(7).toString.toDouble / 3600


//        val dim_source = "直播"

        FamilySampleLiveRegionDuration(
          terminal_num,
          cluster_name,
          citylevel,
          workOrRest,
          dim_hour,
          model,
          region,
          durtaion)


      })
    println(liveRDD.count)

    // TODO 获取到剧点播 workOrRest dim_hour dim_source,plot,durtaion 信息
//    val apkRDD = sqlContext.sql(
//      """
//        SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, lp.dim_region, sum(lp
//        .fact_duration)
//        FROM (
//        	SELECT dim_sn, if(dayOfWeek(date) in (6,7),'周末','工作日') AS workOrRest, dim_hour,dim_awcid,fact_duration
//        		FROM hr.tracker_player_fact_partition
//        		WHERE date > date_sub('""" + analysisDate +
//        """',""" + recentDaysNum +
//        """) AND date <= '""" + analysisDate +
//        """'
//        ) lp
//        JOIN cross_terminal ft ON lp.dim_sn = ft.sn
//        GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.dim_hour, lp.dim_region
//        """.stripMargin)
//      .map(line => {
//
//        val terminal_num = line(0).toString
//        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
//        val citylevel = line(2).toString
//
//        val workOrRest = line(3).toString
//
//        val dim_hour = line(4).toString
//        val region = line(5).toString
//        val durtaion = line(6).toString.toDouble / 3600
//
//
//        val dim_source = "点播"
//
//
//        FamilySampleLiveRegionDuration(
//          terminal_num,
//          cluster_name,
//          citylevel,
//          workOrRest,
//          dim_hour,
//          dim_source,
//          region,
//          durtaion)
//
//      })
//
//    println(apkRDD.count) 只有七个不做
//
//    val resultDF = liveRDD.union(apkRDD).toDF

    val resultDF = liveRDD.toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLiveRegionTimeCnt", false,
      SaveMode.Overwrite)

  }
}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * 对标注样本做强规律梳理
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：标注样本、直播、应用都有的sn
  * 结果维度：家庭结构、城市级别 终端数 (直播/点播）
  */
object FamilySampleLiveTerminal {

  case class FamilySampleLiveTimeCnt2(
                                       terminal_num: String,
                                       cluster_name: String,
                                       citylevel: String

                                     )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilySampleLivePlayTimeCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-31", "31")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO 获取 标注样本sn

    //TODO 标注样本  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("family_terminal")

    //TODO 直播&&标注样本
    sqlContext.sql(
      """
        select ft.*
        from
        (
        select distinct(dim_sn) as dim_sn from hr.tracker_live_fact_partition
        where  date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
        ) lp
        join family_terminal ft
        on lp.dim_sn = ft.sn
        """.stripMargin).registerTempTable("family_live")

    println(sqlContext.sql("select * from family_live").distinct().count())

    //TODO 到剧&&标注样本
    //   sqlContext.sql(
    //      """
    //        select ft.*
    //        from
    //        (
    //        select distinct(dim_sn) as dim_sn from hr.tracker_player_fact_partition
    //        where
    //        date > date_sub('""" + analysisDate +
    //        """',""" + recentDaysNum +
    //        """) and date <= '""" + analysisDate +
    //        """'
    //        ) lp
    //        join family_terminal ft
    //        on lp.dim_sn = ft.sn
    //        """.stripMargin).registerTempTable("family_play")
    //
    //    println(sqlContext.sql("select * from family_play").distinct.count())

    //TODO 直播&&标注样本
    val resultDF = sqlContext.sql(
      "select count(distinct fl.sn) as terminal_num, fl.cluster_id,fl.citylevel from family_live fl  group by fl.cluster_id,fl.citylevel  "
    ).map(line => {
      val terminal_num = line(0).toString
      val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
      val citylevel = line(2).toString

      FamilySampleLiveTimeCnt2(
        terminal_num,
        cluster_name,
        citylevel
      )

    }).toDF


    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleLiveTerminal", false,
      SaveMode.Overwrite)

  }

}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * 对标注样本做强规律梳理
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：标注样本、直播、应用都有的sn
  * 结果维度：家庭结构、城市级别 终端数 (直播/点播）
  */
object FamilySampleOcTerminal {

  case class FamilySampleOcTerminalCnt(
                                       terminal_num: String,
                                       cluster_name: String,
                                       citylevel: String

                                     )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilySampleOcTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-31", "31")
    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO 获取 标注样本sn

    //TODO 标注样本  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("family_terminal")

    //TODO 直播&&标注样本
    sqlContext.sql(
      """
        select ft.*
        from
        (
        select distinct(sn) as sn from hr.tracker_oc_fact_partition
        where  date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """) and date <= '""" + analysisDate +
        """'
        ) lp
        join family_terminal ft
        on lp.sn = ft.sn
        """.stripMargin).registerTempTable("family_oc")

    println(sqlContext.sql("select * from family_oc").distinct().count())
    //TODO 到剧&&标注样本
    //   sqlContext.sql(
    //      """
    //        select ft.*
    //        from
    //        (
    //        select distinct(sn) as sn from hr.tracker_player_fact_partition
    //        where
    //        date > date_sub('""" + analysisDate +
    //        """',""" + recentDaysNum +
    //        """) and date <= '""" + analysisDate +
    //        """'
    //        ) lp
    //        join family_terminal ft
    //        on lp.sn = ft.sn
    //        """.stripMargin).registerTempTable("family_play")
    //
    //    println(sqlContext.sql("select * from family_play").distinct.count())


    //TODO 直播&&标注样本
    val resultDF = sqlContext.sql(
      "select count(distinct fl.sn) as terminal_num, fl.cluster_id,fl.citylevel from family_oc fl  group by fl.cluster_id,fl.citylevel  "
    ).map(line => {
      val terminal_num = line(0).toString
      val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")
      val citylevel = line(2).toString

      FamilySampleOcTerminalCnt(
        terminal_num,
        cluster_name,
        citylevel
      )

    }).toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleOcTerminal", false,
      SaveMode.Overwrite)

  }

}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求5和6：
  * 时间：2017年3月
  * 时长指标单位：小时
  * 终端选取：有开关机的标注样本
  * 家庭结构	城市级别	工作日/周末	时段	开机时长	开机次数

  */
object FamilySampleOcTimeCnt {

  case class FamilySampleOcDurationCnt(
                                        terminal_num: String,
                                        cluster_name: String,
                                        citylevel: String,
                                        workOrRest: String,
                                        dim_hour: String,
                                        duration: Double,
                                        cnt: Double)

  case class FilmPlot(
                       id: String,
                       tag: String
                     )

  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleOcTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

   //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")

    //TODO 获取 标注样本sn

    //TODO 标注样本  cluster_id citylevel 信息
    sqlContext.sql(
      """
      select t.sn,uc.cluster_id,t.citylevel  from
       (
        select * from hr.user_family_compose
        union all
        select * from hr.user_family_compose2
       ) uf
       join hr.user_family_compose_cluster uc
       on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old)
       join
        hr.terminal t
        on uf.sn = t.sn
      """
    ).registerTempTable("cross_terminal")

    sqlContext.sql("select * from cross_terminal").show


    // TODO 获取开关机workOrRest dim_hour dim_source,durtaion,cnt 信息
    val liveRDD = sqlContext.sql(
      """
          SELECT count(distinct ft.sn),ft.cluster_id,ft.citylevel, lp.workOrRest, lp.power_on_time, sum(lp
          .power_on_length),sum(lp.cnt)
          FROM (
            SELECT sn, if(dayOfWeek(date) in (6,7),'周末','工作日') AS workOrRest, power_on_time,power_on_length,cnt
              FROM hr.tracker_oc_fact_partition
              WHERE date > date_sub('""" + analysisDate +"""',""" + recentDaysNum +""") AND date <= '""" +
        analysisDate + """') lp
          JOIN cross_terminal ft ON lp.sn = ft.sn
          GROUP BY ft.cluster_id,ft.citylevel, lp.workOrRest, lp.power_on_time
        """.stripMargin)
      .map(line => {

        val terminal_num = line(0).toString

        val cluster_name = UserVectorConstant.FAMILY_COMPOSE_CLUSTER_MAP.get(line(1).toString.toInt).getOrElse("其他")

        val citylevel = line(2).toString

        val workOrRest = line(3).toString

        val dim_hour = line(4).toString

        val durtaion = line(5).toString.toDouble / 60

        val cnt = line(6).toString.toDouble

        FamilySampleOcDurationCnt(
          terminal_num,
          cluster_name,
          citylevel,
          workOrRest,
          dim_hour,
          durtaion,
          cnt)


      })
    println(liveRDD.count)


    val resultDF = liveRDD.toDF

    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "FamilySampleOcTimeCnt", false,
      SaveMode.Overwrite)

  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalBrandCnt.TerminalBrandDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalBrandDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalBrandDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,brand from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val brand_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
          .TERMINAL_BRAND_ARR)

      (sn + "\t" + stat_date + "\t" + period, brand_map)

    })

    val TerminalBrandDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val brand_map = line._2._2

      val resultArr = new Array[TerminalBrandDistCnt](brand_map.size)

      val brand_arr = brand_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalBrandDistCnt(
          sn,
          stat_date,
          period,
          brand_arr(i),
          brand_map.get(brand_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalBrandDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalBrandDistCntDF.registerTempTable("TerminalBrandDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.brand,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,brand,sum(cnt) as cnt  from TerminalBrandDistCnt group by stat_date,period,cluster_id,brand
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalBrandDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)

    println(resultDF.count())

    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_brand_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalBrandCnt.TerminalBrandDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalBrandDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalBrandDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,brand from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val brand_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
          .TERMINAL_BRAND_ARR)

      (sn + "\t" + stat_date + "\t" + period, brand_map)

    })

    val TerminalBrandDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val brand_map = line._2._2

      val resultArr = new Array[TerminalBrandDistCnt](brand_map.size)

      val brand_arr = brand_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalBrandDistCnt(
          sn,
          stat_date,
          period,
          brand_arr(i),
          brand_map.get(brand_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalBrandDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalBrandDistCntDF.registerTempTable("TerminalBrandDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.brand,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,brand,sum(cnt) as cnt  from TerminalBrandDistCnt group by stat_date,period,cluster_id,brand
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalBrandDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)

    println(resultDF.count())

    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_brand_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalPriceCnt.TerminalPriceDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalPriceDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalPriceDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,price from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val price_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
            .TV_PRICE_ARR)

      (sn + "\t" + stat_date + "\t" + period, price_map)

    })

    val TerminalPriceDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val price_map = line._2._2

      val resultArr = new Array[TerminalPriceDistCnt](price_map.size)

      val price_arr = price_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalPriceDistCnt(
          sn,
          stat_date,
          period,
          price_arr(i),
          price_map.get(price_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalPriceDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalPriceDistCntDF.registerTempTable("TerminalPriceDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.price,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,price,sum(cnt) as cnt  from TerminalPriceDistCnt group by stat_date,period,cluster_id,price
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalPriceDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_price_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalPriceCnt.TerminalPriceDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalPriceDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalPriceDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,price from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val price_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
            .TV_PRICE_ARR)

      (sn + "\t" + stat_date + "\t" + period, price_map)

    })

    val TerminalPriceDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val price_map = line._2._2

      val resultArr = new Array[TerminalPriceDistCnt](price_map.size)

      val price_arr = price_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalPriceDistCnt(
          sn,
          stat_date,
          period,
          price_arr(i),
          price_map.get(price_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalPriceDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalPriceDistCntDF.registerTempTable("TerminalPriceDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.price,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,price,sum(cnt) as cnt  from TerminalPriceDistCnt group by stat_date,period,cluster_id,price
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalPriceDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_price_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalProvinceCnt.TerminalProvinceDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalProvinceDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalProvinceDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,province from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val province_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
            .TERMINAL_PROVINCE_ARR)

      (sn + "\t" + stat_date + "\t" + period, province_map)

    })

    val TerminalProvinceDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val province_map = line._2._2

      val resultArr = new Array[TerminalProvinceDistCnt](province_map.size)

      val province_arr = province_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalProvinceDistCnt(
          sn,
          stat_date,
          period,
          province_arr(i),
          province_map.get(province_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalProvinceDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalProvinceDistCntDF.registerTempTable("TerminalProvinceDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.province,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,province,sum(cnt) as cnt  from TerminalProvinceDistCnt group by stat_date,period,cluster_id,province
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalProvinceDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_province_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalProvinceCnt.TerminalProvinceDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalProvinceDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalProvinceDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,province from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val province_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
            .TERMINAL_PROVINCE_ARR)

      (sn + "\t" + stat_date + "\t" + period, province_map)

    })

    val TerminalProvinceDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val province_map = line._2._2

      val resultArr = new Array[TerminalProvinceDistCnt](province_map.size)

      val province_arr = province_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalProvinceDistCnt(
          sn,
          stat_date,
          period,
          province_arr(i),
          province_map.get(province_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalProvinceDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalProvinceDistCntDF.registerTempTable("TerminalProvinceDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.province,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,province,sum(cnt) as cnt  from TerminalProvinceDistCnt group by stat_date,period,cluster_id,province
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalProvinceDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_province_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalSizeCnt.TerminalSizeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalSizeDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalSizeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,size from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val size_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
              .TV_SIZE_ARR)

      (sn + "\t" + stat_date + "\t" + period, size_map)

    })

    val TerminalSizeDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val size_map = line._2._2

      val resultArr = new Array[TerminalSizeDistCnt](size_map.size)

      val size_arr = size_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalSizeDistCnt(
          sn,
          stat_date,
          period,
          size_arr(i),
          size_map.get(size_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalSizeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalSizeDistCntDF.registerTempTable("TerminalSizeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.size,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,size,sum(cnt) as cnt  from TerminalSizeDistCnt group by stat_date,period,cluster_id,size
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalSizeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_size_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.TerminalSizeCnt.TerminalSizeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyTerminalSizeDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyTerminalSizeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 设备特征-品牌分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,size from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val size_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
              .TV_SIZE_ARR)

      (sn + "\t" + stat_date + "\t" + period, size_map)

    })

    val TerminalSizeDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val size_map = line._2._2

      val resultArr = new Array[TerminalSizeDistCnt](size_map.size)

      val size_arr = size_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalSizeDistCnt(
          sn,
          stat_date,
          period,
          size_arr(i),
          size_map.get(size_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalSizeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalSizeDistCntDF.registerTempTable("TerminalSizeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.size,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,size,sum(cnt) as cnt  from TerminalSizeDistCnt group by stat_date,period,cluster_id,size
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalSizeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_size_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.mllib

import com.avcdata.spark.job.util.HdfsUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object FamilyVectorDataExport {

//  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyVectorDataExport")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    val sqlContext = new HiveContext(sc)

    val familyRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0).toString
      val member_num = line(1).toString
      val has_child = line(2).toString
      val has_old = line(3).toString
      val family_cluster_id = line(4).toString

      (sn, member_num + "\t" + family_cluster_id + "\t" + has_child + "\t" + has_old)
    })


    val clusterResultRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-UserVectorAllETL")
      .map(line => {
        val cols = line.split("\t")
        //        val cluster_id = cols(14)

        var i = 0
        val sn = cols(i)
        i = i + 1
        val stat_date = cols(i)
        i = i + 1
        val period = cols(i)
        i = i + 1

        val brand = cols(i)
        i = i + 1
        val province = cols(i)
        i = i + 1
        val price = cols(i)
        i = i + 1
        val size = cols(i)
        i = i + 1

        val workday_oc_dist = cols(i)
        i = i + 1
        val restday_oc_dist = cols(i)
        i = i + 1
        val workday_channel_dist = cols(i)
        i = i + 1
        val restday_channel_dist = cols(i)
        i = i + 1
        val pg_subject_dist = cols(i)
        i = i + 1
        val pg_year_dist = cols(i)
        i = i + 1
        val pg_region_dist = cols(i)

        (sn, cols)

      })





    val resultRDD = familyRDD.join(clusterResultRDD).map(line => {
      val sn = line._1

      val leftCols = line._2._1.split("\t")
      val member_num = leftCols(0)
      val family_cluster_id = leftCols(1)

      //      has_child+"\t"+has_old
      val has_child = leftCols(2)
      val has_old = leftCols(3)


      val vector_cols = line._2._2

      val brand = vector_cols(3)
      val province = vector_cols(4)
      val price = vector_cols(5)
      val size = vector_cols(6)
      val workday_oc_dist = vector_cols(7)
      val restday_oc_dist = vector_cols(8)
      val workday_channel_dist = vector_cols(9)
      val restday_channel_dist = vector_cols(10)
      val pg_subject_dist = vector_cols(11)
      val pg_year_dist = vector_cols(12)
      val pg_region_dist = vector_cols(13)
      //      val cluster_id = vector_cols(14)

      sn + "\t" + brand + "\t" + province + "\t" + price + "\t" + size + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist + "\t" + member_num + "\t" + family_cluster_id + "\t" + has_child + "\t" +
        has_old
    })

    val outPutPath = "/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-FamilyVectorDataExport"

    //TODO 删除旧目录
    HdfsUtils.rm(outPutPath, true)

    resultRDD.repartition(1)
      .saveAsTextFile(outPutPath)


  }


}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhWorkdayChannelDistCnt.WorkdayChannelDistCnt
import com.avcdata.spark.job.etl.stat.user.BhWorkdayOcTimeDistCnt.WorkdayOcTimeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyWorkdayChannelTimeDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyWorkdayChannelTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_channel_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_channel_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_CHANNEL_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_channel_dist_map)

    })

    val workdayChannelDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_channel_dist_map = line._2._2

      val resultArr = new Array[WorkdayChannelDistCnt](workday_channel_dist_map.size)

      val channel_arr = workday_channel_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayChannelDistCnt(
          sn,
          stat_date,
          period,
          channel_arr(i),
          workday_channel_dist_map.get(channel_arr(i)).get.toDouble,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayChannelDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayChannelDistCntDF.registerTempTable("workdayChannelDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.channel,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,channel,sum(cnt) as cnt  from workdayChannelDistCnt group by stat_date,period,cluster_id,channel
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayChannelDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_workday_channel_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhWorkdayChannelDistCnt.WorkdayChannelDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyWorkdayChannelTimeDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyWorkdayChannelTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_channel_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_channel_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_CHANNEL_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_channel_dist_map)

    })

    val workdayChannelDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_channel_dist_map = line._2._2

      val resultArr = new Array[WorkdayChannelDistCnt](workday_channel_dist_map.size)

      val channel_arr = workday_channel_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayChannelDistCnt(
          sn,
          stat_date,
          period,
          channel_arr(i),
          workday_channel_dist_map.get(channel_arr(i)).get.toDouble,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayChannelDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayChannelDistCntDF.registerTempTable("workdayChannelDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.channel,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,channel,sum(cnt) as cnt  from workdayChannelDistCnt group by stat_date,period,cluster_id,channel
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayChannelDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_workday_channel_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhWorkdayOcTimeDistCnt.WorkdayOcTimeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyWorkdayOcTimeDistCnt {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyWorkdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose2 uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 统计-工作日开机时间分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_oc_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_OC_HOUR_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_oc_dist_map)

    })

    val workdayOcTimeDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_oc_dist_map = line._2._2

      val resultArr = new Array[WorkdayOcTimeDistCnt](workday_oc_dist_map.size)

      val hour_arr = workday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          workday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayOcTimeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayOcTimeDistCntDF.registerTempTable("workdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_workday_oc_time_dist_cnt2", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.family

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.etl.stat.user.BhWorkdayOcTimeDistCnt.WorkdayOcTimeDistCnt
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 家庭结构样本数据
  */
object FamilyWorkdayOcTimeDistCnt01 {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("FamilyWorkdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val snCidRDD = sqlContext.sql("select uf.sn,uf.member_num,uf.has_child,uf.has_old,uc.cluster_id from hr.user_family_compose uf join hr.user_family_compose_cluster uc on (uf.member_num = uc.member_num and uf.has_child = uc.has_child and uf.has_old = uc.has_old) ").map(line => {
      val sn = line(0)
      val member_num = line(1)
      val has_child = line(2)
      val has_old = line(3)
      val cluster_id = line(4).toString
      val stat_date = analysisDate
      val period = recentDaysNum
      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //TODO 统计-工作日开机时间分布-分类数量（频次和)和分类占比
    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_oc_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_OC_HOUR_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_oc_dist_map)

    })

    val workdayOcTimeDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_oc_dist_map = line._2._2

      val resultArr = new Array[WorkdayOcTimeDistCnt](workday_oc_dist_map.size)

      val hour_arr = workday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          workday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayOcTimeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayOcTimeDistCntDF.registerTempTable("workdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "family_stat_workday_oc_time_dist_cnt", false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.test

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by avc on 2017/2/28.
  */
object FilmDataLoadJob {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }


  def run(sc: SparkContext, s: String) = {

    val initRDD = sc.textFile("E:\\aowei\\tracker-job\\doc\\data\\add1.csv")

    println("total:" + initRDD.count())

    val rdd = initRDD.map(_.split(",")).filter(line => {
      println(line(3))
      line(3).contains("电视剧")
    })
    //16732

    //    val rdd = initRDD.filter(_.contains("电视剧"))//16979

    val cnt = rdd.count()
//    rdd.foreach(println(_))
    println("电视剧：" + cnt)

  }


}
package com.avcdata.etl.common.util

import com.avcdata.etl.common.compiler.Compiler

/**
  * 函数编译执行
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/30 11:01
  */
object FunctionCompiler
{
  val functionRegex = """!\{\{\{(.+)\}\}\}""".r

  /**
    * 执行代码行中的函数
    * !{{{code}}}
    * 返回字符串结果
    *
    * @param srcLine 源代码行
    * @return 执行替换后的结果
    */
  def execute(srcLine: String): String =
  {
    var tmpLine = srcLine
    functionRegex.findAllMatchIn(srcLine).foreach(m =>
    {
      val allFunc = m.group(0)
      val exeFunc = m.group(1)

      val script =
        s"""import com.avcdata.etl.common.udf.DateTimeUDF._
            |import com.avcdata.etl.common.udf.AvailableTimePeriodUDF._
            |
            |$exeFunc
      """.stripMargin

      //TODO How to optimize the compiler?
      val result = new Compiler(None).eval[String](script)

      tmpLine = tmpLine.replaceAllLiterally(allFunc, result)
    })

    tmpLine
  }
}
package com.avcdata.etl.common.util

import com.avcdata.etl.common.compiler.Compiler

/**
  * 函数编译执行
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/30 11:01
  */
object FunctionCompiler
{
  val functionRegex = """!\{\{\{(.+)\}\}\}""".r

  /**
    * 执行代码行中的函数
    * !{{{code}}}
    * 返回字符串结果
    *
    * @param srcLine 源代码行
    * @return 执行替换后的结果
    */
  def execute(srcLine: String): String =
  {
    var tmpLine = srcLine
    functionRegex.findAllMatchIn(srcLine).foreach(m =>
    {
      val allFunc = m.group(0)
      val exeFunc = m.group(1)

      val script =
        s"""import com.avcdata.etl.common.udf.DateTimeUDF._
            |import com.avcdata.etl.common.udf.AvailableTimePeriodUDF._
            |
            |$exeFunc
      """.stripMargin

      //TODO How to optimize the compiler?
      val result = new Compiler(None).eval[String](script)

      tmpLine = tmpLine.replaceAllLiterally(allFunc, result)
    })

    tmpLine
  }
}
package com.avcdata.spark.job.mock

import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.mock.GenerateUserTerminalDataHelper.FamilyMember
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.Random


/**
  * member_struct
  * 家庭结构
  * ("单身"，2700)，
  * ("二人世界"，3500)，
  * ("夫妻和小孩"，5000)，
  * ("夫妻和老人"，2000)，
  * ("夫妻、老人和小孩"，1900)，

  * member_id
  * 成员ID
  * 1-6

  * time_range
  * 观看时间段
  * ("00:00-06:00",961)
  * ("06:00-08:00",476)
  * ("08:00-12:00",1633)
  * ("12:00-14:00",1307)
  * ("14:00-17:00",1537)
  * ("17:00-19:00",1033)
  * ("19:00-22:00",2054)
  * ("22:00-24:00",1449)

  * age_range
  * 年龄段
  * ("儿童",1360)
  * ("少年",1076)
  * ("青年",1218)
  * ("中年",5020)
  * ("老年",1326)

  */
object GenerateUserTerminalData {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("GenerateUserTerminalData")
    val sc = new SparkContext(conf)
    run(sc)

    sc.stop()
  }


  case class UserTerminal(
                           sn: String,
                           member_struct: String,
                           member_id: Int,
                           time_range: String,
                           age_range: String,
                           sex: String,
                           brand: String,
                           province: String,
                           city: String,
                           citylevel: String,
                           area: String,
                           price: String,
                           size: String,
                           model: String,
                           license: String)

  def run(sc: SparkContext) = {

    //TODO 目的：生成一个userterminal

    //TODO  从user_vector_terminal 中取terminal相关属性
    val sqlContext = new HiveContext(sc)

    import sqlContext.implicits._

    val resultRDD =
    //      sqlContext.sql("SELECT t.sn, t.brand, t.area, t.province, t.city , t.citylevel, tsp.size, t.model, t.license, tsp.price FROM hr.user_vector_terminal uvt JOIN hr.terminal t ON uvt.sn = t.sn JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand AND t.model = tsp.model ")

      sqlContext.sql("SELECT t.sn, t.brand, t.area, t.province, t.city , t.citylevel, tsp.size, t.model, t.license, tsp.price FROM hr.sample_terminal_three t Left JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand AND t.model = tsp.model ")
        .rdd
        .flatMap(line => {
          val sn = line(0).toString
          val brand = line(1).toString
          val area = line(2).toString
          val province = line(3).toString
          val city = line(4).toString
          val citylevel = line(5).toString

          var size = ""
          if(line(6)!=null){
            size = line(6).toString
          }

          val model = line(7).toString
          val license = line(8).toString

          var price = ""
          if(line(9)!=null){
            price = line(9).toString
          }

          //        val sn = "00:0c:e7:06:00:00"
          //        val brand = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TERMINAL_BRAND_ARR)
          //        val province = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TERMINAL_PROVINCE_ARR)
          //        val city = "北京市"
          //        val citylevel = "特级城市"
          //        val area = "华北"
          //        val price = "2000"
          //        val size = "30"
          //        val model = "TZKM"
          //        val license = "youku"

          //TODO 结果
          val userTerminalArr = scala.collection.mutable.ArrayBuffer[UserTerminal]()

          //TODO 家庭结构
          val member_struct =  RandomUtil.getRandomByWeight(UserVectorConstant.FAMILY_COMPOSE_WEIGHT)

          //TODO 家庭成员
          val fms = GenerateUserTerminalDataHelper.getFmsByMemberStruct(member_struct)

          //TODO 时间段
          for (ele <- 0 until UserVectorConstant.TIME_RANGE.length) {
            val time_range = RandomUtil.getRandomByWeight(UserVectorConstant.TIME_RANGE_WEIGHT)
            //          println("time_range:" + time_range)

            for (fm <- getWatchTVFamilyMembers(fms)) {
              val userTerminal = UserTerminal(
                sn,
                member_struct,
                fm.member_id,
                time_range,
                fm.age_range,
                fm.sex,
                brand,
                province,
                city,
                citylevel,
                area,
                price,
                size,
                model,
                license
              )
              userTerminalArr.+=(userTerminal)
              //            println(userTerminal)
            }
          }
          userTerminalArr

        })

    println(resultRDD.count)

    //    resultRDD.toDF().write.mode(SaveMode.Append).insertInto("")
    //    resultRDD.saveAsTextFile("/user/hdfs/rsync/userterminal")

    resultRDD.toDF.distinct().write.mode(SaveMode.Overwrite).saveAsTable("hr.user_terminal")

    //    sqlContext.sql("use hr")
    //
    //    resultRDD.toDF().registerTempTable("tmp_table")
    //
    //    sqlContext.sql("create table hr.user_terminal as select * from tmp_table")
  }


  /**
    * 随机获取某个家庭某时段观看的家庭成员的集合
    *
    * @param arr
    * @return
    */
  def getWatchTVFamilyMembers(arr: Array[FamilyMember]): Array[FamilyMember] = {
    val resultArr = scala.collection.mutable.ArrayBuffer[FamilyMember]()

    val wt_fm_num_max = arr.length
    for (i <- 1 to Random.nextInt(wt_fm_num_max) + 1) {
      resultArr.+=(arr(i - 1))
    }

    resultArr.toArray
  }


}


//for (i <- 1 to member_id_max) {
//  member_id_arr.+=(i)
//}
//
//  for (ele <- member_id_arr) {
//
//  println("member id:" + ele)
//
//  val age_range = GenerateUserTerminalDataHelper.getEleRandomByArr(Array("青年", "中年", "老年"))
//
//  println("age_range:" + age_range)
//
//  val sex = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.SEX)
//
//  println("sex:" + sex)
//
//}

package com.avcdata.spark.job.mock

import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.mock.GenerateUserTerminalDataHelper.FamilyMember
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.Random

object GenerateUserTerminalData20170925 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("GenerateUserTerminalData")
    val sc = new SparkContext(conf)
    run(sc)

    sc.stop()


  }


  case class UserTerminal(
                           sn: String,
                           member_struct: String,
                           member_id: Int,
                           time_range: String,
                           age_range: String,
                           sex: String,
                           brand: String,
                           province: String,
                           city: String,
                           citylevel: String,
                           area: String,
                           price: String,
                           size: String,
                           model: String,
                           license: String

                         )

  def run(sc: SparkContext) = {

    //TODO 目的：生成一个userterminal

    //TODO  从user_vector_terminal 中取terminal相关属性
    val sqlContext = new HiveContext(sc)

    import sqlContext.implicits._

    val resultRDD =
//      sqlContext.sql("SELECT t.sn, t.brand, t.area, t.province, t.city , t.citylevel, tsp.size, t.model, t.license, tsp.price FROM hr.user_vector_terminal uvt JOIN hr.terminal t ON uvt.sn = t.sn JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand AND t.model = tsp.model ")

      sqlContext.sql("SELECT t.sn, t.brand, t.area, t.province, t.city , t.citylevel, tsp.size, t.model, t.license, tsp.price FROM hr.sample_terminal_three t Left JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand AND t.model = tsp.model ")
      .rdd
      .flatMap(line => {
        val sn = line(0).toString
        val brand = line(1).toString
        val area = line(2).toString
        val province = line(3).toString
        val city = line(4).toString
        val citylevel = line(5).toString
        val size = line(6).toString
        val model = line(7).toString
        val license = line(8).toString
        val price = line(9).toString


        //        val sn = "00:0c:e7:06:00:00"
        //        val brand = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TERMINAL_BRAND_ARR)
        //        val province = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TERMINAL_PROVINCE_ARR)
        //        val city = "北京市"
        //        val citylevel = "特级城市"
        //        val area = "华北"
        //        val price = "2000"
        //        val size = "30"
        //        val model = "TZKM"
        //        val license = "youku"

        //TODO 结果
        val userTerminalArr = scala.collection.mutable.ArrayBuffer[UserTerminal]()

        //TODO 家庭结构
        val member_struct_arr = UserVectorConstant.FAMILY_COMPOSE_MAX_NUM_MAP.keySet.toArray
        val member_struct = GenerateUserTerminalDataHelper.getEleRandomByArr(member_struct_arr)

        //TODO 家庭成员
        val fms = GenerateUserTerminalDataHelper.getFmsByMemberStruct(member_struct)

        //TODO 时间段
        for (ele <- 0 until UserVectorConstant.TIME_RANGE.length) {
          val time_range = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TIME_RANGE)
          //          println("time_range:" + time_range)

          for (fm <- getWatchTVFamilyMembers(fms)) {
            val userTerminal = UserTerminal(
              sn,
              member_struct,
              fm.member_id,
              time_range,
              fm.age_range,
              fm.sex,
              brand,
              province,
              city,
              citylevel,
              area,
              price,
              size,
              model,
              license
            )
            userTerminalArr.+=(userTerminal)
            //            println(userTerminal)
          }
        }
        userTerminalArr

      })

    println(resultRDD.count)

    //    resultRDD.toDF().write.mode(SaveMode.Append).insertInto("")
    //    resultRDD.saveAsTextFile("/user/hdfs/rsync/userterminal")

        resultRDD.toDF.distinct().write.mode(SaveMode.Overwrite).saveAsTable("hr.user_terminal")

//    sqlContext.sql("use hr")
//
//    resultRDD.toDF().registerTempTable("tmp_table")
//
//    sqlContext.sql("create table hr.user_terminal as select * from tmp_table")
  }


  /**
    * 随机获取某个家庭某时段观看的家庭成员的集合
    *
    * @param arr
    * @return
    */
  def getWatchTVFamilyMembers(arr: Array[FamilyMember]): Array[FamilyMember] = {
    val resultArr = scala.collection.mutable.ArrayBuffer[FamilyMember]()

    val wt_fm_num_max = arr.length
    for (i <- 1 to Random.nextInt(wt_fm_num_max) + 1) {
      resultArr.+=(arr(i - 1))
    }

    resultArr.toArray
  }


}


//for (i <- 1 to member_id_max) {
//  member_id_arr.+=(i)
//}
//
//  for (ele <- member_id_arr) {
//
//  println("member id:" + ele)
//
//  val age_range = GenerateUserTerminalDataHelper.getEleRandomByArr(Array("青年", "中年", "老年"))
//
//  println("age_range:" + age_range)
//
//  val sex = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.SEX)
//
//  println("sex:" + sex)
//
//}

package com.avcdata.spark.job.mock

import com.avcdata.spark.job.etl.common.UserVectorConstant

import scala.util.Random

object GenerateUserTerminalDataHelper {

  case class FamilyMember(member_id: Int, age_range: String, sex: String)


  //TODO 枚举法获取家庭成员属性
  def getFmsByMemberStruct(member_struct: String) = {

    val member_id_arr = scala.collection.mutable.ArrayBuffer[Int]()

    val max_memeber_num = UserVectorConstant.FAMILY_COMPOSE_MAX_NUM_MAP.get(member_struct).getOrElse(-1)
    val member_id_max = Random.nextInt(max_memeber_num) + 1

    var fms = member_struct match {
      case "单身" => getFMsOfSingle
      case "二人世界" => getFMsOfCouple
      case "夫妻和小孩" => getFMsOfCoupleAndChildren
      case "夫妻和老人" => getFMsOfCoupleAndOld
      case "夫妻、老人和小孩" => getFMsOfCoupleAndOldAndChildren
    }

    fms
  }


  //单身
  def getFMsOfSingle(): Array[FamilyMember] = {
    Array(FamilyMember(1, RandomUtil.getRandomByWeight(UserVectorConstant.AGE_RANGE_WEIGHT), RandomUtil.getRandomByWeight(UserVectorConstant.SEX_WEIGHT)))
  }

  //二人世界
  def getFMsOfCouple(): Array[FamilyMember] = {

    Array(
      FamilyMember(1, RandomUtil.getRandomByWeight(UserVectorConstant.AGE_RANGE_WEIGHT), "男"),
      FamilyMember(2, RandomUtil.getRandomByWeight(UserVectorConstant.AGE_RANGE_WEIGHT), "女")

    )
  }


  //夫妻和小孩
  def getFMsOfCoupleAndChildren(): Array[FamilyMember] = {

    //    val possibleArr = Array()

    val possibleArr = Array[Array[FamilyMember]](

      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "女")

      ),
      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "男")
      ),

      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男")
      )
      ,
      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "女")
      )
      ,

      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "女")
      ),
      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "男")

      ),
      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男")
      )
      ,
      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "女")

      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "男"),
        FamilyMember(4, "青年", "女")

      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "男"),
        FamilyMember(4, "青年", "男")

      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "男")
      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "女")
      )

    )

    getEleRandomByArr(possibleArr)
  }


  //夫妻和老人
  def getFMsOfCoupleAndOld(): Array[FamilyMember] = {


    val possibleArr = Array[Array[FamilyMember]](

      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "老年", "男"),
        FamilyMember(4, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "老年", "男"),
        FamilyMember(4, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "老年", "男")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "老年", "女")
      )
    )

    getEleRandomByArr(possibleArr)

  }


  //夫妻、老人和小孩
  def getFMsOfCoupleAndOldAndChildren(): Array[FamilyMember] = {

    val possibleArr = Array[Array[FamilyMember]](

      Array[FamilyMember](

        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "女"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "男"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "女"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "女"),
        FamilyMember(5, "老年", "男")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "老年", "男")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "女"),
        FamilyMember(4, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "女"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "男"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "女"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "女"),
        FamilyMember(5, "老年", "男")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "男"),
        FamilyMember(5, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "老年", "男")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "女"),
        FamilyMember(4, "老年", "女")
      )
    )

    getEleRandomByArr(possibleArr)

  }


  def getEleRandomByArr(arr: Array[String]): String = {
    arr(Random.nextInt(arr.length))
  }

  def getEleRandomByArr(arr: Array[Array[FamilyMember]]): Array[FamilyMember] = {
    arr(Random.nextInt(arr.length))
  }

}
package com.avcdata.spark.job.mock

import com.avcdata.spark.job.etl.common.UserVectorConstant

import scala.util.Random

object GenerateUserTerminalDataHelper20170925 {

  case class FamilyMember(member_id: Int, age_range: String, sex: String)


  //TODO 枚举法获取家庭成员属性
  def getFmsByMemberStruct(member_struct: String) = {

    val member_id_arr = scala.collection.mutable.ArrayBuffer[Int]()

    val max_memeber_num = UserVectorConstant.FAMILY_COMPOSE_MAX_NUM_MAP.get(member_struct).getOrElse(-1)
    val member_id_max = Random.nextInt(max_memeber_num) + 1

    var fms = member_struct match {
      case "单身" => getFMsOfSingle
      case "二人世界" => getFMsOfCouple
      case "夫妻和小孩" => getFMsOfCoupleAndChildren
      case "夫妻和老人" => getFMsOfCoupleAndOld
      case "夫妻、老人和小孩" => getFMsOfCoupleAndOldAndChildren
    }

    fms
  }


  //单身
  def getFMsOfSingle(): Array[FamilyMember] = {
    Array(FamilyMember(1, getEleRandomByArr(Array("青年", "中年", "老年")), getEleRandomByArr(UserVectorConstant.SEX)))
  }

  //二人世界
  def getFMsOfCouple(): Array[FamilyMember] = {

    Array(
      FamilyMember(1, getEleRandomByArr(Array("青年", "中年", "老年")), "男"),
      FamilyMember(2, getEleRandomByArr(Array("青年", "中年", "老年")), "女")

    )
  }


  //夫妻和小孩
  def getFMsOfCoupleAndChildren(): Array[FamilyMember] = {

    //    val possibleArr = Array()

    val possibleArr = Array[Array[FamilyMember]](

      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "女")

      ),
      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "男")
      ),

      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男")
      )
      ,
      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "女")
      )
      ,

      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "女")
      ),
      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "男")

      ),
      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男")
      )
      ,
      Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "女")

      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "男"),
        FamilyMember(4, "青年", "女")

      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "男"),
        FamilyMember(4, "青年", "男")

      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "男")
      ),
      Array[FamilyMember](
        FamilyMember(1, "老年", "男"),
        FamilyMember(2, "老年", "女"),
        FamilyMember(3, "青年", "女")
      )

    )

    getEleRandomByArr(possibleArr)
  }


  //夫妻和老人
  def getFMsOfCoupleAndOld(): Array[FamilyMember] = {


    val possibleArr = Array[Array[FamilyMember]](

      Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "老年", "男"),
        FamilyMember(4, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "老年", "男"),
        FamilyMember(4, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "老年", "男")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "老年", "女")
      )
    )

    getEleRandomByArr(possibleArr)

  }


  //夫妻、老人和小孩
  def getFMsOfCoupleAndOldAndChildren(): Array[FamilyMember] = {

    val possibleArr = Array[Array[FamilyMember]](

      Array[FamilyMember](

        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "女"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "男"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "女"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "女"),
        FamilyMember(5, "老年", "男")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "儿童", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "男"),
        FamilyMember(4, "老年", "男")

      ), Array[FamilyMember](
        FamilyMember(1, "青年", "男"),
        FamilyMember(2, "青年", "女"),
        FamilyMember(3, "儿童", "女"),
        FamilyMember(4, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "女"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "男"),
        FamilyMember(5, "老年", "男"),
        FamilyMember(6, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "女"),
        FamilyMember(4, "老年", "男"),
        FamilyMember(5, "老年", "女")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "女"),
        FamilyMember(5, "老年", "男")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "少年", "男"),
        FamilyMember(5, "老年", "女")

      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "男"),
        FamilyMember(4, "老年", "男")
      ), Array[FamilyMember](
        FamilyMember(1, "中年", "男"),
        FamilyMember(2, "中年", "女"),
        FamilyMember(3, "少年", "女"),
        FamilyMember(4, "老年", "女")
      )
    )

    getEleRandomByArr(possibleArr)

  }


  def getEleRandomByArr(arr: Array[String]): String = {
    arr(Random.nextInt(arr.length))
  }

  def getEleRandomByArr(arr: Array[Array[FamilyMember]]): Array[FamilyMember] = {
    arr(Random.nextInt(arr.length))
  }

}
package com.avcdata.spark.job.mock

import com.avcdata.spark.job.etl.common.UserVectorConstant
import com.avcdata.spark.job.mock.GenerateUserTerminalDataHelper.FamilyMember
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.Random

object GenerateUserTerminalData_Hbase {




  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("GenerateUserTerminalData")
    val sc = new SparkContext(conf)
    run(sc)

    sc.stop()


  }


  case class UserTerminal(
                           sn: String,
                           member_struct: String,
                           member_id: Int,
                           time_range: String,
                           age_range: String,
                           sex: String,
                           brand: String,
                           province: String,
                           city: String,
                           citylevel: String,
                           area: String,
                           price: String,
                           size: String,
                           model: String,
                           license: String

                         )

  def run(sc: SparkContext) = {

    //TODO 目的：生成一个userterminal

    //TODO  从user_vector_terminal 中取terminal相关属性
    val sqlContext = new HiveContext(sc)

    val resultRDD = sqlContext.sql("SELECT t.sn, t.brand, t.area, t.province, t.city , t.citylevel, tsp.size, t.model, t.license, tsp.price FROM hr.user_vector_terminal uvt JOIN hr.terminal t ON uvt.sn = t.sn JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand AND t.model = tsp.model ").rdd
      .flatMap(line => {
        val sn = line(0).toString
        val brand = line(1).toString
        val area = line(2).toString
        val province = line(3).toString
        val city = line(4).toString
        val citylevel = line(5).toString
        val size = line(6).toString
        val model = line(7).toString
        val license = line(8).toString
        val price = line(9).toString


        //        val sn = "00:0c:e7:06:00:00"
        //        val brand = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TERMINAL_BRAND_ARR)
        //        val province = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TERMINAL_PROVINCE_ARR)
        //        val city = "北京市"
        //        val citylevel = "特级城市"
        //        val area = "华北"
        //        val price = "2000"
        //        val size = "30"
        //        val model = "TZKM"
        //        val license = "youku"

        //TODO 结果
        val userTerminalArr = scala.collection.mutable.ArrayBuffer[UserTerminal]()



        //TODO 家庭结构
        val member_struct_arr = UserVectorConstant.FAMILY_COMPOSE_MAX_NUM_MAP.keySet.toArray
        val member_struct = GenerateUserTerminalDataHelper.getEleRandomByArr(member_struct_arr)

        //TODO 家庭成员
        val fms = GenerateUserTerminalDataHelper.getFmsByMemberStruct(member_struct)

        //TODO 时间段
        for (ele <- 0 until UserVectorConstant.TIME_RANGE.length) {
          val time_range = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.TIME_RANGE)
          //          println("time_range:" + time_range)

          for (fm <- getWatchTVFamilyMembers(fms)) {
            val userTerminal = UserTerminal(
              sn,
              member_struct,
              fm.member_id,
              time_range,
              fm.age_range,
              fm.sex,
              brand,
              province,
              city,
              citylevel,
              area,
              price,
              size,
              model,
              license
            )
            userTerminalArr.+=(userTerminal)
            //            println(userTerminal)
          }
        }


        userTerminalArr


      })


    //    // TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_terminal")

      try {

        items.foreach(userTerminal => {

          //          info:sn,
          //          info:member_struct,
          //          info:member_id,
          //          info:time_range,
          //          info:brand,
          //          info:age_range,
          //          info:sex,
          //          info:area,
          //          info:province,
          //          info:city,
          //          info:citylevel,
          //          info:price,
          //          info:size,
          //          info:model,
          //          info:license


          val sn = userTerminal.sn
          val member_struct = userTerminal.member_struct
          val member_id = userTerminal.member_id
          val time_range = userTerminal.time_range
          val brand = userTerminal.brand
          val age_range = userTerminal.age_range
          val sex = userTerminal.sex
          val area = userTerminal.area
          val province = userTerminal.province
          val city = userTerminal.city
          val citylevel = userTerminal.citylevel
          val price = userTerminal.price
          val size = userTerminal.size
          val model = userTerminal.model
          val license = userTerminal.license


          val orderedLine =
            sn + "\t" + member_struct + "\t" + member_id + "\t" + time_range + "\t" + brand + "\t" + age_range + "\t" + sex + "\t" + area + "\t" + province + "\t" + city + "\t" + citylevel + "\t" + price + "\t" + size + "\t" + model + "\t" + license

          mutator.mutate(HBaseUtils.getPut_UserTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })


  }


  /**
    * 随机获取某个家庭某时段观看的家庭成员的集合
    *
    * @param arr
    * @return
    */
  def getWatchTVFamilyMembers(arr: Array[FamilyMember]): Array[FamilyMember] = {
    val resultArr = scala.collection.mutable.ArrayBuffer[FamilyMember]()

    val wt_fm_num_max = arr.length
    for (i <- 1 to Random.nextInt(wt_fm_num_max) + 1) {
      resultArr.+=(arr(i - 1))
    }

    resultArr.toArray
  }


}


//for (i <- 1 to member_id_max) {
//  member_id_arr.+=(i)
//}
//
//  for (ele <- member_id_arr) {
//
//  println("member id:" + ele)
//
//  val age_range = GenerateUserTerminalDataHelper.getEleRandomByArr(Array("青年", "中年", "老年"))
//
//  println("age_range:" + age_range)
//
//  val sex = GenerateUserTerminalDataHelper.getEleRandomByArr(UserVectorConstant.SEX)
//
//  println("sex:" + sex)
//
//}

package com.avcdata.vbox.tmp

import com.avcdata.vbox.util.TimeUtils
import org.apache.spark.SparkContext

object GetCCPgOfPlay {

  def main(args: Array[String]): Unit = {
    //
    val haha = "2017-08-30 11:22:39".substring(0, 10)

    val log =
      """
     216ffa9b74ecc5ca93546642d16d1e6c	杨乃武与小白菜	2017-08-30 11:22:39	null	null	湖南省	长沙市	E6200	8S70
      """.stripMargin
    val test = log.split("\t")(2).trim.substring(0, 10)

    println(test)


    //    val conf = new SparkConf()
    //      .setMaster("local[4]")
    //      .setAppName("GetCCPgOfPlay")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-09-01")
    //    sc.stop()
  }

  //    1、老平台剧名：即2017.1.28-2017.1.31剧集名称

  //    2、新平台剧名：即2017.8.26-2017.8.29剧集名称

  //    3、新平台剧集去重终端数：即2017.8.26-2017.8.29每天剧集日志的去重终端数

  def run(sc: SparkContext, currentDate: String) = {

//    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
//      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)
//
//    //TODO 酷开老平台的剧集数据
//    //    157.122.65.204	VIDEO_PLAYER	fca386490c12	PlayerControl	1500540566770		复合大师_33	Start		2017-07-20 16:49:26	广东省	未匹配	G8210	9R50
//
//    val old_date_arr = Array[String](
//      "20170128",
//      "20170129",
//      "20170130",
//      "20170131"
//    )
//
//    var old_initRDD = sc.makeRDD(Array("#"))
//
//    for (i <- 0 until old_date_arr.length) {
//      println(old_date_arr(i))
//      old_initRDD = old_initRDD.union(sc.textFile("/user/hdfs/rsync/COOCAA/" + old_date_arr(i) + "/aowei_PlayerControl" + old_date_arr(i)))
//    }
//
//    old_initRDD = old_initRDD.filter(line => {
//      !line.equals("#")
//    })
//
//
//    old_initRDD
//      .map(line => {
//        val cols = line.split("\t")
//        cols(6).trim
//      }).distinct().saveAsTextFile("/tmp/cc_old_pg" + System.currentTimeMillis())


    ////////////////////////////////////////////////////////////////////
    //TODO 新平台的数据
    //    216ffa9b74ecc5ca93546642d16d1e6c	杨乃武与小白菜	2017-08-30 11:22:39	null	null	湖南省	长沙市	E6200	8S70
    val new_date_arr = Array[String](
      "20170819",
      "20170820",
      "20170821",
      "20170822",
      "20170823",
      "20170824",
      "20170825"
    )

    var new_initRDD = sc.makeRDD(Array("#"))

    for (i <- 0 until new_date_arr.length) {
      println(new_date_arr(i))
      new_initRDD = new_initRDD.union(sc.textFile("/user/hdfs/rsync/COOCAA/" + new_date_arr(i) + "/aowei_PlayerControl" + new_date_arr(i)))
    }
    new_initRDD = new_initRDD.filter(line => {
      !line.equals("#")
    })


    new_initRDD.map(line => {
      val cols = line.split("\t")
      cols(1).trim
    }).distinct().repartition(1).saveAsTextFile("/tmp/cc_new_pg" + System.currentTimeMillis())


//    val new_acnt = new_initRDD.filter(line => {
//      val cols = line.split("\t")
//      val sn = cols(0).trim
//      val time = cols(2).trim
//      time.length > 10
//    }).map(line => {
//      val cols = line.split("\t")
//      val sn = cols(0).trim
//      val date = cols(2).trim.substring(0, 10)
//      (date, sn)
//    })
//      .distinct
//      .map(x => (x._1, 1)).reduceByKey(_ + _).repartition(1)
//      .saveAsTextFile("/tmp/new_acnt" + System
//      .currentTimeMillis())
  }
}package com.avcdata.vbox.tmp

import com.github.nscala_time.time.Imports._
import org.apache.spark.SparkContext

import scala.collection.mutable

object GetChannelOfLive {
  val ch_ChannelMap = mutable.Map(
    ("中央8台" -> "CCTV-8"),
    ("中央12台" -> "CCTV-12"),
    ("中央1台" -> "CCTV-1"),
    ("黑龙江卫视" -> "黑龙江卫视"),
    ("中央9台" -> "CCTV-9"),
    ("中央5台" -> "CCTV-5"),
    ("贵州卫视" -> "贵州卫视"),
    ("中央14台" -> "CCTV-14"),
    ("东方卫视" -> "上海东方卫视"),
    ("上海卫视" -> "上海东方卫视"),
    ("湖北卫视" -> "湖北卫视"),
    ("东南卫视" -> "东南卫视"),
    ("广东卫视" -> "广东卫视"),
    ("中央7台" -> "CCTV-7"),
    ("湖南卫视" -> "湖南卫视"),
    ("山东卫视" -> "山东卫视"),
    ("北京卫视" -> "北京卫视"),
    ("青海卫视" -> "青海卫视"),
    ("中央10台" -> "CCTV-10"),
    ("旅游卫视" -> "旅游卫视"),
    ("甘肃卫视" -> "甘肃卫视"),
    ("重庆卫视" -> "重庆卫视"),
    ("中央3台" -> "CCTV-3"),
    ("新疆卫视" -> "新疆卫视"),
    ("厦门卫视频道" -> "厦门卫视"),
    ("中央13台" -> "CCTV-13"),
    ("辽宁卫视" -> "辽宁卫视"),
    ("山西卫视" -> "山西卫视"),
    ("中央2台" -> "CCTV-2"),
    ("宁夏卫视" -> "宁夏卫视"),
    ("安徽卫视" -> "安徽卫视"),
    ("河北卫视" -> "河北卫视"),
    ("中央6台" -> "CCTV-6"),
    ("浙江卫视" -> "浙江卫视"),
    ("江西卫视" -> "江西卫视"),
    ("河南卫视" -> "河南卫视"),
    ("中央11台" -> "CCTV-11"),
    ("广西卫视" -> "广西卫视"),
    ("江苏卫视" -> "江苏卫视"),
    ("四川卫视" -> "四川卫视"),
    ("中央15台" -> "CCTV-15"),
    ("云南卫视" -> "云南卫视"),
    ("中央4台" -> "CCTV-4"),
    ("内蒙古卫视" -> "内蒙古卫视"),
    ("西藏卫视" -> "西藏卫视"),
    ("深圳卫视" -> "深圳卫视"),
    ("天津卫视" -> "天津卫视"),
    ("陕西卫视" -> "陕西卫视"),
    ("吉林卫视" -> "吉林卫视"),
    ("CCTV-1综合" -> "CCTV-1"),
    ("CCTV-2财经" -> "CCTV-2"),
    ("CCTV-3综艺" -> "CCTV-3"),
    ("CCTV-4中文国际" -> "CCTV-4"),
    ("CCTV-5体育" -> "CCTV-5"),
    ("CCTV-6电影" -> "CCTV-6"),
    ("CCTV-7军事农业" -> "CCTV-7"),
    ("CCTV-8电视剧" -> "CCTV-8"),
    ("CCTV-9纪录" -> "CCTV-9"),
    ("CCTV-10科教" -> "CCTV-10"),
    ("CCTV-11戏曲" -> "CCTV-11"),
    ("CCTV-12社会与法" -> "CCTV-12"),
    ("CCTV-13新闻" -> "CCTV-13"),
    ("CCTV-14少儿" -> "CCTV-14"),
    ("CCTV-15音乐" -> "CCTV-15"),
    ("厦门卫视" -> "厦门卫视"),
    ("CCTV-4中文国际(亚)" -> "CCTV-4")
  )

  val ko_ChannelMap = mutable.Map(
    ("CCTV-8" -> "CCTV-8"),
    ("CCTV-12" -> "CCTV-12"),
    ("CCTV-1" -> "CCTV-1"),
    ("黑龙江卫视" -> "黑龙江卫视"),
    ("CCTV-9" -> "CCTV-9"),
    ("CCTV-5" -> "CCTV-5"),
    ("贵州卫视" -> "贵州卫视"),
    ("CCTV-14" -> "CCTV-14"),
    ("东方卫视" -> "上海东方卫视"),
    ("湖北卫视" -> "湖北卫视"),
    ("东南卫视" -> "东南卫视"),
    ("广东卫视" -> "广东卫视"),
    ("CCTV-7" -> "CCTV-7"),
    ("湖南卫视" -> "湖南卫视"),
    ("山东卫视" -> "山东卫视"),
    ("BTV北京卫视" -> "北京卫视"),
    ("青海卫视" -> "青海卫视"),
    ("CCTV-10" -> "CCTV-10"),
    ("旅游卫视" -> "旅游卫视"),
    ("甘肃卫视" -> "甘肃卫视"),
    ("重庆卫视" -> "重庆卫视"),
    ("CCTV-3" -> "CCTV-3"),
    ("新疆卫视" -> "新疆卫视"),
    ("厦门卫视" -> "厦门卫视"),
    ("CCTV-13" -> "CCTV-13"),
    ("辽宁卫视" -> "辽宁卫视"),
    ("山西卫视" -> "山西卫视"),
    ("CCTV-2" -> "CCTV-2"),
    ("宁夏卫视" -> "宁夏卫视"),
    ("安徽卫视" -> "安徽卫视"),
    ("河北卫视" -> "河北卫视"),
    ("CCTV-6" -> "CCTV-6"),
    ("浙江卫视" -> "浙江卫视"),
    ("江西卫视" -> "江西卫视"),
    ("河南卫视" -> "河南卫视"),
    ("CCTV-11" -> "CCTV-11"),
    ("广西卫视" -> "广西卫视"),
    ("江苏卫视" -> "江苏卫视"),
    ("四川卫视" -> "四川卫视"),
    ("CCTV-15" -> "CCTV-15"),
    ("云南卫视" -> "云南卫视"),
    ("CCTV-4" -> "CCTV-4"),
    ("内蒙古卫视" -> "内蒙古卫视"),
    ("西藏卫视" -> "西藏卫视"),
    ("深圳卫视" -> "深圳卫视"),
    ("天津卫视" -> "天津卫视"),
    ("陕西卫视" -> "陕西卫视"),
    ("吉林卫视" -> "吉林卫视"),
    ("CCTV-1综合" -> "CCTV-1"),
    ("CCTV-2财经" -> "CCTV-2"),
    ("CCTV-3综艺" -> "CCTV-3"),
    ("CCTV-4中文国际" -> "CCTV-4"),
    ("CCTV-5体育" -> "CCTV-5"),
    ("CCTV-6电影" -> "CCTV-6"),
    ("CCTV-7军事农业" -> "CCTV-7"),
    ("CCTV-8电视剧" -> "CCTV-8"),
    ("CCTV-9纪录" -> "CCTV-9"),
    ("CCTV-10科教" -> "CCTV-10"),
    ("CCTV-11戏曲" -> "CCTV-11"),
    ("CCTV-12社会与法" -> "CCTV-12"),
    ("CCTV-13新闻" -> "CCTV-13"),
    ("CCTV-14少儿" -> "CCTV-14"),
    ("CCTV-15音乐" -> "CCTV-15"),
    ("厦门卫视" -> "厦门卫视")
  )


  def run(sc: SparkContext, analysisDate: String) = {


    //val preDate = DateTime.parse(analysisDate).plus(-1).toString("yyyy-MM-dd")--- -1减1  1不变
    val NextDate = DateTime.parse(analysisDate).plusDays(1).toString("yyyy-MM-dd")
    val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

    val chRDD =
      sc.textFile("/user/hdfs/rsync/CH/" + "2017-07-30" + "/livebroadcast.txt")
        .union(sc.textFile("/user/hdfs/rsync/CH/" + "2017-07-31" + "/livebroadcast.txt"))
        .union(sc.textFile("/user/hdfs/rsync/CH/" + "2017-08-01" + "/livebroadcast.txt"))
        .union(sc.textFile("/user/hdfs/rsync/CH/" + "2017-08-02" + "/livebroadcast.txt"))
        .union(sc.textFile("/user/hdfs/rsync/CH/" + "2017-08-03" + "/livebroadcast.txt"))
        .union(sc.textFile("/user/hdfs/rsync/CH/" + "2017-08-04" + "/livebroadcast.txt"))
        .union(sc.textFile("/user/hdfs/rsync/CH/" + "2017-08-05" + "/livebroadcast.txt"))

        .filter(x => x.split('\t').length == 11)
        //      .filter(x => {
        //        val date = x.split('\t')(5).substring(0, 10)
        //        date == analysisDate || date == NextDate || date == preDate || date == yesBeforeDate
        //      })
        .filter(x => x.split('\t')(0).length >= 12)
        .filter(line => {
          val cols = line.split('\t')
          val tv = cols(3) //频道名称
          !ch_ChannelMap.keySet.contains(tv)
        })
        .map(line => {
          val cols = line.split('\t')
          val tv = cols(3) //频道名称
          ("CH", tv)
        }).distinct()


    val koRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-07-30" + "/tv_logo.log.*")
      .union(sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-07-31" + "/tv_logo.log.*"))
      .union(sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-08-01" + "/tv_logo.log.*"))
      .union(sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-08-02" + "/tv_logo.log.*"))
      .union(sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-08-03" + "/tv_logo.log.*"))
      .union(sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-08-04" + "/tv_logo.log.*"))
      .union(sc.textFile("/user/hdfs/rsync/KONKA/" + "2017-08-05" + "/tv_logo.log.*"))

      //val count = sc.textFile("/user/hdfs/rsync/KONKA/history/tv/tv" + analysisDate)
      .filter(x => {
      val cols = x.split('|')
      cols.length == 7 && cols(6) != "" && cols(6).length == 19
    })
      //      .filter(x => {
      //        val date = x.split('|')(6).substring(0, 10)
      //        date == analysisDate || date == preDate || date == yesBeforeDate
      //      })


      .filter(line => {
      val cols = line.split('|')
      val tv = cols(5)
      !ko_ChannelMap.keySet.contains(tv)
    })
      .map(line => {
        val cols = line.split('|')
        val tv = cols(5)
        ("KO", tv)
      }).distinct()

    chRDD.union(koRdd).saveAsTextFile("/tmp/allChannel" + System.currentTimeMillis())

  }
}
package com.avcdata.spark.job.util

import org.apache.hadoop.fs.Path
import org.apache.hadoop.hbase.client.{HTable, Put, Result}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.{HFileOutputFormat, LoadIncrementalHFiles, TableOutputFormat}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, KeyValue}
import org.apache.hadoop.mapreduce.Job
import org.apache.spark.{SparkConf, SparkContext}

object HBaseBulkLoader {

  val sparkConf = new SparkConf()
    .setMaster("local[4]")
    .setAppName("coocaa-ApkDataLoadJob")
  val sc = new SparkContext(sparkConf)

  //  val sparkConf = new SparkConf().
  //    setMaster("spark://lxw1234.com:7077").
  //    setAppName("lxw1234.com")

  val hbaseConf = HBaseConfiguration.create()
  hbaseConf.set("hbase.zookeeper.property.clientPort", "2181")
  hbaseConf.set("hbase.zookeeper.quorum", "")

  def hfile2Table(hfilePath: String, outputTable: String): Unit = {
    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, outputTable)
    val table = new HTable(hbaseConf, outputTable)
    val bulkLoader = new LoadIncrementalHFiles(hbaseConf)
    bulkLoader.doBulkLoad(new Path(hfilePath), table)

    sc.stop()

  }


  def insert(hdfsFilePath: String, hfilePath: String, family: String, col: String, outputTable: String): Unit = {

    val domainUid = sc.textFile(hdfsFilePath).map { x =>
      val a = x.split(",")
      val domain = a(0)
      val uid = a(1)
      (domain, uid)
    }
    val result = domainUid.distinct().sortByKey(numPartitions = 1).map { x =>
      val domain = x._1
      val uid = x._2
      val kv: KeyValue = new KeyValue(Bytes.toBytes(domain), family.getBytes(), col.getBytes(), uid.getBytes())
      (new ImmutableBytesWritable(Bytes.toBytes(domain)), kv)
    }

    val conf = HBaseConfiguration.create()
    conf.set(TableOutputFormat.OUTPUT_TABLE, outputTable)
    result.saveAsNewAPIHadoopFile(hfilePath, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat], conf)

    sc.stop()
  }


  def insert2(hdfsFilePath: String, hfilePath: String, family: String, col: String, outputTable: String): Unit = {

    var rdd1 = sc.makeRDD(Array(("A", 2), ("B", 6), ("C", 7)))
    sc.hadoopConfiguration.set("hbase.zookeeper.quorum ", "zkNode1,zkNode2,zkNode3")
    sc.hadoopConfiguration.set("zookeeper.znode.parent", "/hbase")
    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE, outputTable)

    var job = new Job(sc.hadoopConfiguration)
    job.setOutputKeyClass(classOf[ImmutableBytesWritable])
    job.setOutputValueClass(classOf[Result])
    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])

    rdd1.map(
      x => {
        var put = new Put(Bytes.toBytes(x._1))
        put.add(Bytes.toBytes(family), Bytes.toBytes(col), Bytes.toBytes(x._2))
        (new ImmutableBytesWritable, put)
      }
    ).saveAsNewAPIHadoopDataset(job.getConfiguration)

    sc.stop()
  }


}
package com.avcdata.spark.job.util

import org.apache.hadoop.fs.Path
import org.apache.hadoop.hbase.client.{HTable, Put, Result}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.{HFileOutputFormat, LoadIncrementalHFiles, TableOutputFormat}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, KeyValue}
import org.apache.hadoop.mapreduce.Job
import org.apache.spark.{SparkConf, SparkContext}

object HBaseBulkLoader {

  val sparkConf = new SparkConf()
    .setMaster("local[4]")
    .setAppName("coocaa-ApkDataLoadJob")
  val sc = new SparkContext(sparkConf)

  //  val sparkConf = new SparkConf().
  //    setMaster("spark://lxw1234.com:7077").
  //    setAppName("lxw1234.com")

  val hbaseConf = HBaseConfiguration.create()
  hbaseConf.set("hbase.zookeeper.property.clientPort", "2181")
  hbaseConf.set("hbase.zookeeper.quorum", "")

  def hfile2Table(hfilePath: String, outputTable: String): Unit = {
    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, outputTable)
    val table = new HTable(hbaseConf, outputTable)
    val bulkLoader = new LoadIncrementalHFiles(hbaseConf)
    bulkLoader.doBulkLoad(new Path(hfilePath), table)

    sc.stop()

  }


  def insert(hdfsFilePath: String, hfilePath: String, family: String, col: String, outputTable: String): Unit = {

    val domainUid = sc.textFile(hdfsFilePath).map { x =>
      val a = x.split(",")
      val domain = a(0)
      val uid = a(1)
      (domain, uid)
    }
    val result = domainUid.distinct().sortByKey(numPartitions = 1).map { x =>
      val domain = x._1
      val uid = x._2
      val kv: KeyValue = new KeyValue(Bytes.toBytes(domain), family.getBytes(), col.getBytes(), uid.getBytes())
      (new ImmutableBytesWritable(Bytes.toBytes(domain)), kv)
    }

    val conf = HBaseConfiguration.create()
    conf.set(TableOutputFormat.OUTPUT_TABLE, outputTable)
    result.saveAsNewAPIHadoopFile(hfilePath, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat], conf)

    sc.stop()
  }


  def insert2(hdfsFilePath: String, hfilePath: String, family: String, col: String, outputTable: String): Unit = {

    var rdd1 = sc.makeRDD(Array(("A", 2), ("B", 6), ("C", 7)))
    sc.hadoopConfiguration.set("hbase.zookeeper.quorum ", "zkNode1,zkNode2,zkNode3")
    sc.hadoopConfiguration.set("zookeeper.znode.parent", "/hbase")
    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE, outputTable)

    var job = new Job(sc.hadoopConfiguration)
    job.setOutputKeyClass(classOf[ImmutableBytesWritable])
    job.setOutputValueClass(classOf[Result])
    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])

    rdd1.map(
      x => {
        var put = new Put(Bytes.toBytes(x._1))
        put.add(Bytes.toBytes(family), Bytes.toBytes(col), Bytes.toBytes(x._2))
        (new ImmutableBytesWritable, put)
      }
    ).saveAsNewAPIHadoopDataset(job.getConfiguration)

    sc.stop()
  }


}
package com.avcdata.vbox.util

import org.apache.hadoop.fs.Path
import org.apache.hadoop.hbase.client.{HTable, Put, Result}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.{HFileOutputFormat, LoadIncrementalHFiles, TableOutputFormat}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, KeyValue}
import org.apache.hadoop.mapreduce.Job
import org.apache.spark.{SparkConf, SparkContext}

object HBaseBulkLoader {

  val sparkConf = new SparkConf()
    .setMaster("local[4]")
    .setAppName("coocaa-ApkDataLoadJob")
  val sc = new SparkContext(sparkConf)

  //  val sparkConf = new SparkConf().
  //    setMaster("spark://lxw1234.com:7077").
  //    setAppName("lxw1234.com")

  val hbaseConf = HBaseConfiguration.create()
  hbaseConf.set("hbase.zookeeper.property.clientPort", "2181")
  hbaseConf.set("hbase.zookeeper.quorum", "")

  def hfile2Table(hfilePath: String, outputTable: String): Unit = {
    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, outputTable)
    val table = new HTable(hbaseConf, outputTable)
    val bulkLoader = new LoadIncrementalHFiles(hbaseConf)
    bulkLoader.doBulkLoad(new Path(hfilePath), table)

    sc.stop()

  }


  def insert(hdfsFilePath: String, hfilePath: String, family: String, col: String, outputTable: String): Unit = {

    val domainUid = sc.textFile(hdfsFilePath).map { x =>
      val a = x.split(",")
      val domain = a(0)
      val uid = a(1)
      (domain, uid)
    }
    val result = domainUid.distinct().sortByKey(numPartitions = 1).map { x =>
      val domain = x._1
      val uid = x._2
      val kv: KeyValue = new KeyValue(Bytes.toBytes(domain), family.getBytes(), col.getBytes(), uid.getBytes())
      (new ImmutableBytesWritable(Bytes.toBytes(domain)), kv)
    }

    val conf = HBaseConfiguration.create()
    conf.set(TableOutputFormat.OUTPUT_TABLE, outputTable)
    result.saveAsNewAPIHadoopFile(hfilePath, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat], conf)

    sc.stop()
  }


  def insert2(hdfsFilePath: String, hfilePath: String, family: String, col: String, outputTable: String): Unit = {

    var rdd1 = sc.makeRDD(Array(("A", 2), ("B", 6), ("C", 7)))
    sc.hadoopConfiguration.set("hbase.zookeeper.quorum ", "zkNode1,zkNode2,zkNode3")
    sc.hadoopConfiguration.set("zookeeper.znode.parent", "/hbase")
    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE, outputTable)

    var job = new Job(sc.hadoopConfiguration)
    job.setOutputKeyClass(classOf[ImmutableBytesWritable])
    job.setOutputValueClass(classOf[Result])
    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])

    rdd1.map(
      x => {
        var put = new Put(Bytes.toBytes(x._1))
        put.add(Bytes.toBytes(family), Bytes.toBytes(col), Bytes.toBytes(x._2))
        (new ImmutableBytesWritable, put)
      }
    ).saveAsNewAPIHadoopDataset(job.getConfiguration)

    sc.stop()
  }


}
package com.avcdata.etl.common.pool.hbase

import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory}


/**
  * JDBC数据库连接池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/16 08:18
  */
object HBaseConnectionPool
{
  //数据库连接池
  private var connPool: Map[(String, String), ObjectPool[Connection]] = Map()

  sys.addShutdownHook
  {
    connPool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperQuorum: String, zookeeperClientPort: String): ManagedConnection =
  {
    val pool = connPool.getOrElse((zookeeperQuorum, zookeeperClientPort),
      {
        HBaseConnectionPool.synchronized[ObjectPool[Connection]]
          {
            connPool.getOrElse((zookeeperQuorum, zookeeperClientPort),
              {
                val p = new GenericObjectPool[Connection](new ConnectionFactory(zookeeperQuorum, zookeeperClientPort))
                connPool += (zookeeperQuorum, zookeeperClientPort) -> p

                p
              })
          }
      })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

/**
  * 受管连接对象
  *
  * @param pool       池对象
  * @param connection 连接对象
  */
class ManagedConnection(private val pool: ObjectPool[Connection], val connection: Connection)
{
  def close() = pool.returnObject(connection)
}

/**
  * 连接池创建工场
  */
private class ConnectionFactory(zookeeperQuorum: String, zookeeperClientPort: String) extends BasePooledObjectFactory[Connection]
{
  override def create() =
  {
    val hBaseConfiguration = HBaseConfiguration.create()
    hBaseConfiguration.set("hbase.zookeeper.quorum", zookeeperQuorum)
    hBaseConfiguration.set("hbase.zookeeper.property.clientPort", zookeeperClientPort)

    ConnectionFactory.createConnection(hBaseConfiguration)
  }

  override def wrap(conn: Connection) = new DefaultPooledObject[Connection](conn)

  override def validateObject(po: PooledObject[Connection]) = !po.getObject.isClosed

  override def destroyObject(po: PooledObject[Connection]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Connection]) =
  {}
}package com.avcdata.etl.common.pool.hbase


import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory}


/**
  * JDBC数据库连接池
  */
object HBaseConnectionPool {
  //数据库连接池
  private var connPool: Map[(String, String), ObjectPool[Connection]] = Map()

  sys.addShutdownHook {
    connPool.values.foreach { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperQuorum: String, zookeeperClientPort: String): ManagedConnection = {
    val pool = connPool.getOrElse((zookeeperQuorum, zookeeperClientPort), {
      HBaseConnectionPool.synchronized[ObjectPool[Connection]] {
        connPool.getOrElse((zookeeperQuorum, zookeeperClientPort), {
          val p = new GenericObjectPool[Connection](new ConnectionFactory(zookeeperQuorum, zookeeperClientPort))
          connPool += (zookeeperQuorum, zookeeperClientPort) -> p

          p
        })
      }
    })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

/**
  * 受管连接对象
  *
  * @param pool       池对象
  * @param connection 连接对象
  */
class ManagedConnection(private val pool: ObjectPool[Connection], val connection: Connection) {
  def close() = pool.returnObject(connection)
}

/**
  * 连接池创建工场
  */
private class ConnectionFactory(zookeeperQuorum: String, zookeeperClientPort: String) extends BasePooledObjectFactory[Connection] {
  override def create() = {
    val hBaseConfiguration = HBaseConfiguration.create()
    hBaseConfiguration.set("hbase.zookeeper.quorum", zookeeperQuorum)
    hBaseConfiguration.set("hbase.zookeeper.property.clientPort", zookeeperClientPort)

    ConnectionFactory.createConnection(hBaseConfiguration)
  }

  override def wrap(conn: Connection) = new DefaultPooledObject[Connection](conn)

  override def validateObject(po: PooledObject[Connection]) = !po.getObject.isClosed

  override def destroyObject(po: PooledObject[Connection]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Connection]) = {}
}package com.avcdata.etl.common.pool.hbase


import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory}


/**
  * JDBC数据库连接池
  */
object HBaseConnectionPool {
  //数据库连接池
  private var connPool: Map[(String, String), ObjectPool[Connection]] = Map()

  sys.addShutdownHook {
    connPool.values.foreach { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperQuorum: String, zookeeperClientPort: String): ManagedConnection = {
    val pool = connPool.getOrElse((zookeeperQuorum, zookeeperClientPort), {
      HBaseConnectionPool.synchronized[ObjectPool[Connection]] {
        connPool.getOrElse((zookeeperQuorum, zookeeperClientPort), {
          val p = new GenericObjectPool[Connection](new ConnectionFactory(zookeeperQuorum, zookeeperClientPort))
          connPool += (zookeeperQuorum, zookeeperClientPort) -> p

          p
        })
      }
    })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

/**
  * 受管连接对象
  *
  * @param pool       池对象
  * @param connection 连接对象
  */
class ManagedConnection(private val pool: ObjectPool[Connection], val connection: Connection) {
  def close() = pool.returnObject(connection)
}

/**
  * 连接池创建工场
  */
private class ConnectionFactory(zookeeperQuorum: String, zookeeperClientPort: String) extends BasePooledObjectFactory[Connection] {
  override def create() = {
    val hBaseConfiguration = HBaseConfiguration.create()
    hBaseConfiguration.set("hbase.zookeeper.quorum", zookeeperQuorum)
    hBaseConfiguration.set("hbase.zookeeper.property.clientPort", zookeeperClientPort)

    ConnectionFactory.createConnection(hBaseConfiguration)
  }

  override def wrap(conn: Connection) = new DefaultPooledObject[Connection](conn)

  override def validateObject(po: PooledObject[Connection]) = !po.getObject.isClosed

  override def destroyObject(po: PooledObject[Connection]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Connection]) = {}
}package com.avcdata.etl.common.pool.hbase

import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory}


/**
  * JDBC数据库连接池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/16 08:18
  */
object HBaseConnectionPool
{
  //数据库连接池
  private var connPool: Map[(String, String), ObjectPool[Connection]] = Map()

  sys.addShutdownHook
  {
    connPool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperQuorum: String, zookeeperClientPort: String): ManagedConnection =
  {
    val pool = connPool.getOrElse((zookeeperQuorum, zookeeperClientPort),
      {
        HBaseConnectionPool.synchronized[ObjectPool[Connection]]
          {
            connPool.getOrElse((zookeeperQuorum, zookeeperClientPort),
              {
                val p = new GenericObjectPool[Connection](new ConnectionFactory(zookeeperQuorum, zookeeperClientPort))
                connPool += (zookeeperQuorum, zookeeperClientPort) -> p

                p
              })
          }
      })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

/**
  * 受管连接对象
  *
  * @param pool       池对象
  * @param connection 连接对象
  */
class ManagedConnection(private val pool: ObjectPool[Connection], val connection: Connection)
{
  def close() = pool.returnObject(connection)
}

/**
  * 连接池创建工场
  */
private class ConnectionFactory(zookeeperQuorum: String, zookeeperClientPort: String) extends BasePooledObjectFactory[Connection]
{
  override def create() =
  {
    val hBaseConfiguration = HBaseConfiguration.create()
    hBaseConfiguration.set("hbase.zookeeper.quorum", zookeeperQuorum)
    hBaseConfiguration.set("hbase.zookeeper.property.clientPort", zookeeperClientPort)

    ConnectionFactory.createConnection(hBaseConfiguration)
  }

  override def wrap(conn: Connection) = new DefaultPooledObject[Connection](conn)

  override def validateObject(po: PooledObject[Connection]) = !po.getObject.isClosed

  override def destroyObject(po: PooledObject[Connection]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Connection]) =
  {}
}package com.avcdata.spark.job.util

import java.io.IOException

import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.Delete
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD

object HBaseDao {

  def deleteData(keyRDD: RDD[String], htableName: String): Boolean = {

    keyRDD.foreachPartition(items => {

      val table =  HBaseUtils.getHbaseConn().getTable(TableName.valueOf(htableName))
      try {
        items.foreach(key => {
          table.delete(new Delete(Bytes.toBytes(key)))
        })
      } catch {
        case e: IOException =>
          println("删除记录异常！！！")
          false
      } finally {
        table.close
      }
    })

    true
  }
}
package com.avcdata.spark.job.etl.util

import java.io.IOException
import java.util
import java.util.concurrent.Executors

import org.apache.hadoop.hbase.KeyValue.Type
import org.apache.hadoop.hbase._
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes

/**
  * Created by xiaoyuzhou on 2017/6/17.
  */
object HBaseDao {
  val myConf = HBaseConfiguration.create()

  val connection = getHbaseConn()

  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection = {


    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf);
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tableName"))


    //    val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    //    myConf.set("hbase.zookeeper.quorum", "192.168.79.131")

    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")

    myConf.set("hbase.zookeeper.property.clientPort", "2181")

    myConf.set("hbase.regionserver.thread.compaction.large", "5")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    //    myConf.set("hbase.hregion.majorcompaction","0")

    myConf.set("hbase.hstore.compaction.min", "10")

    myConf.set("hbase.hstore.compaction.max", "10")

    myConf.set("hbase.hstore.blockingStoreFiles", "100")

    myConf.set("hbase.hstore.compactionThreshold", "7")

    myConf.set("hbase.regionserver.handler.count", "100")

    myConf.set("hbase.regionserver.hlog.splitlog.writer.threads", "10")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    myConf.set("hbase.regionserver.thread.compaction.large", "8")

    myConf.set("hbase.hregion.max.filesizes", "4G")

    myConf.set("hbase.hregion.max.filesize", "60G")

    myConf.set("hbase.hregion.memstore.flush.size", "60G")

    myConf.set("hbase.hregion.memstore.block.multiplier", "5")

    //    myConf.set("hbase.client.write.buffer", "8388608")

    myConf.set("hbase.client.pause", "200")

    myConf.set("hbase.client.retries.number", "71")

    myConf.set("hbase.ipc.client.tcpnodelay", "false")

    myConf.set("hbase.client.scanner.caching", "500")

    myConf.set("hbase.htable.threads.max", Integer.MAX_VALUE.toString)

    myConf.set("hbase.htable.threads.keepalivetime", "60")

    myConf.setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 120000)

    val threadPool = Executors.newFixedThreadPool(Runtime.getRuntime()
      .availableProcessors())

    ConnectionFactory.createConnection(myConf, threadPool)

    //    ConnectionFactory.createConnection(myConf)

  }

  def getMutator(tableName: String): BufferedMutator = {
    connection.getBufferedMutator(TableName.valueOf(tableName))
  }

  /**
    * 释放内部缓冲区
    *
    * @param table
    */
  def closeTable(table: Table) {
    if (table != null) {
      try {
        table.close
      }
      catch {
        case e: IOException => {
          e.printStackTrace
        }
      }
    }
  }

  /**
    * 关闭连接
    */
  def closeConn {
    if (connection != null) {
      try {
        connection.close
      }
      catch {
        case e: IOException => {
          e.printStackTrace
        }
      }
    }
  }


  /**
    * 删除表
    *
    * @param tableName
    * @throws IOException
    */
  @throws(classOf[Exception])
  def deleteTable(tableName: String) {
    try {
      val admin: Admin = connection.getAdmin
      admin.disableTable(TableName.valueOf(tableName))
      admin.deleteTable(TableName.valueOf(tableName))
      println("delete table " + tableName + " ok.")
    }
    catch {
      case e: MasterNotRunningException => {
        e.printStackTrace
      }
      case e: ZooKeeperConnectionException => {
        e.printStackTrace
      }
    }
  }


  /**
    * 创建表
    *
    * @param tableName
    * @param familys
    * @throws IOException
    */
  @throws(classOf[IOException])
  def createTable(tableName: String, familys: Array[String]) {
    val admin: Admin = connection.getAdmin
    if (admin.tableExists(TableName.valueOf(tableName))) {
      println("table already exists!")
    }
    else {
      val tableDescriptor: HTableDescriptor = new HTableDescriptor(TableName.valueOf(tableName))
      for (i <- 0 until familys.length) {
        tableDescriptor.addFamily(new HColumnDescriptor(familys(i)))
      }
      admin.createTable(tableDescriptor)
      println("create table " + tableName + " ok.")
    }
  }


  /**
    * 插入数据
    *
    * @param tableName
    * @param rowKey
    * @param family
    * @param qualifier
    * @param value
    * @throws Exception
    */
  @throws(classOf[Exception])
  def insert(tableName: String, rowKey: String, family: String, qualifier: String, value: String) {
    try {
      val table: Table = connection.getTable(TableName.valueOf(tableName))
      val put: Put = new Put(Bytes.toBytes(rowKey))
      put.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier), Bytes.toBytes(value))
      table.put(put)
      println("insert record  " + rowKey + " to table " + tableName + " ok.")
    }
    catch {
      case e: IOException => {
        e.printStackTrace
      }
    }
  }

  /**
    * 删除某条记录
    *
    * @param tableName
    * @param rowKey
    * @throws java.io.IOException
    */
  @throws(classOf[IOException])
  def delete(tableName: String, rowKey: String) {
    val table: Table = connection.getTable(TableName.valueOf(tableName))
    val list = new util.ArrayList[Delete]()

    if (isRowExist(tableName, rowKey)) {
      val delete = new Delete(rowKey.getBytes)
      list.add(delete)
      table.delete(list)
    }
  }

  @throws(classOf[IOException])
  def delete(tableName: String, rowKeys: Array[String]) {
    val table: Table = connection.getTable(TableName.valueOf(tableName))
    val list = new util.ArrayList[Delete]()

    for (rowKey <- rowKeys) {
      if (isRowExist(tableName, rowKey)) {
        val delete = new Delete(rowKey.getBytes)
        list.add(delete)
      }
    }

    if (!list.isEmpty) {
      table.delete(list)
    }
  }


  /**
    * 检查rowkey是否存在
    *
    * @param tableName
    * @param rowKey
    * @throws java.io.IOException
    * @return
    */
  @throws(classOf[IOException])
  def isRowExist(tableName: String, rowKey: String): Boolean = {
    val table: Table = connection.getTable(TableName.valueOf(tableName))
    //    get.setCheckExistenceOnly(true);//只检查是否存在 返回Null
    table.exists(new Get(rowKey.getBytes))
  }

  /**
    * 获取数据
    *
    * @param tableName
    * @param rowKey
    * @throws Exception
    */
  @throws(classOf[IOException])
  def getByRowKey(tableName: String, rowKey: String) {
    val table: Table = connection.getTable(TableName.valueOf(tableName))
    val get: Get = new Get(rowKey.getBytes)

    if (isRowExist(tableName, rowKey)) {
      val rs: Result = table.get(get)
      for (cell <- rs.rawCells) {
        printCell(cell)
      }
    } else {
      throw new RuntimeException("rowkey is not exists")
    }

  }


  def printCell(cell: Cell): Unit = {
    //        println(new String(cell.getRowArray) + " ")
    //        println(new String(cell.getFamilyArray) + ":")
    //        println(new String(cell.getQualifierArray) + " ")
    //        println(cell.getTimestamp + " ")
    //    println(new String(cell.getValueArray, "UTF-8"))
    //        println(Bytes.toString(cell.getValueArray))
    //    println(CellUtil.getCellKeyAsString(cell))

    //    print(new String(cell.getQualifierArray,"UTF-8"));//(01ca770a87207淘气爷孙_082017-02-14CCdimdim_crowd  Z��8非少儿   ?

    print(new String(cell.getQualifierArray, cell.getQualifierOffset(), cell.getQualifierLength(), "UTF-8") + ":")

    // @    (01ca770a87207淘气爷孙_082017-02-14CCdimdim_awcid  Z��846714
    //    print(new String(cell.getValueArray, "UTF-8"))
    println(new String(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength(), "UTF-8"))

    //    println(Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));

  }


  /**
    * 获取某一行
    *
    * @param tableName
    */
  def getAll(tableName: String) {
    try {
      val table: Table = connection.getTable(TableName.valueOf(tableName))
      val scan: Scan = new Scan
      val rs: ResultScanner = table.getScanner(scan)
      import scala.collection.JavaConversions._
      for (r <- rs) {
        for (cell <- r.rawCells) {
          printCell(cell)
        }
      }
    }
    catch {
      case e: IOException => {
        e.printStackTrace
      }
    }
  }


  def main(args: Array[String]) {
    getByRowKey("tracker_player_active_fact_test", "01ca770a87207淘气爷孙_082017-02-14CC")
    //    //    delete("tracker_player_active_fact_test", "01ca770a87207淘气爷孙_082017-02-14CC")
    //    //    getByRowKey("tracker_player_active_fact_test", "01ca770a87207淘气爷孙_082017-02-14CC")
    //    connection.close
    //    System.exit(0)
  }

}
package com.avcdata.vbox.util

import java.io.IOException

import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.Delete
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD

object HBaseDao {

  def deleteData(keyRDD: RDD[String], htableName: String): Boolean = {

    keyRDD.foreachPartition(items => {

      val table = HBaseUtils.getHbaseConn().getTable(TableName.valueOf(htableName))
      try {
        items.foreach(key => {
          table.delete(new Delete(Bytes.toBytes(key)))
        })
      } catch {
        case e: IOException =>
          println("删除记录异常！！！")
          false
      } finally {
        //        table.close
        HBaseUtils.getHbaseConn().close()
      }
    })

    true
  }
}
package com.avcdata.spark.job.changhong

import java.io.IOException

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase._
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.filter.{CompareFilter, RowFilter, SubstringComparator}

/**
  * Created by avc on 2016/11/4.
  */
case class HbaseOb(admin: Admin, hbaseConf: Configuration, hbaseConn: Connection)

object hbaseUtil {

    var ho:HbaseOb = _

    /**
      * hbase连接的初始化
      */
    def hbaseInit(): Unit = {
        val hbaseConf = HBaseConfiguration.create()

        //hbaseConf.set("hbase.zookeeper.quorum", "192.168.2.66")
        hbaseConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
        hbaseConf.set("hbase.zookeeper.property.clientPort", "2181")
        val hbaseConn = ConnectionFactory.createConnection(hbaseConf)
        val admin = hbaseConn.getAdmin
        ho = HbaseOb(admin, hbaseConf, hbaseConn)
    }

    /**
      * 创建一张表
      * @param tableName 表名
      * @param columnFarily 列族
      */
    def createTable(tableName: String, columnFarily: Array[String]): Unit = {

        val userTable = TableName.valueOf(tableName)
        if (ho.admin.tableExists(userTable)) {
            println(tableName + "存在！")
        } else {
            val tableDesc = new HTableDescriptor(userTable)
            for (cf <- columnFarily)
                tableDesc.addFamily(new HColumnDescriptor(cf.getBytes()))
            ho.admin.createTable(tableDesc)
            println("创建表成功！")
        }
    }

    /**
      * Hbase获取所有的表信息
      * @param unit kong
      * @return
      */
    def getAllTables(unit: Unit): List[String] = {
        val tables: List[String] = List()
        if (ho.admin != null) {
            try {
                val allTable = ho.admin.listTables()
                if (allTable != null) {
                    for (hTableDescriptor <- allTable) {
                        if (tables.size == 0) {
                            tables.++(hTableDescriptor.getNameAsString())
                            println(hTableDescriptor.getNameAsString())
                        }
                    }
                }


            } catch {
                case e: IOException =>
                    e.printStackTrace()
            }
        }

        tables
    }

    /**
      * Hbase中往某个表中添加一条记录
      * @param table 表名
      * @param key 行健
      * @param family 列族
      * @param col 列1
      * @param data 数据
      * @return 是否成功
      */
    def addOneRecord(table: Table, key: String, family: String,
                     col: String, data: String): Boolean = {
        //val userTable = TableName.valueOf(table)
        //val table1 = hbaseConn.getTable(userTable)

        var put = new Put(key.getBytes())
        put.addColumn(family.getBytes(), col.getBytes(), data.getBytes())
        try {
            table.put(put)
            println("插入数据条" + key + "成功！！！")
            true
        } catch {
            case e: IOException =>
                println("插入数据条" + key + "失败！！！")
                false;
        }
    }

    /**
      * Hbase表中记录信息的查询
      * @param table 表名
      * @param key 行健
      */
    def getValueFromKey(table: Table, key: String): Unit = {
        var list:List[String] = List()
        list = list :+ key
        val get = new Get(key.getBytes())
        try {
            val rs = table.get(get)
            if (rs.raw().length == 0) {
                println("不存在关键字为" + key + "的行！!")

            } else {
                println(rs.size())
                  for (cell <- rs.rawCells())
                  {
                      println(new String(CellUtil.cloneValue(cell)))
                      list = list :+ new String(CellUtil.cloneValue(cell))
                  }

                println(list)

            }
        } catch {
            case e: IOException =>
                e.printStackTrace()
        }
    }

    /**
      * 显示所有数据，通过Table Scan类获取已有表的信息
      * @param table 表名
      */
    def getAllData(table: Table, cf : String): Unit = {
        //val table = TableName.valueOf(tableName)
        //val tableDesc=new HTableDescriptor(userTable)
        val scan = new Scan()
        scan.setMaxVersions()
        scan.setBatch(1000)
        scan.addFamily(cf.getBytes())
        val rs = table.getScanner(scan)

        while (rs.next() != null)
        {
            println(rs)
            val r = rs.next()
            for (kv <- r.raw()) {
            System.out.println(new String(kv.getKey()) + new String(kv.getValue()))
            }
        }
    }

    /**
      * 获取包含某个字符串的数据，通过Table Scan类获取已有表的信息
      * @param table 表名
      * @param filter 包含指定字符串的过滤条件
      */
    def getDataByFilter(table: Table, filter:RowFilter): Unit = {
        val scan = new Scan()
        scan.setFilter(filter)
        val rs = table.getScanner(scan)

        var i = 0
        var flag = 0

        while (rs.next() != null)
        {
            var list:List[String] = List()
            val r = rs.next()
            for (cell <- r.rawCells()) {
                if (flag == 0) {
                    list = list :+ new String(CellUtil.cloneRow(cell))
                } else {
                    println(new String(CellUtil.cloneValue(cell)))
                    //list = list :+ new String(CellUtil.cloneValue(cell))
                }
            }
            flag = 0
            i = i + 1

        }
        println("count : " + i)
    }

    /**
      * Hbase表中记录信息的删除
      * @param table 表名
      * @param key 行健
      * @return
      */
    def deleteRecordByKey(table: Table, key: String): Boolean = {
        val de = new Delete(key.getBytes())
        try {
            table.delete(de)
            true;
        } catch {
            case e: IOException =>
                println("删除记录" + key + "异常！！！")
                false
        }
    }

    /**
      * Hbase中表的删除
      * @param table 表名
      * @return
      */
    def deleteTable(table: String): Boolean = {
        val userTable = TableName.valueOf(table)
        try {
            if (ho.admin.tableExists(userTable)) {
                ho.admin.disableTable(userTable)
                ho.admin.deleteTable(userTable)
                println("删除表" + table + "!!!")
            }
            true;
        } catch {
            case e: IOException =>
                e.printStackTrace()
                println("删除表" + table + "异常!!!")
                false
        }
    }

    /**
      * Hbase中表的删除
      * @param table 表名
      * @return
      */
    def truncateTable(table: String, ho:HbaseOb): Boolean = {
        val userTable = TableName.valueOf(table)
        try {
            if (ho.admin.tableExists(userTable)) {
                ho.admin.truncateTable(TableName.valueOf(table), true)
                println("清空表" + table + "!!!")
            }
            true;
        } catch {
            case e: IOException =>
                e.printStackTrace()
                println("清空表" + table + "异常!!!")
                false
        }
    }

    def test(): Unit = {
        hbaseInit()
        //val cf = Array("name", "age")
        //createTable("wuxc_test", cf)
        val userTable = TableName.valueOf("test_retain_terminal")
        val table = ho.hbaseConn.getTable(userTable)
        //deleteRecordByKey(table, "")
        println("=======start option=======")
        //addOneRecord(table, "id1", "name", "na", "Jack1")
        //getValueFromKey(table, "(小鹰桌面)bcec23a6b0bd2017-02-012017-03-01")
        val filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator("2017-02-012017-03-01"))
        getDataByFilter(table, filter)
        //createTable("test", cf)
        //getAllTables()
        //getAllData(table, "id1")

        //deleteTable("test")
    }

    def main(args: Array[String]): Unit =
    {
        test()
    }
}
package com.avcdata.spark.job.util

import java.util.concurrent.Executors

import com.avcdata.etl.common.pool.hbase.HBaseConnectionPool
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.rdd.RDD

/**
  * HBase工具类
  */

object HBaseUtils {

  val myConf = HBaseConfiguration.create()

  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection = {

    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf);
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tableName"))

    //    val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    //    myConf.set("hbase.zookeeper.quorum", "192.168.79.131")

//    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")

    myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")

    myConf.set("hbase.zookeeper.property.clientPort", "2181")

    myConf.set("hbase.regionserver.thread.compaction.large", "5")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    //    myConf.set("hbase.hregion.majorcompaction","0")

    myConf.set("hbase.hstore.compaction.min", "10")

    myConf.set("hbase.hstore.compaction.max", "10")

    myConf.set("hbase.hstore.blockingStoreFiles", "100")

    myConf.set("hbase.hstore.compactionThreshold", "7")

    myConf.set("hbase.regionserver.handler.count", "100")

    myConf.set("hbase.regionserver.hlog.splitlog.writer.threads", "10")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    myConf.set("hbase.regionserver.thread.compaction.large", "8")

    myConf.set("hbase.hregion.max.filesizes", "4G")

    myConf.set("hbase.hregion.max.filesize", "60G")

    myConf.set("hbase.hregion.memstore.flush.size", "60G")

    myConf.set("hbase.hregion.memstore.block.multiplier", "5")

    myConf.set("hbase.hstore.compaction.min", "7")

    //    myConf.set("hbase.client.write.buffer", "8388608")

    myConf.set("hbase.client.pause", "200")

    myConf.set("hbase.client.retries.number", "71")

    myConf.set("hbase.ipc.client.tcpnodelay", "false")

    myConf.set("hbase.client.scanner.caching", "500")

    myConf.set("hbase.htable.threads.max", Integer.MAX_VALUE.toString)

    myConf.set("hbase.htable.threads.keepalivetime", "60")

    //    myConf.set("hbase.defaults.for.version.skip", "true")

//    val threadPool = Executors.newFixedThreadPool(Runtime.getRuntime()
//      .availableProcessors())
//
//    ConnectionFactory.createConnection(myConf, threadPool)


    ConnectionFactory.createConnection(myConf);

//    HBaseConnectionPool.apply("192.168.1.11,192.168.1.12,192.168.1.13", "2181").connection

  }

  def getMutator(tableName: String): BufferedMutator = {
    getHbaseConn().getBufferedMutator(TableName.valueOf(tableName))
  }

  def saveToHbase(result: RDD[(String, Double)], tablename: String, column: String, item: String*) = {
    result.foreachPartition { x =>
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum",
        "compute000,compute001,compute002,compute003,compute004," +
          "compute005,compute006,compute007,compute008,compute009,compute010," +
          "compute011,compute012,compute013,compute014,compute015,compute016," +
          "compute017,compute018,compute019,compute020,compute021,compute022," +
          "compute023,compute024,compute025,compute026,compute027,compute028," +
          "compute029,compute030,compute031,compute032,compute033,compute034," +
          "compute035,compute036,compute037,compute038")
      myConf.set("hbase.master", "10.10.10.10:60000")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      myConf.set("hbase.defaults.for.version.skip", "true")
      val myTable = new HTable(myConf, tablename)
      myTable.setAutoFlush(false, false)
      myTable.setWriteBufferSize(3 * 1024 * 1024)

      x.foreach { f =>
        val p = new Put(Bytes.toBytes(f._1))
        for (k <- 0 until item.length) {
          p.addColumn(Bytes.toBytes(column), Bytes.toBytes(item(k)), Bytes.toBytes(f._2.toString))
        }
        myTable.put(p)
      }
      myTable.flushCommits()
    }
  }


  ///////////////////////////////////////////////////数据清洗///////////////////////////////////////////////////////////////

  /**
    * 获取terminal表的Put对象
    *
    * @param brand 厂家
    * @param sortedLine
    * @return
    */
  def getPut_terminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }

  /**
    * 获取SampleTerminal表的Put对象
    *
    * @param brand 厂家
    * @param sortedLine
    * @return
    */
  def getPut_SampleTerminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }


  /**
    * 获取SilentTerminal 沉默终端数的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_SilentTerminal(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + cols(4)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("apk_type"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(2)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("brand"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("license"), Bytes.toBytes(cols(4)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("province"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("silent_cnt"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("all_cnt"), Bytes.toBytes(cols(7)))

    put

  }

  /**
    * 获取EPG表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_epg(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    var i = 0
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("channel"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_date"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("start_end"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("time_range"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg_type"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_hour"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_min"), Bytes.toBytes(cols(i)))

    put

  }

  /**
    * 获取apk表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk(sortedLine: String, brand: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(3) + cols(0) + cols(1) + cols(2) + brand))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取plays_index表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_Plays_SearchIndex(sortedLine: String) = {


    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(3) + cols(4)))

    var i = 0
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("platform"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("category"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("index"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("time"), Bytes.toBytes(cols(i)))


    put


  }

  def getPut_apkGQ(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("logtime"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 龚琴推总测试表
    * sn,
    * apkPackage,
    * date,
    * SUM(duration) dura,
    * SUM(launchCnt) launchCnts
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk_gq(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("this_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))


    put

  }

  //获取到剧表的Put对象
  def getPut_Plays(brand: String, sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    // vid：title 剧名-集数
    val put = new Put(Bytes.toBytes(cols(5) + cols(0) + cols(12) + cols(4) + brand))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_title"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_awcid"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_part"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_vv"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_model"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_year"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_crowd"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_region"), Bytes.toBytes(cols(i)))

    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_name"), Bytes.toBytes(cols(i)))


    put

  }

  //获取到剧终端信息表的Put对象
  def getPut_Plays_Terminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }


  ///////////////////////////////////////推总/////////////////////////////////////////////////////////

  /**
    * 获取开关机 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_oc(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    //    val put = new Put(Bytes.toBytes(cols(0)))
    val put = new Put(Bytes.toBytes(cols(3) + cols(1) + cols(2) + cols(0).substring(cols(0).length - 2)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_day"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_time"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_length"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取直播 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_live(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(5) + cols(4) + cols(3) + cols(2) + cols(1) + cols(0).substring(cols(0).length - 2)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_channel"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_min"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_time_length"), Bytes.toBytes(cols(i)))

    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_time_length"), Bytes.toBytes(cols(i)))
    //    i = i + 1
    //
    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))


    put

  }


  /**
    * 获取APK 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_apk(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(4) + cols(3) + cols(2) + cols(1) + cols(0).substring(cols(0).length - 2)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1


    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }

}
package utils

import org.apache.hadoop.hbase.{HColumnDescriptor, HTableDescriptor}
import org.apache.spark.rdd.RDD
import org.apache.hadoop.hbase.client.{BufferedMutator, Connection}
//import org.apache.hadoop.hbase.client.{BufferedMutator, Connection, ConnectionFactory}
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}
import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}

/**
  * HBase工具类
  */
object HBaseUtils
{


  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection =
  {
    LoanPattern.using(HBaseConnectionPool("192.168.1.11,192.168.1.12,192.168.1.13", "2181"))
    { manageConn =>
      manageConn.connection
    }
  }

  def getMutator(tableName: String): BufferedMutator =
  {
    val admin = getHbaseConn().getAdmin
    val table = TableName.valueOf(tableName)
    if (!admin.tableExists(table))
    {
      //创建Hbase表模式
      val tableDescriptor = new HTableDescriptor(table)
      //创建列簇1    artitle
      tableDescriptor.addFamily(new HColumnDescriptor("status".getBytes()))
      //创建列簇2    author
      tableDescriptor.addFamily(new HColumnDescriptor("content".getBytes()))
      //创建表
      admin.createTable(tableDescriptor)
    }
    getHbaseConn().getBufferedMutator(TableName.valueOf(tableName))
  }

  def saveToHbase(result: RDD[(String, Double)], tablename: String, column: String, item: String*) =
  {
    result.foreachPartition
    { x =>
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum",
        "compute000,compute001,compute002,compute003,compute004," +
          "compute005,compute006,compute007,compute008,compute009,compute010," +
          "compute011,compute012,compute013,compute014,compute015,compute016," +
          "compute017,compute018,compute019,compute020,compute021,compute022," +
          "compute023,compute024,compute025,compute026,compute027,compute028," +
          "compute029,compute030,compute031,compute032,compute033,compute034," +
          "compute035,compute036,compute037,compute038"
      )
      myConf.set("hbase.master", "10.10.10.10:60000")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      myConf.set("hbase.defaults.for.version.skip", "true")
      val myTable = new HTable(myConf, tablename)
      myTable.setAutoFlush(false, false)
      myTable.setWriteBufferSize(3 * 1024 * 1024)

      x.foreach
      { f =>
        val p = new Put(Bytes.toBytes(f._1))
        for (k <- 0 until item.length)
        {
          p.addColumn(Bytes.toBytes(column), Bytes.toBytes(item(k)), Bytes.toBytes(f._2.toString))
        }
        myTable.put(p)
      }
      myTable.flushCommits()
    }
  }
}

object HBaseConnectionPool
{
  //数据库连接池
  private var connPool: Map[(String, String), ObjectPool[Connection]] = Map()

  sys.addShutdownHook
  {
    connPool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperQuorum: String, zookeeperClientPort: String): ManagedConnection =
  {
    val pool = connPool.getOrElse((zookeeperQuorum, zookeeperClientPort),
      {
        HBaseConnectionPool.synchronized[ObjectPool[Connection]]
          {
            connPool.getOrElse((zookeeperQuorum, zookeeperClientPort),
              {
                val p = new GenericObjectPool[Connection](new ConnectionFactory(zookeeperQuorum, zookeeperClientPort))
                connPool += (zookeeperQuorum, zookeeperClientPort) -> p

                p
              }
            )
          }
      }
    )

    new ManagedConnection(pool, pool.borrowObject())
  }
}

/**
  * 受管连接对象
  *
  * @param pool       池对象
  * @param connection 连接对象
  */
class ManagedConnection(private val pool: ObjectPool[Connection], val connection: Connection)
{
  def close() = pool.returnObject(connection)
}

/**
  * 连接池创建工场
  */
private class ConnectionFactory(zookeeperQuorum: String, zookeeperClientPort: String) extends BasePooledObjectFactory[Connection]
{
  override def create() =
  {
    val hBaseConfiguration = HBaseConfiguration.create()
    hBaseConfiguration.set("hbase.zookeeper.quorum", zookeeperQuorum)
    hBaseConfiguration.set("hbase.zookeeper.property.clientPort", zookeeperClientPort)

    ConnectionFactory.createConnection(hBaseConfiguration)
  }

  override def wrap(conn: Connection) = new DefaultPooledObject[Connection](conn)

  override def validateObject(po: PooledObject[Connection]) = !po.getObject.isClosed

  override def destroyObject(po: PooledObject[Connection]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Connection]) =
  {}
}
package com.avcdata.spark.job.util

import org.apache.hadoop.hbase._
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes

/**
  * HBase工具类
  */
object HBaseUtils {

  val myConf = HBaseConfiguration.create()

  val connection = getHbaseConn()

  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection = {


    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf);
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tableName"))


    //    val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    //    myConf.set("hbase.zookeeper.quorum", "192.168.79.131")

    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")

    myConf.set("hbase.zookeeper.property.clientPort", "2181")

    myConf.set("hbase.regionserver.thread.compaction.large", "5")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    //    myConf.set("hbase.hregion.majorcompaction","0")

    myConf.set("hbase.hstore.compaction.min", "10")

    myConf.set("hbase.hstore.compaction.max", "10")

    myConf.set("hbase.hstore.blockingStoreFiles", "100")

    myConf.set("hbase.hstore.compactionThreshold", "7")

    myConf.set("hbase.regionserver.handler.count", "100")

    myConf.set("hbase.regionserver.hlog.splitlog.writer.threads", "10")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    myConf.set("hbase.regionserver.thread.compaction.large", "8")

    myConf.set("hbase.hregion.max.filesizes", "4G")

    myConf.set("hbase.hregion.max.filesize", "60G")

    myConf.set("hbase.hregion.memstore.flush.size", "60G")

    myConf.set("hbase.hregion.memstore.block.multiplier", "5")

    //    myConf.set("hbase.client.write.buffer", "8388608")

    myConf.set("hbase.client.pause", "200")

    myConf.set("hbase.client.retries.number", "71")

    myConf.set("hbase.ipc.client.tcpnodelay", "false")

    myConf.set("hbase.client.scanner.caching", "500")

    myConf.set("hbase.htable.threads.max", Integer.MAX_VALUE.toString)

    myConf.set("hbase.htable.threads.keepalivetime", "60")

    //    val threadPool = Executors.newFixedThreadPool(Runtime.getRuntime()      .availableProcessors())

    //    ConnectionFactory.createConnection(myConf, threadPool)

    ConnectionFactory.createConnection(myConf)

    //     HBaseConnectionPool.apply("192.168.1.11,192.168.1.12,192.168.1.13", "2181").connection


  }

  def getMutator(tableName: String): BufferedMutator = {
    connection.getBufferedMutator(TableName.valueOf(tableName))
  }


  ///////////////////////////////////////////////////特征向量清洗///////////////////////////////////////////////////////////////

  /**
    * 获取UserVectorTerminal表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_UserVectorTerminal(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("brand"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("province"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pro_year"), Bytes.toBytes(cols(5)))

    put

  }


  /**
    * 获取UserVectorPlay表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_UserVectorPlay(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("stat_date"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("period"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pg_subject_dist"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pg_year_dist"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pg_region_dist"), Bytes.toBytes(cols(5)))

    put

  }

  /**
    * 获取user_vector_bh表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_UserVectorBh(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("stat_date"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("period"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("workday_oc_dist"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("restday_oc_dist"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("workday_channel_dist"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("restday_channel_dist"), Bytes.toBytes(cols(6)))

    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("workday_pg_sources_dist"), Bytes.toBytes(cols(7)))
    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("restday_pg_sources_dist"), Bytes.toBytes(cols(8)))

    put

  }

  /**
    * 获取user_vector_all表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_UserVectorAll(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("stat_date"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("period"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("brand"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("province"), Bytes.toBytes(cols(4)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("size"), Bytes.toBytes(cols(6)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("workday_oc_dist"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("restday_oc_dist"), Bytes.toBytes(cols(8)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("workday_channel_dist"), Bytes.toBytes(cols(9)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("restday_channel_dist"), Bytes.toBytes(cols(10)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pg_subject_dist"), Bytes.toBytes(cols(11)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pg_year_dist"), Bytes.toBytes(cols(12)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pg_region_dist"), Bytes.toBytes(cols(13)))


    put

  }


  /**
    * 获取UserTerminal用户画像最终表
    *
    * @param sortedLine
    * @return
    */
  def getPut_UserTerminal(sortedLine: String): Put = {

    //          info:sn,
    //          info:member_struct,
    //          info:member_id,
    //          info:time_range,
    //          info:brand,
    //          info:age_range,
    //          info:sex,
    //          info:area,
    //          info:province,
    //          info:city,
    //          info:citylevel,
    //          info:price,
    //          info:size,
    //          info:model,
    //          info:license


    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(3) + cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("member_struct"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("member_id"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("time_range"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("brand"), Bytes.toBytes(cols(4)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("age_range"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sex"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("province"), Bytes.toBytes(cols(8)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("city"), Bytes.toBytes(cols(9)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(10)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(cols(11)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("size"), Bytes.toBytes(cols(12)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("model"), Bytes.toBytes(cols(13)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("license"), Bytes.toBytes(cols(14)))

    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pro_year"), Bytes.toBytes(cols(5)))

    put

  }

  /**
    * 获取开关机、直播、apk所有的终端信息
    *
    * @param sortedLine
    * @return
    */
  def getPut_AllTerminal(sortedLine: String): Put = {


    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0)))

    val sn = cols(0)
    val brand = cols(1)
    val area = cols(2)
    val province = cols(3)
    val city = cols(4)
    val citylevel = cols(5)
    val size = cols(6)
    val model = cols(7)
    val license = cols(8)
    val price = cols(9)

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("brand"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("area"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("province"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("city"), Bytes.toBytes(cols(4)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("size"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("model"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("license"), Bytes.toBytes(cols(8)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(cols(9)))

    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("pro_year"), Bytes.toBytes(cols(5)))

    put

  }


}
package com.avcdata.vbox.util

import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.rdd.RDD

/**
  * HBase工具类
  */
object HBaseUtils {


  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection = {

    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf);
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tableName"))


    val myConf = HBaseConfiguration.create()

    //    val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    //    myConf.set("hbase.zookeeper.quorum", "192.168.79.131")

//    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")

    myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")

    myConf.set("hbase.zookeeper.property.clientPort", "2181")

    myConf.set("hbase.regionserver.thread.compaction.large", "5")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    //    myConf.set("hbase.hregion.majorcompaction","0")

    myConf.set("hbase.hstore.compaction.min", "10")

    myConf.set("hbase.hstore.compaction.max", "10")

    myConf.set("hbase.hstore.blockingStoreFiles", "100")

    myConf.set("hbase.hstore.compactionThreshold", "7")

    myConf.set("hbase.regionserver.handler.count", "100")

    myConf.set("hbase.regionserver.hlog.splitlog.writer.threads", "10")

    myConf.set("hbase.regionserver.thread.compaction.small", "5")

    myConf.set("hbase.regionserver.thread.compaction.large", "8")

    myConf.set("hbase.hregion.max.filesizes", "4G")

    myConf.set("hbase.hregion.max.filesize", "60G")

    myConf.set("hbase.hregion.memstore.flush.size", "60G")

    myConf.set("hbase.hregion.memstore.block.multiplier", "5")

    myConf.set("hbase.hstore.compaction.min", "7")

    //    myConf.set("hbase.client.write.buffer", "8388608")

    myConf.set("hbase.client.pause", "200")

    myConf.set("hbase.client.retries.number", "71")

    myConf.set("hbase.ipc.client.tcpnodelay", "false")

    myConf.set("hbase.client.scanner.caching", "500")


    //    myConf.set("hbase.defaults.for.version.skip", "true")


    ConnectionFactory.createConnection(myConf);

  }

  def getMutator(tableName: String): BufferedMutator = {
    getHbaseConn().getBufferedMutator(TableName.valueOf(tableName))
  }

  def saveToHbase(result: RDD[(String, Double)], tablename: String, column: String, item: String*) = {
    result.foreachPartition { x =>
      val myConf = HBaseConfiguration.create()
      myConf.set("hbase.zookeeper.quorum",
        "compute000,compute001,compute002,compute003,compute004," +
          "compute005,compute006,compute007,compute008,compute009,compute010," +
          "compute011,compute012,compute013,compute014,compute015,compute016," +
          "compute017,compute018,compute019,compute020,compute021,compute022," +
          "compute023,compute024,compute025,compute026,compute027,compute028," +
          "compute029,compute030,compute031,compute032,compute033,compute034," +
          "compute035,compute036,compute037,compute038")
      myConf.set("hbase.master", "10.10.10.10:60000")
      myConf.set("hbase.zookeeper.property.clientPort", "2181")
      myConf.set("hbase.defaults.for.version.skip", "true")
      val myTable = new HTable(myConf, tablename)
      myTable.setAutoFlush(false, false)
      myTable.setWriteBufferSize(3 * 1024 * 1024)

      x.foreach { f =>
        val p = new Put(Bytes.toBytes(f._1))
        for (k <- 0 until item.length) {
          p.addColumn(Bytes.toBytes(column), Bytes.toBytes(item(k)), Bytes.toBytes(f._2.toString))
        }
        myTable.put(p)
      }
      myTable.flushCommits()
    }
  }


  ///////////////////////////////////////////////////数据清洗///////////////////////////////////////////////////////////////

  /**
    * 获取terminal表的Put对象
    *
    * @param brand 厂家
    * @param sortedLine
    * @return
    */
  def getPut_terminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }


  def getPut_terminal_tcl(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }

  /**
    * 获取SampleTerminal表的Put对象
    *
    * @param brand 厂家
    * @param sortedLine
    * @return
    */
  def getPut_SampleTerminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }


  /**
    * 获取SilentTerminal 沉默终端数的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_SilentTerminal(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + cols(4)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("apk_type"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(2)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("brand"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("license"), Bytes.toBytes(cols(4)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("province"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("silent_cnt"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("all_cnt"), Bytes.toBytes(cols(7)))

    put

  }

  /**
    * 获取EPG表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_epg(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    var i = 0
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("channel"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_date"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("start_end"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("time_range"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg_type"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_hour"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_min"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取开关机清洗的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_oc(sortedLine: String, brand: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    //    val put = new Put(Bytes.toBytes(cols(0)))
    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + brand))

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("power_on_day"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("power_on_time"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("cnt"), Bytes.toBytes(cols(i)))

    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("power_on_length"), Bytes.toBytes(cols(i)))



    put

  }

  /**
    * 获取apk表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk(sortedLine: String, brand: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(3) + cols(0) + cols(1) + cols(2) + brand))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取plays_index表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_Plays_SearchIndex(sortedLine: String) = {


    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(3) + cols(4)))

    var i = 0
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("platform"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("category"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("index"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("time"), Bytes.toBytes(cols(i)))


    put


  }

  def getPut_apkGQ(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("logtime"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 龚琴推总测试表
    * sn,
    * apkPackage,
    * date,
    * SUM(duration) dura,
    * SUM(launchCnt) launchCnts
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk_gq(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("this_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))


    put

  }

  //获取到剧表的Put对象
  def getPut_Plays(brand: String, sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    // vid：title 剧名-集数
    val put = new Put(Bytes.toBytes(cols(5) + cols(0) + cols(12) + cols(4) + brand))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_title"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_awcid"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_part"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_vv"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_model"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_year"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_crowd"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_region"), Bytes.toBytes(cols(i)))

    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_name"), Bytes.toBytes(cols(i)))


    put

  }

  //获取到剧终端信息表的Put对象
  def getPut_Plays_Terminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }


  ///////////////////////////////////////推总/////////////////////////////////////////////////////////

  /**
    * 获取开关机 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_oc(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    //    val put = new Put(Bytes.toBytes(cols(0)))
    val put = new Put(Bytes.toBytes(cols(3) + cols(1) + cols(2) + cols(0).substring(cols(0).length - 2)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_day"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_time"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_length"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取直播 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_live(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(5) + cols(4) + cols(3) + cols(2) + cols(1) + cols(0).substring(cols(0).length - 2)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_channel"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_min"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_time_length"), Bytes.toBytes(cols(i)))

    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_time_length"), Bytes.toBytes(cols(i)))
    //    i = i + 1
    //
    //    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))


    put

  }


  /**
    * 获取APK 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_apk(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(4) + cols(3) + cols(2) + cols(1) + cols(0).substring(cols(0).length - 2)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1


    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }

}
package com.avcdata.spark.job.until

import org.apache.hadoop.hbase.client.{BufferedMutator, Connection, ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}

/**
  * HBase工具类
  */
object HBaseUtils_after_01 {

  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection = {

    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf);
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tableName"))


    val myConf = HBaseConfiguration.create()

    //    val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    //    myConf.set("hbase.zookeeper.quorum", "192.168.79.131")

    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")

    myConf.set("hbase.zookeeper.property.clientPort", "2181")

    ConnectionFactory.createConnection(myConf);

  }

  def getMutator(tableName: String): BufferedMutator = {
    getHbaseConn().getBufferedMutator(TableName.valueOf(tableName))
  }


  ///////////////////////////////////////////////////数据清洗///////////////////////////////////////////////////////////////

  /**
    * 获取terminal表的Put对象
    *
    * @param brand 厂家
    * @param sortedLine
    * @return
    */
  def getPut_terminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }

  /**
    * 获取EPG表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_epg(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    var i = 0
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("channel"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_date"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("start_end"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("time_range"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg_type"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_hour"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_min"), Bytes.toBytes(cols(i)))

    put

  }

  /**
    * 获取apk表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }

  def getPut_apkGQ(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("logtime"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 龚琴推总测试表
    * sn,
    * apkPackage,
    * date,
    * SUM(duration) dura,
    * SUM(launchCnt) launchCnts
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk_gq(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("this_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))


    put

  }

  //获取到剧表的Put对象
  def getPut_Plays(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    // vid：title 剧名-集数
    val put = new Put(Bytes.toBytes(cols(0) + cols(3) + cols(4) + cols(5) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_title"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_awcid"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_part"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_vv"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put

  }


  ///////////////////////////////////////推总/////////////////////////////////////////////////////////

  /**
    * 获取开关机 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_oc(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(0)+cols(1)+cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_day"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_time"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_length"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取直播 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_live(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(0)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_channel"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_min"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_time_length"), Bytes.toBytes(cols(i)))


    put

  }


  /**
    * 获取APK 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_apk(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1


    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }

}
package com.avcdata.spark.job.until

import org.apache.hadoop.hbase.client.{BufferedMutator, Connection, ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}

/**
  * HBase工具类
  */
object HBaseUtils_after_01 {

  /**
    * 获取Hbase连接
    *
    * @return
    */
  def getHbaseConn(): Connection = {

    //      val myConf = HBaseConfiguration.create()
    //      myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
    //      myConf.set("hbase.zookeeper.property.clientPort", "2181")
    //      val hbaseConn = ConnectionFactory.createConnection(myConf);
    //      val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tableName"))


    val myConf = HBaseConfiguration.create()

    //    val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")
    //    myConf.set("hbase.zookeeper.quorum", "192.168.79.131")

    myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")

    myConf.set("hbase.zookeeper.property.clientPort", "2181")

    ConnectionFactory.createConnection(myConf);

  }

  def getMutator(tableName: String): BufferedMutator = {
    getHbaseConn().getBufferedMutator(TableName.valueOf(tableName))
  }


  ///////////////////////////////////////////////////数据清洗///////////////////////////////////////////////////////////////

  /**
    * 获取terminal表的Put对象
    *
    * @param brand 厂家
    * @param sortedLine
    * @return
    */
  def getPut_terminal(brand: String, sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(6) + brand))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("license"), Bytes.toBytes(cols(0)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("province"), Bytes.toBytes(cols(1)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("last_poweron"), Bytes.toBytes(cols(2)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("model"), Bytes.toBytes(cols(3)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("size"), Bytes.toBytes(cols(4)))

    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("city"), Bytes.toBytes(cols(5)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("brand"), Bytes.toBytes(brand))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("sn"), Bytes.toBytes(cols(6)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("area"), Bytes.toBytes(cols(7)))
    put.addColumn(Bytes.toBytes("terminalProperty"), Bytes.toBytes("citylevel"), Bytes.toBytes(cols(8)))

    put

  }

  /**
    * 获取EPG表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_epg(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    var i = 0
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("channel"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_date"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("start_end"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("time_range"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg_type"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("pg"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_hour"), Bytes.toBytes(cols(i)))

    i = i + 1
    put.addColumn(Bytes.toBytes("result"), Bytes.toBytes("tv_min"), Bytes.toBytes(cols(i)))

    put

  }

  /**
    * 获取apk表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }

  def getPut_apkGQ(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("logtime"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 龚琴推总测试表
    * sn,
    * apkPackage,
    * date,
    * SUM(duration) dura,
    * SUM(launchCnt) launchCnts
    *
    * @param sortedLine
    * @return
    */
  def getPut_apk_gq(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    //    val result = sn + "\t" + apkPackage + "\t" + date + "\t" + hour + "\t" + launchCnt + "\t" + duration


    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("this_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))


    put

  }

  //获取到剧表的Put对象
  def getPut_Plays(sortedLine: String): Put = {

    //    val dimAreaCol = Bytes.toBytes("dim_area")

    val cols = sortedLine.split('\t')

    //    val put = new Put(Bytes.toBytes(cols(1) + "-" + cols(2) + "-" + cols(0) + "-" + cols(6) + "-" + cols(7)))

    // vid：title 剧名-集数
    val put = new Put(Bytes.toBytes(cols(0) + cols(3) + cols(4) + cols(5) + "CC"))

    var i = 0
    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_title"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_awcid"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_part"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("dim"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_vv"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("fact"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))
    i = i + 1

    put

  }


  ///////////////////////////////////////推总/////////////////////////////////////////////////////////

  /**
    * 获取开关机 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_oc(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(0)+cols(1)+cols(2)))

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_day"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_time"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("power_on_length"), Bytes.toBytes(cols(i)))

    put

  }


  /**
    * 获取直播 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_live(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    var i = 0

    val put = new Put(Bytes.toBytes(cols(0)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_channel"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_min"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_time_length"), Bytes.toBytes(cols(i)))


    put

  }


  /**
    * 获取APK 推总表的Put对象
    *
    * @param sortedLine
    * @return
    */
  def getPut_total_apk(sortedLine: String): Put = {

    val cols = sortedLine.split('\t')

    val put = new Put(Bytes.toBytes(cols(0) + cols(1) + cols(2) + cols(3) + "CC"))

    var i = 0

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_sn"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_apk"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_date"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("dim_hour"), Bytes.toBytes(cols(i)))
    i = i + 1


    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_cnt"), Bytes.toBytes(cols(i)))
    i = i + 1

    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("fact_duration"), Bytes.toBytes(cols(i)))

    put

  }

}
package com.avcdata.spark.job.view


import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs._

import scala.collection.mutable.HashSet


/**
  * @author Administrator
  *
  */
class Hdfsapi {
  val conf = new Configuration();
  //    conf.addResource(new Path("D:\\myeclipse\\Hadoop\\hadoopEx\\src\\conf\\hadoop.xml"));
  val hdfs = FileSystem.get(conf);

  //    var hashset = new HashSet[Path]()

  def deleteFile(path: String) = {
    val syspath = new Path(path)
    var success = false
    //     f - the path to delete.
    //recursive - if path is a directory and set to true, the directory is deleted else throws an exception.
    //In case of a file the recursive can be set to either true or false.
    if (this.isDirectory(syspath) == true) {
      success = hdfs.delete(syspath, true)
    } else {
      success = hdfs.delete(syspath)
    }
    success
  }


  /**
    * return a list of file Paths
    * need digui visit
    */
  def traverseFiles(folder: String, hashset: HashSet[Path]): HashSet[Path] = {

    // this method just return the child files
    // val listfilesStatus = hdfs.listStatus(new Path(folder), new DefaultFilter())
    //
    //
    //    val listfilesStatus = hdfs.listStatus(new Path(folder), new DefaultFilter())
    //
    //    var hashset = new HashSet[Path]()
    //
    //    for(filestatus <- listfilesStatus)
    //    {
    //
    //      hashset.add(filestatus.getPath())
    //
    //      println(filestatus.getPath())
    //    }


    //  wrong
    //    val iter = listfilesStatus.
    //    while(  listfilesStatus.hasNext() == true )
    //    {
    //
    //      hashset.add(listfilesStatus.next().getPath())
    //
    //    }


    //    var list  = new List()
    val listfilesStatus = hdfs.listStatus(new Path(folder), new DefaultFilter())

    //    var hashset = new HashSet[Path]()
    for (filestatus <- listfilesStatus) {
      val filepath = filestatus.getPath()
      if (hdfs.isFile(filepath) == true) {
        if (filepath.getName().startsWith("part") == true) {
          hashset.add(filestatus.getPath())
        }
      } else {
        traverseFilepath(filepath, hashset)
      }
      //      println(filestatus.getPath())
    }
    hashset
  }


  /**
    * need a result type  :HashSet[Path]
    */
  def traverseFilepath(path: Path, hashset: HashSet[Path]): HashSet[Path] = {
    //    var list  = new List()
    val listfilesStatus = hdfs.listStatus(path, new DefaultFilter())
    //    var hashset = new HashSet[Path]()
    for (filestatus <- listfilesStatus) {
      val filepath = filestatus.getPath()

      if (hdfs.isFile(filepath) == true) {

        if (filepath.getName().startsWith("part") == true) {

          hashset.add(filestatus.getPath())

        }
      } else {
        traverseFilepath(filepath, hashset)
      }
      //      println(filestatus.getPath())
    }

    //    val iter = listfilesStatus.
    //    while(  listfilesStatus.hasNext() == true )
    //    {
    //
    //      hashset.add(listfilesStatus.next().getPath())
    //
    //    }

    hashset
  }


  def copyFiles(frompath: String, topath: String, overrideExist: Boolean): Boolean = {
    // if the frompath or the topath is not valate, it should throw a exception
    val sourcepath = new Path(frompath)
    val destpath = new Path(topath)
    //    println(destpath.getName())
    val sourceInputStream = hdfs.open(sourcepath)
    val destOutputStream = this.createFile(destpath)
    var buffer = new Array[Byte](64000000)
    var len = sourceInputStream.read(buffer)
    //while(  (len = sourceInputStream.read( buffer ) != -1 )) warning: unit !=  int  will always true ===> functional programming
    while (len != -1) {
      println("buffer lenth:" + buffer.length + "   len :" + len)
      destOutputStream.write(buffer, 0, len - 1)
      len = sourceInputStream.read(buffer)
    }
    destOutputStream.hflush()
    sourceInputStream.close()
    destOutputStream.close()
    true
  }

  def copyFiles(frompath: Path, topath: Path): Boolean = {
    //    println(destpath.getName())
    val sourceInputStream = hdfs.open(frompath)
    val destOutputStream = this.createFile(topath)
    var buffer = new Array[Byte](64000000)
    var len = sourceInputStream.read(buffer)

    //while(  (len = sourceInputStream.read( buffer ) != -1 )) warning: unit !=  int  will always true ===> functional programming
    while (len != -1) {
      println("buffer lenth:" + buffer.length + "   len :" + len)
      destOutputStream.write(buffer, 0, len - 1)
      len = sourceInputStream.read(buffer)
    }

    destOutputStream.hflush()
    sourceInputStream.close()
    destOutputStream.close()
    true
  }


  /**
    * copy files from a set which includes all the source paths  to another
    */
  def copyFiles(source: HashSet[Path], destfolder: Path): Boolean = {
    for (path <- source) {
      //      var dest = destfolder.suffix("/").suffix(path.getName())
      var dest = destfolder.toString() + "/" + path.getName()
      println(dest.toString())
      copyFiles(path, new Path(dest))
    }
    true
  }


  /**
    * copy files from one folder to another
    */
  def copyFiles(sourceFolder: String, destfolder: String): Boolean = {
    var hashset = new HashSet[Path]()
    hashset = traverseFiles(sourceFolder, hashset)
    copyFiles(hashset, new Path(destfolder))
    true
  }


  def isFile(path: String) = {
    val syspath = new Path(path)
    hdfs.isFile(syspath)
  }


  def isFile(path: Path) = {
    hdfs.isFile(path)
  }


  def isDirectory(path: String) = {
    val syspath = new Path(path)
    hdfs.isDirectory(syspath)
  }


  def isDirectory(path: Path) = {
    hdfs.isDirectory(path)
  }


  def isValidatePath(path: String) {
    val syspath = new Path(path)
    //     syspath.
  }


  def isexist(path: String) = {
    val syspath = new Path(path)
    hdfs.exists(syspath)
  }


  def isexist(path: Path) = {
    hdfs.exists(path)

  }


  def uploadFiles(frompath: String, toPath: String) = {}

  def uploadFiles(frompath: Path, toPath: Path) = {}

  def dowmloadFiles() = {}

  def createFile(path: String) = {
    val syspath = new Path(path)
    hdfs.create(syspath)
  }

  def createFile(path: Path) = {
    hdfs.create(path)
  }


  def renameFile(path: String, newptah: String) = {}

  def getModifyyTIme(path: String) = {}

  def getHostName(path: String) = {
  }

  def ls(fileSystem: FileSystem, path: String) = {

    println("list path:" + path)
    val fs = fileSystem.listStatus(new Path(path))
    val listPath = FileUtil.stat2Paths(fs)
    for (p <- listPath) {
      println(p)
    }
    println("----------------------------------------")
  }

  def main(args: Array[String]) {
//    val uri = args[0];
    //    val conf = new Configuration();
    //    val hdfs = FileSystem.get(URI.create(uri), conf);
    //    val fs = hdfs.listStatus(new Path(args[0]));
    //    val paths = FileUtil.stat2Paths(fs);
    //
    //    for (Path p : paths)
    //    System.out.println(p);

    val conf = new Configuration()
    //val hdfsCoreSitePath = new Path("core-site.xml")
    // val hdfsHDFSSitePath = new Path("hdfs-site.xml")
    //conf.addResource(hdfsCoreSitePath)
    //conf.addResource(hdfsHDFSSitePath)
    println(conf) //Configuration: core-default.xml, core-site.xml
    //根据这个输出,在这个程序进来之前,conf已经被设置过了

    //目前我知道,定位具体的hdfs的位置,有两种方式
    //一种是在conf配置,一个域名可以绑定多个ip.我们通过这个域名来定位hdfs.
    //另一种是在调用FileSystem.get时指定一个域名或者一个ip,当然仅限一个.

    val fileSystem = FileSystem.get(conf)
    //如果conf设置了hdfs的host和port,此处可以不写
    //hadoop的配置都是一层一层的,后面的会覆盖前面的.

    //String HDFS="hdfs://localhost:9000";
    //FileSystem hdfs = FileSystem.get(URI.create(HDFS),conf);
    //这种写法 只能用一个ip或者域名了.不推荐.

    ls(fileSystem, "/")
    ls(fileSystem, ".")
    ls(fileSystem, "svd")

  }
}


class DefaultFilter extends PathFilter {
  override def accept(path: Path) = {
    //filter the files not endwith "parquent"
    true
  }
}package com.avcdata.vbox.view

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs._

import scala.collection.mutable.HashSet


/**
  * @author Administrator
  *
  */
class Hdfsapi {
  val conf = new Configuration();
  //    conf.addResource(new Path("D:\\myeclipse\\Hadoop\\hadoopEx\\src\\conf\\hadoop.xml"));
  val hdfs = FileSystem.get(conf);

  //    var hashset = new HashSet[Path]()

  def deleteFile(path: String) = {
    val syspath = new Path(path)
    var success = false
    //     f - the path to delete.
    //recursive - if path is a directory and set to true, the directory is deleted else throws an exception.
    //In case of a file the recursive can be set to either true or false.
    if (this.isDirectory(syspath) == true) {
      success = hdfs.delete(syspath, true)
    } else {
      success = hdfs.delete(syspath)
    }
    success
  }


  /**
    * return a list of file Paths
    * need digui visit
    */
  def traverseFiles(folder: String, hashset: HashSet[Path]): HashSet[Path] = {

    // this method just return the child files
    // val listfilesStatus = hdfs.listStatus(new Path(folder), new DefaultFilter())
    //
    //
    //    val listfilesStatus = hdfs.listStatus(new Path(folder), new DefaultFilter())
    //
    //    var hashset = new HashSet[Path]()
    //
    //    for(filestatus <- listfilesStatus)
    //    {
    //
    //      hashset.add(filestatus.getPath())
    //
    //      println(filestatus.getPath())
    //    }


    //  wrong
    //    val iter = listfilesStatus.
    //    while(  listfilesStatus.hasNext() == true )
    //    {
    //
    //      hashset.add(listfilesStatus.next().getPath())
    //
    //    }


    //    var list  = new List()
    val listfilesStatus = hdfs.listStatus(new Path(folder), new DefaultFilter())

    //    var hashset = new HashSet[Path]()
    for (filestatus <- listfilesStatus) {
      val filepath = filestatus.getPath()
      if (hdfs.isFile(filepath) == true) {
        if (filepath.getName().startsWith("part") == true) {
          hashset.add(filestatus.getPath())
        }
      } else {
        traverseFilepath(filepath, hashset)
      }
      //      println(filestatus.getPath())
    }
    hashset
  }


  /**
    * need a result type  :HashSet[Path]
    */
  def traverseFilepath(path: Path, hashset: HashSet[Path]): HashSet[Path] = {
    //    var list  = new List()
    val listfilesStatus = hdfs.listStatus(path, new DefaultFilter())
    //    var hashset = new HashSet[Path]()
    for (filestatus <- listfilesStatus) {
      val filepath = filestatus.getPath()

      if (hdfs.isFile(filepath) == true) {

        if (filepath.getName().startsWith("part") == true) {

          hashset.add(filestatus.getPath())

        }
      } else {
        traverseFilepath(filepath, hashset)
      }
      //      println(filestatus.getPath())
    }

    //    val iter = listfilesStatus.
    //    while(  listfilesStatus.hasNext() == true )
    //    {
    //
    //      hashset.add(listfilesStatus.next().getPath())
    //
    //    }

    hashset
  }


  def copyFiles(frompath: String, topath: String, overrideExist: Boolean): Boolean = {
    // if the frompath or the topath is not valate, it should throw a exception
    val sourcepath = new Path(frompath)
    val destpath = new Path(topath)
    //    println(destpath.getName())
    val sourceInputStream = hdfs.open(sourcepath)
    val destOutputStream = this.createFile(destpath)
    var buffer = new Array[Byte](64000000)
    var len = sourceInputStream.read(buffer)
    //while(  (len = sourceInputStream.read( buffer ) != -1 )) warning: unit !=  int  will always true ===> functional programming
    while (len != -1) {
      println("buffer lenth:" + buffer.length + "   len :" + len)
      destOutputStream.write(buffer, 0, len - 1)
      len = sourceInputStream.read(buffer)
    }
    destOutputStream.hflush()
    sourceInputStream.close()
    destOutputStream.close()
    true
  }

  def copyFiles(frompath: Path, topath: Path): Boolean = {
    //    println(destpath.getName())
    val sourceInputStream = hdfs.open(frompath)
    val destOutputStream = this.createFile(topath)
    var buffer = new Array[Byte](64000000)
    var len = sourceInputStream.read(buffer)

    //while(  (len = sourceInputStream.read( buffer ) != -1 )) warning: unit !=  int  will always true ===> functional programming
    while (len != -1) {
      println("buffer lenth:" + buffer.length + "   len :" + len)
      destOutputStream.write(buffer, 0, len - 1)
      len = sourceInputStream.read(buffer)
    }

    destOutputStream.hflush()
    sourceInputStream.close()
    destOutputStream.close()
    true
  }


  /**
    * copy files from a set which includes all the source paths  to another
    */
  def copyFiles(source: HashSet[Path], destfolder: Path): Boolean = {
    for (path <- source) {
      //      var dest = destfolder.suffix("/").suffix(path.getName())
      var dest = destfolder.toString() + "/" + path.getName()
      println(dest.toString())
      copyFiles(path, new Path(dest))
    }
    true
  }


  /**
    * copy files from one folder to another
    */
  def copyFiles(sourceFolder: String, destfolder: String): Boolean = {
    var hashset = new HashSet[Path]()
    hashset = traverseFiles(sourceFolder, hashset)
    copyFiles(hashset, new Path(destfolder))
    true
  }


  def isFile(path: String) = {
    val syspath = new Path(path)
    hdfs.isFile(syspath)
  }


  def isFile(path: Path) = {
    hdfs.isFile(path)
  }


  def isDirectory(path: String) = {
    val syspath = new Path(path)
    hdfs.isDirectory(syspath)
  }


  def isDirectory(path: Path) = {
    hdfs.isDirectory(path)
  }


  def isValidatePath(path: String) {
    val syspath = new Path(path)
    //     syspath.
  }


  def isexist(path: String) = {
    val syspath = new Path(path)
    hdfs.exists(syspath)
  }


  def isexist(path: Path) = {
    hdfs.exists(path)

  }


  def uploadFiles(frompath: String, toPath: String) = {}

  def uploadFiles(frompath: Path, toPath: Path) = {}

  def dowmloadFiles() = {}

  def createFile(path: String) = {
    val syspath = new Path(path)
    hdfs.create(syspath)
  }

  def createFile(path: Path) = {
    hdfs.create(path)
  }


  def renameFile(path: String, newptah: String) = {}

  def getModifyyTIme(path: String) = {}

  def getHostName(path: String) = {
  }

  def ls(fileSystem: FileSystem, path: String) = {

    println("list path:" + path)
    val fs = fileSystem.listStatus(new Path(path))
    val listPath = FileUtil.stat2Paths(fs)
    for (p <- listPath) {
      println(p)
    }
    println("----------------------------------------")
  }

  def main(args: Array[String]) {
//    val uri = args[0];
    //    val conf = new Configuration();
    //    val hdfs = FileSystem.get(URI.create(uri), conf);
    //    val fs = hdfs.listStatus(new Path(args[0]));
    //    val paths = FileUtil.stat2Paths(fs);
    //
    //    for (Path p : paths)
    //    System.out.println(p);

    val conf = new Configuration()
    //val hdfsCoreSitePath = new Path("core-site.xml")
    // val hdfsHDFSSitePath = new Path("hdfs-site.xml")
    //conf.addResource(hdfsCoreSitePath)
    //conf.addResource(hdfsHDFSSitePath)
    println(conf) //Configuration: core-default.xml, core-site.xml
    //根据这个输出,在这个程序进来之前,conf已经被设置过了

    //目前我知道,定位具体的hdfs的位置,有两种方式
    //一种是在conf配置,一个域名可以绑定多个ip.我们通过这个域名来定位hdfs.
    //另一种是在调用FileSystem.get时指定一个域名或者一个ip,当然仅限一个.

    val fileSystem = FileSystem.get(conf)
    //如果conf设置了hdfs的host和port,此处可以不写
    //hadoop的配置都是一层一层的,后面的会覆盖前面的.

    //String HDFS="hdfs://localhost:9000";
    //FileSystem hdfs = FileSystem.get(URI.create(HDFS),conf);
    //这种写法 只能用一个ip或者域名了.不推荐.

    ls(fileSystem, "/")
    ls(fileSystem, ".")
    ls(fileSystem, "svd")

  }
}


class DefaultFilter extends PathFilter {
  override def accept(path: Path) = {
    //filter the files not endwith "parquent"
    true
  }
}package com.avcdata.etl.common.util

import java.io.{BufferedReader, InputStreamReader}
import java.nio.charset.Charset
import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}


/**
  * HDFS文件读取
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 14:08
  */
object HdfsFileUtil
{
  /**
    * 自适应读取HDFS或本地系统文件
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件内容
    */
  def read(filePath: String, conf: Configuration = new Configuration()): Seq[String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.getFileStatus(path).isFile)
    {
      LoanPattern.using(new BufferedReader(new InputStreamReader(fs.open(path))))
      { reader => Stream.continually(reader.readLine()).takeWhile(_ != null).toArray.toSeq }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 读取配置文件为MAP
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件流
    */
  def readPropertiesToMap(filePath: String, conf: Configuration = new Configuration()): Map[String, String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.isFile(path))
    {
      LoanPattern.using(fs.open(path).getWrappedStream)
      { in =>
        val fileConfigProps = new Properties()
        fileConfigProps.load(in)

        //集合隐式转换
        import scala.collection.JavaConversions._
        fileConfigProps.toMap
      }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 删除文件
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 删除是否成功
    */
  def delete(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      val path = new Path(filePath)

      fs.delete(path, true)
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 重命名文件/目录
    *
    * @param src  源文件/目录
    * @param dest 目的文件/目录
    * @param conf Hadoop配置
    * @return 重命名是否成功
    */
  def rename(src: String, dest: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      fs.rename(new Path(src), new Path(dest))
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 判断文件是否存在
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 文件是否存在
    */
  def exists(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    FileSystem.get(conf).exists(new Path(filePath))
  }

  /**
    * 判断当前文件夹下是否存在子文件
    *
    * @param filePath 文件夹路径
    * @param conf     配置对象
    * @return 是否存在子文件
    */
  def existsChildFile(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val path = new Path(filePath)
    if (FileSystem.get(conf).isDirectory(path)) FileSystem.get(conf).listFiles(path, false).hasNext else false
  }

  /**
    * 追加数据至指定的文件
    *
    * @param filePath 文件路径
    * @param fileName 文件名
    * @param line     需要追加的内容
    */
  def appendLine(filePath: String, fileName: String, line: String, conf: Configuration = new Configuration()): Unit =
  {
    val fs = FileSystem.get(conf)

    if (!fs.exists(new Path(filePath)))
    {
      fs.mkdirs(new Path(filePath))
    }

    val saveFilePath = new Path(s"$filePath/$fileName")
    if (!fs.exists(saveFilePath))
    {
      fs.createNewFile(saveFilePath)
    }

    LoanPattern.using(fs.append(saveFilePath))
    { out =>

      out.write((line + "\n").getBytes(Charset.forName("UTF-8")))
      out.flush()
    }
  }
}
package com.avcdata.etl.common.util

import java.io.{BufferedReader, InputStreamReader}
import java.nio.charset.Charset
import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}


/**
  * HDFS文件读取
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 14:08
  */
object HdfsFileUtil
{
  /**
    * 自适应读取HDFS或本地系统文件
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件内容
    */
  def read(filePath: String, conf: Configuration = new Configuration()): Seq[String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.getFileStatus(path).isFile)
    {
      LoanPattern.using(new BufferedReader(new InputStreamReader(fs.open(path))))
      { reader => Stream.continually(reader.readLine()).takeWhile(_ != null).toArray.toSeq }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 读取配置文件为MAP
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件流
    */
  def readPropertiesToMap(filePath: String, conf: Configuration = new Configuration()): Map[String, String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.isFile(path))
    {
      LoanPattern.using(fs.open(path).getWrappedStream)
      { in =>
        val fileConfigProps = new Properties()
        fileConfigProps.load(in)

        //集合隐式转换
        import scala.collection.JavaConversions._
        fileConfigProps.toMap
      }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 删除文件
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 删除是否成功
    */
  def delete(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      val path = new Path(filePath)

      fs.delete(path, true)
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 重命名文件/目录
    *
    * @param src  源文件/目录
    * @param dest 目的文件/目录
    * @param conf Hadoop配置
    * @return 重命名是否成功
    */
  def rename(src: String, dest: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      fs.rename(new Path(src), new Path(dest))
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 判断文件是否存在
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 文件是否存在
    */
  def exists(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    FileSystem.get(conf).exists(new Path(filePath))
  }

  /**
    * 判断当前文件夹下是否存在子文件
    *
    * @param filePath 文件夹路径
    * @param conf     配置对象
    * @return 是否存在子文件
    */
  def existsChildFile(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val path = new Path(filePath)
    if (FileSystem.get(conf).isDirectory(path)) FileSystem.get(conf).listFiles(path, false).hasNext else false
  }

  /**
    * 追加数据至指定的文件
    *
    * @param filePath 文件路径
    * @param fileName 文件名
    * @param line     需要追加的内容
    */
  def appendLine(filePath: String, fileName: String, line: String, conf: Configuration = new Configuration()): Unit =
  {
    val fs = FileSystem.get(conf)

    if (!fs.exists(new Path(filePath)))
    {
      fs.mkdirs(new Path(filePath))
    }

    val saveFilePath = new Path(s"$filePath/$fileName")
    if (!fs.exists(saveFilePath))
    {
      fs.createNewFile(saveFilePath)
    }

    LoanPattern.using(fs.append(saveFilePath))
    { out =>

      out.write((line + "\n").getBytes(Charset.forName("UTF-8")))
      out.flush()
    }
  }
}
package com.avcdata.etl.common.util

import java.io.{BufferedReader, InputStreamReader}
import java.nio.charset.Charset
import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}


/**
  * HDFS文件读取
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 14:08
  */
object HdfsFileUtil
{
  /**
    * 自适应读取HDFS或本地系统文件
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件内容
    */
  def read(filePath: String, conf: Configuration = new Configuration()): Seq[String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.getFileStatus(path).isFile)
    {
      LoanPattern.using(new BufferedReader(new InputStreamReader(fs.open(path))))
      { reader => Stream.continually(reader.readLine()).takeWhile(_ != null).toArray.toSeq }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 读取配置文件为MAP
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件流
    */
  def readPropertiesToMap(filePath: String, conf: Configuration = new Configuration()): Map[String, String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.isFile(path))
    {
      LoanPattern.using(fs.open(path).getWrappedStream)
      { in =>
        val fileConfigProps = new Properties()
        fileConfigProps.load(in)

        //集合隐式转换
        import scala.collection.JavaConversions._
        fileConfigProps.toMap
      }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 删除文件
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 删除是否成功
    */
  def delete(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      val path = new Path(filePath)

      fs.delete(path, true)
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 重命名文件/目录
    *
    * @param src  源文件/目录
    * @param dest 目的文件/目录
    * @param conf Hadoop配置
    * @return 重命名是否成功
    */
  def rename(src: String, dest: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      fs.rename(new Path(src), new Path(dest))
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 判断文件是否存在
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 文件是否存在
    */
  def exists(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    FileSystem.get(conf).exists(new Path(filePath))
  }

  /**
    * 判断当前文件夹下是否存在子文件
    *
    * @param filePath 文件夹路径
    * @param conf     配置对象
    * @return 是否存在子文件
    */
  def existsChildFile(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val path = new Path(filePath)
    if (FileSystem.get(conf).isDirectory(path)) FileSystem.get(conf).listFiles(path, false).hasNext else false
  }

  /**
    * 追加数据至指定的文件
    *
    * @param filePath 文件路径
    * @param fileName 文件名
    * @param line     需要追加的内容
    */
  def appendLine(filePath: String, fileName: String, line: String, conf: Configuration = new Configuration()): Unit =
  {
    val fs = FileSystem.get(conf)

    if (!fs.exists(new Path(filePath)))
    {
      fs.mkdirs(new Path(filePath))
    }

    val saveFilePath = new Path(s"$filePath/$fileName")
    if (!fs.exists(saveFilePath))
    {
      fs.createNewFile(saveFilePath)
    }

    LoanPattern.using(fs.append(saveFilePath))
    { out =>

      out.write((line + "\n").getBytes(Charset.forName("UTF-8")))
      out.flush()
    }
  }
}
package com.avcdata.etl.common.util

import java.io.{BufferedReader, InputStreamReader}
import java.nio.charset.Charset
import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}


/**
  * HDFS文件读取
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 14:08
  */
object HdfsFileUtil
{
  /**
    * 自适应读取HDFS或本地系统文件
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件内容
    */
  def read(filePath: String, conf: Configuration = new Configuration()): Seq[String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.getFileStatus(path).isFile)
    {
      LoanPattern.using(new BufferedReader(new InputStreamReader(fs.open(path))))
      { reader => Stream.continually(reader.readLine()).takeWhile(_ != null).toArray.toSeq }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 读取配置文件为MAP
    *
    * @param filePath 文件路径
    * @param conf     配置对象
    * @return 文件流
    */
  def readPropertiesToMap(filePath: String, conf: Configuration = new Configuration()): Map[String, String] =
  {
    val fs = FileSystem.get(conf)

    val path = new Path(filePath)
    if (fs.isFile(path))
    {
      LoanPattern.using(fs.open(path).getWrappedStream)
      { in =>
        val fileConfigProps = new Properties()
        fileConfigProps.load(in)

        //集合隐式转换
        import scala.collection.JavaConversions._
        fileConfigProps.toMap
      }
    }
    else
    {
      throw new IllegalArgumentException(s"Path => $filePath is not a file.")
    }
  }

  /**
    * 删除文件
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 删除是否成功
    */
  def delete(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      val path = new Path(filePath)

      fs.delete(path, true)
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 重命名文件/目录
    *
    * @param src  源文件/目录
    * @param dest 目的文件/目录
    * @param conf Hadoop配置
    * @return 重命名是否成功
    */
  def rename(src: String, dest: String, conf: Configuration = new Configuration()): Boolean =
  {
    val fs = FileSystem.get(conf)
    try
    {
      fs.rename(new Path(src), new Path(dest))
    }
    catch
    {
      case ex: Throwable => ex.printStackTrace(); throw ex
    }
  }

  /**
    * 判断文件是否存在
    *
    * @param filePath 文件路径
    * @param conf     HDFS配置
    * @return 文件是否存在
    */
  def exists(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    FileSystem.get(conf).exists(new Path(filePath))
  }

  /**
    * 判断当前文件夹下是否存在子文件
    *
    * @param filePath 文件夹路径
    * @param conf     配置对象
    * @return 是否存在子文件
    */
  def existsChildFile(filePath: String, conf: Configuration = new Configuration()): Boolean =
  {
    val path = new Path(filePath)
    if (FileSystem.get(conf).isDirectory(path)) FileSystem.get(conf).listFiles(path, false).hasNext else false
  }

  /**
    * 追加数据至指定的文件
    *
    * @param filePath 文件路径
    * @param fileName 文件名
    * @param line     需要追加的内容
    */
  def appendLine(filePath: String, fileName: String, line: String, conf: Configuration = new Configuration()): Unit =
  {
    val fs = FileSystem.get(conf)

    if (!fs.exists(new Path(filePath)))
    {
      fs.mkdirs(new Path(filePath))
    }

    val saveFilePath = new Path(s"$filePath/$fileName")
    if (!fs.exists(saveFilePath))
    {
      fs.createNewFile(saveFilePath)
    }

    LoanPattern.using(fs.append(saveFilePath))
    { out =>

      out.write((line + "\n").getBytes(Charset.forName("UTF-8")))
      out.flush()
    }
  }
}
package com.avcdata.spark.job.mllib

import java.io.PrintWriter

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

/**
  * Created by Administrator on 2017/5/4.
  */
object HDFSUtils {
  def getFS():FileSystem = {
    System.setProperty("HADOOP_USER_NAME","hdfs")
    val conf = new Configuration()
     conf.set("fs.defaultFS","hdfs://192.168.1.15:8020/")
    conf.set("mapred.remote.os","Linux")
    FileSystem.get(conf)
  }

  def cluster_resultToFS(iterator: Iterator[(String,String,Int)]): Unit = {
    println("Trying to write to HDFS...")
    val fs = getFS()
    val output = fs.create(new Path("/user/hdfs/rsync/uservector/2017-05-04-ClusterResult/All-result"))

    val writer = new PrintWriter(output)
    iterator.foreach(data => {
      writer.write("["+data._1)
      writer.write(data._2+"]")
      writer.write("\t")
      writer.write(data._3.toString)
      writer.write("\n")
    }
    )
    writer.close()
    println("Closed!")
  }

  def questionnaires_resultToFS(iterator: Iterator[(String,String,String,String)]): Unit = {
    println("Trying to write to HDFS...")
    val fs = getFS()
    val output = fs.create(new Path("/user/hdfs/rsync/uservector/2017-05-04-ClusterResult/questionnaires-result"))

    val writer = new PrintWriter(output)
    iterator.foreach(data => {
      writer.write(data._1)
      writer.write("\t")
      writer.write(data._2)
      writer.write("\t")
      writer.write(data._3)
      writer.write("\t")
      writer.write(data._4)
      writer.write("\n")
    }
    )
    writer.close()
    println("Closed!")
  }
}
package com.avcdata.spark.job.util

import java.io.{File, FileInputStream, FileOutputStream, IOException}

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, FileSystem, FileUtil, Path}
import org.apache.hadoop.io.IOUtils


/**
  * hdfs文件夹操作类
  */
object HdfsUtils {

  def getFS(): FileSystem = {
    //    System.setProperty("hadoop.home.dir", "D:\\04coding\\projects-bigData\\Hadoop\\hadoop-2.5.0")
    System.setProperty("HADOOP_USER_NAME", "hdfs")
    val conf = new Configuration()
    conf.set("fs.defaultFS", "hdfs://srv8.avcdata.com:8020/")
    conf.set("mapred.remote.os", "Linux")
    FileSystem.get(conf)
  }

  /**
    * 关闭FileSystem
    *
    * @param fileSystem
    */
  def closeFS(fileSystem: FileSystem) {
    if (fileSystem != null) {
      try {
        fileSystem.close()
      } catch {
        case e: IOException => e.printStackTrace()
      }
    }
  }


  ///////////////////////////////////////////////////////////////////////////


  /**
    * ls
    * @param hdfsFilePath
    */
  def listFiles(hdfsFilePath: String): Unit = {
    val fileSystem = getFS()
    val fstats = fileSystem.listStatus(new Path(hdfsFilePath))
    try {

      for (fstat <- fstats) {
        if (fstat.isDirectory()) {
          println("directory")
        } else {
          println("file")
        }
        println("Permission:" + fstat.getPermission())
        println("Owner:" + fstat.getOwner())
        println("Group:" + fstat.getGroup())
        println("Size:" + fstat.getLen())
        println("Replication:" + fstat.getReplication())
        println("Block Size:" + fstat.getBlockSize())
        println("Name:" + fstat.getPath())
        println("#############################")
      }

    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      if (fileSystem != null) {
        try {
          fileSystem.close()
        } catch {
          case e: IOException => e.printStackTrace()
        }
      }
    }
  }

  def ls(fileSystem: FileSystem, path: String) = {
    println("list path:" + path)
    val fs = fileSystem.listStatus(new Path(path))
    val listPath = FileUtil.stat2Paths(fs)
    for (p <- listPath) {
      println(p)
    }
    println("----------------------------------------")
  }


  /**
    * 创建目录
    *
    * @param hdfsFilePath
    */
  def mkdir(hdfsFilePath: String) = {
    val fileSystem = getFS()

    try {
      val success = fileSystem.mkdirs(new Path(hdfsFilePath))
      if (success) {
        println("Create directory or file successfully")
      }
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      this.closeFS(fileSystem)
    }
  }

  /**
    * 删除文件或目录
    *
    * @param hdfsFilePath
    * @param recursive 递归
    */
  def rm(hdfsFilePath: String, recursive: Boolean): Unit = {
    val fileSystem = this.getFS()
    try {
      val success = fileSystem.delete(new Path(hdfsFilePath), recursive)
      if (success) {
        System.out.println("delete successfully")
      }
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      this.closeFS(fileSystem)
    }
  }


  /**
    * 上传文件到HDFS
    * @param localPath
    * @param hdfspath
    */
  def write(localPath: String, hdfspath: String) {

    val inStream = new FileInputStream(
      new File(localPath)
    )
    val fileSystem = this.getFS()
    val writePath = new Path(hdfspath)
    val outStream = fileSystem.create(writePath)

    try {
      IOUtils.copyBytes(inStream, outStream, 4096, false)
    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(inStream)
      IOUtils.closeStream(outStream)
    }

  }

  /**
  //    * 上传文件到HDFS
//    *
//    * @param localFilePath
//    * @param hdfsFilePath
//    */
  //  def put(localFilePath: String, hdfsFilePath: String) = {
  //    val fileSystem = this.getFS()
  //    try {
  //      val fdos = fileSystem.create(new Path(hdfsFilePath))
  //      val fis = new FileInputStream(new File(localFilePath))
  //      IOUtils.copyBytes(fis, fdos, 1024)
  //
  //    } catch {
  //      case e: IllegalArgumentException => e.printStackTrace()
  //      case e: IOException => e.printStackTrace()
  //    } finally {
  //      IOUtils.closeStream(fileSystem)
  //    }
  //  }


  /**
    * 打印hdfs上的文件内容
    *
    * @param hdfsFilePath
    */
  def cat(hdfsFilePath: String) {

    val fileSystem = this.getFS()

    val readPath = new Path(hdfsFilePath)

    val inStream = fileSystem.open(readPath)

    try {
      IOUtils.copyBytes(inStream, System.out, 4096, false)
    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(inStream)
    }
  }


  /**
    * 下载文件到本地
    *
    * @param localFilePath
    * @param hdfsFilePath
    */
  def get(localFilePath: String, hdfsFilePath: String) {
    val fileSystem = this.getFS()
    try {
      val fsis = fileSystem.open(new Path(hdfsFilePath))
      val fos = new FileOutputStream(new File(localFilePath))
      IOUtils.copyBytes(fsis, fos, 1024)
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(fileSystem)
    }
  }


  def main(args: Array[String]) {

    val fileSystem = getFS()
    val path = "/user/hdfs/rsync/CH/2016-12-14"

    try {

      //      println("list path:---------" + path)
      //      val fs = fileSystem.listStatus(new Path(path))
      //      val listPath = FileUtil.stat2Paths(fs)
      //      for (p <- listPath) {
      //        println(p)
      //      }
      //      println("----------------------------------------")

      //      val fdis = fileSystem.open(new Path("/user/hdfs"))
      //      IOUtils.copyBytes(fdis, System.out, 1024)


      val fstats = fileSystem.listStatus(new Path(path))
      for (fstat: FileStatus <- fstats) {
        //        println(fstat.isDirectory() ? "directory": "file")
        //        println("Permission:" + fstat.getPermission())
        //        println("Owner:" + fstat.getOwner())
        //        println("Group:" + fstat.getGroup())
        val path = fstat.getPath().toString
        val name = path.substring(path.toString.lastIndexOf("/") + 1)
        println(name)
        //        println("Size:" + fstat.getLen/1024/1024)
        //        println("Replication:" + fstat.getReplication())
        //        println("Block Size:" + fstat.getBlockSize())

        //        println("#############################")
      }

    } catch {
      case ex: IOException => {
        ex.printStackTrace()
        println(ex.getCause)
        println("link err")
      }
    } finally {
      IOUtils.closeStream(fileSystem)
    }


  }
}package com.avcdata.spark.job.util

import java.io.{File, FileInputStream, FileOutputStream, IOException}

import com.avcdata.spark.job.common.Helper
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, FileSystem, FileUtil, Path}
import org.apache.hadoop.io.IOUtils


/**
  * hdfs文件夹操作类
  */
object HdfsUtils {

  def getFS(): FileSystem = {
    //    System.setProperty("hadoop.home.dir", "D:\\04coding\\projects-bigData\\Hadoop\\hadoop-2.5.0")
    System.setProperty("HADOOP_USER_NAME", "hdfs")


    //hadoop 的方式
    //    val conf = new Configuration()
    //    conf.set("fs.defaultFS", "hdfs://srv5.avcdata.com:8020/")
    //    //    conf.set("fs.defaultFS", "hdfs://srv8.avcdata.com:8020/")
    //    conf.set("mapred.remote.os", "Linux")
    //    FileSystem.get(conf)

    //spark 方式
    val hadoopConf = Helper.sparkContext.hadoopConfiguration
     FileSystem.get(hadoopConf)
  }

  /**
    * 关闭FileSystem
    *
    * @param fileSystem
    */
  def closeFS(fileSystem: FileSystem) {
    if (fileSystem != null) {
      try {
        fileSystem.close()
      } catch {
        case e: IOException => e.printStackTrace()
      }
    }
  }


  ///////////////////////////////////////////////////////////////////////////


  /**
    * ls
    *
    * @param hdfsFilePath
    */
  def listFiles(hdfsFilePath: String): Unit = {
    val fileSystem = getFS()
    val fstats = fileSystem.listStatus(new Path(hdfsFilePath))
    try {

      for (fstat <- fstats) {
        if (fstat.isDirectory()) {
          println("directory")
        } else {
          println("file")
        }
        println("Permission:" + fstat.getPermission())
        println("Owner:" + fstat.getOwner())
        println("Group:" + fstat.getGroup())
        println("Size:" + fstat.getLen())
        println("Replication:" + fstat.getReplication())
        println("Block Size:" + fstat.getBlockSize())
        println("Name:" + fstat.getPath())
        println("#############################")
      }

    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      if (fileSystem != null) {
        try {
          fileSystem.close()
        } catch {
          case e: IOException => e.printStackTrace()
        }
      }
    }
  }

  def ls(fileSystem: FileSystem, path: String) = {
    println("list path:" + path)
    val fs = fileSystem.listStatus(new Path(path))
    val listPath = FileUtil.stat2Paths(fs)
    for (p <- listPath) {
      println(p)
    }
    println("----------------------------------------")
  }


  /**
    * 创建目录
    *
    * @param hdfsFilePath
    */
  def mkdir(hdfsFilePath: String) = {
    val fileSystem = getFS()

    try {
      val success = fileSystem.mkdirs(new Path(hdfsFilePath))
      if (success) {
        println("Create directory or file successfully")
      }
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      this.closeFS(fileSystem)
    }
  }

  /**
    * 删除文件或目录
    *
    * @param hdfsFilePath
    * @param recursive 递归
    */
  def rm(hdfsFilePath: String, recursive: Boolean): Unit = {
    val fileSystem = this.getFS()
    val path = new Path(hdfsFilePath)
    try {
      if (fileSystem.exists(path)) {
        val success = fileSystem.delete(path, recursive)
        if (success) {
          System.out.println("delete successfully")
        }
      }

    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      this.closeFS(fileSystem)
    }
  }


  /**
    * 上传文件到HDFS
    *
    * @param localPath
    * @param hdfspath
    */
  def write(localPath: String, hdfspath: String) {

    val inStream = new FileInputStream(
      new File(localPath)
    )
    val fileSystem = this.getFS()
    val writePath = new Path(hdfspath)
    val outStream = fileSystem.create(writePath)

    try {
      IOUtils.copyBytes(inStream, outStream, 4096, false)
    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(inStream)
      IOUtils.closeStream(outStream)
    }

  }

  /**
    * //    * 上传文件到HDFS
    * //    *
    * //    * @param localFilePath
    * //    * @param hdfsFilePath
    * //    */
  //  def put(localFilePath: String, hdfsFilePath: String) = {
  //    val fileSystem = this.getFS()
  //    try {
  //      val fdos = fileSystem.create(new Path(hdfsFilePath))
  //      val fis = new FileInputStream(new File(localFilePath))
  //      IOUtils.copyBytes(fis, fdos, 1024)
  //
  //    } catch {
  //      case e: IllegalArgumentException => e.printStackTrace()
  //      case e: IOException => e.printStackTrace()
  //    } finally {
  //      IOUtils.closeStream(fileSystem)
  //    }
  //  }


  /**
    * 打印hdfs上的文件内容
    *
    * @param hdfsFilePath
    */
  def cat(hdfsFilePath: String) {

    val fileSystem = this.getFS()

    val readPath = new Path(hdfsFilePath)

    val inStream = fileSystem.open(readPath)

    try {
      IOUtils.copyBytes(inStream, System.out, 4096, false)
    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(inStream)
    }
  }


  /**
    * 下载文件到本地
    *
    * @param localFilePath
    * @param hdfsFilePath
    */
  def get(localFilePath: String, hdfsFilePath: String) {
    val fileSystem = this.getFS()
    try {
      val fsis = fileSystem.open(new Path(hdfsFilePath))
      val fos = new FileOutputStream(new File(localFilePath))
      IOUtils.copyBytes(fsis, fos, 1024)
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(fileSystem)
    }
  }


  def main(args: Array[String]) {

    val fileSystem = getFS()
    val path = "/user/hdfs/rsync/CH/2016-12-14"

    try {

      //      println("list path:---------" + path)
      //      val fs = fileSystem.listStatus(new Path(path))
      //      val listPath = FileUtil.stat2Paths(fs)
      //      for (p <- listPath) {
      //        println(p)
      //      }
      //      println("----------------------------------------")

      //      val fdis = fileSystem.open(new Path("/user/hdfs"))
      //      IOUtils.copyBytes(fdis, System.out, 1024)


      val fstats = fileSystem.listStatus(new Path(path))
      for (fstat: FileStatus <- fstats) {
        //        println(fstat.isDirectory() ? "directory": "file")
        //        println("Permission:" + fstat.getPermission())
        //        println("Owner:" + fstat.getOwner())
        //        println("Group:" + fstat.getGroup())
        val path = fstat.getPath().toString
        val name = path.substring(path.toString.lastIndexOf("/") + 1)
        println(name)
        //        println("Size:" + fstat.getLen/1024/1024)
        //        println("Replication:" + fstat.getReplication())
        //        println("Block Size:" + fstat.getBlockSize())

        //        println("#############################")
      }

    } catch {
      case ex: IOException => {
        ex.printStackTrace()
        println(ex.getCause)
        println("link err")
      }
    } finally {
      IOUtils.closeStream(fileSystem)
    }


  }
}package com.avcdata.vbox.util

import java.io.{File, FileInputStream, FileOutputStream, IOException}

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, FileSystem, FileUtil, Path}
import org.apache.hadoop.io.IOUtils


/**
  * hdfs文件夹操作类
  */
object HdfsUtils {

  def getFS(): FileSystem = {
    //    System.setProperty("hadoop.home.dir", "D:\\04coding\\projects-bigData\\Hadoop\\hadoop-2.5.0")
    System.setProperty("HADOOP_USER_NAME", "hdfs")
    val conf = new Configuration()
    conf.set("fs.defaultFS", "hdfs://srv8.avcdata.com:8020/")
    conf.set("mapred.remote.os", "Linux")
    FileSystem.get(conf)
  }

  /**
    * 关闭FileSystem
    *
    * @param fileSystem
    */
  def closeFS(fileSystem: FileSystem) {
    if (fileSystem != null) {
      try {
        fileSystem.close()
      } catch {
        case e: IOException => e.printStackTrace()
      }
    }
  }


  ///////////////////////////////////////////////////////////////////////////


  /**
    * ls
    * @param hdfsFilePath
    */
  def listFiles(hdfsFilePath: String): Unit = {
    val fileSystem = getFS()
    val fstats = fileSystem.listStatus(new Path(hdfsFilePath))
    try {

      for (fstat <- fstats) {
        if (fstat.isDirectory()) {
          println("directory")
        } else {
          println("file")
        }
        println("Permission:" + fstat.getPermission())
        println("Owner:" + fstat.getOwner())
        println("Group:" + fstat.getGroup())
        println("Size:" + fstat.getLen())
        println("Replication:" + fstat.getReplication())
        println("Block Size:" + fstat.getBlockSize())
        println("Name:" + fstat.getPath())
        println("#############################")
      }

    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      if (fileSystem != null) {
        try {
          fileSystem.close()
        } catch {
          case e: IOException => e.printStackTrace()
        }
      }
    }
  }

  def ls(fileSystem: FileSystem, path: String) = {
    println("list path:" + path)
    val fs = fileSystem.listStatus(new Path(path))
    val listPath = FileUtil.stat2Paths(fs)
    for (p <- listPath) {
      println(p)
    }
    println("----------------------------------------")
  }


  /**
    * 创建目录
    *
    * @param hdfsFilePath
    */
  def mkdir(hdfsFilePath: String) = {
    val fileSystem = getFS()

    try {
      val success = fileSystem.mkdirs(new Path(hdfsFilePath))
      if (success) {
        println("Create directory or file successfully")
      }
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      this.closeFS(fileSystem)
    }
  }

  /**
    * 删除文件或目录
    *
    * @param hdfsFilePath
    * @param recursive 递归
    */
  def rm(hdfsFilePath: String, recursive: Boolean): Unit = {
    val fileSystem = this.getFS()
    try {
      val success = fileSystem.delete(new Path(hdfsFilePath), recursive)
      if (success) {
        System.out.println("delete successfully")
      }
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      this.closeFS(fileSystem)
    }
  }


  /**
    * 上传文件到HDFS
    * @param localPath
    * @param hdfspath
    */
  def write(localPath: String, hdfspath: String) {

    val inStream = new FileInputStream(
      new File(localPath)
    )
    val fileSystem = this.getFS()
    val writePath = new Path(hdfspath)
    val outStream = fileSystem.create(writePath)

    try {
      IOUtils.copyBytes(inStream, outStream, 4096, false)
    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(inStream)
      IOUtils.closeStream(outStream)
    }

  }

  /**
  //    * 上传文件到HDFS
//    *
//    * @param localFilePath
//    * @param hdfsFilePath
//    */
  //  def put(localFilePath: String, hdfsFilePath: String) = {
  //    val fileSystem = this.getFS()
  //    try {
  //      val fdos = fileSystem.create(new Path(hdfsFilePath))
  //      val fis = new FileInputStream(new File(localFilePath))
  //      IOUtils.copyBytes(fis, fdos, 1024)
  //
  //    } catch {
  //      case e: IllegalArgumentException => e.printStackTrace()
  //      case e: IOException => e.printStackTrace()
  //    } finally {
  //      IOUtils.closeStream(fileSystem)
  //    }
  //  }


  /**
    * 打印hdfs上的文件内容
    *
    * @param hdfsFilePath
    */
  def cat(hdfsFilePath: String) {

    val fileSystem = this.getFS()

    val readPath = new Path(hdfsFilePath)

    val inStream = fileSystem.open(readPath)

    try {
      IOUtils.copyBytes(inStream, System.out, 4096, false)
    } catch {
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(inStream)
    }
  }


  /**
    * 下载文件到本地
    *
    * @param localFilePath
    * @param hdfsFilePath
    */
  def get(localFilePath: String, hdfsFilePath: String) {
    val fileSystem = this.getFS()
    try {
      val fsis = fileSystem.open(new Path(hdfsFilePath))
      val fos = new FileOutputStream(new File(localFilePath))
      IOUtils.copyBytes(fsis, fos, 1024)
    } catch {
      case e: IllegalArgumentException => e.printStackTrace()
      case e: IOException => e.printStackTrace()
    } finally {
      IOUtils.closeStream(fileSystem)
    }
  }


  def main(args: Array[String]) {

    val fileSystem = getFS()
    val path = "/user/hdfs/rsync/CH/2016-12-14"

    try {

      //      println("list path:---------" + path)
      //      val fs = fileSystem.listStatus(new Path(path))
      //      val listPath = FileUtil.stat2Paths(fs)
      //      for (p <- listPath) {
      //        println(p)
      //      }
      //      println("----------------------------------------")

      //      val fdis = fileSystem.open(new Path("/user/hdfs"))
      //      IOUtils.copyBytes(fdis, System.out, 1024)


      val fstats = fileSystem.listStatus(new Path(path))
      for (fstat: FileStatus <- fstats) {
        //        println(fstat.isDirectory() ? "directory": "file")
        //        println("Permission:" + fstat.getPermission())
        //        println("Owner:" + fstat.getOwner())
        //        println("Group:" + fstat.getGroup())
        val path = fstat.getPath().toString
        val name = path.substring(path.toString.lastIndexOf("/") + 1)
        println(name)
        //        println("Size:" + fstat.getLen/1024/1024)
        //        println("Replication:" + fstat.getReplication())
        //        println("Block Size:" + fstat.getBlockSize())

        //        println("#############################")
      }

    } catch {
      case ex: IOException => {
        ex.printStackTrace()
        println(ex.getCause)
        println("link err")
      }
    } finally {
      IOUtils.closeStream(fileSystem)
    }


  }
}package com.avcdata.spark.job.coocaa

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext}

/**
 * Created by avc on 2016/10/26.
 */
object Helper {

  //默认配置文件读取
  val config: Config = ConfigFactory.load()

  //初始化Spark配置
  val sparkConf = new SparkConf()
    .setIfMissing("spark.master", config.getString("spark.master"))
    .setIfMissing("spark.app.name", config.getString("spark.app.name"))
    .setIfMissing("redis.host", config.getString("redis.host"))
    .setIfMissing("redis.port", config.getString("redis.port"))
    .setIfMissing("es.nodes", config.getString("es.nodes"))
    .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
    .setIfMissing("mysql.host", config.getString("mysql.host"))
    .setIfMissing("mysql.user", config.getString("mysql.user"))
    .setIfMissing("mysql.password", config.getString("mysql.password"))
    .setIfMissing("mysql.db", config.getString("mysql.db"))
    .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
    .setIfMissing("spark.cleaner.ttl", "3600")

  def getNewContext():SparkContext = {
    new SparkContext(sparkConf)
  }

  //初始化SparkContext
  val sparkContext: SparkContext = getNewContext()

  //命令行参数解析
  def parseOptions(args: Array[String], index: Int, defaultValue: String): String = if (args.length > index) args(index) else defaultValue

  def mysqlConf = {
    val prop = new java.util.Properties()
    prop.put("user", sparkContext.getConf.get("mysql.user"))
    prop.put("password", sparkContext.getConf.get("mysql.password"))
    prop.put("driver", "com.mysql.jdbc.Driver")
    prop
  }
  def hiveConf = {
    val prop = new java.util.Properties()
    prop.put("user", "")
    prop.put("password", "")
    prop.put("driver", "org.apache.hive.jdbc.HiveDriver")
    prop
  }
}
package com.avcdata.spark.job.common

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext}

object Helper {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    //初始化Spark配置
    val sparkConf = new SparkConf()
        .setIfMissing("spark.master", config.getString("spark.master"))
        .setIfMissing("spark.app.name", config.getString("spark.app.name"))
        .setIfMissing("redis.host", config.getString("redis.host"))
        .setIfMissing("redis.port", config.getString("redis.port"))
        .setIfMissing("es.nodes", config.getString("es.nodes"))
        .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
        .setIfMissing("mysql.host", config.getString("mysql.host"))
        .setIfMissing("mysql.user", config.getString("mysql.user"))
        .setIfMissing("mysql.password", config.getString("mysql.password"))
        .setIfMissing("mysql.db", config.getString("mysql.db"))
        .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
        .setIfMissing("spark.cleaner.ttl", "3600")

    def getNewContext(): SparkContext = {
        new SparkContext(sparkConf)
    }

    //初始化SparkContext
    val sparkContext: SparkContext = getNewContext()

    //命令行参数解析
    def parseOptions(args: Array[String], index: Int, defaultValue: String): String = if (args.length > index) args(index) else defaultValue

    def mysqlConf = {
        val prop = new java.util.Properties()
        prop.put("user", sparkContext.getConf.get("mysql.user"))
        prop.put("password", sparkContext.getConf.get("mysql.password"))
        prop.put("driver", "com.mysql.jdbc.Driver")
        prop
    }

    def hiveConf = {
        val prop = new java.util.Properties()
        prop.put("user", "")
        prop.put("password", "")
        prop.put("driver", "org.apache.hive.jdbc.HiveDriver")
        prop
    }
}
package com.avcdata.spark.job.common

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext}

/**
 * Created by avc on 2016/10/26.
 */
object Helper {

  //默认配置文件读取
  val config: Config = ConfigFactory.load()

  //初始化Spark配置
  val sparkConf = new SparkConf()
    .setIfMissing("spark.master", config.getString("spark.master"))
    .setIfMissing("spark.app.name", config.getString("spark.app.name"))
    .setIfMissing("redis.host", config.getString("redis.host"))
    .setIfMissing("redis.port", config.getString("redis.port"))
    .setIfMissing("es.nodes", config.getString("es.nodes"))
    .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
    .setIfMissing("mysql.host", config.getString("mysql.host"))
    .setIfMissing("mysql.user", config.getString("mysql.user"))
    .setIfMissing("mysql.password", config.getString("mysql.password"))
    .setIfMissing("mysql.db", config.getString("mysql.db"))
    .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
    .setIfMissing("spark.cleaner.ttl", "3600")

  def getNewContext():SparkContext = {
    new SparkContext(sparkConf)
  }

  //初始化SparkContext
  val sparkContext: SparkContext = getNewContext()

  //命令行参数解析
  def parseOptions(args: Array[String], index: Int, defaultValue: String): String = if (args.length > index) args(index) else defaultValue

  def mysqlConf = {
    val prop = new java.util.Properties()
    prop.put("user", sparkContext.getConf.get("mysql.user"))
    prop.put("password", sparkContext.getConf.get("mysql.password"))
    prop.put("driver", "com.mysql.jdbc.Driver")
    prop
  }
  def hiveConf = {
    val prop = new java.util.Properties()
    prop.put("user", "")
    prop.put("password", "")
    prop.put("driver", "org.apache.hive.jdbc.HiveDriver")
    prop
  }
}
package com.avcdata.vbox.common

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext}

object Helper {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    //初始化Spark配置
    val sparkConf = new SparkConf()
        .setIfMissing("spark.master", config.getString("spark.master"))
        .setIfMissing("spark.app.name", config.getString("spark.app.name"))
        .setIfMissing("redis.host", config.getString("redis.host"))
        .setIfMissing("redis.port", config.getString("redis.port"))
        .setIfMissing("es.nodes", config.getString("es.nodes"))
        .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
        .setIfMissing("mysql.host", config.getString("mysql.host"))
        .setIfMissing("mysql.user", config.getString("mysql.user"))
        .setIfMissing("mysql.password", config.getString("mysql.password"))
        .setIfMissing("mysql.db", config.getString("mysql.db"))
        .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
        .setIfMissing("spark.cleaner.ttl", "3600")

    def getNewContext(): SparkContext = {
        new SparkContext(sparkConf)
    }

    //初始化SparkContext
    val sparkContext: SparkContext = getNewContext()

    //命令行参数解析
    def parseOptions(args: Array[String], index: Int, defaultValue: String): String = if (args.length > index) args(index) else defaultValue

    def mysqlConf = {
        val prop = new java.util.Properties()
        prop.put("user", sparkContext.getConf.get("mysql.user"))
        prop.put("password", sparkContext.getConf.get("mysql.password"))
        prop.put("driver", "com.mysql.jdbc.Driver")
        prop
    }

    def hiveConf = {
        val prop = new java.util.Properties()
        prop.put("user", "")
        prop.put("password", "")
        prop.put("driver", "org.apache.hive.jdbc.HiveDriver")
        prop
    }
}
package com.avcdata.vbox.launcher

import com.avcdata.vbox.clean.apk._
import com.avcdata.vbox.clean.coocaa.{DataCleanCCTerminal, DataCleanKOTerminal}
import com.avcdata.vbox.clean.epg.DataCleanEpg
import com.avcdata.vbox.clean.live.{LiveData2Partition, DataCleanCHLive, DataCleanKOLive}
import com.avcdata.vbox.clean.oc._
import com.avcdata.vbox.clean.play.{DataCleanCCPlay, DataCleanCHPlay}
import com.avcdata.vbox.clean.terminal.{DataCleanCHTerminal, DataCleanTCLTerminal}
import com.avcdata.vbox.common.Helper
import org.apache.log4j.Logger

object HisExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    val sc = Helper.sparkContext

    println("===============?????????????????????=======================")

    ///////////////////////////////////epg////////////////////////////
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@DataCleanEpg start...")
      DataCleanEpg.run(sc, analysisDate)
      println(analysisDate + "@DataCleanEpg end....")
    }

    //////////////////////////////////终端数////////////////////////////
    //TODO 酷开终端信息清洗
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@DataCleanCCTerminal start....")
      DataCleanCCTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCCTerminal end....")
    }

    //TODO 长虹终端信息清洗
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@DataCleanCHTerminal start....")
      DataCleanCHTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanCHTerminal end....")
    }

    //TODO 康佳终端信息清洗
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@DataCleanKOTerminal start....")
      DataCleanKOTerminal.run(sc, analysisDate)
      println(analysisDate + "@DataCleanKOTerminal end....")
    }




    ////////////////////////搜索指数/////////////////////////////////////////
//    //TODO 爬虫搜索指数清洗
//    if (executePart.charAt(5) == '1') {
//      println(analysisDate + "@DataCleanPlaySearchIndex start....")
//      DataCleanPlaySearchIndex.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanPlaySearchIndex end....")
//    }
//
//
//    //////////////////////////开关机////////////////////////////////////
//


    // TODO 康佳开关机清洗
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@DataCleanKOPowerOn start....")
      DataCleanKOPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOPowerOn end....")
    }

    //TODO TCL开关机日志清洗
//    if (executePart.charAt(7) == '1') {
//      println(analysisDate + "@DataCleanTCLPowerOn start....")
//      DataCleanTCLPowerOn.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanTCLPowerOn end....")
//    }


    // TODO 长虹开关机清洗
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@DataCleanCHPowerOn start....")
      DataCleanCHPowerOn.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPowerOn end....")
    }
//
//
//
//
//    ///////////////////////////直播///////////////////////////////////

    // TODO 康佳直播清洗
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@DataCleanKOLive start....")
      DataCleanKOLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOLive end....")
    }

    // TODO 长虹直播清洗-8月28日
//    if (executePart.charAt(10) == '1') {
//      println(analysisDate + "@DataCleanCHLiveOld start....")
//      DataCleanCHLiveOld.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCHLiveOld end....")
//    }
    //8月28日-今
    if (executePart.charAt(10) == '1') {
      println(analysisDate + "@DataCleanCHLive start....")
      DataCleanCHLive.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHLive end....")
    }

//
//    ///////////////////////////apk///////////////////////////////////////

    //TODO 康佳apk清洗
    if (executePart.charAt(11) == '1') {
      println(analysisDate + "@DataCleanKOApk start....")
      DataCleanKOApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanKOApk end....")
    }

    // TODO 长虹apk清洗
    if (executePart.charAt(12) == '1') {
      println(analysisDate + "@DataCleanCHApk start....")
      DataCleanCHApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHApk end....")
    }


    //TODO TCL APk日志清洗
//    if (executePart.charAt(13) == '1') {
//      println(analysisDate + "@DataCleanTCLApk start....")
//      DataCleanTCLApk.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanTCLApk end....")
//    }


    // TODO 酷开apk清洗
    if (executePart.charAt(14) == '1') {
      println(analysisDate + "@DataCleanCCApk start....")
      DataCleanCCApk.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCCApk end....")
    }


    ////////////////////////导入分区表////////////////////////////////////

    if(1==2){
      //TODO 开关机（非TCL)导入分区表
      if (executePart.charAt(15) == '1') {
        println(analysisDate + "@OCData2Partition start....")
        OCData2Partition.run(sc, analysisDate);
        println(analysisDate + "@OCData2Partition end....")
      }

      //TODO 开关机（TCL)导入分区表
      if (executePart.charAt(16) == '1') {
        println(analysisDate + "@OCData2PartitionTCL start....")
        OCData2PartitionTCL.run(sc, analysisDate);
        println(analysisDate + "@OCData2PartitionTCL end....")
      }


      //TODO apk（非TCL)导入分区表
      if (executePart.charAt(17) == '1') {
        println(analysisDate + "@ApkData2Partition start....")
        ApkData2Partition.run(sc, analysisDate);
        println(analysisDate + "@ApkData2Partition end....")
      }
      //TODO apk（TCL)导入分区表
      if (executePart.charAt(18) == '1') {
        println(analysisDate + "@ApkData2PartitionTCL start....")
        ApkData2PartitionTCL.run(sc, analysisDate);
        println(analysisDate + "@ApkData2PartitionTCL end....")
      }

      //TODO 直播（非TCL)导入分区表
      if (executePart.charAt(19) == '1') {
        println(analysisDate + "@LiveData2Partition start....")
        LiveData2Partition.run(sc, analysisDate);
        println(analysisDate + "@LiveData2Partition end....")
      }
    }


    //////////////////////////剧集//////////////////////

    //TODO 长虹到剧清洗
    if (executePart.charAt(20) == '1') {
      println(analysisDate + "@DataCleanCHPlay start....")
      DataCleanCHPlay.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHPlay end....")
    }


    //TODO 酷开到剧清洗
//    if (executePart.charAt(21) == '1') {
//      println(analysisDate + "@DataCleanCCPlay start....")
//      DataCleanCCPlay.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCCPlay end....")
//    }




    //TODO TCL终端信息清洗
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@DataCleanTCLTerminal start....")
    //      DataCleanTCLTerminal.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanTCLTerminal end....")
    //    }


    //TODO 计算月沉默终端数
    //    if (executePart.charAt(22) == '1') {
    //      println(analysisDate + "@SilentTerminal start....")
    //      SilentTerminal.run(sc, analysisDate);
    //      println(analysisDate + "@SilentTerminal end....")
    //    }
    //
    //    //TODO 月沉默终端数推总
    //    if (executePart.charAt(23) == '1') {
    //      println(analysisDate + "@SilentTerminalTotal start....")
    //      SilentTerminalTotal.run(sc, analysisDate);
    //      println(analysisDate + "@SilentTerminalTotal end....")
    //    }



























































    ////////////////////////////剧集///////////////////////////////////////////


    //    // 长虹对数
    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@GetChannelOfLive start....")
    //      GetChannelOfLive.run(sc, analysisDate);
    //      println(analysisDate + "@GetChannelOfLive end....")
    //    }


    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@DataCleanCHPowerOn01 start....")
    //      DataCleanCHPowerOn01.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHPowerOn01 end....")
    //    }


    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@DataCleanCHPowerOnForCH start....")
    //      DataCleanCHPowerOnForCH.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHPowerOnForCH end....")
    //    }
    //
    //
    //
    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@DataCleanCHApkForCH start....")
    //      DataCleanCHApkForCH.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHApkForCH end....")
    //    }
    //
    //
    //    if (executePart.charAt(2) == '1') {
    //      println(analysisDate + "@DataCleanCHPowerOnCnt start....")
    //      DataCleanCHPowerOnCnt.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHPowerOnCnt end....")
    //    }
    //
    //    if (executePart.charAt(3) == '1') {
    //      println(analysisDate + "@DataCleanCHApkCnt start....")
    //      DataCleanCHApkCnt.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHApkCnt end....")
    //    }
    //
    //
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@DataCleanCHApkOTTCnt start....")
    //      DataCleanCHApkOTTCnt.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCHApkOTTCnt end....")
    //    }


    //


    //////////////////测试数据支持//////////////////////
    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanAllPowerOnCnt start....")
    //          DataCleanAllPowerOnCnt.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanAllPowerOnCnt end....")
    //        }

    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanAllApkOTTCnt start....")
    //          DataCleanAllApkOTTCnt.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanAllApkOTTCnt end....")
    //        }

    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanAll9ApkCnt start....")
    //          DataCleanAll9ApkCnt.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanAll9ApkCnt end....")
    //        }

    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@GetCCPgOfPlay start....")
    //          GetCCPgOfPlay.run(sc, analysisDate);
    //          println(analysisDate + "@GetCCPgOfPlay end....")
    //        }

    //
    //        if (executePart.charAt(1) == '1') {
    //          println(analysisDate + "@DataCleanCCPlayTest start....")
    //          DataCleanCCPlayTest.run(sc, analysisDate);
    //          println(analysisDate + "@DataCleanCCPlayTest end....")
    //        }


    ////////////////////////////////EPG数据清洗/////////////////////////////////////
    //    if (executePart.charAt(0) == '1') {
    //      println(analysisDate + "@DataCleanEpg start...")
    //      DataCleanEpg.run(sc, analysisDate)
    //      println(analysisDate + "@DataCleanEpg end....")
    //    }
    //
    //    //    if (executePart.charAt(0) == '1') {
    //    //      println(analysisDate + "@EpgDataClean start...")
    //    //      EpgDataClean.run(sc, analysisDate)
    //    //      println(analysisDate + "@EpgDataClean end....")
    //    //    }
    //
    //
    //    //    if (executePart.charAt(0) == '1') {
    //    //      println(analysisDate + "@EpgDataDao start...")
    //    //      if (args.length < 4) {
    //    //        println(" Not enough arguments")
    //    //        System.exit(0)
    //    //      }
    //    //      EpgDataDao.delete(sc, args(2), args(3))
    //    //      println(analysisDate + "@EpgDataDao end....")
    //    //    }
    //
    //
    //    //////////////////////////////COOCAA数据清洗////////////////////////////////////////
    //
    //    //终端
    //    if (executePart.charAt(1) == '1') {
    //      println(analysisDate + "@COOCAA-TerminalDataLoadJob start....")
    //      TerminalDataLoadJob.run(sc, analysisDate)
    //      println(analysisDate + "@COOCAA-TerminalDataLoadJob end....")
    //    }
    //
    //
    //    //开关机
    //    //    if (executePart.charAt(2) == '1') {
    //    //      println(analysisDate + "@TerminalPowerOnDataLoadJob start....")
    //    //      TerminalPowerOnDataLoadJob.run(sc);
    //    //      println(analysisDate + "@TerminalPowerOnDataLoadJob end....")
    //    //    }
    //
    //
    //    //直播
    //    //    if (executePart.charAt(3) == '1') {
    //    //      println(analysisDate + "@LiveDataLoadJob start....")
    //    //      LiveDataLoadJob.run(sc, analysisDate)
    //    //      println(analysisDate + "@LiveDataLoadJob end....")
    //    //    }
    //
    //
    //    //apk
    //    //    if (executePart.charAt(4) == '1') {
    //    //      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
    //    //      ApkDataLoadJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    //    //    }
    //    if (executePart.charAt(4) == '1') {
    //      println(analysisDate + "@COOCAA-ApkDataLoadJob start....")
    //      ApkDataLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkDataLoadJob end....")
    //    }
    //
    //    //////////////////////////到剧数据清洗////////////////////////
    //
    //    //到剧
    //    if (executePart.charAt(5) == '1') {
    //      println(analysisDate + "@COOCAA-DataCleanCCPlay start....")
    //      DataCleanCCPlay.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-DataCleanCCPlay end....")
    //      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob start....")
    //      //      PlaysUnpassDataLoadJob.run(sc, analysisDate);
    //      //      println(analysisDate + "@COOCAA-PlaysUnpassDataLoadJob end....")
    //    }
    //
    //
    //
    //    //搜索指数
    //    if (executePart.charAt(6) == '1') {
    //      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob start....")
    //      SearchIndexDataLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob end....")
    //    }
    //
    //
    //    //////////////////////////////推总////////////////////////////////////////
    //
    //    //开关机推总
    //    if (executePart.charAt(7) == '1') {
    //      println(analysisDate + "@TerminalPowerTimeTotalJob start ... ")
    //      TerminalPowerTimeTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@TerminalPowerTimeTotalJob end ... ")
    //    }
    //
    //
    //    //直播推总
    //    if (executePart.charAt(8) == '1') {
    //      println(analysisDate + "@LiveTimeTotalJob start ... ")
    //      LiveTimeTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@LiveTimeTotalJob end ... ")
    //    }
    //
    //    //APK推总
    //    if (executePart.charAt(9) == '1') {
    //      println(analysisDate + "@ApkTimeTotalJob start ... ")
    //      ApkTimeTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@ApkTimeTotalJob end ... ")
    //    }
    //
    //    //////////////////////////统计日志信息///////////////////////////
    //    //    if (executePart.charAt(10) == '1') {
    //    //      println(analysisDate + "@LogViewJob start ... ")
    //    //      LogViewJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@LogViewJob end ... ")
    //    //    }
    //
    //
    //    //////////////////////////统计Hive表信息///////////////////////////
    //    //    if (executePart.charAt(11) == '1') {
    //    //      println(analysisDate + "@HiveTableViewJob start ... ")
    //    //      HiveTableViewJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@HiveTableViewJob end ... ")
    //    //    }
    //
    //    ////////////////////////teset 龚琴推总////////////////////////
    //
    //    //    if (executePart.charAt(12) == '1') {
    //    //      println(analysisDate + "@ApkDataLoadJobGQ start ... ")
    //    //      ApkDataLoadJobGQ.run(sc, analysisDate);
    //    //      println(analysisDate + "@ApkDataLoadJobGQ end ... ")
    //    //    }
    //    //
    //    //    //terminal 样本库
    //    //    if (executePart.charAt(13) == '1') {
    //    //      println(analysisDate + "@COOCAA-Apk2SampleTerminalLoadJob start ... ")
    //    //      Apk2SampleTerminalLoadJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-Apk2SampleTerminalLoadJob end ... ")
    //    //    }
    //    //
    //    //    //terminal 样本库2
    //    //    if (executePart.charAt(14) == '1') {
    //    //      println(analysisDate + "@COOCAA-TerminalPartition2SampleTerminalTwoLoadJob start ... ")
    //    //      TerminalPartition2SampleTerminalTwoLoadJob.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-TerminalPartition2SampleTerminalTwoLoadJob end ... ")
    //    //    }
    //
    //    //TODO TerminalDataLoad=>SampleTerminalTwo
    //    //    if (executePart.charAt(14) == '1') {
    //    //      println(analysisDate + "@COOCAA-TerminalDataLoad2SampleTerminalTwo start ... ")
    //    //      TerminalDataLoad2SampleTerminalTwo.run(sc, analysisDate);
    //    //      println(analysisDate + "@COOCAA-TerminalDataLoad2SampleTerminalTwo end ... ")
    //    //    }
    //
    //
    //    //TODO 将apk清洗数据放入分区表
    //    if (executePart.charAt(15) == '1') {
    //      println(analysisDate + "@COOCAA-ApkData2Partition start ... ")
    //      ApkData2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkData2Partition end ... ")
    //    }
    //
    //    //TODO 开关机次数和时长推总数据=>分区表
    //    if (executePart.charAt(16) == '1') {
    //      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition start ... ")
    //      TerminalPowerTimeTotal2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition end ... ")
    //    }
    //
    //    //TODO Live次数和时长推总数据=>分区表
    //    if (executePart.charAt(17) == '1') {
    //      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition start ... ")
    //      LiveTimeTotal2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition end ... ")
    //    }
    //
    //
    //    //TODO APK次数和时长推总数据=>分区表
    //    if (executePart.charAt(18) == '1') {
    //      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition start ... ")
    //      ApkTimeTotal2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition end ... ")
    //    }
    //
    //
    //    //TODO 到剧终端信息清洗
    //    if (executePart.charAt(19) == '1') {
    //      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob start ... ")
    //      PlayerTerminalLoadJob.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-PlayerTerminalLoadJob end ... ")
    //    }
    //
    //
    //    //TODO 到剧清洗=>分区表
    //    if (executePart.charAt(20) == '1') {
    //      println(analysisDate + "@COOCAA-PlayData2Partition start....")
    //      PlayData2Partition.run(sc, analysisDate);
    //      println(analysisDate + "@COOCAA-PlayData2Partition end....")
    //    }
    //
    //    //TODO 计算月沉默终端数
    //    if (executePart.charAt(21) == '1') {
    //      println(analysisDate + "@SilentTerminal start....")
    //      SilentTerminal.run(sc, analysisDate);
    //      println(analysisDate + "@SilentTerminal end....")
    //    }
    //
    //    //TODO 月沉默终端数推总
    //    if (executePart.charAt(22) == '1') {
    //      println(analysisDate + "@SilentTerminalTotal start....")
    //      SilentTerminalTotal.run(sc, analysisDate);
    //      println(analysisDate + "@SilentTerminalTotal end....")
    //    }
    //
    //    //TODO 过滤掉2017.04直播多余省份（31个 省市之外的）的数据
    //    if (executePart.charAt(23) == '1') {
    //      println(analysisDate + "@LiveTimeTotalProvinceFilter start....")
    //      LiveTimeTotalProvinceFilter.run(sc, analysisDate);
    //      println(analysisDate + "@LiveTimeTotalProvinceFilter end....")
    //    }
    //
    //
    //
    //
    //    //////////////////////////////////////////////////////////////////
    //
    //
    //    ///////////////////////////HiveSQL TO Mysql////////////////////
    //
    //    //地区分布


    sc.stop()
  }

}
package com.avcdata.spark.job.common

import org.joda.time.DateTime

/**
  * Created by avc on 2016/11/18.
  */
object HiveSql {
    //tv总览
    val tracker_tv_overviewtable = "tracker_tv_overview"
    //val tracker_tv_overviewtable = "test_tv_overview"

    def getTracker_tv_overview_ottsql_daily(date: String) : String = {
        val tracker_tv_overview_ottsql1 =
            """
              |select brand,license, province, "" city, "OTT" behavior_type, "daily" period, dim_date as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date, dim_sn from hr.tracker_apk_fact_partition where date = '"""+ date + """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,dim_date
            """
        tracker_tv_overview_ottsql1.stripMargin
    }

    def getTracker_tv_overview_ottsql_weekly(date: String) : String = {
        val tracker_tv_overview_ottsql2 =
            """
              |select brand,license, province, "" city, "OTT" behavior_type, "weekly" period, DATE_SUB('""" + date + """', 7) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ottsql2.stripMargin
    }

    // where date like """+ similarYear + "-" + month +"""%
    def getTracker_tv_overview_ottsql_monthly(month: String, date: String) : String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_overview_ottsql3 =
            """
              |select brand,license, province, "" city, "OTT" behavior_type, "monthly" period, """"+ similarYear + "-" + mon +"""-01" as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ottsql3.stripMargin
    }

    def getTracker_tv_overview_ottsql_30days(date: String): String = {
        val tracker_tv_overview_ottsql4 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, "OTT" behavior_type, "30days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
              |)apc
            """

        tracker_tv_overview_ottsql4.stripMargin
    }

    def getTracker_tv_overview_ottsql_7days(date: String): String = {
        val tracker_tv_overview_ottsql5 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, "OTT" behavior_type, "7days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
              |)apc
            """

        tracker_tv_overview_ottsql5.stripMargin
    }

    def getTracker_tv_overview_livesql_daily(date: String): String = {
        val tracker_tv_overview_livesql1 =
            """
              |select brand as brand,license, province, "" city, "直播" behavior_type, "daily" period, dim_date as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_date,dim_sn from hr.tracker_live_fact_partition where date = '"""+ date + """') ai
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province,dim_date
            """
        tracker_tv_overview_livesql1.stripMargin
    }

    def getTracker_tv_overview_livesql_weekly(date: String): String = {
        val tracker_tv_overview_livesql2 =
            """
              |select brand as brand,license, province, "" city, "直播" behavior_type, "weekly" period, DATE_SUB('""" + date + """', 7) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_date,dim_sn from hr.tracker_live_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_livesql2.stripMargin
    }

    def getTracker_tv_overview_livesql_monthly(month: String, date: String): String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_overview_livesql3 =
            """
              |select brand as brand,license, province, "" city, "直播" behavior_type, "monthly" period, """"+ similarYear + "-" + mon +"""-01" as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_date,dim_sn from hr.tracker_live_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_livesql3.stripMargin
    }

    def getTracker_tv_overview_livesql_30days(date: String): String = {
        val tracker_tv_overview_livesql4 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, "直播" behavior_type, "30days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_date,dim_sn from hr.tracker_live_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province
              |)lpc
            """

        tracker_tv_overview_livesql4.stripMargin
    }

    def getTracker_tv_overview_livesql_7days(date: String): String = {
        val tracker_tv_overview_livesql5 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, "直播" behavior_type, "7days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_date,dim_sn from hr.tracker_live_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province
              |)lpc
            """

        tracker_tv_overview_livesql5.stripMargin
    }

    def getTracker_tv_overview_ocsql_daily(date: String): String = {
        val tracker_tv_overview_ocsql1 =
            """
              |select brand as brand,license, province, "" city, "智能电视开机" behavior_type, "daily" period, power_on_day as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date = '"""+ date + """') ai
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province,power_on_day
            """
        tracker_tv_overview_ocsql1.stripMargin
    }

    def getTracker_tv_overview_ocsql_weekly(date: String): String = {
        val tracker_tv_overview_ocsql2 =
            """
              |select brand as brand,license, province, "" city, "智能电视开机" behavior_type, "weekly" period, DATE_SUB('""" + date + """', 7) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ocsql2.stripMargin
    }

    def getTracker_tv_overview_ocsql_monthly(month: String, date: String): String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_overview_ocsql3 =
            """
              |select brand as brand,license, province, "" city, "智能电视开机" behavior_type, "monthly" period, """"+ similarYear + "-" + mon +"""-01" as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ocsql3.stripMargin
    }

    def getTracker_tv_overview_ocsql_30days(date: String): String = {
        val tracker_tv_overview_ocsql4 =
            """
              |select cpc.brand as brand,cpc.license, cpc.province, "" city, "智能电视开机" behavior_type, "30days" period, cpc.power_on_day as tv_date, cpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" power_on_day, count(distinct dim_sn) as tsn
              |from
              |(select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') ci
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ci.dim_sn=pc.sn
              |group by brand,license,province
              |)cpc
            """

        tracker_tv_overview_ocsql4.stripMargin
    }

    def getTracker_tv_overview_ocsql_7days(date: String): String = {
        val tracker_tv_overview_ocsql5 =
            """
              |select cpc.brand as brand,cpc.license, cpc.province, "" city, "智能电视开机" behavior_type, "7days" period, cpc.power_on_day as tv_date, cpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" power_on_day, count(distinct dim_sn) as tsn
              |from
              |(select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') ci
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ci.dim_sn=pc.sn
              |group by brand,license,province
              |)cpc
            """

        tracker_tv_overview_ocsql5.stripMargin
    }

    //分时tv总览
    val tracker_tv_overview_hourtable = "tracker_tv_overview_hour"
    //val tracker_tv_overview_hourtable = "test_tv_overview_hour"

    def getTracker_tv_overview_hoursql_ott(date: String): String = {
        val tracker_tv_overview_hoursql1 =
            """
              |select ta.brand as brand,ta.license license , ta.province province, "" city, "OTT" behavior_type, ai.dim_date as tv_date, ai.dim_hour as tv_hour, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date, dim_hour, dim_sn from hr.tracker_apk_fact_partition where date = '""" + date +
                """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) ta
              |on an.packagename=ai.dim_apk and ai.dim_sn=ta.sn
              |group by brand,license,province,dim_date,dim_hour
            """

        tracker_tv_overview_hoursql1.stripMargin
    }

    def getTracker_tv_overview_hoursql_oc(date: String): String = {
        val tracker_tv_overview_hoursql2 =
              """
              |select tc.brand as brand,tc.license license, tc.province province, "" city, "智能电视开机" behavior_type, ci.power_on_day as tv_date, ci.power_on_time as tv_hour, count(distinct ci.dim_sn) as terminal_cnt
              |from (select power_on_day, power_on_time, sn as dim_sn from hr.tracker_oc_fact_partition where date = '""" + date + """') ci
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) tc
              |on ci.dim_sn=tc.sn
              |group by brand,license,province,power_on_day,power_on_time
              """
        tracker_tv_overview_hoursql2.stripMargin
    }

    def getTracker_tv_overview_hoursql_live(date: String): String = {
        val tracker_tv_overview_hoursql3 =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, "直播" behavior_type, li.dim_date as tv_date, li.dim_hour as tv_hour, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_date, dim_hour, dim_sn from hr.tracker_live_fact_partition where date = '""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal) tl
              |on li.dim_sn=tl.sn
              |group by brand,license,province,dim_date,dim_hour
            """
        tracker_tv_overview_hoursql3.stripMargin
    }

    val tracker_tv_livetable = "tracker_tv_live"
    //val tracker_tv_livetable = "test_tv_live"

    def getTracker_tv_livesql_daily(date: String): String ={
        val tracker_tv_livesql1 =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, li.dim_channel channel, "daily" period, li.dim_date as tv_date, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date = '""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal) tl
              |on li.dim_sn=tl.sn
              |group by dim_date,province,brand,license,dim_channel
            """
        tracker_tv_livesql1.stripMargin
    }

    def getTracker_tv_livesql_weekly(date: String): String ={
        val tracker_tv_livesql2 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, li.dim_channel channel, "weekly" period, DATE_SUB('""" + date + """', 7) as tv_date, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province,dim_channel
            """
        tracker_tv_livesql2.stripMargin
    }

    def getTracker_tv_livesql_monthly(month: String, date: String): String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_livesql3 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, li.dim_channel channel, "monthly" period, """"+ similarYear + "-" + mon +"""-01" as tv_date, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date like """"+ similarYear + "-" + mon +"""%") li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province,dim_channel
            """
        tracker_tv_livesql3.stripMargin
    }

    def getTracker_tv_livesql_30days(date: String): String = {
        val tracker_tv_livesql4 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, lpc.dim_channel channel, "30days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, dim_channel, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province,dim_channel
              |)lpc
            """
        tracker_tv_livesql4.stripMargin
    }

    def getTracker_tv_livesql_7days(date: String): String = {
        val tracker_tv_livesql5 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, lpc.dim_channel channel, "7days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, dim_channel, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province, dim_channel
              |)lpc
            """
        tracker_tv_livesql5.stripMargin
    }

    //分时直播
    val tracker_tv_live_hourtable = "tracker_tv_live_hour"
    //val tracker_tv_live_hourtable = "test_tv_live_hour"

    def getTracker_tv_live_hoursql(date: String): String = {
        val tracker_tv_live_hoursql =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, li.dim_channel channel, li.dim_date as tv_date, li.dim_hour as tv_hour, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date, dim_hour,dim_sn from hr.tracker_live_fact_partition where date = '""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal) tl
              |on li.dim_sn=tl.sn
              |group by brand,license,province,dim_date,dim_hour,dim_channel
            """
        tracker_tv_live_hoursql.stripMargin
    }

    //新增终端
    val tracker_tv_newaddtable = "tracker_tv_newadd"

    //mon已经-1
    def getTracker_tv_new_monthly(month: String, date: String) : String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_newaddsql =
            """
              |select brand,license,province,"" city,"monthly" period,dim_apk apk,dim_date,count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_new_terminal where dim_date = '"""+ similarYear + "-"+mon +"""-01') ai
              |join
              |(select appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province,sn,license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn and an.appname=ai.dim_apk
              |group by brand,license,province,dim_date,dim_apk
            """
        tracker_tv_newaddsql.stripMargin
    }

    //留存终端
    val tracker_tv_retaintable = "tracker_tv_retain"

    def getTracker_tv_retainsql(new_date: String, retain_date: String): String = {
        var tracker_tv_retainsql =
            """
              |select brand,license,province,"" city,"monthly" period,dim_apk apk,new_date,retain_date,
              |count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,new_date,dim_sn,retain_date from hr.tracker_retain_terminal
              |where new_date = '""" + new_date + """' and retain_date = '""" + retain_date + """') ai
              |join
              |(select appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province,sn,license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn and an.appname=ai.dim_apk
              |group by brand,license,province,new_date,retain_date,dim_apk
            """

        tracker_tv_retainsql.stripMargin
    }

    val tracker_tv_apktable = "tracker_tv_apk"
    //val tracker_tv_apktable = "test_tv_apk"
    //val tracker_tv_apktable = "test_tv_apk1"

    def getTracker_tv_apksql_daily(date: String) : String = {
        val tracker_tv_apksql1 =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, an.appname apk, "daily" period, ai.dim_date as tv_date, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date = '""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) tl
              |on an.packagename=ai.dim_apk and ai.dim_sn=tl.sn
              |group by brand,license,province,appname,dim_date
            """

        tracker_tv_apksql1.stripMargin
    }

    def getTracker_tv_apksql_weekly(date: String) : String = {
        val tracker_tv_apksql2 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, an.appname apk, "weekly" period, DATE_SUB('""" + date + """', 7) as tv_date, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
            """

        tracker_tv_apksql2.stripMargin
    }

    def getTracker_tv_apksql_monthly(month: String, date: String) : String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_apksql3 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, an.appname apk, "monthly" period, """"+ similarYear + "-" + mon +"""-01" as tv_date, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
            """
        tracker_tv_apksql3.stripMargin
    }

    def getTracker_tv_apksql_30days(date: String): String = {
        val tracker_tv_apksql4 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, apc.appname apk, "30days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, appname, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
              |)apc
            """

        tracker_tv_apksql4.stripMargin
    }

    def getTracker_tv_apksql_7days(date: String): String = {
        val tracker_tv_apksql5 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, apc.appname apk, "7days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, appname, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
              |)apc
            """

        tracker_tv_apksql5.stripMargin
    }

    //分时应用
    val tracker_tv_apk_hourtable = "tracker_tv_apk_hour"
    //val tracker_tv_apk_hourtable = "test_tv_apk_hour"

    def getTracker_tv_apk_hoursql(date: String): String = {
        val tracker_tv_apk_hoursql =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, an.appname apk, ai.dim_date as tv_date, ai.dim_hour as tv_hour, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_hour,dim_sn from hr.tracker_apk_fact_partition where date = '""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) tl
              |on an.packagename=ai.dim_apk and ai.dim_sn=tl.sn
              |group by brand,license,province,dim_date,dim_hour,appname
            """
        tracker_tv_apk_hoursql.stripMargin
    }

    val tracker_total_tv_overviewtable = "tracker_total_tv_overview"
    //val tracker_total_tv_overviewtable = "test_total_tv_overview"

    def getTracker_total_tv_overviewsql_daily(date: String): String = {
        val preDate = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_overviewsql =
            """
              |select key, brand,license,province,city,behavior_type,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = 'OTT' and period = 'daily' and date = '""" + date +"""') ov
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='"""+ preDate +"""') sec
              |on ov.brand=sec.br and ov.license=sec.lic and ov.province=sec.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = '直播' and period = 'daily' and date = '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date='"""+ preDate +"""') fst
              |on ov.brand=fst.br and ov.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = '智能电视开机' and period = 'daily' and date='""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst where date='"""+ preDate +"""') fst
              |on ov.brand=fst.br and ov.province=fst.pro
            """

        tracker_total_tv_overviewsql.stripMargin
    }

    def getTracker_total_tv_overviewsql_weekly(date: String): String = {
        val currWeeklyDate = DateTime.parse(date).plusDays(-7).toString("yyyy-MM-dd")
        val tracker_total_tv_overviewsql =
            """
              |select key, brand,license,province,city,behavior_type,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = 'OTT' and period='weekly' and date>='""" + currWeeklyDate +"""' and date < '""" + date +"""') ov
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec where date = '"""+ currWeeklyDate +"""') sec
              |on ov.brand=sec.br and ov.license=sec.lic and ov.province=sec.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = '直播' and period='weekly' and date>='""" + currWeeklyDate +"""' and date < '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date = '"""+ currWeeklyDate +"""') fst
              |on ov.brand=fst.br and ov.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = '智能电视开机' and period='weekly' and date>='""" + currWeeklyDate +"""' and date < '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst where date = '"""+ currWeeklyDate +"""') fst
              |on ov.brand=fst.br and ov.province=fst.pro
            """

        tracker_total_tv_overviewsql.stripMargin
    }

    def getTracker_total_tv_overviewsql_monthly(date: String): String = {
        val preMonthDate = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_overviewsql =
            """
              |select key, brand,license,province,city,behavior_type,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = 'OTT' and period='monthly' and date>='""" + preMonthDate +"""' and date < '""" + date +"""') ov
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec where date = '"""+ preMonthDate +"""') sec
              |on ov.brand=sec.br and ov.license=sec.lic and ov.province=sec.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = '直播' and period='monthly' and date>='""" + preMonthDate +"""' and date <= '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date = '"""+ preMonthDate +"""') fst
              |on ov.brand=fst.br and ov.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = '智能电视开机' and period='monthly' and date>='""" + preMonthDate +"""' and date <= '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst where date = '"""+ preMonthDate +"""') fst
              |on ov.brand=fst.br and ov.province=fst.pro
            """

        tracker_total_tv_overviewsql.stripMargin
    }

    val tracker_total_tv_overview_hourtable = "tracker_total_tv_overview_hour"
    //val tracker_total_tv_overview_hourtable = "test_total_tv_overview_hour"

    def getTracker_total_tv_overview_hoursql(date: String): String = {
        val preDate = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_overview_hoursql =
            """
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = 'OTT' and date='""" + date +"""') ovh
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec where date ='"""+ preDate +"""') sec
              |on ovh.brand=sec.br and ovh.license=sec.lic and ovh.province=sec.pro
              |union all
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = '直播' and date='""" + date + """') ovh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date ='"""+ preDate +"""') fst
              |on ovh.brand=fst.br and ovh.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = '智能电视开机' and date='""" + date + """') ovh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst where date ='"""+ preDate +"""') fst
              |on ovh.brand=fst.br and ovh.province=fst.pro
            """

        tracker_total_tv_overview_hoursql.stripMargin
    }

    val tracker_total_tv_livetable = "tracker_total_tv_live"
    //val tracker_total_tv_livetable = "test_total_tv_live"

    def getTracker_total_tv_livesql_daily(date: String): String = {
        val preDate = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_livesql =
            """
              |select key, brand,license, province, city, channel, period, tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,channel,period,tv_date,terminal_cnt from hr.tracker_tv_live_partition
              |where period in ('daily','7days','30days') and date= '""" + date +"""') tl
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date='"""+ preDate +"""') fst
              |on tl.brand=fst.br and tl.province=fst.pro
            """
        tracker_total_tv_livesql.stripMargin
    }

    def getTracker_total_tv_livesql_weekly(date: String): String = {
        val currWeeklyDate = DateTime.parse(date).plusDays(-7).toString("yyyy-MM-dd")
        val tracker_total_tv_livesql =
            """
              |select key, brand,license, province, city, channel, period, tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,channel,period,tv_date,terminal_cnt from hr.tracker_tv_live_partition
              |where period='weekly' and date>='""" + currWeeklyDate +"""' and date < '""" + date +"""') tl
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date='"""+ currWeeklyDate +"""') fst
              |on tl.brand=fst.br and tl.province=fst.pro
            """
        tracker_total_tv_livesql.stripMargin
    }

    def getTracker_total_tv_livesql_monthly(date: String): String = {
        val preMonthDate = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_livesql =
            """
              |select key, brand,license, province, city, channel, period, tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,channel,period,tv_date,terminal_cnt from hr.tracker_tv_live_partition
              |where period='monthly' and date>='""" + preMonthDate +"""' and date < '""" + date +"""') tl
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date='"""+ preMonthDate +"""') fst
              |on tl.brand=fst.br and tl.province=fst.pro
            """
        tracker_total_tv_livesql.stripMargin
    }

    val tracker_total_tv_live_hourtable = "tracker_total_tv_live_hour"
    //val tracker_total_tv_live_hourtable = "test_total_tv_live_hour"

    def getTracker_total_tv_live_hoursql(date: String): String = {
        val preDate = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_live_hoursql =
            """
              |select key, brand,license, province, city, channel, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,channel,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_live_hour_partition
              |where date='""" + date + """') tlh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst where date='"""+ preDate +"""') fst
              |on tlh.brand=fst.br and tlh.province=fst.pro
            """
        tracker_total_tv_live_hoursql.stripMargin
    }

    val tracker_total_tv_apktable = "tracker_total_tv_apk"
    //val tracker_total_tv_apktable = "test_total_tv_apk"

    def getTracker_total_tv_apksql_daily(date: String): String = {
        val preDate = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apksql =
            """
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where period in ('daily','7days','30days') and apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date='""" + date +"""') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='"""+ preDate +"""') sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where period in ('daily','7days','30days') and apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date='""" + date +"""') ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec2 where date='"""+ preDate +"""') vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """
        tracker_total_tv_apksql.stripMargin
    }

    def getTracker_total_tv_apksql_weekly(date: String): String = {
        val currWeeklyDate = DateTime.parse(date).plusDays(-7).toString("yyyy-MM-dd")
        val tracker_total_tv_apksql =
            """
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where period='weekly' and apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date>='""" + currWeeklyDate +"""' and date <= '""" + date +"""') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='"""+ currWeeklyDate +"""') sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where period='weekly' and apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date>='""" + currWeeklyDate +"""' and date <= '""" + date +"""') ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec2 where date='"""+ currWeeklyDate +"""') vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """
        tracker_total_tv_apksql.stripMargin
    }

    def getTracker_total_tv_apksql_monthly(date: String): String = {
        val preMonthDate = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apksql =
            """
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where period='monthly' and apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date>='""" + preMonthDate +"""' and date <= '""" + date +"""') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='"""+ preMonthDate +"""') sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where period='monthly' and apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date>='""" + preMonthDate +"""' and date <= '""" + date +"""') ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec2 where date='"""+ preMonthDate +"""') vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """
        tracker_total_tv_apksql.stripMargin
    }

    val tracker_total_tv_apk_hourtable = "tracker_total_tv_apk_hour"
    //val tracker_total_tv_apk_hourtable = "test_total_tv_apk_hour"

    def getTracker_total_tv_apk_hoursql(date: String): String = {
        val preDate = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apk_hoursql =
            """
              |select key,brand,license,province,city,apk,tv_date,tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_apk_hour_partition
              |where apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date='""" + date + """') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='"""+ preDate +"""') sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,apk,tv_date,tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_apk_hour_partition
              |where apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date='""" + date + """') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec2 where date='"""+ preDate +"""') vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """
        tracker_total_tv_apk_hoursql.stripMargin
    }

    //新增推总
    val tracker_total_tv_newaddtable = "tracker_total_tv_newadd"

    def getTracker_total_tv_newaddsql(date: String): String = {
        val td = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apklicsql =
            """
              |select key,brand,license,province,city,period,apk,dim_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,period,apk,dim_date,terminal_cnt from hr.tracker_tv_newadd
              |where apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and dim_date='""" + td + """') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='""" + td + """') sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,period,apk,dim_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,period,apk,dim_date,terminal_cnt from hr.tracker_tv_newadd
              |where apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and dim_date='""" + td + """') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec2 where date='""" + td + """') vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """

        tracker_total_tv_apklicsql.stripMargin
    }

    val tracker_total_tv_retaintable = "tracker_total_tv_retain"

    def getTracker_total_tv_retainsql(date: String): String = {
        val td = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apklic_hoursql =
            """
              |select key,brand,license,province,city,period,apk,new_date,retain_date,
              |(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,period,apk,new_date,retain_date,terminal_cnt
              |from hr.tracker_tv_retain where retain_date='""" + td + """'
              |and apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端')) ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec where date='""" + td + """') sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,period,apk,new_date,retain_date,
              |(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,period,apk,new_date,retain_date,terminal_cnt
              |from hr.tracker_tv_retain where retain_date='""" + td + """'
              |and apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端')) ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec2 where date='""" + td + """') vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """

        tracker_total_tv_apklic_hoursql.stripMargin
    }

    /**
      * 获取统计月前一个月和基础月 中间包含哪几个月
      * @param preStaticsDate 统计月前月份
      * @return
      */
    def getnewMonths(baseDate: String, preStaticsDate : String) : List[String]={
        var i = 0
        var list:List[String] =List()

        for (i<-0 to 20) {
            if (DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd").equals(preStaticsDate)) {
                //list = list :+ DateTime.parse("2017-01-01").plusMonths(i).toString("yyyy-MM-dd")
                return list
            }
            list = list :+ DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd")
        }

        list
    }
}

object hiveTrst {
    def main(args: Array[String]): Unit = {
        //println(HiveSql.getTracker_tv_overview_ottsql("2016-11-10"))
        //println("union all")
        //println(HiveSql.getTracker_tv_overview_livesql("2016-11-10"))
        //println(HiveSql.getTracker_tv_overview_ocsql("2016-11-10"))
        /*println(HiveSql.getTracker_total_tv_livesql("2016-11-30", "7"))

        println(HiveSql.getTracker_total_tv_apksql("2016-11-30", "0"))

        println(HiveSql.getTracker_total_tv_apk_hoursql("2016-12-04"))*/
        //println(HiveSql.getTracker_total_tv_retainsql("2017-02-01"))
        //println(HiveSql.getTracker_tv_apksql_monthly("4","2017-05-01"))
        println(HiveSql.getTracker_total_tv_livesql_daily("2017-05-01"))
        println(HiveSql.getTracker_total_tv_livesql_weekly("2017-05-01"))
        println(HiveSql.getTracker_total_tv_livesql_monthly("2017-05-01"))
        println(HiveSql.getTracker_total_tv_live_hoursql("2017-05-01"))
        println(HiveSql.getTracker_total_tv_apksql_daily("2017-05-01"))
        println(HiveSql.getTracker_total_tv_apksql_weekly("2017-05-01"))
        println(HiveSql.getTracker_total_tv_apk_hoursql("2017-05-01"))
    }
}
package com.avcdata.vbox.common

import org.joda.time.DateTime

/**
  * Created by avc on 2016/11/18.
  */
object HiveSql {
    //tv总览
    val tracker_tv_overviewtable = "tracker_tv_overview"
    //val tracker_tv_overviewtable = "test_tv_overview"

    def getTracker_tv_overview_ottsql_daily(date: String) : String = {
        val tracker_tv_overview_ottsql1 =
            """
              |select brand,license, province, "" city, "OTT" behavior_type, "daily" period, dim_date as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date, dim_sn from hr.tracker_apk_fact_partition where date = '"""+ date + """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,dim_date
            """
        tracker_tv_overview_ottsql1.stripMargin
    }

    def getTracker_tv_overview_ottsql_weekly(date: String) : String = {
        val tracker_tv_overview_ottsql2 =
            """
              |select brand,license, province, "" city, "OTT" behavior_type, "weekly" period, min(dim_date) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ottsql2.stripMargin
    }

    // where date like """+ similarYear + "-" + month +"""%
    def getTracker_tv_overview_ottsql_monthly(month: String, date: String) : String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_overview_ottsql3 =
            """
              |select brand,license, province, "" city, "OTT" behavior_type, "monthly" period, min(dim_date) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ottsql3.stripMargin
    }

    def getTracker_tv_overview_ottsql_30days(date: String): String = {
        val tracker_tv_overview_ottsql4 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, "OTT" behavior_type, "30days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
              |)apc
            """

        tracker_tv_overview_ottsql4.stripMargin
    }

    def getTracker_tv_overview_ottsql_7days(date: String): String = {
        val tracker_tv_overview_ottsql5 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, "OTT" behavior_type, "7days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province
              |)apc
            """

        tracker_tv_overview_ottsql5.stripMargin
    }

    def getTracker_tv_overview_livesql_daily(date: String): String = {
        val tracker_tv_overview_livesql1 =
            """
              |select brand as brand,license, province, "" city, "直播" behavior_type, "daily" period, dim_date as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_date,dim_sn from hr.tracker_live_fact_partition where date = '"""+ date + """') ai
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province,dim_date
            """
        tracker_tv_overview_livesql1.stripMargin
    }

    def getTracker_tv_overview_livesql_weekly(date: String): String = {
        val tracker_tv_overview_livesql2 =
            """
              |select brand as brand,license, province, "" city, "直播" behavior_type, "weekly" period, min(dim_date) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_date,dim_sn from hr.tracker_live_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_livesql2.stripMargin
    }

    def getTracker_tv_overview_livesql_monthly(month: String, date: String): String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_overview_livesql3 =
            """
              |select brand as brand,license, province, "" city, "直播" behavior_type, "monthly" period, min(dim_date) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select dim_date,dim_sn from hr.tracker_live_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_livesql3.stripMargin
    }

    def getTracker_tv_overview_livesql_30days(date: String): String = {
        val tracker_tv_overview_livesql4 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, "直播" behavior_type, "30days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_date,dim_sn from hr.tracker_live_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province
              |)lpc
            """

        tracker_tv_overview_livesql4.stripMargin
    }

    def getTracker_tv_overview_livesql_7days(date: String): String = {
        val tracker_tv_overview_livesql5 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, "直播" behavior_type, "7days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_date,dim_sn from hr.tracker_live_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province
              |)lpc
            """

        tracker_tv_overview_livesql5.stripMargin
    }

    def getTracker_tv_overview_ocsql_daily(date: String): String = {
        val tracker_tv_overview_ocsql1 =
            """
              |select brand as brand,license, province, "" city, "智能电视开机" behavior_type, "daily" period, power_on_day as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date = '"""+ date + """') ai
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province,power_on_day
            """
        tracker_tv_overview_ocsql1.stripMargin
    }

    def getTracker_tv_overview_ocsql_weekly(date: String): String = {
        val tracker_tv_overview_ocsql2 =
            """
              |select brand as brand,license, province, "" city, "智能电视开机" behavior_type, "weekly" period, min(power_on_day) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ocsql2.stripMargin
    }

    def getTracker_tv_overview_ocsql_monthly(month: String, date: String): String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_overview_ocsql3 =
            """
              |select brand as brand,license, province, "" city, "智能电视开机" behavior_type, "monthly" period, min(power_on_day) as tv_date, count(distinct dim_sn) as terminal_cnt
              |from (select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province
            """
        tracker_tv_overview_ocsql3.stripMargin
    }

    def getTracker_tv_overview_ocsql_30days(date: String): String = {
        val tracker_tv_overview_ocsql4 =
            """
              |select cpc.brand as brand,cpc.license, cpc.province, "" city, "智能电视开机" behavior_type, "30days" period, cpc.power_on_day as tv_date, cpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" power_on_day, count(distinct dim_sn) as tsn
              |from
              |(select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') ci
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ci.dim_sn=pc.sn
              |group by brand,license,province
              |)cpc
            """

        tracker_tv_overview_ocsql4.stripMargin
    }

    def getTracker_tv_overview_ocsql_7days(date: String): String = {
        val tracker_tv_overview_ocsql5 =
            """
              |select cpc.brand as brand,cpc.license, cpc.province, "" city, "智能电视开机" behavior_type, "7days" period, cpc.power_on_day as tv_date, cpc.tsn terminal_cnt
              |from
              |(select brand, license, province, """" + date + """" power_on_day, count(distinct dim_sn) as tsn
              |from
              |(select power_on_day,sn as dim_sn from hr.tracker_oc_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') ci
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on ci.dim_sn=pc.sn
              |group by brand,license,province
              |)cpc
            """

        tracker_tv_overview_ocsql5.stripMargin
    }

    //分时tv总览
    val tracker_tv_overview_hourtable = "tracker_tv_overview_hour"
    //val tracker_tv_overview_hourtable = "test_tv_overview_hour"

    def getTracker_tv_overview_hoursql_ott(date: String): String = {
        val tracker_tv_overview_hoursql1 =
            """
              |select ta.brand as brand,ta.license license , ta.province province, "" city, "OTT" behavior_type, ai.dim_date as tv_date, ai.dim_hour as tv_hour, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date, dim_hour, dim_sn from hr.tracker_apk_fact_partition where date = '""" + date +
                """') ai
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) ta
              |on an.packagename=ai.dim_apk and ai.dim_sn=ta.sn
              |group by brand,license,province,dim_date,dim_hour
            """

        tracker_tv_overview_hoursql1.stripMargin
    }

    def getTracker_tv_overview_hoursql_oc(date: String): String = {
        val tracker_tv_overview_hoursql2 =
              """
              |select tc.brand as brand,tc.license license, tc.province province, "" city, "智能电视开机" behavior_type, ci.power_on_day as tv_date, ci.power_on_time as tv_hour, count(distinct ci.dim_sn) as terminal_cnt
              |from (select power_on_day, power_on_time, sn as dim_sn from hr.tracker_oc_fact_partition where date = '""" + date + """') ci
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) tc
              |on ci.dim_sn=tc.sn
              |group by brand,license,province,power_on_day,power_on_time
              """
        tracker_tv_overview_hoursql2.stripMargin
    }

    def getTracker_tv_overview_hoursql_live(date: String): String = {
        val tracker_tv_overview_hoursql3 =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, "直播" behavior_type, li.dim_date as tv_date, li.dim_hour as tv_hour, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_date, dim_hour, dim_sn from hr.tracker_live_fact_partition where date = '""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal) tl
              |on li.dim_sn=tl.sn
              |group by brand,license,province,dim_date,dim_hour
            """
        tracker_tv_overview_hoursql3.stripMargin
    }

    val tracker_tv_livetable = "tracker_tv_live"
    //val tracker_tv_livetable = "test_tv_live"

    def getTracker_tv_livesql_daily(date: String): String ={
        val tracker_tv_livesql1 =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, li.dim_channel channel, "daily" period, li.dim_date as tv_date, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date = '""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal) tl
              |on li.dim_sn=tl.sn
              |group by dim_date,province,brand,license,dim_channel
            """
        tracker_tv_livesql1.stripMargin
    }

    def getTracker_tv_livesql_weekly(date: String): String ={
        val tracker_tv_livesql2 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, li.dim_channel channel, "weekly" period, min(li.dim_date) as tv_date, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province,dim_channel
            """
        tracker_tv_livesql2.stripMargin
    }

    def getTracker_tv_livesql_monthly(month: String, date: String): String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_livesql3 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, li.dim_channel channel, "monthly" period, min(li.dim_date) as tv_date, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date like """"+ similarYear + "-" + mon +"""%") li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province,dim_channel
            """
        tracker_tv_livesql3.stripMargin
    }

    def getTracker_tv_livesql_30days(date: String): String = {
        val tracker_tv_livesql4 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, lpc.dim_channel channel, "30days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, dim_channel, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province,dim_channel
              |)lpc
            """
        tracker_tv_livesql4.stripMargin
    }

    def getTracker_tv_livesql_7days(date: String): String = {
        val tracker_tv_livesql5 =
            """
              |select lpc.brand as brand,lpc.license, lpc.province, "" city, lpc.dim_channel channel, "7days" period, lpc.dim_date as tv_date, lpc.tsn terminal_cnt
              |from
              |(select brand, license, province, dim_channel, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_channel,dim_date,dim_sn from hr.tracker_live_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal t) pc
              |on li.dim_sn=pc.sn
              |group by brand,license,province, dim_channel
              |)lpc
            """
        tracker_tv_livesql5.stripMargin
    }

    //分时直播
    val tracker_tv_live_hourtable = "tracker_tv_live_hour"
    //val tracker_tv_live_hourtable = "test_tv_live_hour"

    def getTracker_tv_live_hoursql(date: String): String = {
        val tracker_tv_live_hoursql =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, li.dim_channel channel, li.dim_date as tv_date, li.dim_hour as tv_hour, count(distinct li.dim_sn) as terminal_cnt
              |from (select dim_channel,dim_date, dim_hour,dim_sn from hr.tracker_live_fact_partition where date = '""" + date + """') li
              |join
              |(select brand, province, sn, license
              |from hr.live_terminal) tl
              |on li.dim_sn=tl.sn
              |group by brand,license,province,dim_date,dim_hour,dim_channel
            """
        tracker_tv_live_hoursql.stripMargin
    }

    //新增终端
    val tracker_tv_newaddtable = "tracker_tv_newadd"

    //mon已经-1
    def getTracker_tv_new_monthly(month: String, date: String) : String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_newaddsql =
            """
              |select brand,license,province,"" city,"monthly" period,dim_apk apk,dim_date,count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_new_terminal where dim_date = '"""+ similarYear + "-"+mon +"""-01') ai
              |join
              |(select brand, province,sn,license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province,dim_date,dim_apk
            """
        tracker_tv_newaddsql.stripMargin
    }

    //留存终端
    val tracker_tv_retaintable = "tracker_tv_retain"

    def getTracker_tv_retainsql(new_date: String, retain_date: String): String = {
        var tracker_tv_retainsql =
            """
              |select brand,license,province,"" city,"monthly" period,dim_apk apk,new_date,retain_date,
              |count(distinct ai.dim_sn) as terminal_cnt,sum(duration) duration,sum(cnt) cnt
              |from (select dim_apk,new_date,dim_sn,retain_date,duration,cnt from hr.tracker_retain_terminal
              |where new_date = '""" + new_date + """' and retain_date = '""" + retain_date + """') ai
              |join
              |(select brand, province,sn,license
              |from hr.sample_terminal_three t) pc
              |on ai.dim_sn=pc.sn
              |group by brand,license,province,new_date,retain_date,dim_apk
            """

        tracker_tv_retainsql.stripMargin
    }

    val tracker_tv_apktable = "tracker_tv_apk"
    //val tracker_tv_apktable = "test_tv_apk"
    //val tracker_tv_apktable = "test_tv_apk1"

    def getTracker_tv_apksql_daily(date: String) : String = {
        val tracker_tv_apksql1 =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, an.appname apk, "daily" period, ai.dim_date as tv_date, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date = '""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) tl
              |on an.packagename=ai.dim_apk and ai.dim_sn=tl.sn
              |group by brand,license,province,appname,dim_date
            """

        tracker_tv_apksql1.stripMargin
    }

    def getTracker_tv_apksql_weekly(date: String) : String = {
        val tracker_tv_apksql2 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, an.appname apk, "weekly" period, min(ai.dim_date) as tv_date, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>= DATE_SUB('""" + date + """', 7) and date < '"""+ date +"""') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
            """

        tracker_tv_apksql2.stripMargin
    }

    def getTracker_tv_apksql_monthly(month: String, date: String) : String = {
        var similarYear = date.substring(0, 4)
        if (month.equals("12")) {
            similarYear = (date.substring(0, 4).toInt - 1).toString
        }

        var mon = month
        if (mon.toInt < 10) {
            mon = "0"+mon
        }

        val tracker_tv_apksql3 =
            """
              |select pc.brand as brand,pc.license license, pc.province province, "" city, an.appname apk, "monthly" period, min(ai.dim_date) as tv_date, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date like """"+ similarYear + "-" + mon +"""%") ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
            """
        tracker_tv_apksql3.stripMargin
    }

    def getTracker_tv_apksql_30days(date: String): String = {
        val tracker_tv_apksql4 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, apc.appname apk, "30days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, appname, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date >DATE_SUB('""" + date + """', 30) and date<='""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
              |)apc
            """

        tracker_tv_apksql4.stripMargin
    }

    def getTracker_tv_apksql_7days(date: String): String = {
        val tracker_tv_apksql5 =
            """
              |select apc.brand as brand,apc.license, apc.province, "" city, apc.appname apk, "7days" period, apc.dim_date as tv_date, apc.tsn terminal_cnt
              |from
              |(select brand, license, province, appname, """" + date + """" dim_date, count(distinct dim_sn) as tsn
              |from
              |(select dim_apk,dim_date,dim_sn from hr.tracker_apk_fact_partition where date>DATE_SUB('""" + date + """', 7) and date<='""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three t) pc
              |on an.packagename=ai.dim_apk and ai.dim_sn=pc.sn
              |group by brand,license,province,appname
              |)apc
            """

        tracker_tv_apksql5.stripMargin
    }

    //分时应用
    val tracker_tv_apk_hourtable = "tracker_tv_apk_hour"
    //val tracker_tv_apk_hourtable = "test_tv_apk_hour"

    def getTracker_tv_apk_hoursql(date: String): String = {
        val tracker_tv_apk_hoursql =
            """
              |select tl.brand as brand,tl.license license, tl.province province, "" city, an.appname apk, ai.dim_date as tv_date, ai.dim_hour as tv_hour, count(distinct ai.dim_sn) as terminal_cnt
              |from (select dim_apk,dim_date,dim_hour,dim_sn from hr.tracker_apk_fact_partition where date = '""" + date + """') ai
              |join
              |(select distinct packagename,appname
              |from hr.apkinfo where onelevel='视频') an
              |join
              |(select brand, province, sn, license
              |from hr.sample_terminal_three) tl
              |on an.packagename=ai.dim_apk and ai.dim_sn=tl.sn
              |group by brand,license,province,dim_date,dim_hour,appname
            """
        tracker_tv_apk_hoursql.stripMargin
    }

    val tracker_total_tv_overviewtable = "tracker_total_tv_overview"
    //val tracker_total_tv_overviewtable = "test_total_tv_overview"

    def getTracker_total_tv_overviewsql(date: String, subDate: String): String = {

        val tracker_total_tv_overviewsql_1 =
            """
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition where behavior_type = '直播' and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst) fst
              |on ov.brand=fst.br and ov.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition where behavior_type = '智能电视开机' and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst) fst
              |on ov.brand=fst.br and ov.province=fst.pro
            """
        val tracker_total_tv_overviewsql_2 =
            """
              |select key, brand,license,province,city,behavior_type,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = 'OTT' and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ov
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ov.brand=sec.br and ov.license=sec.lic and ov.province=sec.pro
            """

        val tracker_total_tv_overviewsql =
            """
              |select key, brand,license,province,city,behavior_type,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition
              |where behavior_type = 'OTT' and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ov
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ov.brand=sec.br and ov.license=sec.lic and ov.province=sec.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition where behavior_type = '直播' and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst) fst
              |on ov.brand=fst.br and ov.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, period, tv_date, (terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,period,tv_date,terminal_cnt from hr.tracker_tv_overview_partition where behavior_type = '智能电视开机' and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ov
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst) fst
              |on ov.brand=fst.br and ov.province=fst.pro
            """

        tracker_total_tv_overviewsql.stripMargin
    }

    val tracker_total_tv_overview_hourtable = "tracker_total_tv_overview_hour"
    //val tracker_total_tv_overview_hourtable = "test_total_tv_overview_hour"

    def getTracker_total_tv_overview_hoursql(date: String): String = {
        val tracker_total_tv_overview_hoursql_1 =
            """
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = '直播' and date=DATE_SUB('""" + date + """', 0)) ovh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst) fst
              |on ovh.brand=fst.br and ovh.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = '智能电视开机' and date=DATE_SUB('""" + date + """', 0)) ovh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst) fst
              |on ovh.brand=fst.br and ovh.province=fst.pro
            """

        val tracker_total_tv_overview_hoursql_2 =
            """
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = 'OTT' and date=DATE_SUB('""" + date +"""', 0)) ovh
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ovh.brand=sec.br and ovh.license=sec.lic and ovh.province=sec.pro
            """

        val tracker_total_tv_overview_hoursql =
            """
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = 'OTT' and date=DATE_SUB('""" + date +"""', 0)) ovh
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ovh.brand=sec.br and ovh.license=sec.lic and ovh.province=sec.pro
              |union all
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = '直播' and date=DATE_SUB('""" + date + """', 0)) ovh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst) fst
              |on ovh.brand=fst.br and ovh.province=fst.pro
              |union all
              |select key, brand,license, province, city, behavior_type, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_overview_hour_partition where behavior_type = '智能电视开机' and date=DATE_SUB('""" + date + """', 0)) ovh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_oc_fst) fst
              |on ovh.brand=fst.br and ovh.province=fst.pro
            """

        tracker_total_tv_overview_hoursql.stripMargin
    }

    val tracker_total_tv_livetable = "tracker_total_tv_live"
    //val tracker_total_tv_livetable = "test_total_tv_live"

    def getTracker_total_tv_livesql(date: String, subDate: String): String = {
        val tracker_total_tv_livesql =
            """
              |select key, brand,license, province, city, channel, period, tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,channel,period,tv_date,terminal_cnt from hr.tracker_tv_live_partition where date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') tl
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst) fst
              |on tl.brand=fst.br and tl.province=fst.pro
            """
        tracker_total_tv_livesql.stripMargin
    }

    val tracker_total_tv_live_hourtable = "tracker_total_tv_live_hour"
    //val tracker_total_tv_live_hourtable = "test_total_tv_live_hour"

    def getTracker_total_tv_live_hoursql(date: String): String = {
        val tracker_total_tv_live_hoursql =
            """
              |select key, brand,license, province, city, channel, tv_date, tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,channel,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_live_hour_partition where date=DATE_SUB('""" + date + """', 0)) tlh
              |join
              |(select brand as br, province as pro,ratio from hr.tracker_total_dim_ratio_fst) fst
              |on tlh.brand=fst.br and tlh.province=fst.pro
            """
        tracker_total_tv_live_hoursql.stripMargin
    }

    val tracker_total_tv_apktable = "tracker_total_tv_apk"
    //val tracker_total_tv_apktable = "test_total_tv_apk"

    def getTracker_total_tv_apksql(date: String, subDate: String): String = {
        val tracker_total_tv_apksql =
            """
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,apk,period,tv_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,period,tv_date,terminal_cnt from hr.tracker_tv_apk_partition
              |where apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date>=DATE_SUB('""" + date +"""', """+ subDate +""") and date <= '""" + date +"""') ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec2) vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """
        tracker_total_tv_apksql.stripMargin
    }

    val tracker_total_tv_apk_hourtable = "tracker_total_tv_apk_hour"
    //val tracker_total_tv_apk_hourtable = "test_total_tv_apk_hour"

    def getTracker_total_tv_apk_hoursql(date: String): String = {
        val tracker_total_tv_apk_hoursql =
            """
              |select key,brand,license,province,city,apk,tv_date,tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_apk_hour_partition
              |where apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date=DATE_SUB('""" + date + """', 0)) ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,apk,tv_date,tv_hour,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key, brand,license,province,city,apk,tv_date,tv_hour,terminal_cnt from hr.tracker_tv_apk_hour_partition
              |where apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and date=DATE_SUB('""" + date + """', 0)) ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec2) vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """
        tracker_total_tv_apk_hoursql.stripMargin
    }

    //新增推总
    val tracker_total_tv_newaddtable = "tracker_total_tv_newadd"

    def getTracker_total_tv_newaddsql(date: String, subDate: String): String = {
        val td = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apklicsql =
            """
              |select key,brand,license,province,city,period,apk,dim_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,period,apk,dim_date,terminal_cnt from hr.tracker_tv_newadd
              |where apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and dim_date=DATE_SUB('""" + td + """', 0)) ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,period,apk,dim_date,(terminal_cnt*ratio) as terminal_cnt
              |from
              |(select key,brand,license,province,city,period,apk,dim_date,terminal_cnt from hr.tracker_tv_newadd
              |where apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端') and dim_date=DATE_SUB('""" + td + """', 0)) ta
              |join
              |(select brand as br, license as lic, province as pro,ratio from hr.tracker_total_dim_ratio_sec2) vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """

        tracker_total_tv_apklicsql.stripMargin
    }

    val tracker_total_tv_retaintable = "tracker_total_tv_retain"

    def getTracker_total_tv_retainsql(date: String): String = {
        val td = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val tracker_total_tv_apklic_hoursql =
            """
              |select key,brand,license,province,city,period,apk,new_date,retain_date,
              |(terminal_cnt*ratio) as terminal_cnt,(duration*ratio) duration,(cnt*ratio)cnt
              |from
              |(select key,brand,license,province,city,period,apk,new_date,retain_date,duration,cnt,terminal_cnt
              |from hr.tracker_tv_retain where retain_date=DATE_SUB('""" + td + """', 0)
              |and apk not in('CIBN环球影视','银河·奇异果','腾讯视频TV端')) ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec) sec
              |on ta.brand=sec.br and ta.license=sec.lic and ta.province=sec.pro
              |union all
              |select key,brand,license,province,city,period,apk,new_date,retain_date,
              |(terminal_cnt*ratio) as terminal_cnt,(duration*ratio) duration,(cnt*ratio)cnt
              |from
              |(select key,brand,license,province,city,period,apk,new_date,retain_date,duration,cnt,terminal_cnt
              |from hr.tracker_tv_retain where retain_date=DATE_SUB('""" + td + """', 0)
              |and apk in('CIBN环球影视','银河·奇异果','腾讯视频TV端')) ta
              |join
              |(select brand as br,license as lic,province as pro,ratio from hr.tracker_total_dim_ratio_sec2) vssec
              |on ta.brand=vssec.br and ta.license=vssec.lic and ta.province=vssec.pro
            """

        tracker_total_tv_apklic_hoursql.stripMargin
    }

    /**
      * 获取统计月前一个月和基础月 中间包含哪几个月
      * @param preStaticsDate 统计月前月份
      * @return
      */
    def getnewMonths(baseDate: String, preStaticsDate : String) : List[String]={
        var i = 0
        var list:List[String] =List()

        for (i<-0 to 20) {
            if (DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd").equals(preStaticsDate)) {
                //list = list :+ DateTime.parse("2017-01-01").plusMonths(i).toString("yyyy-MM-dd")
                return list
            }
            list = list :+ DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd")
        }

        list
    }
}

object hiveTrst {
    def main(args: Array[String]): Unit = {
        //println(HiveSql.getTracker_tv_overview_ottsql("2016-11-10"))
        //println("union all")
        //println(HiveSql.getTracker_tv_overview_livesql("2016-11-10"))
        //println(HiveSql.getTracker_tv_overview_ocsql("2016-11-10"))
        /*println(HiveSql.getTracker_total_tv_livesql("2016-11-30", "7"))

        println(HiveSql.getTracker_total_tv_apksql("2016-11-30", "0"))

        println(HiveSql.getTracker_total_tv_apk_hoursql("2016-12-04"))*/
        println(HiveSql.getTracker_tv_retainsql("2017-01-01", "2017-02-01"))
    }
}
package com.avcdata.spark.job.view

import java.io.Serializable

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 数据监控任务 (统计hive表信息)
  */
object HiveTableViewJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    sc.stop()
  }

  case class DataView(date: String, name: String, tpe: String, source: String, cnt: String) extends Serializable

  def run(sc: SparkContext, analysisDate: String) = {

    val resultArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //统计hive表信息
    //    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\view\\hr-tables.csv"

    val hdfsPath = "/user/hdfs/rsync/view/hr-tables.csv"

    val initRDD = sc.textFile(hdfsPath)

    val sqlArr = initRDD.map(line => {

      val cols = line.split(",")

      val tableName = cols(0)

      val dateCol = cols(1)

      var sql = "select count(1) cnt from hr." + tableName + " where " + dateCol + " = '" + analysisDate + "'"

      if (dateCol.equals("all")) {
        sql = "select count(1) cnt from hr." + tableName
      }
      sql + "," + tableName
    }).collect()

    //查询所有的hive表
    val sqlContext = new HiveContext(sc);


    for (line <- sqlArr) {

      val cols = line.split(",")

      val cntHQL = cols(0)

      //        date: String, name: String, tpe: String, source: String, cnt: String
      val date = analysisDate
      val name = cols(1)
      val tpe = "hive"
      val source = "aowei"
      val cnt = sqlContext.sql(cntHQL).collect()(0).get(0).toString

      val dataView = DataView(date, name, tpe, source, cnt)

      resultArrayBuffer += dataView

    }

    println("resultArrayBuffer.size:" + resultArrayBuffer.size)

    import sqlContext.implicits._
    //    println(resultArrayBuffer.length)
    //将统计结果写入Mysql
    val df = sc.parallelize(resultArrayBuffer).toDF()

    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "vboxDB", "check_view", false)


  }
}
package com.avcdata.vbox.view

import java.io.Serializable

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.JdbcUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 数据监控任务 (统计hive表信息)
  */
object HiveTableViewJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    sc.stop()
  }

  case class DataView(date: String, name: String, tpe: String, source: String, cnt: String) extends Serializable

  def run(sc: SparkContext, analysisDate: String) = {

    val resultArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //统计hive表信息
    //    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\view\\hr-tables.csv"

    val hdfsPath = "/user/hdfs/rsync/view/hr-tables.csv"

    val initRDD = sc.textFile(hdfsPath)

    val sqlArr = initRDD.map(line => {

      val cols = line.split(",")

      val tableName = cols(0)

      val dateCol = cols(1)

      var sql = "select count(1) cnt from hr." + tableName + " where " + dateCol + " = '" + analysisDate + "'"

      if (dateCol.equals("all")) {
        sql = "select count(1) cnt from hr." + tableName
      }
      sql + "," + tableName
    }).collect()

    //查询所有的hive表
    val sqlContext = new HiveContext(sc);


    for (line <- sqlArr) {

      val cols = line.split(",")

      val cntHQL = cols(0)

      //        date: String, name: String, tpe: String, source: String, cnt: String
      val date = analysisDate
      val name = cols(1)
      val tpe = "hive"
      val source = "aowei"
      val cnt = sqlContext.sql(cntHQL).collect()(0).get(0).toString

      val dataView = DataView(date, name, tpe, source, cnt)

      resultArrayBuffer += dataView

    }

    println("resultArrayBuffer.size:" + resultArrayBuffer.size)

    import sqlContext.implicits._
    //    println(resultArrayBuffer.length)
    //将统计结果写入Mysql
    val df = sc.parallelize(resultArrayBuffer).toDF()

//    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "vboxDB", "check_view", false)


  }
}
package com.avcdata.spark.job.view

import java.io.Serializable

import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 数据监控任务 (统计hive表信息)
  */
object HiveTableViewJob01 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    sc.stop()
  }

  case class DataView(date: String, name: String, tpe: String, source: String, cnt: String) extends Serializable

  def run(sc: SparkContext, analysisDate: String) = {

    val resultArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //统计hive表信息
    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\view\\hr-tables.csv"

    //    val hdfsPath = "/user/hdfs/rsync/view/hr-tables.csv"

    val initRDD = sc.textFile(hdfsPath)

    initRDD.foreach(line => {


      val cols = line.split(",")

      val tableName = cols(0)
      val dateCol = cols(1)
      val brandCol = cols(2)

      var sql = "select bd,count(1) from " + tableName + " where " + dateCol + " = '" + analysisDate + "' group by " +
        brandCol

      if (dateCol.equals("all")) {
        if (brandCol.equals("all")) {
          sql =  "select 'source' brand,count(1) from " + tableName
          brandCol

        } else {

        }
      } else {

        if (brandCol.equals("all")) {

        } else {

        }


      }






      println(sql)
    })


    //统计Hive表信息
    val tableArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //查询所有的hive表
    val sqlContext = new SQLContext(sc);


    //将统计结果写入Mysql
    //    val df = sc.parallelize(tableArrayBuffer).toDF()

    //    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "vboxDB", "check_view", false)


    //统计hive表信息

  }


}
package com.avcdata.vbox.view

import java.io.Serializable

import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 数据监控任务 (统计hive表信息)
  */
object HiveTableViewJob01 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    sc.stop()
  }

  case class DataView(date: String, name: String, tpe: String, source: String, cnt: String) extends Serializable

  def run(sc: SparkContext, analysisDate: String) = {

    val resultArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //统计hive表信息
    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\view\\hr-tables.csv"

    //    val hdfsPath = "/user/hdfs/rsync/view/hr-tables.csv"

    val initRDD = sc.textFile(hdfsPath)

    initRDD.foreach(line => {


      val cols = line.split(",")

      val tableName = cols(0)
      val dateCol = cols(1)
      val brandCol = cols(2)

      var sql = "select bd,count(1) from " + tableName + " where " + dateCol + " = '" + analysisDate + "' group by " +
        brandCol

      if (dateCol.equals("all")) {
        if (brandCol.equals("all")) {
          sql =  "select 'source' brand,count(1) from " + tableName
          brandCol

        } else {

        }
      } else {

        if (brandCol.equals("all")) {

        } else {

        }


      }






      println(sql)
    })


    //统计Hive表信息
    val tableArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //查询所有的hive表
    val sqlContext = new SQLContext(sc);


    //将统计结果写入Mysql
    //    val df = sc.parallelize(tableArrayBuffer).toDF()

    //    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "vboxDB", "check_view", false)


    //统计hive表信息

  }


}
package com.avcdata.spark.job.common

import java.util.UUID

import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2016/11/18.
  * 读取hive数据，写入hbase
  */
object HiveToHbase {
    def write(sqlc: HiveContext, sql: String, hbaseTable:String, flag : Int) = {
        val infoFamilyCol = Bytes.toBytes("info")

        //brand  license  province  city  behavior_type  period  tv_date  terminal_cnt
        val infoBrandCol = Bytes.toBytes("brand")  //厂商
        val infoLicenseCol = Bytes.toBytes("license") //牌照
        val infoProvinceCol = Bytes.toBytes("province") //省
        val infoCityCol = Bytes.toBytes("city") //市
        val infoTypeCol = Bytes.toBytes("behavior_type") //行为类型
        val infoPeriodCol = Bytes.toBytes("period") //周期
        val infoDateCol = Bytes.toBytes("tv_date") //日期
        val infoApkCol = Bytes.toBytes("apk") //应用
        val infoChannelCol = Bytes.toBytes("channel") //频道
        val infoHourCol = Bytes.toBytes("tv_hour") //整点
        val infoTerminalcntCol = Bytes.toBytes("terminal_cnt") //终端数
        val infoDimDateCol = Bytes.toBytes("dim_date") //新增终端里面的
        val infoNewdateCol = Bytes.toBytes("new_date") //留存终端
        val infoRetaindateCol = Bytes.toBytes("retain_date") //留存终端
        //val infoDuraCol = Bytes.toBytes("duration") //留存终端时长
        //val infoCntCol = Bytes.toBytes("cnt") //留存终端次数

        println(sql)
        val hiveDataFrame = sqlc.sql(sql)

        hiveDataFrame.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf(hbaseTable))
            try {
                items.foreach(x => {
                    var brand = ""
                    var license = ""
                    var province = ""
                    var city = ""
                    var tcnt = ""
                    var dura = ""
                    var cnt = ""
                    var newDate = ""
                    var retainDate = ""
                    var col4 = ""
                    var col5 = ""
                    var col6 = ""
                    var put:Put = null
                    if (flag == 20 || flag == 21 || flag == 22 || flag == 23 || flag == 24 || flag == 25) {
                        brand = x(0).toString
                        license = x(1).toString
                        province = x(2).toString
                        city = x(3).toString
                        tcnt = x(7).toString
                        //dura = x(8).toString
                        //cnt = x(9).toString
                        col4 = x(4).toString
                        col5 = x(5).toString
                        col6 = x(6).toString

                        put = new Put(Bytes.toBytes(brand + license + province + city + col4 + col5 + col6))
                    } else if (flag == 26 || flag == 27 || flag == 28 || flag == 29 || flag == 30 || flag == 31) {
                        brand = x(1).toString
                        license = x(2).toString
                        province = x(3).toString
                        city = x(4).toString
                        tcnt = x(8).toString
                        col4 = x(5).toString
                        col5 = x(6).toString
                        col6 = x(7).toString
                        put = new Put(Bytes.toBytes(x(0).toString))
                    } else if (flag == 32) {
                        brand = x(0).toString
                        license = x(1).toString
                        province = x(2).toString
                        city = x(3).toString
                        col4 = x(4).toString //period
                        col5 = x(5).toString //apk
                        col6 = x(6).toString //date
                        tcnt = x(7).toString
                        put = new Put(Bytes.toBytes(brand + license + province + city + col4 + col5 + col6))
                    } else if (flag == 34) {
                        brand = x(1).toString
                        license = x(2).toString
                        province = x(3).toString
                        city = x(4).toString
                        col4 = x(5).toString
                        col5 = x(6).toString
                        col6 = x(7).toString
                        tcnt = x(8).toString
                        put = new Put(Bytes.toBytes(x(0).toString))
                    } else if (flag == 33) {
                        brand = x(0).toString
                        license = x(1).toString
                        province = x(2).toString
                        city = x(3).toString
                        col4 = x(4).toString //period
                        col5 = x(5).toString //apk
                        newDate = x(6).toString //new_date
                        retainDate = x(7).toString //retain_date
                        tcnt = x(8).toString  //终端数
                        //dura = x(9).toString
                        //cnt = x(10).toString  //次数

                        put = new Put(Bytes.toBytes(brand + license + province + city + col4 + col5 + newDate + retainDate))
                    } else if (flag == 35) {
                        brand = x(1).toString
                        license = x(2).toString
                        province = x(3).toString
                        city = x(4).toString
                        col4 = x(5).toString //period
                        col5 = x(6).toString //apk
                        newDate = x(7).toString
                        retainDate = x(8).toString
                        tcnt = x(9).toString  //终端数
                        //dura = x(10).toString
                        //cnt = x(11).toString  //次数
                        put = new Put(Bytes.toBytes(x(0).toString))
                    }

                    put.addColumn(infoFamilyCol, infoBrandCol, Bytes.toBytes(brand))
                    put.addColumn(infoFamilyCol, infoLicenseCol, Bytes.toBytes(license))
                    put.addColumn(infoFamilyCol, infoProvinceCol, Bytes.toBytes(province))
                    put.addColumn(infoFamilyCol, infoCityCol, Bytes.toBytes(city))
                    put.addColumn(infoFamilyCol, infoTerminalcntCol, Bytes.toBytes(tcnt))
                    //put.addColumn(infoFamilyCol, infoDuraCol, Bytes.toBytes(dura))
                    //put.addColumn(infoFamilyCol, infoCntCol, Bytes.toBytes(cnt))
                    if(flag == 20 || flag == 26) {
                        //tv总览
                        put.addColumn(infoFamilyCol, infoTypeCol, Bytes.toBytes(col4))  //behavior_type
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col5))  //period
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 21 || flag == 27) {
                        //分时tv总览
                        put.addColumn(infoFamilyCol, infoTypeCol, Bytes.toBytes(col4))  //behavior_type
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col5))  //date
                        put.addColumn(infoFamilyCol, infoHourCol, Bytes.toBytes(col6))  //hour
                    } else if (flag == 22 || flag == 28) {
                        //直播
                        put.addColumn(infoFamilyCol, infoChannelCol, Bytes.toBytes(col4))  //channel
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col5))  //period
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 23 || flag == 29) {
                        //分时直播
                        put.addColumn(infoFamilyCol, infoChannelCol, Bytes.toBytes(col4))  //channel
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col5))  //date
                        put.addColumn(infoFamilyCol, infoHourCol, Bytes.toBytes(col6))  //hour
                    } else if (flag == 24 || flag == 30) {
                        //应用
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col4))  //apk
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col5))  //period
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 25 || flag == 31) {
                        //分时应用
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col4))  //apk
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col5))  //date
                        put.addColumn(infoFamilyCol, infoHourCol, Bytes.toBytes(col6))  //hour
                    } else if (flag == 32 || flag == 34) {
                        //新增终端
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col4))  //period
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col5))  //apk
                        put.addColumn(infoFamilyCol, infoDimDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 33 || flag == 35) {
                        //留存终端
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col4))  //period
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col5))  //apk
                        put.addColumn(infoFamilyCol, infoNewdateCol, Bytes.toBytes(newDate))
                        put.addColumn(infoFamilyCol, infoRetaindateCol, Bytes.toBytes(retainDate))
                        //put.addColumn(infoFamilyCol, infoDuraCol, Bytes.toBytes(dura))
                        //put.addColumn(infoFamilyCol, infoCntCol, Bytes.toBytes(cnt))
                    }

                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
        println("----------------------------------------------------------------------")

    }
}
package com.avcdata.vbox.common

import java.util.UUID

import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2016/11/18.
  * 读取hive数据，写入hbase
  */
object HiveToHbase {
    def write(sqlc: HiveContext, sql: String, hbaseTable:String, flag : Int) = {
        val infoFamilyCol = Bytes.toBytes("info")

        //brand  license  province  city  behavior_type  period  tv_date  terminal_cnt
        val infoBrandCol = Bytes.toBytes("brand")  //厂商
        val infoLicenseCol = Bytes.toBytes("license") //牌照
        val infoProvinceCol = Bytes.toBytes("province") //省
        val infoCityCol = Bytes.toBytes("city") //市
        val infoTypeCol = Bytes.toBytes("behavior_type") //行为类型
        val infoPeriodCol = Bytes.toBytes("period") //周期
        val infoDateCol = Bytes.toBytes("tv_date") //日期
        val infoApkCol = Bytes.toBytes("apk") //应用
        val infoChannelCol = Bytes.toBytes("channel") //频道
        val infoHourCol = Bytes.toBytes("tv_hour") //整点
        val infoTerminalcntCol = Bytes.toBytes("terminal_cnt") //终端数
        val infoDimDateCol = Bytes.toBytes("dim_date") //新增终端里面的
        val infoNewdateCol = Bytes.toBytes("new_date") //留存终端
        val infoRetaindateCol = Bytes.toBytes("retain_date") //留存终端
        val infoDuraCol = Bytes.toBytes("duration") //留存终端时长
        val infoCntCol = Bytes.toBytes("cnt") //留存终端次数

        println(sql)
        val hiveDataFrame = sqlc.sql(sql)

        hiveDataFrame.rdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.20.210,192.168.20.211,192.168.20.212")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf(hbaseTable))
            try {
                items.foreach(x => {
                    var brand = ""
                    var license = ""
                    var province = ""
                    var city = ""
                    var tcnt = ""
                    var dura = ""
                    var cnt = ""
                    var newDate = ""
                    var retainDate = ""
                    var col4 = ""
                    var col5 = ""
                    var col6 = ""
                    var put:Put = null
                    if (flag == 20 || flag == 21 || flag == 22 || flag == 23 || flag == 24 || flag == 25) {
                        //println("-------------------------------1---------------------------------------")
                        brand = x(0).toString
                        license = x(1).toString
                        province = x(2).toString
                        city = x(3).toString
                        tcnt = x(7).toString
                        //dura = x(8).toString
                        //cnt = x(9).toString
                        col4 = x(4).toString
                        col5 = x(5).toString
                        col6 = x(6).toString

                        put = new Put(Bytes.toBytes(brand + license + province + city + col4 + col5 + col6))
                    } else if (flag == 26 || flag == 27 || flag == 28 || flag == 29 || flag == 30 || flag == 31) {
                        //println("-------------------------------2---------------------------------------")
                        brand = x(1).toString
                        license = x(2).toString
                        province = x(3).toString
                        city = x(4).toString
                        tcnt = x(8).toString
                        col4 = x(5).toString
                        col5 = x(6).toString
                        col6 = x(7).toString
                        put = new Put(Bytes.toBytes(x(0).toString))
                    } else if (flag == 32) {
                        brand = x(0).toString
                        license = x(1).toString
                        province = x(2).toString
                        city = x(3).toString
                        col4 = x(4).toString //period
                        col5 = x(5).toString //apk
                        col6 = x(6).toString //date
                        tcnt = x(7).toString
                        put = new Put(Bytes.toBytes(brand + license + province + city + col4 + col5 + col6))
                    } else if (flag == 34) {
                        brand = x(1).toString
                        license = x(2).toString
                        province = x(3).toString
                        city = x(4).toString
                        col4 = x(5).toString
                        col5 = x(6).toString
                        col6 = x(7).toString
                        tcnt = x(8).toString
                        put = new Put(Bytes.toBytes(x(0).toString))
                    } else if (flag == 33) {
                        brand = x(0).toString
                        license = x(1).toString
                        province = x(2).toString
                        city = x(3).toString
                        col4 = x(4).toString //period
                        col5 = x(5).toString //apk
                        newDate = x(6).toString //new_date
                        retainDate = x(7).toString //retain_date
                        tcnt = x(8).toString  //终端数
                        dura = x(9).toString
                        cnt = x(10).toString  //次数

                        put = new Put(Bytes.toBytes(brand + license + province + city + col4 + col5 + newDate + retainDate))
                    } else if (flag == 35) {
                        brand = x(1).toString
                        license = x(2).toString
                        province = x(3).toString
                        city = x(4).toString
                        col4 = x(5).toString //period
                        col5 = x(6).toString //apk
                        newDate = x(7).toString
                        retainDate = x(8).toString
                        tcnt = x(9).toString  //终端数
                        dura = x(10).toString
                        cnt = x(11).toString  //次数
                        put = new Put(Bytes.toBytes(x(0).toString))
                    }

                    put.addColumn(infoFamilyCol, infoBrandCol, Bytes.toBytes(brand))
                    put.addColumn(infoFamilyCol, infoLicenseCol, Bytes.toBytes(license))
                    put.addColumn(infoFamilyCol, infoProvinceCol, Bytes.toBytes(province))
                    put.addColumn(infoFamilyCol, infoCityCol, Bytes.toBytes(city))
                    put.addColumn(infoFamilyCol, infoTerminalcntCol, Bytes.toBytes(tcnt))
                    //put.addColumn(infoFamilyCol, infoDuraCol, Bytes.toBytes(dura))
                    //put.addColumn(infoFamilyCol, infoCntCol, Bytes.toBytes(cnt))
                    if(flag == 20 || flag == 26) {
                        //tv总览
                        put.addColumn(infoFamilyCol, infoTypeCol, Bytes.toBytes(col4))  //behavior_type
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col5))  //period
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 21 || flag == 27) {
                        //分时tv总览
                        put.addColumn(infoFamilyCol, infoTypeCol, Bytes.toBytes(col4))  //behavior_type
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col5))  //date
                        put.addColumn(infoFamilyCol, infoHourCol, Bytes.toBytes(col6))  //hour
                    } else if (flag == 22 || flag == 28) {
                        //直播
                        put.addColumn(infoFamilyCol, infoChannelCol, Bytes.toBytes(col4))  //channel
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col5))  //period
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 23 || flag == 29) {
                        //分时直播
                        println("23 : " + col4 + "---" + col5 + "---" + col6)
                        put.addColumn(infoFamilyCol, infoChannelCol, Bytes.toBytes(col4))  //channel
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col5))  //date
                        put.addColumn(infoFamilyCol, infoHourCol, Bytes.toBytes(col6))  //hour
                    } else if (flag == 24 || flag == 30) {
                        //应用
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col4))  //apk
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col5))  //period
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 25 || flag == 31) {
                        //分时应用
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col4))  //apk
                        put.addColumn(infoFamilyCol, infoDateCol, Bytes.toBytes(col5))  //date
                        put.addColumn(infoFamilyCol, infoHourCol, Bytes.toBytes(col6))  //hour
                    } else if (flag == 32 || flag == 34) {
                        //新增终端
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col4))  //period
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col5))  //apk
                        put.addColumn(infoFamilyCol, infoDimDateCol, Bytes.toBytes(col6))  //date
                    } else if (flag == 33 || flag == 35) {
                        //留存终端
                        put.addColumn(infoFamilyCol, infoPeriodCol, Bytes.toBytes(col4))  //period
                        put.addColumn(infoFamilyCol, infoApkCol, Bytes.toBytes(col5))  //apk
                        put.addColumn(infoFamilyCol, infoNewdateCol, Bytes.toBytes(newDate))
                        put.addColumn(infoFamilyCol, infoRetaindateCol, Bytes.toBytes(retainDate))
                        put.addColumn(infoFamilyCol, infoDuraCol, Bytes.toBytes(dura))
                        put.addColumn(infoFamilyCol, infoCntCol, Bytes.toBytes(cnt))
                    }

                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
        println("----------------------------------------------------------------------")

    }
}
package com.avcdata.spark.job.total

import java.sql.DriverManager

import com.avcdata.spark.job.common.Helper
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SaveMode, DataFrame}
import org.apache.spark.sql.hive.HiveContext

import scala.tools.nsc.Properties

/**
  * @author zhangyongtian
  * @define 查询Hive写入mysql工具类
  */
object HiveToMysql {
  def main(args: Array[String]): Unit = {

    //默认配置文件读取
    val sc = Helper.sparkContext

    //地区分布
    //    val sql_day = Sql.vbox_tv_eara_daily_hql
    //
    //    val sql_week = Sql.vbox_tv_eara_weekly_hql
    //
    //    val sql_month = Sql.vbox_tv_eara_monthly_hql
    //
    //    val tableName = Sql.vbox_tv_eara_tableName
    //
    //    run(sc, Helper.mysqlConf, tableName, sql_day, sql_week, sql_month)


    //排行榜
    val sql_day = Sql.vbox_tv_eara_hql

    //    val sql_day = Sql.vbox_tv_ranking_hql

    val sql_week = Sql.vbox_tv_ranking_weekly_hql

    val sql_month = Sql.vbox_tv_ranking_monthly_hql

    val tableName = Sql.vbox_tv_eara_tableName

    //    val tableName = Sql.vbox_tv_ranking_tableName

    run(sc, Helper.mysqlConf, tableName, sql_day, sql_week, sql_month)

    sc.stop()
  }

  def run(sc: SparkContext,
          prop: java.util.Properties, tableName: String, sql_day: String, sql_week: String, sql_month: String) = {

    val log = Logger.getLogger(getClass.getName)

    ///////////////////////////////////从hive中查询数据
    val hiveContext: HiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr") //使用hive 数据库

    val df_day: DataFrame = hiveContext.sql(sql_day).distinct()

    //    val df_week: DataFrame = hiveContext.sql(sql_week)

    //    val df_month: DataFrame = hiveContext.sql(sql_month)

    //    val df_all = df_day.unionAll(df_day).unionAll(df_week).unionAll(df_month).coalesce(1).distinct()



    ////////////////////////////////////通过jdbc 将数据写入mysql
    //    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + sc.getConf.get("mysql.db") + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/vboxDB" + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    //    清空数据
    //    truncate(url, tableName, prop)


    df_day.write
      .mode(SaveMode.Append)
      .jdbc(url, tableName, prop)


  }

  def truncate(url: String, tableName: String, prop: java.util.Properties) = {
    classOf[com.mysql.jdbc.Driver]
    val conn = DriverManager.getConnection(url + "&user=" + prop.getProperty("user") + "&password=" + prop.getProperty("password"))
    try {
      val statement = conn.createStatement()
      statement.executeUpdate("TRUNCATE " + tableName);
    }

    catch {
      case e: Exception => e.printStackTrace
    }
    finally {
      conn.close
    }
  }


}
package com.avcdata.spark.job.konka

import java.sql.DriverManager

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.{SaveMode, SQLContext}

object HiveToMysql {

  def write(
             sc: SparkContext,
             sql: String,
             targetTable: String,
             prop: java.util.Properties) = {


    val sqlc = new HiveContext(sc)

    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + sc.getConf.get("mysql.db") + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    truncate(url,targetTable,prop)

    sqlc.sql(sql).write
      .mode(SaveMode.Append)
      .jdbc(url, targetTable, prop)

  }

  def truncate(url: String, tableName: String,prop: java.util.Properties) = {
    classOf[com.mysql.jdbc.Driver]
    val conn = DriverManager.getConnection(url+"&user="+prop.getProperty("user")+"&password="+prop.getProperty("password"))
    try {
      val statement = conn.createStatement()
      statement.executeUpdate("TRUNCATE " + tableName);
    }

    catch {
      case e: Exception => e.printStackTrace
    }
    finally {
      conn.close
    }
  }
}
package com.avcdata.spark.job.konka

import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import com.redislabs.provider.redis._

object HiveToRedis {
  def writeToHash(sc: SparkContext,sqlc:SQLContext,sql:String,key:String,keyCol:String,ttl:Int=3600) = {

    val df = sqlc.sql(sql)

    val cols = df.columns

    val rdd = df.map(row=>{
      var json = "{"
      cols.foreach(col=>{
        json+= "\""+col+"\":\""+row.getAs[String](col)+"\","
      })
      json = json.substring(0,json.length-1)
      json+="}"
      (row.getAs[String](keyCol),json)
    }).toJavaRDD().rdd

    sc.toRedisHASH(rdd,key,ttl);

  }
}
package com.avcdata.spark.job.util

import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

object HiveUtils {

  case class Person(name: String, col1: Int, col2: String)

  def saveAsTable(sc: SparkContext, filePath: String, tableName: String) {

    //    insertInto函数是向表中写入数据，可以看出此函数不能指定数据库和分区等信息，不可以直接进行写入。
    //    向hive数据仓库写入数据必须指定数据库，hive数据表建立可以在hive上建立，或者使用hiveContext.sql（“create table ...."）

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    val data = sc.textFile(filePath).map(x => x.split("\\s+"))
      .map(x => Person(x(0), x(1).toInt, x(2)))
    data.toDF().write.mode(SaveMode.Append).insertInto(tableName)
  }


  def insert2tableByTempTable(sc: SparkContext, filePath: String, tableName: String) {

    //    insertInto函数是向表中写入数据，可以看出此函数不能指定数据库和分区等信息，不可以直接进行写入。
    //    向hive数据仓库写入数据必须指定数据库，hive数据表建立可以在hive上建立，或者使用hiveContext.sql（“create table ...."）

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    //DataFrame数据写入hive指定数据表的分区中时产生大量小文件该怎么解决
    //    hiveContext.setConf("spark.sql.shuffle.partitions", "1")

    val data = sc.textFile(filePath).map(x => x.split("\\s+"))
      .map(x => Person(x(0), x(1).toInt, x(2)))
    data.toDF().registerTempTable("table1")

    hiveContext.sql("insert into " + tableName + " partition(date='2015-04-02') select name,col1,col2 from table1")
  }


  def load2tableByHdfsFile(sc: SparkContext, filePath: String, output_tmp_dir: String, tableName: String, date: String):
  Unit = {
    //output_tmp_dir = "/user/hdfs/rsync/tmp/apk"

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    sc.textFile(filePath)
      .map { r => r.mkString("\001") }.repartition(100).saveAsTextFile(output_tmp_dir)

    hiveContext.sql(s"""load data inpath '$output_tmp_dir' overwrite into table $tableName partition (dt='$date')""")


  }


  //    dataframe.registerTempTable("result")
  //    sql(s"""INSERT OVERWRITE Table $outputTable PARTITION (dt ='$outputDate') select * from result""")
  //    而整个结果数据的产生只需要4分钟左右的时间，比如以下方式：将结果以textfile存入hdfs：
  //    result.rdd.saveAsTextFile(output_tmp_dir)
  //    由此可见，对hive的写入操作耗用了大量的时间。


  //    对此现象的优化可以是，将文件存为符合hive table文件的格式，然后使用hive load将产生的结果文件直接move到指定目录下。代码如下：
  //    result.rdd.map { r => r.mkString("\001") }.repartition(partitions).saveAsTextFile(output_tmp_dir)
  //    sql(s"""load data inpath '$output_tmp_dir' overwrite into table $output partition (dt='$dt')""")


}
package utils

import java.text.SimpleDateFormat

import com.github.nscala_time.time.Imports._
import org.apache.hadoop.fs.Path
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.ListBuffer
import scala.util.control.Breaks._

/**
  * Created by guxiaoyang on 2017/6/16.
  */
object HiveUtils {

  case class snTime(sn: String, durantion: Int, cnt: Int) extends Serializable

  case class snOCTImeRow(key: String, sn: String, length: String, cnt: String, date: String, hour: String)

  case class playData(key: String, sn: String, originalName: String, standardName: String, awcid: String, part: String, vv: String, duration: String, year: String, model: String, crowd: String, region: String, date: String, hour: String, apk: String)

  case class apkData(key: String, sn: String, apk: String, cnt: String, duration: String, date: String, hour: String)

  /*
  开关机分区
   */
  def OCDataPartition(sc: SparkContext, now: String): Unit = {
    val hiveContext = HiveContextHolder.getInstance(sc)

    val tableName = "tracker_sdk_stream" + now.split(" ")(0).replace("-", "_")
    hiveContext.sql("use hr")
    val crealTableSql =
      s"""CREATE EXTERNAL TABLE if not exists $tableName (key string, sn string,inserttime string,date string,hour string, manufacturer string,messagetype string,
build_date string,product string,version_release string,name string,description string,model string,restarttime string,duration string,programstart string,programtype string,
provider string,programend string,programname string,package_name string,order string,ipmac string,province string,city string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" =
"status:sn,status:inserttime,status:date,status:hour,status:manufacturer,status:messagetype,content:build_date,content:product,content:version_release,content:name,content:description,content:model,
content:restarttime,content:duration,content:programstart,content:programtype,content:provider,content:programend,content:programname,content:package_name,content:order,content:ipmac,
content:province,content:city")
TBLPROPERTIES("hbase.table.name" = "$tableName")"""
    hiveContext.sql(crealTableSql)

    import hiveContext.implicits._
    case class hiveOCItem(key: String, sn: String, power_on_time: String, power_on_length: String, cnt: String)

    //获取当前小时段内终端的开机和心跳数据
    val power_on_day = now.split(" ")(0)
    val power_on_time = now.split(" ")(1).split(":")(0)
    val dateDF = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val insertTimeStart = now.split(":")(0)
    val insertTimeEnd = power_on_day + " " + (power_on_time.toInt + 1)

    val sqlOCHour =
      s"""select sn,date, hour,messagetype from hr.$tableName where  inserttime >='$insertTimeStart:00:00'
          |and inserttime<'$insertTimeEnd:00:00'
          | order by sn,date, hour """.stripMargin

    println("读取插入tracker_oc_sdk_partition分区表的基础数据SQL:" + sqlOCHour)
    val initRDD = hiveContext.sql(sqlOCHour).rdd.map(line => {
      val sn = line(0)
      val date = line(1)
      val hour = line(2)
      val messageType = line(3)
      (sn.toString, date + " " + hour, messageType.toString)
    }
    )

    //计算每个sn在小时内的使用时长,需要过滤掉错误日期数据
    val snTimeRDD = initRDD.map(row => (row._1, row._2 + "#" + row._3)).reduceByKey((pre, post) => {
      pre + "|" + post
    }
    ).flatMap(row => {
      val sn = row._1
      //2017-07-03 20:22:35#1|2017-07-03 20:25:35#1|2017-07-03 20:27:35#1|2017-07-03 20:31:35#2|2017-07-03 20:32:35#1
      var durationTotal = 0
      var cnt = 0
      val timeTypeArr = row._2.split("\\|").map(a => {
        val date = a.split("#")(0)
        val messageType = a.split("#")(1)
        (date, messageType)
      }
      )

      val list = new ListBuffer[snTime]()
      for (i <- 0 until timeTypeArr.length) {
        val date = timeTypeArr(i)._1
        val messageType = timeTypeArr(i)._2
        if (i == 0 && messageType == "20") {
          //如果这个sn的第一个时间是待机时间,那么要拿这个待机时间减去该时间所处日期的开始整点
          val hourBegin = date.split(":")(0) + ":00:00"
          val span = (dateDF.parse(date).getTime - dateDF.parse(hourBegin).getTime) / 1000
          if (span <= 240) durationTotal += span.toInt
        }
        if (messageType == "1" || messageType == "21") {
          cnt += 1
          if (i == timeTypeArr.length - 1) {
            //如果这个sn的最后一个时间是开机时间,那么要拿这个开机时间减去该时间所处日期的结束整点
            val hourEnd = DateTime.parse(date, DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")).plusHours(1).toString("yyyy-MM-dd HH") + ":00:00"
            val span = (dateDF.parse(hourEnd).getTime - dateDF.parse(date).getTime) / 1000
            if (span <= 240) durationTotal += span.toInt
          }
          else {
            breakable {
              //从开机时间往后找开机时间和待机时间.找到后面的开机时间on2,则拿on2-1的时间减去on1的时间. 找到待机时间,则拿待机的时间减去on1的时间
              for (j <- i until timeTypeArr.length) {
                val messageTypeRight = timeTypeArr(j)._2
                if ((messageTypeRight == "1" || messageType == "21") && i != j) {
                  val dateRight = timeTypeArr(j - 1)._1
                  val span = (dateDF.parse(dateRight).getTime - dateDF.parse(date).getTime) / 1000
                  durationTotal += span.toInt
                  break
                }
                else if (messageTypeRight == "20") {
                  val dateRight = timeTypeArr(j)._1
                  val span = (dateDF.parse(dateRight).getTime - dateDF.parse(date).getTime) / 1000
                  durationTotal += span.toInt
                  break
                }
              }
            }
          }
        }
      }
      //如果没有开机时间,则计算所有行为的时间间隔,直到第一个待机时间为止
      if (cnt == 0) {
        breakable {
          for (i <- 0 until timeTypeArr.length - 1) {
            if (i == 0) {
              //第1个时间减去该小时的开始时间为该使用时长
              val date = timeTypeArr(0)._1
              val hourBegin = date.split(":")(0) + ":00:00"
              val span = (dateDF.parse(date).getTime - dateDF.parse(hourBegin).getTime) / 1000
              if (span <= 240) durationTotal += span.toInt
            }
            else {
              val leftDate = timeTypeArr(i)._1
              val leftType = timeTypeArr(i)._2

              val rightDate = timeTypeArr(i + 1)._1
              val rightType = timeTypeArr(i + 1)._2

              val span = (dateDF.parse(rightDate).getTime - dateDF.parse(leftDate).getTime) / 1000
              if (span <= 240) durationTotal += span.toInt

              if (rightType == "20") break
            }
          }
        }
      }
      list += snTime(sn, durationTotal, cnt)
    }
    )
    val resultRdd = snTimeRDD.map(row => {
      val sn = row.sn
      val power_on_length = row.durantion
      val cnt = row.cnt
      val key = sn + power_on_day + power_on_time
      snOCTImeRow(key, sn, power_on_length.toString, cnt.toString, power_on_day, power_on_time)
    }
    ).filter(row => row.length != "0" || row.cnt != "0").cache()

    //hiveContext.createDataFrame(resultRdd).coalesce(1).write.mode(SaveMode.Append).partitionBy("date").orc("/user/hive/warehouse/hr.db/tracker_oc_sdk_partition_test")


    val output_tmp_dir =s"""/user/hive/warehouse/hr.db/tracker_oc_sdk_partition_test/tracker_oc_sdk_partition_temp/date=$power_on_day/hour=$power_on_time"""
    resultRdd.toDF.map(a => a.mkString("\001")).repartition(20).saveAsTextFile(output_tmp_dir)
    val insertSql =s"""load data inpath '$output_tmp_dir' overwrite into table hr.tracker_oc_sdk_partition_test partition (date='$power_on_day',hour='$power_on_time')"""
    hiveContext.sql(insertSql)

    println("插入tracker_oc_sdk_partition分区表数据成功")
  }

  /*
  到剧和apk数据清洗
   */
  def PlayDataPartition(sc: SparkContext, nowTime: String): Unit = {
    //val hiveContext = new HiveContext(sc)
    val hiveContext = HiveContextHolder.getInstance(sc)
    val tableName = "tracker_sdk_stream" + nowTime.split(" ")(0).replace("-", "_")
    hiveContext.sql("use hr")
    val crealTableSql =
      s"""CREATE EXTERNAL TABLE if not exists $tableName (key string, sn string,inserttime string,date string,hour string, manufacturer string,messagetype string,
build_date string,product string,version_release string,name string,description string,model string,restarttime string,duration string,programstart string,programtype string,
provider string,programend string,programname string,package_name string,order string,ipmac string,province string,city string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" =
"status:sn,status:inserttime,status:date,status:hour,status:manufacturer,status:messagetype,content:build_date,content:product,content:version_release,content:name,
content:description,content:model,
content:restarttime,content:duration,content:programstart,content:programtype,content:provider,content:programend,content:programname,content:package_name,content:order,content:ipmac,
content:province,content:city")
TBLPROPERTIES("hbase.table.name" = "$tableName")"""
    hiveContext.sql(crealTableSql)

    import hiveContext.implicits._
    val nowDay = nowTime.split(" ")(0)
    val nowHour = nowTime.split(" ")(1).split(":")(0)
    val insertTimeStart = nowTime.split(":")(0)
    val insertTimeEnd = nowDay + " " + (nowHour.toInt + 1)

    //取得所有剧集相关数据
    val sqlAll =
      s"""select sn,date,duration,programtype,provider,programname,order,package_name,programstart,
          |programend, hour,messagetype from hr.$tableName where  inserttime >='$insertTimeStart:00:00'
          |and inserttime<'$insertTimeEnd:00:00'
          | order by sn,programname,date, hour """.stripMargin

    println("读取插入tracker_play_sdk_partition分区表的基础数据SQL:" + sqlAll)

    val allRdd = hiveContext.sql(sqlAll).rdd.cache()

    //纯剧集数据
    val playInitRdd = allRdd.filter(row => row(11).toString == "3").map(row => {
      val sn = row(0).toString
      val date = row(1).toString
      val duration = row(2).toString
      val programType = row(3).toString
      val provider = row(4).toString
      val programName = row(5).toString
      val order = row(6).toString
      val packageName = row(7).toString
      val programStart = row(8).toString
      val programEnd = row(9).toString
      val hour = row(10).toString

      (programName, sn + "#" + date + " " + hour + "#" + duration + "#" + programType + "#" + provider + "#" + order + "#" + packageName + "#" + programStart + "#" + programEnd)
    }
    ).cache()

    //取得标准库数据
    val sqlStandard =
      """select id,original_name,standard_name,year,model,crowd,region from hr.film_properties where original_name is not null and standard_name is not null""".stripMargin
    val standardRdd = hiveContext.sql(sqlStandard).rdd.map(row => {
      val awcid = row(0).toString
      val originalName = row(1).toString
      val standardName = row(2).toString
      val year = if (row(3) == null) "" else row(3).toString
      val model = if (row(4) == null) "" else row(4).toString
      val crowd = if (row(5) == null) "" else row(5).toString
      val region = if (row(6) == null) "" else row(6).toString
      //val columnCount=(awcid + "#" + standardName + "#" + year + "#" + model + "#" + crowd + "#" + region).split("#").length
      (originalName, awcid + "#" + standardName + "#" + year + "#" + model + "#" + crowd + "#" + region)
    }
    ).cache()

    //原始数据与标准库进行匹配并筛选
    val filterRdd = playInitRdd.leftOuterJoin(standardRdd).filter(row => {
      val packageName = row._2._1.split("#")(6)
      val programType = row._2._1.split("#")(3)
      val model = if (row._2._2.isEmpty) null else row._2._2.get.split("#")(3)
      if (row._2._2.isEmpty) {
        true
      }
      else if (packageName.equals("-1") || packageName.equals("com.yunos.tv.edu")) {
        model == "动画片"
      }
      else if (programType.equals("1") ) {
        model == "电影"
      }
      else if (programType.equals("3")) {
        model == "电视剧"
      }
      else if (programType.equals("4") ) {
        model == "综艺"
      }
      else if (programType.equals("0")) {
        true
      }
      else {
        false
      }
    }
    ).filter(row => {
      val programStart = row._2._1.split("#")(7)
      val programEnd = row._2._1.split("#")(8)

      if (programStart == "1970-01-01 08:00:00") {
        programEnd != "9999-12-31 23:59:59" && programEnd != "1970-01-01 08:00:00"
      }
      else if (programEnd == "9999-12-31 23:59:59" || programEnd == "1970-01-01 08:00:00") {
        programStart != "1970-01-01 08:00:00"
      }
      else {
        false
      }
    }
    ).cache()

    val timeDF = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")

    //心跳数据
    val heartBeatRdd = allRdd.map(line => {
      val sn = line(0)
      val date = line(1)
      val hour = line(10)
      val messageType = line(11)

      (sn.toString, messageType.toString + "#" + date + " " + hour.toString)
    }
    ).reduceByKey((x, y) => x + "|" + y).cache()

    //剧集数据
    val playRddAll = filterRdd.map(row => {
      val sn = row._2._1.split("#")(0)
      val programName = row._1
      (sn, row._2._1 + "#" + programName + "&&" + (if (row._2._2.isEmpty) "-1" else row._2._2.get))
    }
    ).reduceByKey((x, y) => x + "|" + y).cache()

    //实际计算过程
    val resultRdd = playRddAll.leftOuterJoin(heartBeatRdd)
      .flatMap(row => {
        val list = new ListBuffer[playData]()
        val sn = row._1
        val playAllData = row._2._1
        val heartBeatData = row._2._2.get

        val listHour = new ListBuffer[timeType]()
        case class timeType(time: String, messageType: Int)

        //该小时看过的所有剧集
        val playArr = playAllData.split("\\|")
        //如果只有一个剧集的开始或结束时间
        if (playArr.length == 1) {
          val originalPlay = playArr(0).split("&&")(0).split("#")
          val standardPlay = playArr(0).split("&&")(1).split("#")

          val standardName = if (standardPlay.length == 1) "-1" else standardPlay(1)
          val awcid = if (standardPlay.length == 1) "-1" else standardPlay(0)
          val year = if (standardPlay.length == 1) "-1" else standardPlay(2)
          val model = if (standardPlay.length == 1) "-1" else standardPlay(3)
          val crowd = if (standardPlay.length == 1) "-1" else standardPlay(4)
          //此处若是region信息缺失,数组长度则为5.
          val region = if (standardPlay.length == 1) "-1" else if (standardPlay.length < 6) "-1" else standardPlay(5)

          val originalName = originalPlay(9)
          val order = originalPlay(5)


          val allTime = heartBeatData.split("\\|").flatMap(a => {
            val time = a.split("#")(1)
            val messageType = Tools.getInt(a.split("#")(0))
            listHour += timeType(time, messageType)
          }
          ).sortBy(a => a.time)

          val part = order
          val date = nowDay
          val hour = nowHour
          var vv = 0

          val startTime = originalPlay(7)
          val endTime = originalPlay(8)
          val packageName = originalPlay(6)

          val keyPlay = sn + (if (standardName == "-1") originalName else standardName) + "第" + order + "集" + nowDay + nowHour

          //剧集点播是开始时间
          if (startTime != "1970-01-01 08:00:00") {
            vv += 1
            //从剧集开始时间往后找开机或者待机时间,如果都没有,默认从剧集开始时间看全了本小时时间
            breakable {
              for (i <- allTime.indices) {
                if ((allTime(i).messageType == 1 || allTime(i).messageType == 21) && timeDF.parse(allTime(i).time).after(timeDF.parse(startTime))) {
                  //找到第一个开机时间,拿第一个开机时间前一个时间减去剧集开始时间即为该剧集该小时的观看时间
                  val beforeTime = allTime(i - 1).time
                  val span = (timeDF.parse(beforeTime).getTime - timeDF.parse(startTime).getTime) / 1000
                  list += playData(keyPlay, sn, originalName, standardName, awcid, part, vv.toString, span.toString, year, model, crowd, region, date, hour, packageName)
                  break
                }
                if (allTime(i).messageType == 20 && timeDF.parse(allTime(i).time).after(timeDF.parse(startTime))) {
                  //找到第一个关机时间,拿第一个关机时间减去剧集开始时间即为该剧集该小时的观看时长
                  val endTime = allTime(i).time
                  val span = (timeDF.parse(endTime).getTime - timeDF.parse(startTime).getTime) / 1000
                  list += playData(keyPlay, sn, originalName, standardName, awcid, part, vv.toString, span.toString, year, model, crowd, region, date, hour, packageName)
                  break
                }
                if (i == allTime.length - 1) {
                  //此处是剧集开始时间所处小时段的小时结束时间,即下一小时的开始时间
                  val hourEnd = DateTime.parse(startTime, DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")).plusHours(1).toString("yyyy-MM-dd HH") + ":00:00"
                  val span = (timeDF.parse(hourEnd).getTime - timeDF.parse(startTime).getTime) / 1000
                  list += playData(keyPlay, sn, originalName, standardName, awcid, part, vv.toString, span.toString, year, model, crowd, region, date, hour, packageName)
                }
              }
            }
          }
          else {
            //此处是该剧集的结束时间.此处拿结束时间减去结束时间前最后一个开机时间的后一个时间.如果没有开机时间,默认该剧集观看时间为该小时开始时间持续到剧集结束时间
            val descTime = allTime.sortBy(a => a.time)(Ordering[String].reverse)
            breakable {
              for (i <- descTime.indices) {
                if ((descTime(i).messageType == 1 || descTime(i).messageType == 21) && timeDF.parse(descTime(i).time).before(timeDF.parse(endTime))) {
                  val beforeTime = descTime(i - 1).time
                  val span = (timeDF.parse(endTime).getTime - timeDF.parse(beforeTime).getTime) / 1000
                  list += playData(keyPlay, sn, originalName, standardName, awcid, part, vv.toString, span.toString, year, model, crowd, region, date, hour, packageName)
                  break
                }
                if (i == descTime.length - 1) {
                  val hourBegin = endTime.split(":")(0) + ":00:00"
                  val span = (timeDF.parse(endTime).getTime - timeDF.parse(hourBegin).getTime) / 1000
                  list += playData(keyPlay, sn, originalName, standardName, awcid, part, vv.toString, span.toString, year, model, crowd, region, date, hour, packageName)
                }
              }
            }
          }
        }
        else {
          //重写逻辑.当一个剧集(同一集)的记录出现两次以上时,表示该剧集被观看一次或以上,则一定有开始时间,和结束时间. 结束时间可能在开始时间之前.
          val listStandard = new ListBuffer[String]()

          //将该sn所有时间节点进行排序
          val allTime = heartBeatData.split("\\|").flatMap(a => {
            val time = a.split("#")(1)
            val messageType = Tools.getInt(a.split("#")(0))
            listHour += timeType(time, messageType)
          }
          ).sortBy(a => a.time)

          for (i <- 0 until playArr.length - 1) {

            val startTime = playArr(i).split("&&")(0).split("#")(7)
            val packageName = playArr(i).split("&&")(0).split("#")(6)
            val endTime = playArr(i).split("&&")(0).split("#")(8)
            val order = playArr(i).split("&&")(0).split("#")(5)
            val originalName = playArr(i).split("&&")(0).split("#")(9)
            val standardString = playArr(i).split("&&")(1)
            if (startTime != "1970-01-01 08:00:00") {
              //如果第一个时间是剧集开始时间,则往后找第一个剧集结束时间.然后拿剧集结束时间减去剧集开始时间
              breakable {
                for (j <- i + 1 until playArr.length) {
                  val rightPlayData = playArr(j).split("&&")(0).split("#")
                  val rightOriginalName = rightPlayData(9)
                  val rightOrder = rightPlayData(5)
                  val rightStartTime = rightPlayData(7)
                  val rightEndTime = rightPlayData(8)

                  if (originalName == rightOriginalName && order == rightOrder && rightEndTime != "1970-01-01 08:00:00") {
                    val span = (timeDF.parse(rightEndTime).getTime - timeDF.parse(startTime).getTime) / 1000
                    list += playData(originalName + "#" + order + "#" + sn + "#" + packageName + "&&" + standardString, "", "", "", "", "", "1", span.toString, "", "", "", "", "", "99999", "")
                    break
                  }
                }
              }
              if (i == playArr.length - 2) {
                var durationTotal = 0l
                //如果开始时间是最后一个.则执行到该小时结束时间的观看时长逻辑
                breakable {
                  for (i <- allTime.indices) {
                    if ((allTime(i).messageType == 1 || allTime(i).messageType == 21) && timeDF.parse(allTime(i).time).after(timeDF.parse(startTime))) {
                      //找到第一个开机时间,拿第一个开机时间前一个时间减去剧集开始时间即为该剧集该小时的观看时间
                      val beforeTime = allTime(i - 1).time
                      val span = (timeDF.parse(beforeTime).getTime - timeDF.parse(startTime).getTime) / 1000
                      durationTotal += span
                      break
                    }
                    if (allTime(i).messageType == 20 && timeDF.parse(allTime(i).time).after(timeDF.parse(startTime))) {
                      //找到第一个关机时间,拿第一个关机时间减去剧集开始时间即为该剧集该小时的观看时长
                      val endTime = allTime(i).time
                      val span = (timeDF.parse(endTime).getTime - timeDF.parse(startTime).getTime) / 1000
                      durationTotal += span
                      break
                    }
                    if (i == allTime.length - 1) {
                      //此处是剧集开始时间所处小时段的小时结束时间,即下一小时的开始时间
                      val hourEnd = DateTime.parse(startTime, DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")).plusHours(1).toString("yyyy-MM-dd HH") + ":00:00"
                      val span = (timeDF.parse(hourEnd).getTime - timeDF.parse(startTime).getTime) / 1000
                      durationTotal += span
                    }
                  }
                }
                list += playData(originalName + "#" + order + "#" + sn + "#" + packageName + "&&" + standardString, "", "", "", "", "", "1", durationTotal.toString, "", "", "", "", "", "99999", "")

              }
            }
            else if (i == 0 && endTime != "1970-01-01 08:00:00") {
              var durationTotal = 0l
              //此处是该剧集的结束时间.此处拿结束时间减去结束时间前最后一个开机时间的后一个时间.如果没有开机时间,默认该剧集观看时间为该小时开始时间持续到剧集结束时间
              val descTime = allTime.sortBy(a => a.time)(Ordering[String].reverse)
              breakable {
                for (i <- 0 until descTime.length) {
                  if ((descTime(i).messageType == 1 || descTime(i).messageType == 21) && timeDF.parse(descTime(i).time).before(timeDF.parse(endTime))) {
                    val beforeTime = descTime(i - 1).time
                    val span = (timeDF.parse(endTime).getTime - timeDF.parse(beforeTime).getTime) / 1000
                    durationTotal += span
                    break;
                  }
                  if (i == descTime.length - 1) {
                    val hourBegin = endTime.split(":")(0) + ":00:00"
                    val span = (timeDF.parse(endTime).getTime - timeDF.parse(hourBegin).getTime) / 1000
                    durationTotal += span
                  }
                }
              }
              list += playData(originalName + "#" + order + "#" + sn + "#" + packageName + "&&" + standardString, "", "", "", "", "", "1", durationTotal.toString, "", "", "", "", "", "99999", "")

            }
          }
        }
        list
      }
      ).cache()

    val singlePlay = resultRdd.filter(row => row.hour != "9999")
    val multiData = resultRdd.filter(row => row.hour == "9999").cache()

    val multiDuration = multiData.map(row => {
      val allData = row.key
      val durationTotal = row.duration.toInt
      (allData, durationTotal)
    }
    ).reduceByKey((x, y) => x + y)

    val multiVV = multiData.map(row => {
      val allData = row.key
      val vv = row.vv.toInt
      (allData, vv)
    }
    ).reduceByKey((x, y) => x + y)

    //val temp = multiDuration.join(multiVV)
    // alldata,(durationTotal,vv)

    val multiPlay = multiDuration.join(multiVV).flatMap(row => {
      val multiList = new ListBuffer[playData]()
      val standardData = row._1.split("&&")(1)
      val durationTotal = row._2._1.toString

      val originalName = row._1.split("&&")(0).split("#")(0)
      val part = row._1.split("&&")(0).split("#")(1)
      val sn = row._1.split("&&")(0).split("#")(2)
      val packageName = row._1.split("&&")(0).split("#")(3)

      val vv = row._2._2.toString
      val date = nowDay
      val hour = nowHour
      val standardName = if (standardData == "-1") "-1" else standardData.split("#")(1)
      val awcid = if (standardData == "-1") "-1" else standardData.split("#")(0)
      val year = if (standardData == "-1") "-1" else standardData.split("#")(2)
      val model = if (standardData == "-1") "-1" else standardData.split("#")(3)
      val crowd = if (standardData == "-1") "-1" else standardData.split("#")(4)
      val region = if (standardData == "-1") "-1" else if (standardData.split("#").length == 6) standardData.split("#")(5) else "-1"

      val multiKey = sn + (if (standardName == "-1") originalName else standardName) + "第" + part + "集" + nowDay + nowHour
      multiList += new playData(multiKey, sn, originalName, standardName, awcid, part, vv, durationTotal, year, model, crowd, region, date, hour, packageName)

    }
    ).cache()

    val finalRdd = singlePlay.union(multiPlay).filter(a => a.vv != "0" || a.duration != "0").cache()

    val output_tmp_dir_play =s"""/user/hive/warehouse/hr.db/tracker_play_sdk_partition_test/tracker_play_sdk_partition_temp/date=$nowDay/hour=$nowHour"""
    val pathPlayTemp = new Path(output_tmp_dir_play)
    val hadoopConf = sc.hadoopConfiguration
    val hdfs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)
    if (hdfs.exists(pathPlayTemp)) {
      hdfs.delete(pathPlayTemp, true)
    }
    finalRdd.toDF.map(a => a.mkString("\001")).saveAsTextFile(output_tmp_dir_play)
    val insertSqlPlay =s"""load data inpath '$output_tmp_dir_play' overwrite into table hr.tracker_play_sdk_partition_test partition (date='$nowDay',hour='$nowHour')"""
    hiveContext.sql(insertSqlPlay)

    println("插入tracker_play_sdk_partition分区表数据成功")

    val apkResult = finalRdd.map(row => {
      val key = row.sn + row.apk + row.date + row.hour
      val sn = row.sn
      val apk = row.apk
      val cnt = row.vv
      val duration = row.duration
      apkData(key, sn, apk, cnt, duration, nowDay, nowHour)
    }
    )

    val output_tmp_dir_apk =
      s"""/user/hive/warehouse/hr.db/tracker_apk_sdk_partition_test/tracker_apk_sdk_partition_temp/date=$nowDay/hour=$nowHour"""
    val pathApkTemp = new Path(output_tmp_dir_apk)
    if (hdfs.exists(pathApkTemp)) {
      hdfs.delete(pathApkTemp, true)
    }
    apkResult.toDF.map(a => a.mkString("\001")).saveAsTextFile(output_tmp_dir_apk)
    val insertSqlApk =s"""load data inpath '$output_tmp_dir_apk' overwrite into table hr.tracker_apk_sdk_partition_test partition (date='$nowDay',hour='$nowHour')"""
    hiveContext.sql(insertSqlApk)

    println("插入tracker_apk_sdk_partition分区表数据R成功")

  }
}

object HiveContextHolder {
  @transient
  private var sqlContext: HiveContext = _

  def getInstance(sc: SparkContext): HiveContext = {
    if (null == sqlContext) {
      HiveContextHolder.synchronized[HiveContext] {
        if (null == sqlContext) {
          sqlContext = new HiveContext(sc)

          sqlContext
        }
        else {
          sqlContext
        }
      }
    }
    else {
      sqlContext
    }
  }
}


// hiveContext.createDataFrame(apkResult).coalesce(1).write.mode(SaveMode.Append).partitionBy("date").orc("/user/hive/warehouse/hr.db/tracker_apk_sdk_partition_test")
//
//hiveContext.createDataFrame(resultRdd).coalesce(1).write.mode(SaveMode.Append).partitionBy("date").orc("/user/hive/warehouse/hr.db/tracker_play_sdk_partition_test")

//    resultRdd.toDF.registerTempTable("tempplaytable")
//
//    val insertPlaySql =
//      s"""insert overwrite table hr.tracker_play_sdk_partition_test partition(date='$nowDay',hour='$nowHour')
//         |select key,sn,originalName,standardName, awcid, part, vv,duration,year,model,crowd,region from tempplaytable
//       """.
//        stripMargin
//    hiveContext.sql(insertPlaySql)

//    apkResult.toDF.registerTempTable("tempapktable")
//
//    val insertApkSql =
//      s"""insert overwrite table hr.tracker_apk_sdk_partition_test partition(date='$nowDay',hour='$nowHour')
//         |select key,sn,apk,cnt,duration from tempapktable
//       """.
//        stripMargin
//    hiveContext.sql(insertApkSql)package com.avcdata.spark.job.util

import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

object HiveUtils {

  case class Person(name: String, col1: Int, col2: String)

  def saveAsTable(sc: SparkContext, filePath: String, tableName: String) {

    //    insertInto函数是向表中写入数据，可以看出此函数不能指定数据库和分区等信息，不可以直接进行写入。
    //    向hive数据仓库写入数据必须指定数据库，hive数据表建立可以在hive上建立，或者使用hiveContext.sql（“create table ...."）

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    val data = sc.textFile(filePath).map(x => x.split("\\s+"))
      .map(x => Person(x(0), x(1).toInt, x(2)))
    data.toDF().write.mode(SaveMode.Append).insertInto(tableName)
  }


  def insert2tableByTempTable(sc: SparkContext, filePath: String, tableName: String) {

    //    insertInto函数是向表中写入数据，可以看出此函数不能指定数据库和分区等信息，不可以直接进行写入。
    //    向hive数据仓库写入数据必须指定数据库，hive数据表建立可以在hive上建立，或者使用hiveContext.sql（“create table ...."）

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    //DataFrame数据写入hive指定数据表的分区中时产生大量小文件该怎么解决
    //    hiveContext.setConf("spark.sql.shuffle.partitions", "1")

    val data = sc.textFile(filePath).map(x => x.split("\\s+"))
      .map(x => Person(x(0), x(1).toInt, x(2)))
    data.toDF().registerTempTable("table1")

    hiveContext.sql("insert into " + tableName + " partition(date='2015-04-02') select name,col1,col2 from table1")
  }


  def load2tableByHdfsFile(sc: SparkContext, filePath: String, output_tmp_dir: String, tableName: String, date: String):
  Unit = {
    //output_tmp_dir = "/user/hdfs/rsync/tmp/apk"

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    sc.textFile(filePath)
      .map { r => r.mkString("\001") }.repartition(100).saveAsTextFile(output_tmp_dir)

    hiveContext.sql(s"""load data inpath '$output_tmp_dir' overwrite into table $tableName partition (dt='$date')""")


  }


  //    dataframe.registerTempTable("result")
  //    sql(s"""INSERT OVERWRITE Table $outputTable PARTITION (dt ='$outputDate') select * from result""")
  //    而整个结果数据的产生只需要4分钟左右的时间，比如以下方式：将结果以textfile存入hdfs：
  //    result.rdd.saveAsTextFile(output_tmp_dir)
  //    由此可见，对hive的写入操作耗用了大量的时间。


  //    对此现象的优化可以是，将文件存为符合hive table文件的格式，然后使用hive load将产生的结果文件直接move到指定目录下。代码如下：
  //    result.rdd.map { r => r.mkString("\001") }.repartition(partitions).saveAsTextFile(output_tmp_dir)
  //    sql(s"""load data inpath '$output_tmp_dir' overwrite into table $output partition (dt='$dt')""")


}
package com.avcdata.vbox.util

import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

object HiveUtils {

  case class Person(name: String, col1: Int, col2: String)

  def saveAsTable(sc: SparkContext, filePath: String, tableName: String) {

    //    insertInto函数是向表中写入数据，可以看出此函数不能指定数据库和分区等信息，不可以直接进行写入。
    //    向hive数据仓库写入数据必须指定数据库，hive数据表建立可以在hive上建立，或者使用hiveContext.sql（“create table ...."）

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    val data = sc.textFile(filePath).map(x => x.split("\\s+"))
      .map(x => Person(x(0), x(1).toInt, x(2)))
    data.toDF().write.mode(SaveMode.Append).insertInto(tableName)
  }


  def insert2tableByTempTable(sc: SparkContext, filePath: String, tableName: String) {

    //    insertInto函数是向表中写入数据，可以看出此函数不能指定数据库和分区等信息，不可以直接进行写入。
    //    向hive数据仓库写入数据必须指定数据库，hive数据表建立可以在hive上建立，或者使用hiveContext.sql（“create table ...."）

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    //DataFrame数据写入hive指定数据表的分区中时产生大量小文件该怎么解决
    //    hiveContext.setConf("spark.sql.shuffle.partitions", "1")

    val data = sc.textFile(filePath).map(x => x.split("\\s+"))
      .map(x => Person(x(0), x(1).toInt, x(2)))
    data.toDF().registerTempTable("table1")

    hiveContext.sql("insert into " + tableName + " partition(date='2015-04-02') select name,col1,col2 from table1")
  }


  def load2tableByHdfsFile(sc: SparkContext, filePath: String, output_tmp_dir: String, tableName: String, date: String):
  Unit = {
    //output_tmp_dir = "/user/hdfs/rsync/tmp/apk"

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    hiveContext.sql("use hr")
    sc.textFile(filePath)
      .map { r => r.mkString("\001") }.repartition(100).saveAsTextFile(output_tmp_dir)

    hiveContext.sql(s"""load data inpath '$output_tmp_dir' overwrite into table $tableName partition (dt='$date')""")


  }


  //    dataframe.registerTempTable("result")
  //    sql(s"""INSERT OVERWRITE Table $outputTable PARTITION (dt ='$outputDate') select * from result""")
  //    而整个结果数据的产生只需要4分钟左右的时间，比如以下方式：将结果以textfile存入hdfs：
  //    result.rdd.saveAsTextFile(output_tmp_dir)
  //    由此可见，对hive的写入操作耗用了大量的时间。


  //    对此现象的优化可以是，将文件存为符合hive table文件的格式，然后使用hive load将产生的结果文件直接move到指定目录下。代码如下：
  //    result.rdd.map { r => r.mkString("\001") }.repartition(partitions).saveAsTextFile(output_tmp_dir)
  //    sql(s"""load data inpath '$output_tmp_dir' overwrite into table $output partition (dt='$dt')""")


}
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext}
import utils.HiveUtils

object InsertIntoAPKAndPlayPartition
{
  def main(args: Array[String]): Unit =
  {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val conf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))

    val sc = new SparkContext(conf)

    val nowTime = args(0) + " " + args(1)

    println("开始执行插入tracker_play_sdk_partition和tracker_apk_sdk_partition分区表操作,当前时间为:" + nowTime)
    HiveUtils.PlayDataPartition(sc, nowTime)
  }
}
import java.text.SimpleDateFormat
import java.util.Date

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.{SparkConf, SparkContext}
import utils.HiveUtils

/**
  * Created by guxiaoyang on 2017/7/6.
  */
object InsertIntoOCPartition
{
  def main(args: Array[String]): Unit =
  {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val conf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))

    val sc = new SparkContext(conf)

    val nowTime = args(0) + " " + args(1)

    println("开始执行插入tracker_oc_sdk_partition分区表操作,当前时间为:" + nowTime)
    HiveUtils.OCDataPartition(sc, nowTime)

  }
}
package com.avcdata.vbox.util

import java.sql.{Connection, Date, DriverManager, PreparedStatement}

import org.apache.spark.{SparkConf, SparkContext}

object IPLocation {

  val data2MySQL = (iterator: Iterator[(String, Int)]) => {
    var conn: Connection = null
    var ps: PreparedStatement = null
    val sql = "INSERT INTO location_info1 (location,counts,accesse_date) VALUES(?,?,?)"
    try {
      conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/bigdata?useUnicode=true&characterEncoding=UTF-8", "root", "root")
      iterator.foreach(line => {
        ps = conn.prepareStatement(sql)
        ps.setString(1, line._1)
        ps.setInt(2, line._2)
        ps.setDate(3, new Date(System.currentTimeMillis()))
        ps.executeUpdate()
      })
    } catch {
      case e: Exception => println("Mysql Exception")
    } finally {
      if (ps != null)
        ps.close()
      if (conn != null)
        conn.close()
    }
  }

  def ip2Long(ip: String): Long = {
    val fragments = ip.split("[.]")
    var ipNum = 0L
    for (i <- 0 until fragments.length) {
      ipNum = fragments(i).toLong | ipNum << 8L
    }
    ipNum
  }

  def binarySearch(lines: Array[(String, String, String)], ip: Long): Int = {
    var low = 0
    var high = lines.length - 1
    while (low <= high) {
      val middle = (low + high) / 2
      if ((ip >= lines(middle)._1.toLong) && (ip <= lines(middle)._2.toLong))
        return middle
      if (ip < lines(middle)._1.toLong)
        high = middle - 1
      else {
        low = middle + 1
      }
    }
    -1
  }

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("IPLocation").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val ipRulesRdd = sc.textFile("e://Test/ip.txt").map(lines => {
      val fields = lines.split("\\|")
      val start_num = fields(2)
      val end_num = fields(3)
      val province = fields(6)
      (start_num, end_num, province)
    })
    //全部的IP映射规则
    val ipRulesArrary = ipRulesRdd.collect()

    //广播规则,这个是由Driver向worker中广播规则
    val ipRulesBroadcast = sc.broadcast(ipRulesArrary)

    //加载要处理的数据
    val ipsRdd = sc.textFile("e://Test/access_log").map(line => {
      val fields = line.split("\\|")
      fields(1)
    })

    val result = ipsRdd.map(ip => {
      val ipNum = ip2Long(ip)
      val index = binarySearch(ipRulesBroadcast.value, ipNum)
      val info = ipRulesBroadcast.value(index)
      info
    }).map(t => {
      (t._3, 1)
    }).reduceByKey(_ + _)

    //将数据写入数据库中
    result.foreachPartition(data2MySQL)

    println(result.collect().toBuffer)
    sc.stop()

  }


}package com.avcdata.etl.util.db

/**
  * JDBC执行配置
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/30 12:08
  */
case class JDBCConfig(connectUri: String, username: String, password: String, transactional:Boolean)package com.avcdata.etl.common.pool.jdbc

import java.sql.Connection
import javax.sql.DataSource

import com.mchange.v2.c3p0.ComboPooledDataSource
import org.slf4j.LoggerFactory

/**
  * JDBC数据库连接池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/16 08:18
  */
object JDBCConnectionPool
{
  private val logger = LoggerFactory.getLogger(JDBCConnectionPool.getClass)

  //数据源连接池
  private var dataSourcePool: Map[(String, String, String), ComboPooledDataSource] = Map()

  val JDBC_URI_PREFIX_TO_DRIVER_NAME = Map("jdbc:mysql://" -> "com.mysql.jdbc.Driver")

  sys.addShutdownHook
  {
    dataSourcePool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(connectUri: String, user: String, password: String): Connection =
  {
    dataSourcePool.getOrElse((connectUri, user, password),
      {
        JDBCConnectionPool.synchronized[DataSource](
          dataSourcePool.getOrElse((connectUri, user, password),
            {
              logger.info("Initialize c3p0 database connection pool.")

              //创建数据源
              val dataSource = new ComboPooledDataSource
              dataSource.setDriverClass(getDriverClass(connectUri))
              dataSource.setJdbcUrl(connectUri)
              dataSource.setUser(user)
              dataSource.setPassword(password)

              //初始化时获取一个连接，取值应在minPoolSize与maxPoolSize之间。Default: 3
              dataSource.setInitialPoolSize(1)
              //最大空闲时间,60秒内未使用则连接被丢弃。若为0则永不丢弃。Default: 0
              dataSource.setMaxIdleTime(60)
              //连接池中保留的最大连接数。Default: 15
              dataSource.setMaxPoolSize(10)
              //初始化时获取三个连接，取值应在minPoolSize与maxPoolSize之间。Default: 3
              dataSource.setMinPoolSize(1)

              //当连接池中的连接耗尽的时候c3p0一次同时获取的连接数。Default: 3
              dataSource.setAcquireIncrement(3)
              //定义所有连接测试都执行的测试语句。
              dataSource.setPreferredTestQuery("SELECT SYSDATE()")
              //每60秒检查所有连接池中的空闲连接。Default: 0
              dataSource.setIdleConnectionTestPeriod(60)

              dataSourcePool += (connectUri, user, password) -> dataSource

              dataSource
            })
        )
      }).getConnection
  }

  /**
    * 获取数据库驱动名称
    *
    * @param connectUri 连接URI
    */
  private def getDriverClass(connectUri: String): String =
  {
    val driverName = JDBC_URI_PREFIX_TO_DRIVER_NAME.find(e =>
    {
      val (uriPrefix, _) = e
      connectUri.startsWith(uriPrefix)
    })

    driverName match
    {
      case Some((_, driverClass)) => driverClass
      case None => throw new IllegalArgumentException(s"Can not find => $connectUri driver.")
    }
  }
}package com.avcdata.etl.common.pool.jdbc

import java.sql.Connection
import javax.sql.DataSource

import com.mchange.v2.c3p0.ComboPooledDataSource
import org.slf4j.LoggerFactory

/**
  * JDBC数据库连接池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/16 08:18
  */
object JDBCConnectionPool
{
  private val logger = LoggerFactory.getLogger(JDBCConnectionPool.getClass)

  //数据源连接池
  private var dataSourcePool: Map[(String, String, String), ComboPooledDataSource] = Map()

  val JDBC_URI_PREFIX_TO_DRIVER_NAME = Map("jdbc:mysql://" -> "com.mysql.jdbc.Driver")

  sys.addShutdownHook
  {
    dataSourcePool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(connectUri: String, user: String, password: String): Connection =
  {
    dataSourcePool.getOrElse((connectUri, user, password),
      {
        JDBCConnectionPool.synchronized[DataSource](
          dataSourcePool.getOrElse((connectUri, user, password),
            {
              logger.info("Initialize c3p0 database connection pool.")

              //创建数据源
              val dataSource = new ComboPooledDataSource
              dataSource.setDriverClass(getDriverClass(connectUri))
              dataSource.setJdbcUrl(connectUri)
              dataSource.setUser(user)
              dataSource.setPassword(password)

              //初始化时获取一个连接，取值应在minPoolSize与maxPoolSize之间。Default: 3
              dataSource.setInitialPoolSize(1)
              //最大空闲时间,60秒内未使用则连接被丢弃。若为0则永不丢弃。Default: 0
              dataSource.setMaxIdleTime(60)
              //连接池中保留的最大连接数。Default: 15
              dataSource.setMaxPoolSize(10)
              //初始化时获取三个连接，取值应在minPoolSize与maxPoolSize之间。Default: 3
              dataSource.setMinPoolSize(1)

              //当连接池中的连接耗尽的时候c3p0一次同时获取的连接数。Default: 3
              dataSource.setAcquireIncrement(3)
              //定义所有连接测试都执行的测试语句。
              dataSource.setPreferredTestQuery("SELECT SYSDATE()")
              //每60秒检查所有连接池中的空闲连接。Default: 0
              dataSource.setIdleConnectionTestPeriod(60)

              dataSourcePool += (connectUri, user, password) -> dataSource

              dataSource
            })
        )
      }).getConnection
  }

  /**
    * 获取数据库驱动名称
    *
    * @param connectUri 连接URI
    */
  private def getDriverClass(connectUri: String): String =
  {
    val driverName = JDBC_URI_PREFIX_TO_DRIVER_NAME.find(e =>
    {
      val (uriPrefix, _) = e
      connectUri.startsWith(uriPrefix)
    })

    driverName match
    {
      case Some((_, driverClass)) => driverClass
      case None => throw new IllegalArgumentException(s"Can not find => $connectUri driver.")
    }
  }
}package com.avcdata.etl.util.db

import java.sql.Connection

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import org.slf4j.LoggerFactory

/**
  * JDBC脚本执行
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/30 12:05
  */
object JDBCExecutor
{
  private val logger = LoggerFactory.getLogger(JDBCExecutor.getClass)

  def execute(executableScripts: Seq[String], config: JDBCConfig): Unit =
  {
    logger.info(s"Begin to execute JDBCExecutor.execute($config)")

    LoanPattern.using(JDBCConnectionPool(config.connectUri, config.username, config.password))
    { conn =>

      if (config.transactional)
      {
        transactionExecute(conn, executableScripts)
      }
      else
      {
        executeLines(conn, executableScripts)
      }
    }

    logger.info(s"End to execute JDBCExecutor.execute($config)")
  }

  private def transactionExecute(conn: Connection, executableScripts: Seq[String]): Unit =
  {
    val autoCommit = conn.getAutoCommit
    try
    {
      conn.setAutoCommit(false)
      conn.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)

      executeLines(conn, executableScripts)

      conn.commit()
    }
    catch
    {
      case ex: Throwable => conn.rollback(); throw ex
    }
    finally
    {
      conn.setAutoCommit(autoCommit)
    }
  }

  private def executeLines(conn: Connection, executableScripts: Seq[String]): Unit =
  {
    executableScripts.foreach(scriptLine =>
    {
      logger.info(s"Begin to execute script -> $scriptLine")

      LoanPattern.using(conn.prepareStatement(scriptLine))(_.execute())
    })
  }
}
package com.avcdata.vbox.util

import java.util.Properties

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

class JdbcOnMysql {

  def main(args: Array[String]) {

    val sparkConf = new SparkConf().setMaster("spark://OPENFIRE-DEV:7080").setAppName("spark sql test");
    val sc = new SparkContext(sparkConf);
    val sqlContext = new SQLContext(sc);

    //1. 不指定查询条件
    //这个方式链接MySql的函数原型是：
    //我们只需要提供Driver的url，需要查询的表名，以及连接表相关属性properties。下面是具体例子：
    val url = "jdbc:mysql://192.168.0.101:3306/sas_vip?user=root&password=123456";
    val prop = new Properties();
    val df = sqlContext.read.jdbc(url, "stock", prop);
    println("第一种方法输出：" + df.count());
    println("1.------------->" + df.count());
    println("1.------------->" + df.rdd.partitions.size);

    //2.指定数据库字段的范围
    //这种方式就是通过指定数据库中某个字段的范围，但是遗憾的是，这个字段必须是数字，来看看这个函数的函数原型：
    /* def jdbc(
    url: String,
    table: String,
    columnName: String,
    lowerBound: Long,
    upperBound: Long,
    numPartitions: Int,
    connectionProperties: Properties): DataFrame*/
    //前两个字段的含义和方法一类似。columnName就是需要分区的字段，这个字段在数据库中的类型必须是数字；
    //lowerBound就是分区的下界；upperBound就是分区的上界；numPartitions是分区的个数。同样，我们也来看看如何使用：
    val lowerBound = 1;
    val upperBound = 6;
    val numPartitions = 2;
    val url1 = "jdbc:mysql://192.168.0.101:3306/sas_vip?user=root&password=123456";
    val prop1 = new Properties();
    val df1 = sqlContext.read.jdbc(url1, "stock", "id", lowerBound, upperBound, numPartitions, prop1);
    println("第二种方法输出：" + df1.rdd.partitions.size);
    df1.collect().foreach(println)

    /*这个方法可以将iteblog表的数据分布到RDD的几个分区中，分区的数量由numPartitions参数决定，在理想情况下，每个分区处理相同数量的数据，我们在使用的时候不建议将这个值设置的比较大，因为这可能导致数据库挂掉！但是根据前面介绍，这个函数的缺点就是只能使用整形数据字段作为分区关键字。
这个函数在极端情况下，也就是设置将numPartitions设置为1，其含义和第一种方式一致。*/

    //3.根据任意字段进行分区
    //基于前面两种方法的限制， Spark 还提供了根据任意字段进行分区的方法，函数原型如下：
    /*def jdbc(
    url: String,
    table: String,
    predicates: Array[String],
    connectionProperties: Properties): DataFrame*/
    //这个函数相比第一种方式多了predicates参数，我们可以通过这个参数设置分区的依据，来看看例子：
    //这个函数相比第一种方式多了predicates参数，我们可以通过这个参数设置分区的依据，来看看例子：
    val predicates = Array[String]("id <= 2", "id >= 4 and id <= 5 ")
    val url2 = "jdbc:mysql://192.168.0.101:3306/sas_vip?user=root&password=123456"
    val prop2 = new Properties()
    val df2 = sqlContext.read.jdbc(url, "stock", predicates, prop2)
    println("第三种方法输出：" + df2.rdd.partitions.size + "," + predicates.length);
    df2.collect().foreach(println)
    //最后rdd的分区数量就等于predicates.length。


    //4.通过load获取
    //Spark还提供通过load的方式来读取数据。
    val url3 = "jdbc:mysql://192.168.0.101:3306/sas_vip?user=root&password=123456"
    val df3 = sqlContext.read.format("jdbc").option("url", url).option("dbtable", "stock").load()
    println("第四种方法输出：" + df3.rdd.partitions.size);
    df.collect().foreach(println)

    sc.stop()
  }
}package com.avcdata.vbox.util

import org.apache.spark.SparkContext

class JdbcOnMysql02 {

  def main(args: Array[String]) {

    val sc = new SparkContext("local[2]", "demo")

    def getConnection() = {
      Class.forName("com.mysql.jdbc.Driver").newInstance()
      java.sql.DriverManager.getConnection("jdbc:mysql://hadoop000:3306/hive", "root", "root")
    }

    def flatValue(result: java.sql.ResultSet) = {
      (result.getInt("TBL_ID"), result.getString("TBL_NAME"))
    }

    //select * from TBLS WHERE TBL_ID>=1 AND TBL_ID<=10
    val data = new org.apache.spark.rdd.JdbcRDD(
      sc,
      getConnection,
      "select * from TBLS where TBL_ID >= ? and TBL_ID <= ?",
      1,
      10,
      2,
      flatValue
    )

    println(data.collect().toList)

    sc.stop
  }
}package com.avcdata.vbox.util

import java.sql.DriverManager

import org.apache.spark.rdd.JdbcRDD
import org.apache.spark.{SparkConf, SparkContext}


object JDBCReader {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("JdbcRDDDemo_3").setMaster("local[2]")
    val sc = new SparkContext(conf)

    val connection = () => {
      Class.forName("com.mysql.jdbc.Driver").newInstance()
      DriverManager.getConnection("jdbc:mysql://localhost:3306/bigdata", "root", "root")
    }
    val jdbcRDD = new JdbcRDD(
      sc,
      connection,
      "SELECT * from location_info where id >= ? AND id <= ?",
      1, 5, 2,
      r => {
        val id = r.getInt(1)
        val location = r.getString(2)
        val counts = r.getInt(3)
        val access_date = r.getDate(4)
        (id, location, counts, access_date)
      }
    )
    jdbcRDD.collect().map(line => {
      println("id:" + line._1)
      println("location:" + line._2)
      println("counts:" + line._3)
      println("date:" + line._4)
      println("------------------------------")
    })
    sc.stop
  }
}
package com.avcdata.spark.job.util

import java.sql.DriverManager

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.{DataFrame, SQLContext, SaveMode}
import org.apache.spark.{SparkConf, SparkContext}


object JdbcUtils {

  //TODO 读取Mysql表=>DataFrame
  def readMysql2DF(sc: SparkContext, host: String, prop: java.util.Properties, db: String, tableName: String): DataFrame
  = {

    val sqlContext = new SQLContext(sc)
    val url: String = "jdbc:mysql://" + host + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    sqlContext.read.jdbc(url, tableName, prop)
  }


  //TODO 通过jdbc 将DataFrame写入mysql
  def writeDF2Mysql(sc: SparkContext, df: DataFrame,
                    prop: java.util.Properties, db: String, tableName: String, isTruncate: Boolean) = {

    //    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + sc.getConf.get("mysql.db") + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    //清空数据
    if (isTruncate) truncate(url, tableName, prop)

    df.write
      .mode(SaveMode.Append)
      .jdbc(url, tableName, prop)

  }

  //TODO 清空表数据
  def truncate(url: String, tableName: String, prop: java.util.Properties) = {
    classOf[com.mysql.jdbc.Driver]
    val conn = DriverManager.getConnection(url + "&user=" + prop.getProperty("user") + "&password=" + prop.getProperty("password"))
    try {
      val statement = conn.createStatement()
      statement.executeUpdate("TRUNCATE " + tableName);
    }
    catch {
      case e: Exception => e.printStackTrace
    }
    finally {
      conn.close
    }
  }


  //TODO 通过JDBC操作Hive
  def hiveJdbc() {
    Class.forName("org.apache.hive.jdbc.HiveDriver")
    val conn = DriverManager.getConnection("jdbc:hive2://hadoop2:10000/hive", "hadoop", "")
    try {
      val statement = conn.createStatement
      val rs = statement.executeQuery("select ordernumber,amount from tbStockDetail  where amount>3000")
      while (rs.next) {
        val ordernumber = rs.getString("ordernumber")
        val amount = rs.getString("amount")
        println("ordernumber = %s, amount = %s".format(ordernumber, amount))
      }
    } catch {
      case e: Exception => e.printStackTrace
    }
    conn.close
  }


  //TODO Main
  def main(args: Array[String]) {
    val config: Config = ConfigFactory.load()
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("test")
    val sc = new SparkContext(conf)
    val prop = new java.util.Properties()
    prop.put("user", config.getString("mysql.user"))
    prop.put("password", config.getString("mysql.password"))
    prop.put("driver", "com.mysql.jdbc.Driver")
    val tbDF = readMysql2DF(sc, config.getString("mysql.host"), prop, "vboxDB", "SYS_SHOW_USER")
    tbDF.foreach(print(_))
    sc.stop()
  }

}
package com.avcdata.spark.job.util

import java.sql.DriverManager

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.{DataFrame, SQLContext, SaveMode}
import org.apache.spark.{SparkConf, SparkContext}


object JdbcUtils {

  //TODO 读取Mysql表=>DataFrame
  def readMysql2DF(sc: SparkContext, host: String, prop: java.util.Properties, db: String, tableName: String): DataFrame
  = {

    val sqlContext = new SQLContext(sc)
    val url: String = "jdbc:mysql://" + host + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    sqlContext.read.jdbc(url, tableName, prop)
  }


  //TODO 通过jdbc 将DataFrame写入mysql
  def writeDF2Mysql(sc: SparkContext, df: DataFrame,
                    prop: java.util.Properties, db: String, tableName: String, isTruncate: Boolean,saveMode:SaveMode) = {

    //    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + sc.getConf.get("mysql.db") + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    //清空数据
//    if (isTruncate)
      truncate(url, tableName, prop)

    df.write
      .mode(saveMode)
      .jdbc(url, tableName, prop)

  }

  //TODO 清空表数据
  def truncate(url: String, tableName: String, prop: java.util.Properties) = {
    classOf[com.mysql.jdbc.Driver]
    val conn = DriverManager.getConnection(url + "&user=" + prop.getProperty("user") + "&password=" + prop.getProperty("password"))
    try {
      val statement = conn.createStatement()
      statement.executeUpdate("TRUNCATE " + tableName);
    }
    catch {
      case e: Exception => e.printStackTrace
    }
    finally {
      conn.close
    }
  }


  //TODO 通过JDBC操作Hive
  def hiveJdbc() {
    Class.forName("org.apache.hive.jdbc.HiveDriver")
    val conn = DriverManager.getConnection("jdbc:hive2://hadoop2:10000/hive", "hadoop", "")
    try {
      val statement = conn.createStatement
      val rs = statement.executeQuery("select ordernumber,amount from tbStockDetail  where amount>3000")
      while (rs.next) {
        val ordernumber = rs.getString("ordernumber")
        val amount = rs.getString("amount")
        println("ordernumber = %s, amount = %s".format(ordernumber, amount))
      }
    } catch {
      case e: Exception => e.printStackTrace
    }
    conn.close
  }


  //TODO Main
  def main(args: Array[String]) {
    val config: Config = ConfigFactory.load()
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("test")
    val sc = new SparkContext(conf)
    val prop = new java.util.Properties()
    prop.put("user", config.getString("mysql.user"))
    prop.put("password", config.getString("mysql.password"))
    prop.put("driver", "com.mysql.jdbc.Driver")
    val tbDF = readMysql2DF(sc, config.getString("mysql.host"), prop, "vboxDB", "SYS_SHOW_USER")
    tbDF.foreach(print(_))
    sc.stop()
  }

}
package com.avcdata.vbox.util

import java.sql.{Connection, DriverManager}

import com.mchange.v2.c3p0.ComboPooledDataSource
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.{DataFrame, SQLContext, SaveMode}
import org.apache.spark.{SparkConf, SparkContext}


object JdbcUtils {

  //TODO 读取Mysql表=>DataFrame
  def readMysql2DF(sc: SparkContext, host: String, prop: java.util.Properties, db: String, tableName: String): DataFrame
  = {

    val sqlContext = new SQLContext(sc)
    val url: String = "jdbc:mysql://" + host + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    sqlContext.read.jdbc(url, tableName, prop)
  }


  //TODO 通过jdbc 将DataFrame写入mysql
  def writeDF2Mysql(sc: SparkContext, df: DataFrame,
                    prop: java.util.Properties, db: String, tableName: String, isTruncate: Boolean, saveMode: SaveMode) = {

    //    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + sc.getConf.get("mysql.db") + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false"

    //清空数据
    if (isTruncate)
      truncate(url, tableName, prop)

    df.write
      .mode(SaveMode.Append)
      .jdbc(url, tableName, prop)

  }

  //TODO 清空表数据
  def truncate(url: String, tableName: String, prop: java.util.Properties) = {
    classOf[com.mysql.jdbc.Driver]
    val conn = DriverManager.getConnection(url + "&user=" + prop.getProperty("user") + "&password=" + prop.getProperty("password"))
    try {
      val statement = conn.createStatement()
      statement.executeUpdate("TRUNCATE " + tableName);
    }
    catch {
      case e: Exception => e.printStackTrace
    }
    finally {
      conn.close
    }
  }


  //TODO 通过JDBC操作Hive
  def hiveJdbc() {
    Class.forName("org.apache.hive.jdbc.HiveDriver")
    val conn = DriverManager.getConnection("jdbc:hive2://hadoop2:10000/hive", "hadoop", "")
    try {
      val statement = conn.createStatement
      val rs = statement.executeQuery("select ordernumber,amount from tbStockDetail  where amount>3000")
      while (rs.next) {
        val ordernumber = rs.getString("ordernumber")
        val amount = rs.getString("amount")
        println("ordernumber = %s, amount = %s".format(ordernumber, amount))
      }
    } catch {
      case e: Exception => e.printStackTrace
    }
    conn.close
  }

  //TODO c3p0连接池
  def getConnFromPool(): Connection = {
    val dataSource = new ComboPooledDataSource();
    dataSource.setUser("root");
    dataSource.setPassword("new.1234");
    dataSource.setJdbcUrl("jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false");
    dataSource.setDriverClass("com.mysql.jdbc.Driver");
    dataSource.setInitialPoolSize(10);
    dataSource.setMinPoolSize(1);
    dataSource.setMaxPoolSize(10);
    dataSource.setMaxStatements(50);
    dataSource.setMaxIdleTime(25000);
    dataSource.setTestConnectionOnCheckin(true)
    dataSource.setIdleConnectionTestPeriod(18000)
    dataSource.setTestConnectionOnCheckout(true)
//    //获取connnection时测试是否有效
//    testConnectionOnCheckin = true
//
//    //自动测试的table名称
//    automaticTestTable=C3P0TestTable
//
//    //set to something much less than wait_timeout, prevents connections from going stale
//    idleConnectionTestPeriod = 18000
//
//    //set to something slightly less than wait_timeout, preventing 'stale' connections from being handed out
//    maxIdleTime = 25000
//
//    //if you can take the performance 'hit', set to "true"
//    testConnectionOnCheckout = true
    return dataSource.getConnection()
  }


  //TODO Main
  def main(args: Array[String]) {
    val config: Config = ConfigFactory.load()
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("test")
    val sc = new SparkContext(conf)
//    val prop = new java.util.Properties()
//    prop.put("user", config.getString("mysql.user"))
//    prop.put("password", config.getString("mysql.password"))
//    prop.put("driver", "com.mysql.jdbc.Driver")
//    val tbDF = readMysql2DF(sc, config.getString("mysql.host"), prop, "vboxDB", "SYS_SHOW_USER")
//    tbDF.foreach(print(_))

    val sqlContext = new SQLContext(sc)

        val df = sqlContext.read
          .format("jdbc")
          .option("driver", "com.mysql.jdbc.Driver")
          .option("url", "jdbc:mysql://192.168.20.29:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false")
          .option("user", "root")
          .option("password", "new.1234")
          .option("dbtable", "(SELECT maxip,minip,multiarea FROM ip_geo  WHERE country = '中国' limit 1) as tmp")
          .load()

    df.foreach(println(_))
    //          .load(sql)



    //    val tableQuery = "(SELECT maxip,minip,multiarea FROM ip_geo  WHERE country = '中国' limit 1) tmp"
//    val ipGeoRDD = sqlContext.read.format("jdbc").options(
//      Map("url" -> "jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false&user=root&password=new.1234",
//        "dbtable" -> tableQuery,
//        "fetchSize" -> "1",
//        "numPartitions" -> "300"
//      )).load()
//      .map(row => {
//        val maxip = row.getLong(0)
//        val minip = row.getLong(1)
//        val multiarea = row.getString(2)
//        maxip + "\t" + minip + "\t" + multiarea
//      }).foreach(println(_))



    sc.stop()


  }

}
package com.avcdata.vbox.util

import java.sql.DriverManager

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.{DataFrame, SQLContext, SaveMode}
import org.apache.spark.{SparkConf, SparkContext}


object JdbcUtils03 {

  //TODO 读取Mysql表=>DataFrame
  def readMysql2DF(sc: SparkContext, host: String, prop: java.util.Properties, db: String, tableName: String): DataFrame
  = {

    val sqlContext = new SQLContext(sc)
    val url: String = "jdbc:mysql://" + host + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    sqlContext.read.jdbc(url, tableName, prop)
  }


  //TODO 通过jdbc 将DataFrame写入mysql
  def writeDF2Mysql(sc: SparkContext, df: DataFrame,
                    prop: java.util.Properties, db: String, tableName: String, isTruncate: Boolean) = {

    //    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + sc.getConf.get("mysql.db") + "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    val url: String = "jdbc:mysql://" + sc.getConf.get("mysql.host") + ":3306/" + db +
      "?useUnicode=true&characterEncoding=utf-8&useSSL=false"

    //清空数据
    if (isTruncate) truncate(url, tableName, prop)

    df.write
      .mode(SaveMode.Append)
      .jdbc(url, tableName, prop)

  }

  //TODO 清空表数据
  def truncate(url: String, tableName: String, prop: java.util.Properties) = {
    classOf[com.mysql.jdbc.Driver]
    val conn = DriverManager.getConnection(url + "&user=" + prop.getProperty("user") + "&password=" + prop.getProperty("password"))
    try {
      val statement = conn.createStatement()
      statement.executeUpdate("TRUNCATE " + tableName);
    }
    catch {
      case e: Exception => e.printStackTrace
    }
    finally {
      conn.close
    }
  }


  //TODO 通过JDBC操作Hive
  def hiveJdbc() {
    Class.forName("org.apache.hive.jdbc.HiveDriver")
    val conn = DriverManager.getConnection("jdbc:hive2://hadoop2:10000/hive", "hadoop", "")
    try {
      val statement = conn.createStatement
      val rs = statement.executeQuery("select ordernumber,amount from tbStockDetail  where amount>3000")
      while (rs.next) {
        val ordernumber = rs.getString("ordernumber")
        val amount = rs.getString("amount")
        println("ordernumber = %s, amount = %s".format(ordernumber, amount))
      }
    } catch {
      case e: Exception => e.printStackTrace
    }
    conn.close
  }


  //TODO Main
  def main(args: Array[String]) {
    val config: Config = ConfigFactory.load()
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("test")
    val sc = new SparkContext(conf)
    val prop = new java.util.Properties()
    prop.put("user", config.getString("mysql.user"))
    prop.put("password", config.getString("mysql.password"))
    prop.put("driver", "com.mysql.jdbc.Driver")
    val tbDF = readMysql2DF(sc, config.getString("mysql.host"), prop, "vboxDB", "SYS_SHOW_USER")
    tbDF.foreach(print(_))
    sc.stop()
  }

}
package com.avcdata.spark.job.util

import java.text.SimpleDateFormat

import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.json4s.jackson.JsonMethods._
import org.json4s.jackson.Serialization
import org.json4s.{JValue, _}

object JsonUtils {

  implicit val formats = new DefaultFormats {
    override def dateFormatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
  }
  implicit val formats2 = Serialization.formats(ShortTypeHints(List()))


  def jValue2JsonString(obj: JValue): String = {
    compact(render(obj))
  }

  def jValue2PrettyJsonString(obj: JValue): String = {
    pretty(render(obj))
  }

  ///////////////////////////////////////////////////////////////////////////////////////////
  def jsonObjStr2Map(json: String): Map[String, Any] = {
    org.json4s.jackson.JsonMethods.parse(json, useBigDecimalForDouble = true).values.asInstanceOf[Map[String, Any]]
  }

  def jsonStr2ObjectDemo(): Unit = {
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    implicit val formats = DefaultFormats // Brings in default date formats etc.
    case class Child(name: String, age: Int, birthdate: Option[java.util.Date])
    case class Address(street: String, city: String)
    case class Person(name: String, address: Address, children: List[Child])
    val json = parse(
      """
         { "name": "joe",
           "address": {
             "street": "Bulevard",
             "city": "Helsinki"
           },
           "children": [
             {
               "name": "Mary",
               "age": 5,
               "birthdate": "2004-09-04T18:06:22Z"
             },
             {
               "name": "Mazy",
               "age": 3
             }
           ]
         }
      """)
    println(json.extract[Person].name)

  }

  //////////////////////////////////////spark json///////////////////////////////////////////////
  def sparkReadFromJsonFile2DF(sc: SparkContext, jsonFilePath: String): DataFrame = {
    val sqlContext = new SQLContext(sc);
    val df = sqlContext.read.format("json").load(jsonFilePath);
    df
  }

  def sparkReadFromJsonFileFilter2DF(sc: SparkContext, jsonFilePath: String): DataFrame = {
    val sqlContext = new SQLContext(sc);
    val df = sqlContext.jsonFile(jsonFilePath).registerTempTable("jsonTable")
    val jsonQuery = sqlContext.sql("select * from jsonTable")
    jsonQuery.printSchema
    jsonQuery.queryExecution
    jsonQuery
  }


  ///////////////////////////////////////////////////////////test////////////////////////////////////////////////////

  def main(args: Array[String]): Unit = {
    tmp01()
  }

  def tmp01() = {
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    //    parse("""{"numbers":[1,2,3,4]}""").values
    //    parse("""{"name":"Toy","price":35.35}""", useBigDecimalForDouble = true)

    val json2 = parse(
      """
         {
           "name": "joe",
           "addresses": {
             "address1": {
               "street": "Bulevard",
               "city": "Helsinki"
             },
             "address2": {
               "street": "Soho",
               "city": "London"
             }
           }
         }""")

    val json = parse(
      """
         { "name": "joe",
           "children": [
             {
               "name": "Mary",
               "age": 5
             },
             {
               "name": "Mazy",
               "age": 3
             }
           ]
         }
      """)

    for (JArray(child) <- json) {
      println(child)
    }

    for {
      JObject(child) <- json
      JField("age", JInt(age)) <- child
    } yield age

    for {
      JObject(child) <- json
      JField("name", JString(name)) <- child
      JField("age", JInt(age)) <- child
      if age > 4
    } yield (name, age)

  }


  def tmp02() = {
    //    val json =
    //      ("person" ->
    //        ("name" -> "Joe") ~
    //          ("age" -> 35) ~
    //          ("spouse" ->
    //            ("person" ->
    //              ("name" -> "Marilyn") ~
    //                ("age" -> 33)
    //              )
    //            )
    //        )

  }


}
package com.avcdata.spark.job.util

import java.text.SimpleDateFormat

import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.json4s.jackson.JsonMethods._
import org.json4s.jackson.Serialization
import org.json4s.{JValue, _}

object JsonUtils {

  implicit val formats = new DefaultFormats {
    override def dateFormatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
  }
  implicit val formats2 = Serialization.formats(ShortTypeHints(List()))


  def jValue2JsonString(obj: JValue): String = {
    compact(render(obj))
  }

  def jValue2PrettyJsonString(obj: JValue): String = {
    pretty(render(obj))
  }

  ///////////////////////////////////////////////////////////////////////////////////////////
  def jsonObjStr2Map(json: String): Map[String, Any] = {
    org.json4s.jackson.JsonMethods.parse(json, useBigDecimalForDouble = true).values.asInstanceOf[Map[String, Any]]
  }

  def jsonStr2ObjectDemo(): Unit = {
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    implicit val formats = DefaultFormats // Brings in default date formats etc.
    case class Child(name: String, age: Int, birthdate: Option[java.util.Date])
    case class Address(street: String, city: String)
    case class Person(name: String, address: Address, children: List[Child])
    val json = parse(
      """
         { "name": "joe",
           "address": {
             "street": "Bulevard",
             "city": "Helsinki"
           },
           "children": [
             {
               "name": "Mary",
               "age": 5,
               "birthdate": "2004-09-04T18:06:22Z"
             },
             {
               "name": "Mazy",
               "age": 3
             }
           ]
         }
      """)
    println(json.extract[Person].name)

  }

  //////////////////////////////////////spark json///////////////////////////////////////////////
  def sparkReadFromJsonFile2DF(sc: SparkContext, jsonFilePath: String): DataFrame = {
    val sqlContext = new SQLContext(sc);
    val df = sqlContext.read.format("json").load(jsonFilePath);
    df
  }

  def sparkReadFromJsonFileFilter2DF(sc: SparkContext, jsonFilePath: String): DataFrame = {
    val sqlContext = new SQLContext(sc);
    val df = sqlContext.jsonFile(jsonFilePath).registerTempTable("jsonTable")
    val jsonQuery = sqlContext.sql("select * from jsonTable")
    jsonQuery.printSchema
    jsonQuery.queryExecution
    jsonQuery
  }


  ///////////////////////////////////////////////////////////test////////////////////////////////////////////////////

  def main(args: Array[String]): Unit = {
    tmp01()
  }

  def tmp01() = {
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    //    parse("""{"numbers":[1,2,3,4]}""").values
    //    parse("""{"name":"Toy","price":35.35}""", useBigDecimalForDouble = true)

    val json2 = parse(
      """
         {
           "name": "joe",
           "addresses": {
             "address1": {
               "street": "Bulevard",
               "city": "Helsinki"
             },
             "address2": {
               "street": "Soho",
               "city": "London"
             }
           }
         }""")

    val json = parse(
      """
         { "name": "joe",
           "children": [
             {
               "name": "Mary",
               "age": 5
             },
             {
               "name": "Mazy",
               "age": 3
             }
           ]
         }
      """)

    for (JArray(child) <- json) {
      println(child)
    }

    for {
      JObject(child) <- json
      JField("age", JInt(age)) <- child
    } yield age

    for {
      JObject(child) <- json
      JField("name", JString(name)) <- child
      JField("age", JInt(age)) <- child
      if age > 4
    } yield (name, age)

  }


  def tmp02() = {
    //    val json =
    //      ("person" ->
    //        ("name" -> "Joe") ~
    //          ("age" -> 35) ~
    //          ("spouse" ->
    //            ("person" ->
    //              ("name" -> "Marilyn") ~
    //                ("age" -> 33)
    //              )
    //            )
    //        )

  }


}
package com.avcdata.vbox.util

import java.text.SimpleDateFormat

import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.json4s.jackson.JsonMethods._
import org.json4s.jackson.Serialization
import org.json4s.{JValue, _}

object JsonUtils {

  implicit val formats = new DefaultFormats {
    override def dateFormatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
  }
  implicit val formats2 = Serialization.formats(ShortTypeHints(List()))


  def jValue2JsonString(obj: JValue): String = {
    compact(render(obj))
  }

  def jValue2PrettyJsonString(obj: JValue): String = {
    pretty(render(obj))
  }

  ///////////////////////////////////////////////////////////////////////////////////////////
  def jsonObjStr2Map(json: String): Map[String, Any] = {
    org.json4s.jackson.JsonMethods.parse(json, useBigDecimalForDouble = true).values.asInstanceOf[Map[String, Any]]
  }

  def jsonStr2ObjectDemo(): Unit = {
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    implicit val formats = DefaultFormats // Brings in default date formats etc.
    case class Child(name: String, age: Int, birthdate: Option[java.util.Date])
    case class Address(street: String, city: String)
    case class Person(name: String, address: Address, children: List[Child])
    val json = parse(
      """
         { "name": "joe",
           "address": {
             "street": "Bulevard",
             "city": "Helsinki"
           },
           "children": [
             {
               "name": "Mary",
               "age": 5,
               "birthdate": "2004-09-04T18:06:22Z"
             },
             {
               "name": "Mazy",
               "age": 3
             }
           ]
         }
      """)
    println(json.extract[Person].name)

  }

  //////////////////////////////////////spark json///////////////////////////////////////////////
  def sparkReadFromJsonFile2DF(sc: SparkContext, jsonFilePath: String): DataFrame = {
    val sqlContext = new SQLContext(sc);
    val df = sqlContext.read.format("json").load(jsonFilePath);
    df
  }

  def sparkReadFromJsonFileFilter2DF(sc: SparkContext, jsonFilePath: String): DataFrame = {
    val sqlContext = new SQLContext(sc);
    val df = sqlContext.jsonFile(jsonFilePath).registerTempTable("jsonTable")
    val jsonQuery = sqlContext.sql("select * from jsonTable")
    jsonQuery.printSchema
    jsonQuery.queryExecution
    jsonQuery
  }


  ///////////////////////////////////////////////////////////test////////////////////////////////////////////////////

  def main(args: Array[String]): Unit = {
    tmp01()
  }

  def tmp01() = {
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    //    parse("""{"numbers":[1,2,3,4]}""").values
    //    parse("""{"name":"Toy","price":35.35}""", useBigDecimalForDouble = true)

    val json2 = parse(
      """
         {
           "name": "joe",
           "addresses": {
             "address1": {
               "street": "Bulevard",
               "city": "Helsinki"
             },
             "address2": {
               "street": "Soho",
               "city": "London"
             }
           }
         }""")

    val json = parse(
      """
         { "name": "joe",
           "children": [
             {
               "name": "Mary",
               "age": 5
             },
             {
               "name": "Mazy",
               "age": 3
             }
           ]
         }
      """)

    for (JArray(child) <- json) {
      println(child)
    }

    for {
      JObject(child) <- json
      JField("age", JInt(age)) <- child
    } yield age

    for {
      JObject(child) <- json
      JField("name", JString(name)) <- child
      JField("age", JInt(age)) <- child
      if age > 4
    } yield (name, age)

  }


  def tmp02() = {
    //    val json =
    //      ("person" ->
    //        ("name" -> "Joe") ~
    //          ("age" -> 35) ~
    //          ("spouse" ->
    //            ("person" ->
    //              ("name" -> "Marilyn") ~
    //                ("age" -> 33)
    //              )
    //            )
    //        )

  }

  def tmp03() = {
    val line = """{"ip":"/1.119.129.16:48506","timestamp":1497241435117,"mac":"{"Mac":"d8:47:10:a7:82:c3"}"}"""


    scala.util.parsing.json.JSON.parseFull(line)
    match {
      // Matches if jsonStr is valid JSON and represents a Map of Strings to Any
      case Some(map: Map[String, Any]) => println(map)
      case None => println("Parsing failed")
      case other => println("Unknown data structure: " + other)
    }

  }

  def tmp04() = {
    case class HeartItem(mac: String, timestamp: String, ip: String)

    val json = """{"ip":"/1.119.129.16:48506","timestamp":1497241435117,"mac":"d8:47:10:a7:82:c3"}"""
    import org.json4s._
    import org.json4s.jackson.JsonMethods._
    implicit val formats = DefaultFormats
    val heart = parse(json).extract[HeartItem]
  }


}
import com.typesafe.config.{Config, ConfigFactory}
import kafka.serializer.StringDecoder
import org.apache.log4j.Logger
import org.apache.spark.{SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import stores.ZooKeeperOffsetsStore
import utils.{HBaseUtils, Tools}
import java.text.SimpleDateFormat
import java.util.Date


/**
  * Created by wxy on 4/28/17.
  */
object KafkaOffsetExecutor
{
  def main(arg: Array[String])
  {
    val log = Logger.getLogger(getClass.getName)

    //命令行参数
    def parseOptions(args: Array[String], index: Int, defaultValue: String): String =
    {
      if (args.length > 0) args(index) else defaultValue
    }

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val conf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))

    val ssc = new StreamingContext(conf, Seconds(120))

    val zoostore = new ZooKeeperOffsetsStore("srv1.avcdata.com", "/home/cronjob/guxiaoyang1")

    val stream = KafkaSource.kafkaStream[String, String, StringDecoder, StringDecoder](ssc, "srv1.avcdata.com:9092", zoostore, "vboxtopictest")

    stream.map(_._2).foreachRDD(rdd =>
    {
      val originalRDD = rdd.mapPartitions(items =>
      {
        items.map(s =>
        {
          Tools.getJsonBean(s)
        }
        )
      }
      ).cache()

      val date = new SimpleDateFormat("yyyy_MM_dd").format(new Date())
      val tableName = "tracker_sdk_stream" + date
      //插入原始数据到hbase
      originalRDD.foreachPartition(items =>
      {
        val mutator = HBaseUtils.getMutator(tableName)
        items.foreach(listItem =>
        {
          if (listItem != null)
          {
            listItem.foreach(item =>
            {
              if (item != null)
              {
                item.data.foreach(dataDateil =>
                {
                  val put = Tools.getPutForStream(dataDateil, item.sn, item.manufacturer)
                  mutator.mutate(put)
                }
                )
              }
            }
            )
          }
        }
        )
        mutator.flush()
      }
      )
    }
    )
    ssc.start()
    ssc.awaitTermination()
  }
}
package com.avcdata.etl.export

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.kafka.KafkaProducerPool
import kafka.producer.KeyedMessage
import org.apache.spark.sql.DataFrame
import org.slf4j.LoggerFactory

/**
  * Kafka操作
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/22 10:12
  */
trait KafkaOperation
{
  private val logger = LoggerFactory.getLogger(classOf[KafkaOperation])

  def writeData2Kafka(df: DataFrame, zookeeperList: String, topic: String, parallelWrites: Int): Unit =
  {
    logger.info(s"The parallel write num is $parallelWrites.")

    logger.info(s"Prepared to insert data to kafka, the data size is ${df.count()}.")

    df.toJSON.coalesce(parallelWrites, shuffle = true).foreachPartition(dataIt =>
    {
      LoanPattern.using(KafkaProducerPool(zookeeperList))
      { managedProducer =>

        val producer = managedProducer.connection
        dataIt.foreach(data => producer.send(new KeyedMessage[String, String](topic, data)))
      }
    })

    logger.info(s"End to insert data to kafka.")
  }
}
package com.avcdata.etl.common.pool.kafka

import com.avcdata.etl.common.util.KafkaUtil
import kafka.producer.{Producer, ProducerConfig}
import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}

/**
  * Kafka生产者线程池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/10/22 16:55
  */
object KafkaProducerPool
{
  private var connPool: Map[String, ObjectPool[Producer[String, String]]] = Map()

  sys.addShutdownHook
  {
    connPool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperAddress: String): ManagedConnection =
  {
    val pool = connPool.getOrElse(zookeeperAddress,
      {
        KafkaProducerPool.synchronized[ObjectPool[Producer[String, String]]]
          {
            connPool.getOrElse(zookeeperAddress,
              {
                val p = new GenericObjectPool[Producer[String, String]](new ConnectionFactory(zookeeperAddress))
                connPool += zookeeperAddress -> p

                p
              })
          }
      })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

class ManagedConnection(private val pool: ObjectPool[Producer[String, String]], val connection: Producer[String, String])
{
  def close() = pool.returnObject(connection)
}

private class ConnectionFactory(zookeeperAddress: String)
  extends BasePooledObjectFactory[Producer[String, String]]
{
  override def create() =
  {
    new Producer[String, String](new ProducerConfig(KafkaUtil.defaultConfigs(zookeeperAddress)))
  }

  override def wrap(conn: Producer[String, String]) = new DefaultPooledObject[Producer[String, String]](conn)

  override def destroyObject(po: PooledObject[Producer[String, String]]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Producer[String, String]]) =
  {}
}package com.avcdata.etl.common.pool.kafka

import com.avcdata.etl.common.util.KafkaUtil
import kafka.producer.{Producer, ProducerConfig}
import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}

/**
  * Kafka生产者线程池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/10/22 16:55
  */
object KafkaProducerPool
{
  private var connPool: Map[String, ObjectPool[Producer[String, String]]] = Map()

  sys.addShutdownHook
  {
    connPool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(zookeeperAddress: String): ManagedConnection =
  {
    val pool = connPool.getOrElse(zookeeperAddress,
      {
        KafkaProducerPool.synchronized[ObjectPool[Producer[String, String]]]
          {
            connPool.getOrElse(zookeeperAddress,
              {
                val p = new GenericObjectPool[Producer[String, String]](new ConnectionFactory(zookeeperAddress))
                connPool += zookeeperAddress -> p

                p
              })
          }
      })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

class ManagedConnection(private val pool: ObjectPool[Producer[String, String]], val connection: Producer[String, String])
{
  def close() = pool.returnObject(connection)
}

private class ConnectionFactory(zookeeperAddress: String)
  extends BasePooledObjectFactory[Producer[String, String]]
{
  override def create() =
  {
    new Producer[String, String](new ProducerConfig(KafkaUtil.defaultConfigs(zookeeperAddress)))
  }

  override def wrap(conn: Producer[String, String]) = new DefaultPooledObject[Producer[String, String]](conn)

  override def destroyObject(po: PooledObject[Producer[String, String]]) = po.getObject.close()

  override def passivateObject(po: PooledObject[Producer[String, String]]) =
  {}
}
import kafka.message.MessageAndMetadata
import kafka.serializer.Decoder
import org.apache.spark.Logging
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka.KafkaUtils
import stores.OffsetsStore

import scala.reflect.ClassTag

object KafkaSource extends Logging {

  def kafkaStream[K: ClassTag, V: ClassTag, KD <: Decoder[K] : ClassTag, VD <: Decoder[V] : ClassTag]
  (ssc: StreamingContext, kafkaParams: Map[String, String], offsetsStore: OffsetsStore, topic: String): InputDStream[(K, V)] = {

    val topics = Set(topic)

    val storedOffsets = offsetsStore.readOffsets(topic)
    val kafkaStream = storedOffsets match {
      case None =>
        // start from the latest offsets
        KafkaUtils.createDirectStream[K, V, KD, VD](ssc, kafkaParams, topics)
      case Some(fromOffsets) =>
        // start from previously saved offsets
        val messageHandler = (mmd: MessageAndMetadata[K, V]) => (mmd.key, mmd.message)
        KafkaUtils.createDirectStream[K, V, KD, VD, (K, V)](ssc, kafkaParams, fromOffsets, messageHandler)
    }

    // save the offsets
    kafkaStream.foreachRDD(rdd => offsetsStore.saveOffsets(topic, rdd))

    kafkaStream
  }

  // Kafka input stream
  def kafkaStream[K: ClassTag, V: ClassTag, KD <: Decoder[K] : ClassTag, VD <: Decoder[V] : ClassTag]
  (ssc: StreamingContext, brokers: String, offsetsStore: OffsetsStore, topic: String): InputDStream[(K, V)] =
  kafkaStream(ssc, Map("metadata.broker.list" -> brokers,"auto.offset.reset" -> "largest"), offsetsStore, topic)

}package com.avcdata.etl.common.util

import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import kafka.cluster.Broker
import kafka.common.TopicAndPartition
import kafka.serializer.StringEncoder
import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.zookeeper.ZooKeeper
import org.slf4j.LoggerFactory

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * Kafka Broker工具
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/10/22 22:37
  */
object KafkaUtil
{
  private val logger = LoggerFactory.getLogger(KafkaUtil.getClass)

  val KAFKA_TOPIC_OFFSET_TBL_NAME = "kafka_topic_offset"

  /**
    * 获取Kafka的Broker信息
    *
    * @param zookeeperAddress ZK的地址
    * @return Broker的信息
    */
  def getBrokerList(zookeeperAddress: String): String =
  {
    LoanPattern.using(new ZooKeeper(zookeeperAddress, 10000, null))
    { zk =>

      zk.getChildren("/brokers/ids", false).map(id =>
      {
        val brokerInfoString = new String(zk.getData("/brokers/ids/" + id, false, null))
        Broker.createBroker(Integer.valueOf(id), brokerInfoString)
      }).filter(_ != null).map(_.endPoints.values).flatMap(endPointIt => endPointIt.map(endPoint => endPoint))
        .map(endPoint => s"${endPoint.host}:${endPoint.port}").mkString(",")
    }
  }

  /**
    * 获取默认的Kafka配置
    *
    * @param zookeeperAddress zk的地址
    * @return 默认的Kafka配置
    */
  def defaultConfigs(zookeeperAddress: String): Properties =
  {
    val config = new Properties
    config.put("metadata.broker.list", getBrokerList(zookeeperAddress))
    config.put("serializer.class", classOf[StringEncoder].getName)
    config.put("key.serializer.class", classOf[StringEncoder].getName)

    config
  }

  /**
    * 获取topic的分区数
    *
    * @param zookeeperAddress ZK的地址
    * @param topic            主题
    * @return 分区数
    */
  def topicPartitionNum(zookeeperAddress: String, topic: String): Int =
  {
    val configs = new Properties
    configs.put("bootstrap.servers", getBrokerList(zookeeperAddress))
    configs.put("key.deserializer", classOf[StringDeserializer].getName)
    configs.put("value.deserializer", classOf[StringDeserializer].getName)

    LoanPattern.using(new KafkaConsumer[String, String](configs))(_.partitionsFor(topic).size())
  }

  def fromOffsets(connectUri: String, username: String, password: String
                  , topics: Set[String], zookeeperAddress: String): Option[Map[TopicAndPartition, Long]] =
  {
    val topicToOffsets = LoanPattern.using(JDBCConnectionPool(connectUri, username, password))
    { conn =>

      topics.map(topic =>
      {
        //获取数据库topic分区偏移量
        val tpo = mutable.Map[TopicAndPartition, Long]()

        LoanPattern.using(conn.prepareStatement(s"SELECT partition_num, partition_until_offset FROM $KAFKA_TOPIC_OFFSET_TBL_NAME WHERE topic_name = ?"))
        { ps =>

          ps.setString(1, topic)
          LoanPattern.using(ps.executeQuery())
          { rs =>

            while (rs.next())
            {
              tpo.put(TopicAndPartition(topic, rs.getInt("partition_num")), rs.getLong("partition_until_offset"))
            }
          }
        }

        val topicPartitions = topicPartitionNum(zookeeperAddress, topic)
        if (topicPartitions == tpo.size)
        {
          //更新topic起始偏移量为终止偏移量
          LoanPattern.using(conn.prepareStatement(s"UPDATE $KAFKA_TOPIC_OFFSET_TBL_NAME SET partition_from_offset = partition_until_offset WHERE topic_name = ?"))
          { ps =>

            ps.setString(1, topic)
            ps.execute()
          }

          Some(tpo.toMap)
        }
        else
        {
          logger.info(s"Current topic <$topic> offsets <$topicPartitions> doesn't match DB partition size <${tpo.size}>" +
            s", so gonna rely on Spark checkpoint to reuse Kafka offsets")

          None
        }
      })
    }

    if (topicToOffsets.count(_.isDefined) == topics.size)
    {
      Some(topicToOffsets.map(_.get).flatMap(topicToOffsetMap =>
      {
        topicToOffsetMap.map(topicToOffset => topicToOffset)
      }).toMap)
    }
    else
    {
      None
    }
  }
}
package com.avcdata.etl.common.util

import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import kafka.cluster.Broker
import kafka.common.TopicAndPartition
import kafka.serializer.StringEncoder
import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.zookeeper.ZooKeeper
import org.slf4j.LoggerFactory

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * Kafka Broker工具
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/10/22 22:37
  */
object KafkaUtil
{
  private val logger = LoggerFactory.getLogger(KafkaUtil.getClass)

  val KAFKA_TOPIC_OFFSET_TBL_NAME = "kafka_topic_offset"

  /**
    * 获取Kafka的Broker信息
    *
    * @param zookeeperAddress ZK的地址
    * @return Broker的信息
    */
  def getBrokerList(zookeeperAddress: String): String =
  {
    LoanPattern.using(new ZooKeeper(zookeeperAddress, 10000, null))
    { zk =>

      zk.getChildren("/brokers/ids", false).map(id =>
      {
        val brokerInfoString = new String(zk.getData("/brokers/ids/" + id, false, null))
        Broker.createBroker(Integer.valueOf(id), brokerInfoString)
      }).filter(_ != null).map(_.endPoints.values).flatMap(endPointIt => endPointIt.map(endPoint => endPoint))
        .map(endPoint => s"${endPoint.host}:${endPoint.port}").mkString(",")
    }
  }

  /**
    * 获取默认的Kafka配置
    *
    * @param zookeeperAddress zk的地址
    * @return 默认的Kafka配置
    */
  def defaultConfigs(zookeeperAddress: String): Properties =
  {
    val config = new Properties
    config.put("metadata.broker.list", getBrokerList(zookeeperAddress))
    config.put("serializer.class", classOf[StringEncoder].getName)
    config.put("key.serializer.class", classOf[StringEncoder].getName)

    config
  }

  /**
    * 获取topic的分区数
    *
    * @param zookeeperAddress ZK的地址
    * @param topic            主题
    * @return 分区数
    */
  def topicPartitionNum(zookeeperAddress: String, topic: String): Int =
  {
    val configs = new Properties
    configs.put("bootstrap.servers", getBrokerList(zookeeperAddress))
    configs.put("key.deserializer", classOf[StringDeserializer].getName)
    configs.put("value.deserializer", classOf[StringDeserializer].getName)

    LoanPattern.using(new KafkaConsumer[String, String](configs))(_.partitionsFor(topic).size())
  }

  def fromOffsets(connectUri: String, username: String, password: String
                  , topics: Set[String], zookeeperAddress: String): Option[Map[TopicAndPartition, Long]] =
  {
    val topicToOffsets = LoanPattern.using(JDBCConnectionPool(connectUri, username, password))
    { conn =>

      topics.map(topic =>
      {
        //获取数据库topic分区偏移量
        val tpo = mutable.Map[TopicAndPartition, Long]()

        LoanPattern.using(conn.prepareStatement(s"SELECT partition_num, partition_until_offset FROM $KAFKA_TOPIC_OFFSET_TBL_NAME WHERE topic_name = ?"))
        { ps =>

          ps.setString(1, topic)
          LoanPattern.using(ps.executeQuery())
          { rs =>

            while (rs.next())
            {
              tpo.put(TopicAndPartition(topic, rs.getInt("partition_num")), rs.getLong("partition_until_offset"))
            }
          }
        }

        val topicPartitions = topicPartitionNum(zookeeperAddress, topic)
        if (topicPartitions == tpo.size)
        {
          //更新topic起始偏移量为终止偏移量
          LoanPattern.using(conn.prepareStatement(s"UPDATE $KAFKA_TOPIC_OFFSET_TBL_NAME SET partition_from_offset = partition_until_offset WHERE topic_name = ?"))
          { ps =>

            ps.setString(1, topic)
            ps.execute()
          }

          Some(tpo.toMap)
        }
        else
        {
          logger.info(s"Current topic <$topic> offsets <$topicPartitions> doesn't match DB partition size <${tpo.size}>" +
            s", so gonna rely on Spark checkpoint to reuse Kafka offsets")

          None
        }
      })
    }

    if (topicToOffsets.count(_.isDefined) == topics.size)
    {
      Some(topicToOffsets.map(_.get).flatMap(topicToOffsetMap =>
      {
        topicToOffsetMap.map(topicToOffset => topicToOffset)
      }).toMap)
    }
    else
    {
      None
    }
  }
}
package com.avcdata.spark.job.mllib

import com.avcdata.spark.job.common.Helper
import org.apache.spark.SparkContext
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors


/**
  * Created by Administrator on 2017/4/20.
  */
object kmeans {

//  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
//  Logger.getLogger("org.apache.eclipse.jetty.server").setLevel(Level.OFF)


  def main(args: Array[String]) {

    val sc = Helper.sparkContext

    val sourcePath = "/user/hdfs/rsync/uservector/2017-03-15-151493853344253UserVectorAllETL"

    run(sc, sourcePath)

    sc.stop()

  }

  def run(sc: SparkContext, dataPath: String) = {

    val initRDD = sc.textFile(dataPath)

    val vec = initRDD.map(line => {

      val li = line.replaceAll("\"", "")

      val arr = li.split("\t")

      val sn = arr(0)

      var vectorStr = ""

      for (i <- 3 until arr.length)
        vectorStr = vectorStr + "," + arr(i)

      val vector = Vectors.dense(vectorStr.split(",")
        .filter(ele => {
          !ele.isEmpty && !ele.contains(",")
        }).map(_.toDouble))

      (vector)

    })
    //    vec.foreach(println(_))

    val label_feature_vec = initRDD.map(line => {

      val li = line.replaceAll("\"", "")

      val arr = li.split("\t")

      val sn = arr(0)
      val stat_data = arr(1)
      val period = arr(2)
      val brand = arr(3)
      val province = arr(4)
      val price = arr(5)
      val size = arr(6)
      val workday_oc_dist = arr(7)
      val restday_oc_dist = arr(8)
      val workday_channel_dist = arr(9)
      val restday_channel_dist = arr(10)
      val pg_subject_dist = arr(11)
      val pg_yeay_dist = arr(12)
      val pg_region_dist = arr(13)

      var vectorStr = ""
      var featureStr = ""

      for (i <- 3 until arr.length)
        vectorStr = vectorStr + "," + arr(i)
      //      for(i <- 3 until arr.length)
      //        featureStr = featureStr+"\t"+arr(i)

      (sn, stat_data, period, brand, province, price, size, workday_oc_dist, restday_oc_dist, workday_channel_dist, restday_channel_dist, pg_subject_dist, pg_yeay_dist, pg_region_dist, vectorStr)

    })

    /////////////////////////////////////////////////////////////////////////////////////////
    //模型训练
    // 设置簇的个数
    val dataModelNumber = 8
    // 设置最大迭代次数
    val dataModelTrainTimes = 30
    // 运行10次选出最优解
    val runs = 10
    // 初始聚类中心的选取为k-means++
    val initMode = "k-means||"
    val model_k = KMeans.train(vec, dataModelNumber, dataModelTrainTimes, runs, initMode)
    // 计算集合内方差和，数值越小说明同一簇实例之间的距离越小
    val WSSSE = model_k.computeCost(vec)
    println("Within Set Sum of Squared Errors = " + WSSSE)
    //预测样本类别
    val clu_label_vec = label_feature_vec.map(x => {
      val v = Vectors.dense(x._15.split(",")
        .filter(ele => {
          !ele.isEmpty && !ele.contains(",")
        }).map(_.toDouble))

      (x._1, x._2, x._3, x._4, x._5, x._6, x._7, x._8, x._9, x._10, x._11, x._12, x._13, x._14, model_k.predict(v)) //每个样本对应的sn号,特征,样本所属id

    })
    //    clu_label_vec.foreach(println(_))
    clu_label_vec
      .map(x => {
        x._1 + "\t" + x._2 + "\t" + x._3 + "\t" + x._4 + "\t" + x._5 + "\t" + x._6 + "\t" + x._7 + "\t" + x._8 + "\t" + x._9 + "\t" + x._10 + "\t" + x._11 + "\t" + x._12 + "\t" + x._13 + "\t" + x._14 + "\t" + x._15
      })
      .saveAsTextFile("/user/hdfs/rsync/uservector/" + System.currentTimeMillis()
        + "-ClusterResult")

    //          .foreachPartition(cluster_resultToDB)    //预测结果写入数据库
    //    clu_label_vec.foreachPartition(cluster_resultToFS)      //预测结果写入HDFS
    //获取对象类型
    //    println(model_k.clusterCenters.getClass.getName)
    //
    //    println("Cluster centers:")
    //    for(c <- model_k.clusterCenters){
    //      println(c.getClass.getName)
    //      println(" "+c.toString)
    //    }
  }

}

package com.avc.spark.mllib


import java.sql.DriverManager

import breeze.numerics.abs
import com.avc.spark.mllib.CosineSimilarity.newVector
import com.avc.util.{Helper, mysqlDB}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext

import scala.util.Random

/**
  * Created by Administrator on 2017/5/16.
  * 修改为初始化聚类中心从问卷样本随机抽取
  */
object KMeans_V2 {
  def main(args: Array[String]) {
    val sc = Helper.sparkContext
    run(sc, "2017-03-15", "15")
    sc.stop()
  }
  def run(sc:SparkContext,analysisDate: String, recentDaysNum: String): Unit ={
    algorithm(sc, analysisDate, recentDaysNum, 4,0.5)
    algorithm(sc, analysisDate, recentDaysNum, 8,0.5)
    algorithm(sc, analysisDate, recentDaysNum, 12,0.5)
    algorithm(sc, analysisDate, recentDaysNum, 16,0.5)
  }

  //聚类算法
  def algorithm(sc: SparkContext,analysisDate: String, recentDaysNum: String,k:Int,k_weight:Double) = {
    println("聚类中心数为："+k)
    //输入路径
    val input = "/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-UserVectorAllETL"
    //聚类结果输出路径
    val output = "/user/hdfs/rsync/uservector/test/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + k
    //聚类中心向量输出路径
    val centerOutput = "/user/hdfs/rsync/uservector/test/" + analysisDate + "-" + recentDaysNum + "-centers-" + k
    //问卷样本路径
    val samplePath = "/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-FamilyVectorDataExport"

    val inputRDD = sc.textFile(input)
    val sampleRDD = sc.textFile(samplePath)
    //迭代变化
    var s = 0d
    //迭代收敛阈值
    val shold = 0.1
    //迭代次数,累加器的形式
    var time = sc.accumulator(0)
    var s1 = Double.MaxValue
    //最大限制迭代次数
    val MaxTimes = 30
    //是否进入下一轮迭代
    var ReadyForIteration = true
    //全向量数据
    val points = inputRDD.map(line => {
      val cols = line.split("\t")
      var i = 0
      val sn = cols(i)
      i = i + 1
      val stat_date = cols(i)
      i = i + 1
      val period = cols(i)
      i = i + 1

      val brand = cols(i)
      i = i + 1
      val province = cols(i)
      i = i + 1
      val price = cols(i)
      i = i + 1
      val size = cols(i)
      i = i + 1

      val workday_oc_dist = cols(i)
      i = i + 1
      val restday_oc_dist = cols(i)
      i = i + 1
      val workday_channel_dist = cols(i)
      i = i + 1
      val restday_channel_dist = cols(i)
      i = i + 1
      val pg_subject_dist = cols(i)
      i = i + 1
      val pg_year_dist = cols(i)
      i = i + 1
      val pg_region_dist = cols(i)
      //TODO 合并向量字符串
      val sb = new StringBuilder
      for (i <- 3 until cols.length) {
        if (i < (cols.length - 1)) {
          sb.append(cols(i) + ",")
        } else {
          sb.append(cols(i))
        }
      }
      //TODO 转换成大向量
      val vector = Vectors.dense(sb.toString.split(",").map(_.toDouble))

      (sn,stat_date,period,brand,province,price,size,workday_oc_dist,restday_oc_dist,workday_channel_dist,restday_channel_dist,pg_subject_dist,pg_year_dist,pg_region_dist,vector)

    })

    //样本数据(带有sn号和向量的字段，样本向量，样本中的家庭构成)
    val samples = sc.textFile(samplePath).map(line => {
      val arr = line.split("\t")
      val sn = arr(0)

      val member_num = arr(12)
      val has_child = arr(14)
      val has_old = arr(15)

      var sample_vectorStr = ""
      for (i <- 1 until arr.length - 4)
        if(i>1){
          sample_vectorStr = sample_vectorStr + "," +arr(i)
        }else{
          sample_vectorStr = arr(i)
        }

      val sample_vector = Vectors.dense(sample_vectorStr.split(",").map(_.toDouble))

      (sn, sample_vector, member_num, has_child, has_old)
    })



    //问卷样本向量
    val sampleVector = samples.map(x => {
      val sample_vector = x._2
      //带有sn号和向量的字段，样本向量，样本中的家庭构成
      (sample_vector)
    })
    //从样本中随机生成聚类中心向量
    var centers = sampleVector.takeSample(false, k, new Random().nextLong())

    //定义样本Map存放sn->vector
//    var sampleMap:Map[String,String] = Map()
    val samplemap=  sc.textFile(samplePath).map(x=>{
      val arr = x.split("\t")
      val sn = arr(0)
      val member_num = arr(12)
      val has_child = arr(14)
      val has_old = arr(15)
      val tags = member_num + "\t" + has_child + "\t" + has_old
      (sn,tags.toString())
    }).collect().toMap
    val sampleMapBr=sc.broadcast(samplemap).value

    while (ReadyForIteration) {
      time += 1
      val ClusteringResult = points.map(v => {
        val (centerId, minDistance) = getClosestCenter(centers, v._15)
        val newvector = (newVector(v._15),minDistance)
        (v._1, v._2, v._3, v._4, v._5, v._6, v._7, v._8, v._9, v._10, v._11, v._12, v._13, v._14, newvector,centerId)
      })
      val NewCentersRdd = ClusteringResult.map(ele => {
        val newCenter = (ele._15)._1 * (1d / ele._15._2)
        val sumOfDistance = (ele._15)._2
        (newCenter.point, sumOfDistance)
      })
      var s2 = getNewCenters(NewCentersRdd, centers)
      s = abs(s2 - s1)
      if (s < shold || time.value > MaxTimes) {
        ReadyForIteration = false
        //聚类结果
        val finalResult = ClusteringResult.map(x => {
          (x._1 + "\t" + x._2 + "\t" + x._3 + "\t" + x._4 + "\t" + x._5 + "\t" + x._6 + "\t" + x._7 + "\t" + x._8 + "\t" + x._9 + "\t" + x._10 + "\t" + x._11 + "\t" + x._12 + "\t" + x._13 + "\t" + x._14 + "\t" + x._16)
        }).saveAsTextFile(output)

        //聚类中心
        val c = centers.map(item => {
          val i = centers.indexOf(item)
          val centerArray = item.toArray
          val center = new StringBuilder
          for(i <- 0 until centerArray.length){
            if (i < (centerArray.length - 1)) {
              center.append(centerArray(i) + ",")
            } else {
              center.append(centerArray(i))
            }
          }
          (i,center)
        }).saveAsTextFile(centerOutput)


        //指标一结果
//        val ssd = s2
        val ssd = Math.pow(k,k_weight)*Math.pow((s2+1),(1-k_weight))
        mysqlDB.index_resultTODB(k,ssd)

        //指标二结果
        ClusteringResult.foreachPartition(items => {
          val conn = DriverManager.getConnection("jdbc:MySQL://192.168.1.201:3306/test_kmeans", "root", "new.1234")
          val sql = "insert into sample_allTags_cluster_result_k" + k +"(sn,cluster_id,family_compose, has_child,has_old) values (?,?,?,?,?)"
          val ps = conn.prepareStatement(sql)
          items.foreach(item => {
            if(sampleMapBr.contains(item._1)){
              val sn = item._1
              val clusterId = item._16
              val tagsArray = sampleMapBr.get(sn).toString.replaceAll("Some","").replaceAll("\\(","").replaceAll("\\)","").split("\t")
              val member_num = tagsArray(0)
              val has_child = tagsArray(1)
              val has_old = tagsArray(2)
              ps.setString(1,sn)
              ps.setInt(2,clusterId)
              ps.setString(3,member_num)
              ps.setString(4,has_child)
              ps.setString(5,has_old)
              ps.executeUpdate()
            }
          })

          conn.close()
        })
      }
      s1 = s2
    }
  }

  implicit def toNewVector(point: Vector) = newVector(point)


  //寻找最近簇点及最短距离
  def getClosestCenter(centers: Array[Vector], point: Vector): (Int, Double) = {
    var minDistance = Double.MaxValue
    var centerId = 0
    for (i <- 0 until centers.length) {
      if (point.distance(point,centers(i)) < minDistance) {
        minDistance = point.distance(point,centers(i))
        centerId = i
      }
    }
    (centerId, minDistance)
  }

  def getNewCenters(rdd: RDD[(Vector, Double)], centers: Array[Vector]): Double ={
    //take用于获取RDD中从0到centers.length下标的元素，不排序。
    val res = rdd.take(centers.length)
    var sumOfDistance = 0d
    for (i <- 0 until centers.length) {
      //更新聚类中心簇
      centers(i) = res.apply(i)._1
      sumOfDistance += res.apply(i)._2
    }
    val sum = sumOfDistance / centers.length
    sum
  }

  def printCenters(centers: Array[Vector]) {
    for (v <- centers) {
      v.toArray.foreach(x => print(x+","));print("\n")
    }
  }
}

package com.avcdata.spark.job.executor

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.konka._
import org.apache.log4j.Logger

object KODataCleanExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext

    //TODO 终端   4core 5G 4
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@KO-TerminalDataLoadJob start....")
      TerminalDataLoadJob.run(sc, analysisDate)
      SampleTerminal.run(sc, analysisDate)
      println(analysisDate + "@KO-TerminalDataLoadJob end....")
    }

    //TODO 开关机  4core 5G 4
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@KO-ChTerminalPowerOnDataLoadJob start....")
      TerminalPowerOnDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@KO-ChTerminalPowerOnDataLoadJob end....")
    }


    //TODO 直播  4core 5G 8
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@KO-ChLiveDataLoadJob start....")
      LiveTerminal.run(sc, analysisDate)
      LiveDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@KO-ChLiveDataLoadJob end....")
    }


    //TODO apk  4core 5G 4
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@KO-ChApkDataLoadJob start....")
      ApkDataLoadJob.run(sc, analysisDate)
      println(analysisDate + "@KO-ChApkDataLoadJob end....")
    }


    //TODO 到剧 8core 10G 8


  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveData2Partition {

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)


    val sql = "INSERT OVERWRITE TABLE hr.tracker_live_fact_partition partition(date='" + currentDate + "')  select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_time_length,fact_cnt,dim_date from hr.tracker_live_fact where dim_date='" + currentDate + "'"


    sqlContext.sql(sql)

    println(sql)

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")


  }


}
package com.avcdata.vbox.clean.live

import com.avcdata.vbox.util.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveData2Partition {

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)


    val sql = "INSERT OVERWRITE TABLE hr.tracker_live_fact_partition partition(date='" + twoDaysAgoDate + "')  select  key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_time_length,fact_cnt from hr.tracker_live_fact03 where dim_date='" +
      twoDaysAgoDate +"' and key not like '%TCL%'"


    sqlContext.sql(sql)

    println(sql)

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")


  }


}
package com.avcdata.spark.job.konka

import java.util.Date

import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.joda.time.Seconds
import scala.collection.mutable
import com.github.nscala_time.time.Imports._

/*
**Konka的直播处理
 */
case class TVWatching(sn: String, date: String, actions: mutable.MutableList[TVWatchedItem]) {
}

case class TVWatchedItem(tv: String, date: String)

case class TVWatchedResult(tv: String, date: String, hour: String, duration: Int)

case class TVWatchedResultByMinute(tv: String, date: String, hour: String, minute: String, duration: Int, cnt: Int)

case class Live(name:String,col1:String,col2:String,col3:String,col4:String,col5:String,col6:String,col7:String)

object LiveDataLoadJob {


    def getSecondsToHourEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.hours).withSecondOfMinute(0).withMinuteOfHour(0)).getSeconds.abs
    }

    def getSecondsToMinuteEnd(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime, (dateTime + 1.minutes).withSecondOfMinute(0)).getSeconds.abs
    }

    def getSecondsFromHourBegin(dateTime: DateTime): Int = {
        Seconds.secondsBetween(dateTime.withSecondOfMinute(0).withMinuteOfHour(0), dateTime).getSeconds.abs
    }

    def getSecondsAbs(from: DateTime, to: DateTime): Int = {
        Seconds.secondsBetween(from, to).getSeconds.abs
    }


    def getSeconds(from: DateTime, to: DateTime): Int = {
        Seconds.secondsBetween(from, to).getSeconds
    }

    def getTVWatchedResultByMinute(item: TVWatching): mutable.MutableList[TVWatchedResultByMinute] = {

        val format = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss")
        var startItem: TVWatchedItem = null
        var startTime: DateTime = null
        var endItem: TVWatchedItem = null
        var endTime: DateTime = null
        var tempItem: TVWatchedItem = null
        var tempTime: DateTime = null
        val result = mutable.MutableList[TVWatchedResultByMinute]()

        for (i <- 0 until item.actions.size) {


            if (startItem == null) {
                startItem = item.actions.get(i).get
                startTime = DateTime.parse(startItem.date, format)
            }
            else {
                endItem = item.actions.get(i).get
                endTime = DateTime.parse(endItem.date, format)

                if (endItem.tv != startItem.tv || getSecondsAbs(tempTime, endTime) > 630) {
                    if (getSecondsAbs(tempTime, endTime) > 630)
                        endTime = DateTime.parse(tempItem.date, format) + 5.minutes

                    for (gap <- 0 to org.joda.time.Minutes.minutesBetween(startTime, endTime).getMinutes.abs) {
                        val time: DateTime = startTime + gap.minutes
                        if (time.getDayOfMonth == startTime.getDayOfMonth) {
                            if (gap == 0)
                                result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, getSecondsToMinuteEnd(startTime), 1)
                            else {
                                if (getSeconds(startTime + gap.minutes, endTime) < 60 && getSeconds(startTime + gap.minutes, endTime) > -60)
                                    result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, endTime.getSecondOfMinute, 0)
                                else
                                    result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, 60, 0)
                            }
                        }

                    }
                    startItem = endItem
                    startTime = DateTime.parse(startItem.date, format)
                    endItem = null
                    endTime = null
                }
            }


            tempItem = item.actions.get(i).get
            tempTime = DateTime.parse(tempItem.date, format)

        }

        if (startItem != null) {
            if (endItem == null)
                endTime = startTime + 5.minutes
            for (gap <- 0 to org.joda.time.Minutes.minutesBetween(startTime, endTime).getMinutes.abs) {
                val time: DateTime = startTime + gap.minutes
                if (time.getDayOfMonth == startTime.getDayOfMonth) {
                    if (gap == 0)
                        result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, getSecondsToMinuteEnd(startTime), 1)
                    else {
                        if (getSeconds(startTime + gap.minutes, endTime) < 60 && getSeconds(startTime + gap.minutes, endTime) > -60)
                            result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, endTime.getSecondOfMinute, 0)
                        else
                            result += new TVWatchedResultByMinute(startItem.tv, startItem.date, time.getHourOfDay.toString, time.getMinuteOfHour.toString, 60, 0)
                    }
                }


            }
        }



        result
    }



    def run(sc: SparkContext, analysisDate: String) = {

        //val zookeeper_quorum = sc.getConf.get("hbase.zookeeper.quorum")

        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimChannelCol = Bytes.toBytes("dim_channel")
        val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimDateCol = Bytes.toBytes("dim_date")
        val dimHourCol = Bytes.toBytes("dim_hour")
        val dimMinCol = Bytes.toBytes("dim_min")
        val factCntCol = Bytes.toBytes("fact_cnt")
        val factTimeLenghtCol = Bytes.toBytes("fact_time_length")

        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

        //val count = sc.textFile("F:/avc/docs/konka/tv_logo.log.2016-08-22")
        val baseRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/tv_logo.log." + analysisDate)
        //val count = sc.textFile("/user/hdfs/rsync/KONKA/history/tv/tv" + analysisDate)
                .filter(x =>{
                    val cols = x.split('|')
                    cols.length == 7 && cols(6) != "" && cols(6).length == 19
                })
                .filter(x => {
                    val date = x.split('|')(6).substring(0, 10)
                    date == analysisDate || date == preDate || date == yesBeforeDate
                })
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split('|')
                    val tv = cols(5)
                    val sn = cols(0)
                    val date = cols(6)
                    val xs = new TVWatchedItem(tv, date)
                    val terminalTV = new TVWatching(sn, date.substring(0, 10), mutable.MutableList(xs))
                    (sn + date.substring(0, 10), terminalTV)
                })
            }).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        }).mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.date).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })

        baseRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_active_fact02")) //tracker_live_active_fact
            //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_live_active_fact"))

            try {
                items.foreach(item => {

                    val sn = item.sn

                    getTVWatchedResultByMinute(item).foreach(r => {
                        val put = new Put(Bytes.toBytes(sn + r.tv + item.date + r.hour + r.minute + "KO"))
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimChannelCol, Bytes.toBytes(KONKATVMapping.getTVStandName(r.tv)))
                        //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(item.date))
                        put.addColumn(dimFamilyCol, dimHourCol, Bytes.toBytes(r.hour))
                        put.addColumn(dimFamilyCol, dimMinCol, Bytes.toBytes(r.minute))
                        put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(r.cnt.toString))
                        put.addColumn(factFamilyCol, factTimeLenghtCol, Bytes.toBytes(r.duration.toString))
                        //println(item.sn + "\t" + item.date + "\t" + r.tv + "\t" + r.hour + "\t" + r.minute + "\t" + r.duration.toString + "\t" + r.cnt)
                        mutator.mutate(put)
                    })
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

object KONKATVMapping extends Serializable {
    private val mapping = mutable.Map(
        ("CCTV-8" -> "CCTV-8"),
        ("CCTV-12" -> "CCTV-12"),
        ("CCTV-1" -> "CCTV-1"),
        ("黑龙江卫视" -> "黑龙江卫视"),
        ("CCTV-9" -> "CCTV-9"),
        ("CCTV-5" -> "CCTV-5"),
        ("贵州卫视" -> "贵州卫视"),
        ("CCTV-14" -> "CCTV-14"),
        ("东方卫视" -> "上海东方卫视"),
        ("湖北卫视" -> "湖北卫视"),
        ("东南卫视" -> "东南卫视"),
        ("广东卫视" -> "广东卫视"),
        ("CCTV-7" -> "CCTV-7"),
        ("湖南卫视" -> "湖南卫视"),
        ("山东卫视" -> "山东卫视"),
        ("BTV北京卫视" -> "北京卫视"),
        ("青海卫视" -> "青海卫视"),
        ("CCTV-10" -> "CCTV-10"),
        ("旅游卫视" -> "旅游卫视"),
        ("甘肃卫视" -> "甘肃卫视"),
        ("重庆卫视" -> "重庆卫视"),
        ("CCTV-3" -> "CCTV-3"),
        ("新疆卫视" -> "新疆卫视"),
        ("厦门卫视" -> "厦门卫视"),
        ("CCTV-13" -> "CCTV-13"),
        ("辽宁卫视" -> "辽宁卫视"),
        ("山西卫视" -> "山西卫视"),
        ("CCTV-2" -> "CCTV-2"),
        ("宁夏卫视" -> "宁夏卫视"),
        ("安徽卫视" -> "安徽卫视"),
        ("河北卫视" -> "河北卫视"),
        ("CCTV-6" -> "CCTV-6"),
        ("浙江卫视" -> "浙江卫视"),
        ("江西卫视" -> "江西卫视"),
        ("河南卫视" -> "河南卫视"),
        ("CCTV-11" -> "CCTV-11"),
        ("广西卫视" -> "广西卫视"),
        ("江苏卫视" -> "江苏卫视"),
        ("四川卫视" -> "四川卫视"),
        ("CCTV-15" -> "CCTV-15"),
        ("云南卫视" -> "云南卫视"),
        ("CCTV-4" -> "CCTV-4"),
        ("内蒙古卫视" -> "内蒙古卫视"),
        ("西藏卫视" -> "西藏卫视"),
        ("深圳卫视" -> "深圳卫视"),
        ("天津卫视" -> "天津卫视"),
        ("陕西卫视" -> "陕西卫视"),
        ("吉林卫视" -> "吉林卫视"),
        ("CCTV-1综合" -> "CCTV-1"),
        ("CCTV-2财经" -> "CCTV-2"),
        ("CCTV-3综艺" -> "CCTV-3"),
        ("CCTV-4中文国际" -> "CCTV-4"),
        ("CCTV-5体育" -> "CCTV-5"),
        ("CCTV-6电影" -> "CCTV-6"),
        ("CCTV-7军事农业" -> "CCTV-7"),
        ("CCTV-8电视剧" -> "CCTV-8"),
        ("CCTV-9纪录" -> "CCTV-9"),
        ("CCTV-10科教" -> "CCTV-10"),
        ("CCTV-11戏曲" -> "CCTV-11"),
        ("CCTV-12社会与法" -> "CCTV-12"),
        ("CCTV-13新闻" -> "CCTV-13"),
        ("CCTV-14少儿" -> "CCTV-14"),
        ("CCTV-15音乐" -> "CCTV-15"),
        ("厦门卫视" -> "厦门卫视")
    )

    def getTVStandName(name: String): String = {
        mapping.get(name).getOrElse("其他")
    }

    def main(args: Array[String]): Unit = {
        println(DateTime.parse("2016-12-14 11:11:11".substring(0, 10)).weekOfWeekyear().get())
        println(DateTime.parse("2016-12-14 11:11:11".substring(0, 10)).monthOfYear().get())
        println(DateTime.parse("2016-12-14".substring(0, 10)).dayOfYear().get())
        println(DateTime.parse("2016-12-14".substring(0, 10)).dayOfMonth().get())

        println(DateTime.parse("2017-03-01").plusDays(-2).toString("yyyy-MM-dd").substring(5,7))
    }
}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveHourTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")


        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_live_hoursql(analysisDate), HiveSql.tracker_tv_live_hourtable, 23)

        val parsql =
            """
              |insert overwrite table hr.tracker_tv_live_hour_partition partition(date)
              |select key,brand,license,province, city,channel,tv_date,tv_hour,terminal_cnt,tv_date from hr.tracker_tv_live_hour
              |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
            """

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveHourTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveHourTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")
        HiveToHbase.write(sqlc, HiveSql.getTracker_total_tv_live_hoursql(analysisDate), HiveSql.tracker_total_tv_live_hourtable, 29)

        val parsql =
            """
              |insert overwrite table hr.tracker_total_tv_live_hour_partition partition(date)
              |select key,brand,license,province,city,channel,tv_date,tv_hour,terminal_cnt,tv_date from hr.tracker_total_tv_live_hour
              |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
            """

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.konka

import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable

/*
**直播终端处理，最近三个月的，给推总用
 */
object LiveTerminal {
    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")

        //live
        val baseRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/tv_logo.log." + analysisDate)
        //val baseRdd = sc.textFile("/user/hdfs/rsync/KONKA/history/tv_logo.log.since_10-01")
            .filter(x => x.split('|').length == 7)
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split('|')
                    val sn = cols(0)
                    val model = cols(2)  //机型
                    val plat = cols(3)   //平台
                    val size = cols(4)
                    val brand = "KO"
                    (sn, plat + "," + model + "," + size + "," + brand)
                })
            }).distinct()

        //apk
        val apkrdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/activity.log." + analysisDate)
            .mapPartitions(items => {
            items.map(line => {
                val cols = line.split('|')
                val sn = cols(0)
                val model = cols(2)  //机型
                val plat = cols(3)   //平台
                val size = cols(4)
                val brand = "KO"
                (sn , plat + "," + model + "," + size + "," + brand)
            })
        }).distinct()

        val hiveContext = new HiveContext(sc)
        val teRdd = hiveContext.sql("select sn, province, city from hr.terminal where brand = 'KO'").mapPartitions(items =>{
            items.map(item => {
                val sn = item(0).toString
                val province = item(1).toString
                val city = item(2).toString
                (sn, province+","+city)
            })
        })

        val tvRdd = apkrdd.leftOuterJoin(teRdd).filter(x => !x._2.toString().contains("None"))

        tvRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_live_terminal_test")) //tracker_live_terminal
            try {
                items.foreach(item => {
                    val sn = item._1
                    val plat = item._2._1.split(",")(0)
                    val model = item._2._1.split(",")(1)
                    val size = item._2._1.split(",")(2)
                    val brand = item._2._1.split(",")(3)
                    val province = item._2._2.get.split(",")(0)
                    val city = item._2._2.get.split(",")(1)

                    val put = new Put(Bytes.toBytes(sn + "KO"))
                    put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes(""))
                    put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                    put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(""))
                    put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                    put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                    put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                    put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(""))
                    put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(""))

                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}

package com.avcdata.spark.job.total.time

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将直播推总数据导入到分区表
  */
object LiveTimeTotal2Partition {

  def main(args: Array[String]) {

    val currentDate = "2017-01-01"
    val sql = "INSERT OVERWRITE TABLE hr.tracker_total_live_fact_partition PARTITION (date='" + currentDate + "') " +
      "select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_total_live_fact where dim_date='" + currentDate + "'"

    println(sql)


  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    val sql = "INSERT OVERWRITE TABLE hr.tracker_total_live_fact_partition PARTITION (date='" + currentDate + "') " +
          "select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_total_live_fact where dim_date='" + currentDate + "'"

    println(sql)

    sqlContext.sql(sql)

    //加载三天前的数据
//    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate,TimeUtils.DAY_DATE_FORMAT_ONE,-2)
//
//    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime,TimeUtils.DAY_DATE_FORMAT_ONE)
//
//    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_total_live_fact_partition PARTITION (date='" + twoDaysAgoDate + "') " +
//      "select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_time_length,fact_cnt from hr.tracker_total_live_fact where dim_date='" + twoDaysAgoDate + "'")

//    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_total_live_fact_partition PARTITION (date) select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_time_length,fact_cnt,dim_date as date from hr.tracker_total_live_fact")


  }

}
package com.avcdata.spark.job.total.time

import com.avcdata.spark.job.total.Sql
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zhangyongtian
  * @define 直播次数和时长推总
  *
  */
object LiveTimeTotalJob {

  val log = Logger.getLogger(getClass.getName)


  //////////////////////////////////test////////////////////////////////////////////
  //  def main(args: Array[String]) {
  //    val conf = new SparkConf()
  //      .setMaster("local[1]")
  //      .setAppName("LiveTimeTotalJob")
  //    val sc = new SparkContext(conf)
  //    run(sc, "2016-11-15")
  //    sc.stop()
  //
  //  }
  //////////////////////////////////test////////////////////////////////////////////


  def run(sc: SparkContext, analysisDate: String) = {

    //TODO    读取hive数据
    val hiveContext: HiveContext = new HiveContext(sc)

    val cache_sql =
      """
       select
          sn,ratio
          from
           (select * from hr.tracker_total_dim_ratio_fst where date =date_sub('""" + analysisDate +
    """',0))  ratio
       join
        (select sn,brand,province from hr.live_terminal) terminal
       on
        (ratio.brand = terminal.brand and ratio.province=terminal.province)
      """.stripMargin

    println(cache_sql)
    hiveContext.sql(cache_sql).registerTempTable("sr_cache")

    //    //TODO 6月1号
    //    hiveContext.sql(
    //      """
    //        select
    //        distinct sn,ratio
    //        from
    //         (select * from hr.tracker_total_dim_ratio_fst where date ="2017-06-01")  ratio
    //        join
    //         (select sn,brand,province from hr.live_terminal) terminal
    //        on
    //         (ratio.brand = terminal.brand and ratio.province=terminal.province)
    //      """.stripMargin).registerTempTable("sr_cache")


    hiveContext.cacheTable("sr_cache")

    val totalDF: DataFrame = hiveContext.sql(
      Sql.getTracker_total_live_active_fact_HQL(analysisDate))

    println(Sql.getTracker_total_live_active_fact_HQL(analysisDate))

    //    totalDF.write.mode(SaveMode.Append).saveAsTable("hr.tracker_total_apk_fact")

    val totalRDD = totalDF.rdd

      //TODO 过滤异常值
      .filter(line=>{
      var i = 0

      val key = line(i).toString
      i = i + 1

      val sn = line(i).toString
      i = i + 1

      val channel = line(i).toString
      i = i + 1

      val date = line(i).toString
      i = i + 1

      val hour = line(i).toString
      i = i + 1

      val min = line(i).toString
      i = i + 1

      val cnt = line(i).toString
      i = i + 1

      val duration = line(i).toString

      duration.toDouble >= 0

    })

    totalRDD
      .foreachPartition(lines => {

        val mutator = HBaseUtils.getMutator(Sql.tracker_total_live_active_fact_HtableName)

        try {

          lines.foreach(line => {

            var i = 0

            val key = line(i).toString
            i = i + 1

            val sn = line(i).toString
            i = i + 1

            val channel = line(i).toString
            i = i + 1

            val date = line(i).toString
            i = i + 1

            val hour = line(i).toString
            i = i + 1

            val min = line(i).toString
            i = i + 1

            val cnt = line(i).toString
            i = i + 1

            val duration = line(i).toString

            val sortedLine = key + "\t" + sn + "\t" + channel + "\t" + date + "\t" + hour + "\t" + min + "\t" + cnt + "\t" + duration

            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_total_live(sortedLine))
          })
          mutator.flush()

        } finally {
          mutator.close()
        }
      }

      )

    hiveContext.uncacheTable("sr_cache")

    //    //加载数据到hive分区表
    //    hiveContext.sql("set hive.exec.dynamic.partition=true")
    //    hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //    hiveContext.sql("INSERT INTO TABLE hr.tracker_total_live_fact_partition PARTITION (date='" + analysisDate + "') " +
    //      "select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_total_live_fact where dim_date='" + analysisDate + "'")

  }


}package com.avcdata.spark.job.total.time

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * 过滤掉2017.04直播多余省份（31个 省市之外的）的数据
  */
object LiveTimeTotalProvinceFilter {

  //////////////////////////////////test////////////////////////////////////////////
  //  def main(args: Array[String]) {
  //    val conf = new SparkConf()
  //      .setMaster("local[1]")
  //      .setAppName("LiveTimeTotalJob")
  //    val sc = new SparkContext(conf)
  //    run(sc, "2016-11-15")
  //    sc.stop()
  //
  //  }
  //////////////////////////////////test////////////////////////////////////////////


  def run(sc: SparkContext, analysisDate: String) = {


    val provinceArr = Array[String](
      "上海市", "云南省", "内蒙古自治区", "北京市", "吉林省", "四川省", "天津市", "宁夏回族自治区", "安徽省", "山东省", "山西省", "广东省", "广西壮族自治区", "新疆维吾尔自治区", "江苏省", "江西省", "河北省", "河南省", "浙江省", "海南省", "湖北省", "湖南省", "甘肃省", "福建省", "西藏自治区", "贵州省", "辽宁省", "重庆市", "陕西省", "青海省", "黑龙江省"
    )

    //TODO    读取hive数据
    val hiveContext: HiveContext = new HiveContext(sc)

    hiveContext.sql("select distinct lt.sn from hr.live_terminal lt  join (select distinct province from hr.tracker_total_dim_ratio_fst)  ts on lt.province = ts.province").registerTempTable("province_sn")

    val sql =
      """
        select tf.key,tf.dim_sn,tf.dim_channel,tf.date as dim_date,tf.dim_hour,tf.dim_min,tf.fact_time_length,tf.fact_cnt  from (select * from hr. tracker_total_live_fact_partition where date = '""" + analysisDate +
        """')  tf join province_sn ps on tf.dim_sn = ps.sn
        """.stripMargin

    println(sql)

    hiveContext.sql(sql).registerTempTable("total_time_live_tmp")

    //    val rdd1 = hiveContext.sql("select * from hr. tracker_total_live_fact_partition where date = '" + analysisDate +
    //      "'")
    //
    //    println(rdd1.count)
    //
    //    val rdd2 = hiveContext.sql("select * from total_time_live_tmp")
    //
    //    println(rdd2.count)

    hiveContext.sql(
      """
              INSERT OVERWRITE TABLE hr.tracker_total_live_fact_partition PARTITION (date='""" + analysisDate +
        """') select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_time_length,fact_cnt from
           total_time_live_tmp
        """
          .stripMargin)


  }
}
package com.avcdata.spark.job.total.tnumpre

import java.util.Date

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt - 1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt - 1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" + "'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt - 1).toString
            }
        }

        var month = ""
        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_livesql_daily(analysisDate), HiveSql.tracker_tv_livetable, 22)

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_livesql_weekly(analysisDate), HiveSql.tracker_tv_livetable, 22)
        }

        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_livesql_monthly(month, analysisDate), HiveSql.tracker_tv_livetable, 22)
        }

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_livesql_30days(analysisDate), HiveSql.tracker_tv_livetable, 22)

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_livesql_7days(analysisDate), HiveSql.tracker_tv_livetable, 22)

        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_live_partition partition(date)
                  |select key,brand,license,province,city,channel,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_live
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<='""" + analysisDate +"""'
                """
        }
        else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_live_partition partition(date)
                  |select key,brand,license,province,city,channel,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_live
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 7) and tv_date<='""" + analysisDate +"""'
                """
        } else {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_live_partition partition(date)
                  |select key,brand,license,province,city,channel,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_live
                  |WHERE tv_date='""" + analysisDate +"""'
                """
        }

        sqlc.sql(parsql.stripMargin)
    }
}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object LiveTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt - 1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt - 1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" + "'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt - 1).toString
            }

        }

        var subDate = "0"
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            subDate = "32"
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            subDate = "7"
        }

//        HiveToHbase.write(sqlc, HiveSql.getTracker_total_tv_livesql(analysisDate, subDate), HiveSql.tracker_total_tv_livetable, 28)

        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_live_partition partition(date)
                  |select key,brand,license,province,city,channel,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_live
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<=DATE_SUB('""" + analysisDate +"""', 0)
                """
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_live_partition partition(date)
                  |select key,brand,license,province,city,channel,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_live
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 7) and tv_date<=DATE_SUB('""" + analysisDate +"""', 0)
                """
        } else {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_live_partition partition(date)
                  |select key,brand,license,province,city,channel,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_live
                  |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
                """
        }

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.etl.common.pattern


import scala.language.reflectiveCalls
import util.control.Exception._
import scala.concurrent.{ExecutionContext, Future}

/**
  * 借贷模式实现
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/18 10:35
  */
object LoanPattern extends LoanPattern

trait LoanPattern
{
  type Closable =
  {
    def close()
  }

  def using[R <: Closable, A](resource: R)(f: R => A): A =
  {
    try
    {
      f(resource)
    }
    finally
    {
      ignoring(classOf[Throwable]) apply
        {
          resource.close()
        }
    }
  }

  /**
    * Guarantees a Closeable resource will be closed after being passed to a block that takes
    * the resource as a parameter and returns a Future.
    */
  def futureUsing[R <: Closable, A](resource: R)(f: R => Future[A])(implicit ec: ExecutionContext): Future[A] =
  {
    f(resource) andThen
      { case _ => resource.close() } // close no matter what
  }
}package com.avcdata.etl.common.pattern


import scala.language.reflectiveCalls
import util.control.Exception._
import scala.concurrent.{ExecutionContext, Future}

/**
  * 借贷模式实现
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/18 10:35
  */
object LoanPattern extends LoanPattern

trait LoanPattern
{
  type Closable =
  {
    def close()
  }

  def using[R <: Closable, A](resource: R)(f: R => A): A =
  {
    try
    {
      f(resource)
    }
    finally
    {
      ignoring(classOf[Throwable]) apply
        {
          resource.close()
        }
    }
  }

  /**
    * Guarantees a Closeable resource will be closed after being passed to a block that takes
    * the resource as a parameter and returns a Future.
    */
  def futureUsing[R <: Closable, A](resource: R)(f: R => Future[A])(implicit ec: ExecutionContext): Future[A] =
  {
    f(resource) andThen
      { case _ => resource.close() } // close no matter what
  }
}package utils


import scala.language.reflectiveCalls
import scala.util.control.Exception._
import scala.concurrent.{ExecutionContext, Future}

/**
  * Created by guxiaoyang on 2017/7/9.
  */
object LoanPattern extends LoanPattern

trait LoanPattern
{
  type Closable =
    {
      def close()
    }

  def using[R <: Closable, A](resource: R)(f: R => A): A =
  {
    try
    {
      f(resource)
    }
    finally
    {
      ignoring(classOf[Throwable]) apply
        {
          resource.close()
        }
    }
  }

  /**
    * Guarantees a Closeable resource will be closed after being passed to a block that takes
    * the resource as a parameter and returns a Future.
    */
  def futureUsing[R <: Closable, A](resource: R)(f: R => Future[A])(implicit ec: ExecutionContext): Future[A] =
  {
    f(resource) andThen
      { case _ => resource.close() } // close no matter what
  }
}
package com.avcdata.etl.common.pattern


import scala.language.reflectiveCalls
import util.control.Exception._
import scala.concurrent.{ExecutionContext, Future}

/**
  * 借贷模式实现
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/18 10:35
  */
object LoanPattern extends LoanPattern

trait LoanPattern
{
  type Closable =
  {
    def close()
  }

  def using[R <: Closable, A](resource: R)(f: R => A): A =
  {
    try
    {
      f(resource)
    }
    finally
    {
      ignoring(classOf[Throwable]) apply
        {
          resource.close()
        }
    }
  }

  /**
    * Guarantees a Closeable resource will be closed after being passed to a block that takes
    * the resource as a parameter and returns a Future.
    */
  def futureUsing[R <: Closable, A](resource: R)(f: R => Future[A])(implicit ec: ExecutionContext): Future[A] =
  {
    f(resource) andThen
      { case _ => resource.close() } // close no matter what
  }
}package com.avcdata.etl.common.pattern


import scala.language.reflectiveCalls
import util.control.Exception._
import scala.concurrent.{ExecutionContext, Future}

/**
  * 借贷模式实现
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/8/18 10:35
  */
object LoanPattern extends LoanPattern

trait LoanPattern
{
  type Closable =
  {
    def close()
  }

  def using[R <: Closable, A](resource: R)(f: R => A): A =
  {
    try
    {
      f(resource)
    }
    finally
    {
      ignoring(classOf[Throwable]) apply
        {
          resource.close()
        }
    }
  }

  /**
    * Guarantees a Closeable resource will be closed after being passed to a block that takes
    * the resource as a parameter and returns a Future.
    */
  def futureUsing[R <: Closable, A](resource: R)(f: R => Future[A])(implicit ec: ExecutionContext): Future[A] =
  {
    f(resource) andThen
      { case _ => resource.close() } // close no matter what
  }
}package com.avcdata.spark.job.view

import java.io.Serializable

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.{HdfsUtils, JdbcUtils}
import org.apache.hadoop.fs.{FileStatus, Path}
import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 数据监控任务 (统计日志文件信息)
  */
object LogViewJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    for (i <- 10 to 60) {
      println(i+"|")
    }
  }

  //  def main(args: Array[String]) {
  //    val conf = new SparkConf()
  //      .setMaster("local[1]")
  //      .setAppName("coocaa-PlaysDataLoadJob")
  //    val sc = new SparkContext(conf)
  //    run(sc, "2016-12-01")
  //    sc.stop()
  //  }

  case class DataView(date: String, name: String, tpe: String, source: String, cnt: Long) extends Serializable

  def run(sc: SparkContext, analysisDate: String) = {

    val resultArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //统计日志文件信息

    //KONKA
    val fstatsKONKA = HdfsUtils.getFS().listStatus(new Path("/user/hdfs/rsync/KONKA/" + analysisDate))
    for (fstat: FileStatus <- fstatsKONKA) {
      val path = fstat.getPath().toString
      val date = analysisDate
      val name = path.substring(path.toString.lastIndexOf("/") + 1)
      val tpe = "log"
      val source = "KONKA"

      val cnt = sc.textFile(path).count()
      //形成统计结果
      val dateView = DataView(date, name, tpe, source, cnt)
      resultArrayBuffer += dateView
    }


    //CH
    val fstatsCH = HdfsUtils.getFS().listStatus(new Path("/user/hdfs/rsync/CH/" + analysisDate))
    for (fstat: FileStatus <- fstatsCH) {
      val path = fstat.getPath().toString
      val date = analysisDate
      val name = path.substring(path.toString.lastIndexOf("/") + 1)
      val tpe = "log"
      val source = "CH"

      val cnt = sc.textFile(path).count()

      //形成统计结果
      val dateView = DataView(date, name, tpe, source, cnt)

      resultArrayBuffer += dateView

    }


    //COOCAA
    val fstatsCOOCAA = HdfsUtils.getFS().listStatus(new Path("/user/hdfs/rsync/COOCAA/" + TimeUtils.dateStrPatternOne2Two(analysisDate)))
    for (fstat: FileStatus <- fstatsCOOCAA) {
      val path = fstat.getPath().toString
      val date = analysisDate
      val name = path.substring(path.toString.lastIndexOf("/") + 1)
      val tpe = "log"
      val source = "COOCAA"

      val cnt = sc.textFile(path).count()

      //形成统计结果
      if (!name.endsWith("md5")) {
        val dateView = DataView(date, name, tpe, source, cnt)
        resultArrayBuffer += dateView
      }


    }


    val sqlContext = new SQLContext(sc);
    import sqlContext.implicits._

    //将统计结果写入Mysql
    val df = sc.parallelize(resultArrayBuffer).toDF()

    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "vboxDB", "check_view", false)


  }


}
package com.avcdata.vbox.view

import java.io.Serializable

import com.avcdata.vbox.common.Helper
import com.avcdata.vbox.util.{JdbcUtils, TimeUtils, HdfsUtils}
import org.apache.hadoop.fs.{FileStatus, Path}
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext

/**
  * @author zhangyongtian
  * @define 数据监控任务 (统计日志文件信息)
  */
object LogViewJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    for (i <- 10 to 60) {
      println(i+"|")
    }
  }

  //  def main(args: Array[String]) {
  //    val conf = new SparkConf()
  //      .setMaster("local[1]")
  //      .setAppName("coocaa-PlaysDataLoadJob")
  //    val sc = new SparkContext(conf)
  //    run(sc, "2016-12-01")
  //    sc.stop()
  //  }

  case class DataView(date: String, name: String, tpe: String, source: String, cnt: Long) extends Serializable

  def run(sc: SparkContext, analysisDate: String) = {

    val resultArrayBuffer = collection.mutable.ArrayBuffer[DataView]()

    //统计日志文件信息

    //KONKA
    val fstatsKONKA = HdfsUtils.getFS().listStatus(new Path("/user/hdfs/rsync/KONKA/" + analysisDate))
    for (fstat: FileStatus <- fstatsKONKA) {
      val path = fstat.getPath().toString
      val date = analysisDate
      val name = path.substring(path.toString.lastIndexOf("/") + 1)
      val tpe = "log"
      val source = "KONKA"

      val cnt = sc.textFile(path).count()
      //形成统计结果
      val dateView = DataView(date, name, tpe, source, cnt)
      resultArrayBuffer += dateView
    }


    //CH
    val fstatsCH = HdfsUtils.getFS().listStatus(new Path("/user/hdfs/rsync/CH/" + analysisDate))
    for (fstat: FileStatus <- fstatsCH) {
      val path = fstat.getPath().toString
      val date = analysisDate
      val name = path.substring(path.toString.lastIndexOf("/") + 1)
      val tpe = "log"
      val source = "CH"

      val cnt = sc.textFile(path).count()

      //形成统计结果
      val dateView = DataView(date, name, tpe, source, cnt)

      resultArrayBuffer += dateView

    }


    //COOCAA
    val fstatsCOOCAA = HdfsUtils.getFS().listStatus(new Path("/user/hdfs/rsync/COOCAA/" + TimeUtils.dateStrPatternOne2Two(analysisDate)))
    for (fstat: FileStatus <- fstatsCOOCAA) {
      val path = fstat.getPath().toString
      val date = analysisDate
      val name = path.substring(path.toString.lastIndexOf("/") + 1)
      val tpe = "log"
      val source = "COOCAA"

      val cnt = sc.textFile(path).count()

      //形成统计结果
      if (!name.endsWith("md5")) {
        val dateView = DataView(date, name, tpe, source, cnt)
        resultArrayBuffer += dateView
      }


    }


    val sqlContext = new SQLContext(sc);
    import sqlContext.implicits._

    //将统计结果写入Mysql
    val df = sc.parallelize(resultArrayBuffer).toDF()

//    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "vboxDB", "check_view", false)


  }


}
package com.avcdata.spark.job.until

import scala.collection.mutable

/**
  * @author zhangyongtian
  * @define 映射工具类
  */
object MapingUtils {

  val provinceMap = Map(
    ("北京市" -> "华北"),
    ("天津市" -> "华北"),
    ("上海市" -> "华东"),
    ("重庆市" -> "西南"),
    ("香港特别行政区" -> "港澳台"),
    ("河北省" -> "华北"),
    ("山西省" -> "华北"),
    ("内蒙古自治区" -> "华北"),
    ("辽宁省" -> "华北"),
    ("吉林省" -> "东北"),
    ("黑龙江省" -> "东北"),
    ("江苏省" -> "华东"),
    ("浙江省" -> "华东"),
    ("安徽省" -> "华东"),
    ("福建省" -> "华东"),
    ("江西省" -> "华东"),
    ("山东省" -> "华东"),
    ("河南省" -> "华中"),
    ("湖北省" -> "华中"),
    ("湖南省" -> "华中"),
    ("广东省" -> "华南"),
    ("广西壮族自治区" -> "华南"),
    ("海南省" -> "西南"),
    ("四川省" -> "西南"),
    ("贵州省" -> "西南"),
    ("云南省" -> "西南"),
    ("西藏自治区" -> "西南"),
    ("陕西省" -> "西北"),
    ("甘肃省" -> "西北"),
    ("青海省" -> "西北"),
    ("宁夏回族自治区" -> "西北"),
    ("新疆维吾尔自治区" -> "西北"),
    ("台湾省" -> "港澳台")
  )

  def getArea(province: String): String = {
    return provinceMap.get(province).getOrElse("其他")
  }

  val cityLevel = Map(
    ("北京市" -> "特级城市"),
    ("上海市" -> "特级城市"),
    ("广州市" -> "特级城市"),
    ("深圳市" -> "特级城市"),
    ("成都市" -> "一线城市"),
    ("杭州市" -> "一线城市"),
    ("武汉市" -> "一线城市"),
    ("天津市" -> "一线城市"),
    ("南京市" -> "一线城市"),
    ("重庆市" -> "一线城市"),
    ("西安市" -> "一线城市"),
    ("长沙市" -> "一线城市"),
    ("青岛市" -> "一线城市"),
    ("沈阳市" -> "一线城市"),
    ("大连市" -> "一线城市"),
    ("厦门市" -> "一线城市"),
    ("苏州市" -> "一线城市"),
    ("宁波市" -> "一线城市"),
    ("无锡市" -> "一线城市"),
    ("福州市" -> "二线城市"),
    ("合肥市" -> "二线城市"),
    ("郑州市" -> "二线城市"),
    ("哈尔滨市" -> "二线城市"),
    ("佛山市" -> "二线城市"),
    ("济南市" -> "二线城市"),
    ("东莞市" -> "二线城市"),
    ("昆明市" -> "二线城市"),
    ("太原市" -> "二线城市"),
    ("南昌市" -> "二线城市"),
    ("南宁市" -> "二线城市"),
    ("温州市" -> "二线城市"),
    ("石家庄市" -> "二线城市"),
    ("长春市" -> "二线城市"),
    ("泉州市" -> "二线城市"),
    ("贵阳市" -> "二线城市"),
    ("常州市" -> "二线城市"),
    ("珠海市" -> "二线城市"),
    ("金华市" -> "二线城市"),
    ("烟台市" -> "二线城市"),
    ("海口市" -> "二线城市"),
    ("惠州市" -> "二线城市"),
    ("乌鲁木齐市" -> "二线城市"),
    ("徐州市" -> "二线城市"),
    ("嘉兴市" -> "二线城市"),
    ("潍坊市" -> "二线城市"),
    ("洛阳市" -> "二线城市"),
    ("南通市" -> "二线城市"),
    ("扬州市" -> "二线城市"),
    ("汕头市" -> "二线城市"),
    ("兰州市" -> "三线城市"),
    ("桂林市" -> "三线城市"),
    ("三亚市" -> "三线城市"),
    ("呼和浩特市" -> "三线城市"),
    ("绍兴市" -> "三线城市"),
    ("泰州市" -> "三线城市"),
    ("银川市" -> "三线城市"),
    ("中山市" -> "三线城市"),
    ("保定市" -> "三线城市"),
    ("西宁市" -> "三线城市"),
    ("芜湖市" -> "三线城市"),
    ("赣州市" -> "三线城市"),
    ("绵阳市" -> "三线城市"),
    ("漳州市" -> "三线城市"),
    ("莆田市" -> "三线城市"),
    ("威海市" -> "三线城市"),
    ("邯郸市" -> "三线城市"),
    ("临沂市" -> "三线城市"),
    ("唐山市" -> "三线城市"),
    ("台州市" -> "三线城市"),
    ("宜昌市" -> "三线城市"),
    ("湖州市" -> "三线城市"),
    ("包头市" -> "三线城市"),
    ("济宁市" -> "三线城市"),
    ("盐城市" -> "三线城市"),
    ("鞍山市" -> "三线城市"),
    ("廊坊市" -> "三线城市"),
    ("衡阳市" -> "三线城市"),
    ("秦皇岛市" -> "三线城市"),
    ("吉林市" -> "三线城市"),
    ("大庆市" -> "三线城市"),
    ("淮安市" -> "三线城市"),
    ("丽江市" -> "三线城市"),
    ("揭阳市" -> "三线城市"),
    ("荆州市" -> "三线城市"),
    ("连云港市" -> "三线城市"),
    ("张家口市" -> "三线城市"),
    ("遵义市" -> "三线城市"),
    ("上饶市" -> "三线城市"),
    ("龙岩市" -> "三线城市"),
    ("衢州市" -> "三线城市"),
    ("赤峰市" -> "三线城市"),
    ("湛江市" -> "三线城市"),
    ("运城市" -> "三线城市"),
    ("鄂尔多斯市" -> "三线城市"),
    ("岳阳市" -> "三线城市"),
    ("安阳市" -> "三线城市"),
    ("株洲市" -> "三线城市"),
    ("镇江市" -> "三线城市"),
    ("淄博市" -> "三线城市"),
    ("郴州市" -> "三线城市"),
    ("南平市" -> "三线城市"),
    ("齐齐哈尔市" -> "三线城市"),
    ("常德市" -> "三线城市"),
    ("柳州市" -> "三线城市"),
    ("咸阳市" -> "三线城市"),
    ("南充市" -> "三线城市"),
    ("泸州市" -> "三线城市"),
    ("蚌埠市" -> "三线城市"),
    ("邢台市" -> "三线城市"),
    ("舟山市" -> "三线城市"),
    ("宝鸡市" -> "三线城市"),
    ("常德市" -> "三线城市"),
    ("抚顺市" -> "三线城市"),
    ("宜宾市" -> "三线城市"),
    ("宜春市" -> "三线城市"),
    ("怀化市" -> "三线城市"),
    ("榆林市" -> "三线城市"),
    ("梅州市" -> "三线城市"),
    ("呼伦贝尔市" -> "三线城市")
  )

  /**
    * 根据城市获取城市级别
    *
    * @param code
    * @return
    */
  def getCl(code: String): String = {
    cityLevel.get(code).getOrElse("其他")
  }


  val epgChannelMapping = mutable.Map(
    ("CCTV-1" -> "CCTV-1"),
    ("cctv-1" -> "CCTV-1"),
    ("CCTV-1 综合" -> "CCTV-1"),
    ("cctv-1 综合" -> "CCTV-1"),
    ("CCTV-2" -> "CCTV-2"),
    ("cctv-2" -> "CCTV-2"),
    ("CCTV-3" -> "CCTV-3"),
    ("cctv-3" -> "CCTV-3"),
    ("CCTV-4" -> "CCTV-4"),
    ("cctv-4" -> "CCTV-4"),
    ("CCTV-5" -> "CCTV-5"),
    ("cctv-5" -> "CCTV-5"),
    ("CCTV-5+" -> "CCTV-5+"),
    ("cctv-5+" -> "CCTV-5+"),
    ("CCTV-6" -> "CCTV-6"),
    ("cctv-6" -> "CCTV-6"),
    ("CCTV-7" -> "CCTV-7"),
    ("cctv-7" -> "CCTV-7"),
    ("CCTV-8" -> "CCTV-8"),
    ("cctv-8" -> "CCTV-8"),
    ("CCTV-纪录" -> "CCTV-9"),
    ("cctv-纪录" -> "CCTV-9"),
    ("CCTV-9" -> "CCTV-9"),
    ("cctv-9" -> "CCTV-9"),
    ("CCTV-10" -> "CCTV-10"),
    ("cctv-10" -> "CCTV-10"),
    ("CCTV-11" -> "CCTV-11"),
    ("cctv-11" -> "CCTV-11"),
    ("CCTV-12" -> "CCTV-12"),
    ("cctv-12" -> "CCTV-12"),
    ("CCTV-13" -> "CCTV-13"),
    ("cctv-13" -> "CCTV-13"),
    ("CCTV-新闻" -> "CCTV-13"),
    ("cctv-新闻" -> "CCTV-13"),
    ("CCTV-少儿" -> "CCTV-14"),
    ("cctv-少儿" -> "CCTV-14"),
    ("CCTV-14" -> "CCTV-14"),
    ("cctv-14" -> "CCTV-14"),
    ("CCTV-15" -> "CCTV-15"),
    ("cctv-15" -> "CCTV-15"),
    ("CCTV-音乐" -> "CCTV-15"),
    ("cctv-音乐" -> "CCTV-15"),
    ("黑龙江卫视" -> "黑龙江卫视"),
    ("北京卫视" -> "北京卫视"),
    ("贵州卫视" -> "贵州卫视"),
    ("东方卫视" -> "上海东方卫视"),
    ("上海卫视" -> "上海东方卫视"),
    ("上海东方卫视" -> "上海东方卫视"),
    ("湖北卫视" -> "湖北卫视"),
    ("东南卫视" -> "东南卫视"),
    ("广东卫视" -> "广东卫视"),
    ("湖南卫视" -> "湖南卫视"),
    ("山东卫视" -> "山东卫视"),
    ("BTV北京卫视" -> "北京卫视"),
    ("青海卫视" -> "青海卫视"),
    ("旅游卫视" -> "旅游卫视"),
    ("甘肃卫视" -> "甘肃卫视"),
    ("重庆卫视" -> "重庆卫视"),
    ("新疆卫视" -> "新疆卫视"),
    ("厦门卫视" -> "厦门卫视"),
    ("辽宁卫视" -> "辽宁卫视"),
    ("山西卫视" -> "山西卫视"),
    ("宁夏卫视" -> "宁夏卫视"),
    ("安徽卫视" -> "安徽卫视"),
    ("河北卫视" -> "河北卫视"),
    ("浙江卫视" -> "浙江卫视"),
    ("江西卫视" -> "江西卫视"),
    ("河南卫视" -> "河南卫视"),
    ("广西卫视" -> "广西卫视"),
    ("江苏卫视" -> "江苏卫视"),
    ("四川卫视" -> "四川卫视"),
    ("云南卫视" -> "云南卫视"),
    ("内蒙古卫视" -> "内蒙古卫视"),
    ("西藏卫视" -> "西藏卫视"),
    ("深圳卫视" -> "深圳卫视"),
    ("天津卫视" -> "天津卫视"),
    ("陕西卫视" -> "陕西卫视"),
    ("吉林卫视" -> "吉林卫视")
  )
//  val epgChannelMapping = mutable.Map(
//    ("CCTV-1" -> "CCTV-1 综合"),
//    ("CCTV-1 综合" -> "CCTV-1 综合"),
//    ("CCTV-2" -> "CCTV-2 财经"),
//    ("CCTV-3" -> "CCTV-3 综艺"),
//    ("CCTV-4" -> "CCTV-4 中文国际"),
//    ("CCTV-8" -> "CCTV-8 电视剧"),
//    ("CCTV-纪录" -> "CCTV-9 纪录"),
//    ("CCTV-9" -> "CCTV-9 纪录"),
//    ("CCTV-5" -> "CCTV-5 体育"),
//    ("CCTV-6" -> "CCTV-6 电影"),
//    ("CCTV-7" -> "CCTV-7 军事·农业"),
//    ("CCTV-10" -> "CCTV-10 科教"),
//    ("CCTV-11" -> "CCTV-11 戏曲"),
//    ("CCTV-12" -> "CCTV-12 社会与法"),
//    ("CCTV-13" -> "CCTV-13 新闻"),
//    ("CCTV-新闻" -> "CCTV-13 新闻"),
//    ("CCTV-少儿" -> "CCTV-14 少儿"),
//    ("CCTV-15" -> "CCTV-15 音乐"),
//    ("CCTV-音乐" -> "CCTV-15 音乐"),
//    ("黑龙江卫视" -> "黑龙江卫视"),
//    ("贵州卫视" -> "贵州卫视"),
//    ("东方卫视" -> "上海东方卫视"),
//    ("湖北卫视" -> "湖北卫视"),
//    ("东南卫视" -> "东南卫视"),
//    ("广东卫视" -> "广东卫视"),
//    ("湖南卫视" -> "湖南卫视"),
//    ("山东卫视" -> "山东卫视"),
//    ("BTV北京卫视" -> "北京卫视"),
//    ("青海卫视" -> "青海卫视"),
//    ("旅游卫视" -> "旅游卫视"),
//    ("甘肃卫视" -> "甘肃卫视"),
//    ("重庆卫视" -> "重庆卫视"),
//    ("新疆卫视" -> "新疆卫视"),
//    ("厦门卫视" -> "厦门卫视"),
//    ("辽宁卫视" -> "辽宁卫视"),
//    ("山西卫视" -> "山西卫视"),
//    ("宁夏卫视" -> "宁夏卫视"),
//    ("安徽卫视" -> "安徽卫视"),
//    ("河北卫视" -> "河北卫视"),
//    ("浙江卫视" -> "浙江卫视"),
//    ("江西卫视" -> "江西卫视"),
//    ("河南卫视" -> "河南电视台卫星频道"),
//    ("广西卫视" -> "广西电视台卫星频道"),
//    ("江苏卫视" -> "江苏卫视"),
//    ("四川卫视" -> "四川卫视"),
//    ("云南卫视" -> "云南广播电视台卫视频道"),
//    ("内蒙古卫视" -> "内蒙古卫视"),
//    ("西藏卫视" -> "西藏二套"),
//    ("深圳卫视" -> "深圳卫视"),
//    ("天津卫视" -> "天津卫视"),
//    ("陕西卫视" -> "陕西卫视"),
//    ("吉林卫视" -> "吉林卫视")
//  )

  def epgChannel2Stand(name: String): String = {
    epgChannelMapping.get(name.trim).getOrElse("其他")
  }


}
package com.avcdata.spark.job.until

import scala.collection.mutable

/**
  * @author zhangyongtian
  * @define 映射工具类
  */
object MapingUtils {

  val provinceMap = Map(
    ("北京市" -> "华北"),
    ("天津市" -> "华北"),
    ("上海市" -> "华东"),
    ("重庆市" -> "西南"),
    ("香港特别行政区" -> "港澳台"),
    ("河北省" -> "华北"),
    ("山西省" -> "华北"),
    ("内蒙古自治区" -> "华北"),
    ("辽宁省" -> "华北"),
    ("吉林省" -> "东北"),
    ("黑龙江省" -> "东北"),
    ("江苏省" -> "华东"),
    ("浙江省" -> "华东"),
    ("安徽省" -> "华东"),
    ("福建省" -> "华东"),
    ("江西省" -> "华东"),
    ("山东省" -> "华东"),
    ("河南省" -> "华中"),
    ("湖北省" -> "华中"),
    ("湖南省" -> "华中"),
    ("广东省" -> "华南"),
    ("广西壮族自治区" -> "华南"),
    ("海南省" -> "西南"),
    ("四川省" -> "西南"),
    ("贵州省" -> "西南"),
    ("云南省" -> "西南"),
    ("西藏自治区" -> "西南"),
    ("陕西省" -> "西北"),
    ("甘肃省" -> "西北"),
    ("青海省" -> "西北"),
    ("宁夏回族自治区" -> "西北"),
    ("新疆维吾尔自治区" -> "西北"),
    ("台湾省" -> "港澳台")
  )

  def getArea(province: String): String = {
    return provinceMap.get(province).getOrElse("其他")
  }

  val cityLevel = Map(
    ("北京市" -> "特级城市"),
    ("上海市" -> "特级城市"),
    ("广州市" -> "特级城市"),
    ("深圳市" -> "特级城市"),
    ("成都市" -> "一线城市"),
    ("杭州市" -> "一线城市"),
    ("武汉市" -> "一线城市"),
    ("天津市" -> "一线城市"),
    ("南京市" -> "一线城市"),
    ("重庆市" -> "一线城市"),
    ("西安市" -> "一线城市"),
    ("长沙市" -> "一线城市"),
    ("青岛市" -> "一线城市"),
    ("沈阳市" -> "一线城市"),
    ("大连市" -> "一线城市"),
    ("厦门市" -> "一线城市"),
    ("苏州市" -> "一线城市"),
    ("宁波市" -> "一线城市"),
    ("无锡市" -> "一线城市"),
    ("福州市" -> "二线城市"),
    ("合肥市" -> "二线城市"),
    ("郑州市" -> "二线城市"),
    ("哈尔滨市" -> "二线城市"),
    ("佛山市" -> "二线城市"),
    ("济南市" -> "二线城市"),
    ("东莞市" -> "二线城市"),
    ("昆明市" -> "二线城市"),
    ("太原市" -> "二线城市"),
    ("南昌市" -> "二线城市"),
    ("南宁市" -> "二线城市"),
    ("温州市" -> "二线城市"),
    ("石家庄市" -> "二线城市"),
    ("长春市" -> "二线城市"),
    ("泉州市" -> "二线城市"),
    ("贵阳市" -> "二线城市"),
    ("常州市" -> "二线城市"),
    ("珠海市" -> "二线城市"),
    ("金华市" -> "二线城市"),
    ("烟台市" -> "二线城市"),
    ("海口市" -> "二线城市"),
    ("惠州市" -> "二线城市"),
    ("乌鲁木齐市" -> "二线城市"),
    ("徐州市" -> "二线城市"),
    ("嘉兴市" -> "二线城市"),
    ("潍坊市" -> "二线城市"),
    ("洛阳市" -> "二线城市"),
    ("南通市" -> "二线城市"),
    ("扬州市" -> "二线城市"),
    ("汕头市" -> "二线城市"),
    ("兰州市" -> "三线城市"),
    ("桂林市" -> "三线城市"),
    ("三亚市" -> "三线城市"),
    ("呼和浩特市" -> "三线城市"),
    ("绍兴市" -> "三线城市"),
    ("泰州市" -> "三线城市"),
    ("银川市" -> "三线城市"),
    ("中山市" -> "三线城市"),
    ("保定市" -> "三线城市"),
    ("西宁市" -> "三线城市"),
    ("芜湖市" -> "三线城市"),
    ("赣州市" -> "三线城市"),
    ("绵阳市" -> "三线城市"),
    ("漳州市" -> "三线城市"),
    ("莆田市" -> "三线城市"),
    ("威海市" -> "三线城市"),
    ("邯郸市" -> "三线城市"),
    ("临沂市" -> "三线城市"),
    ("唐山市" -> "三线城市"),
    ("台州市" -> "三线城市"),
    ("宜昌市" -> "三线城市"),
    ("湖州市" -> "三线城市"),
    ("包头市" -> "三线城市"),
    ("济宁市" -> "三线城市"),
    ("盐城市" -> "三线城市"),
    ("鞍山市" -> "三线城市"),
    ("廊坊市" -> "三线城市"),
    ("衡阳市" -> "三线城市"),
    ("秦皇岛市" -> "三线城市"),
    ("吉林市" -> "三线城市"),
    ("大庆市" -> "三线城市"),
    ("淮安市" -> "三线城市"),
    ("丽江市" -> "三线城市"),
    ("揭阳市" -> "三线城市"),
    ("荆州市" -> "三线城市"),
    ("连云港市" -> "三线城市"),
    ("张家口市" -> "三线城市"),
    ("遵义市" -> "三线城市"),
    ("上饶市" -> "三线城市"),
    ("龙岩市" -> "三线城市"),
    ("衢州市" -> "三线城市"),
    ("赤峰市" -> "三线城市"),
    ("湛江市" -> "三线城市"),
    ("运城市" -> "三线城市"),
    ("鄂尔多斯市" -> "三线城市"),
    ("岳阳市" -> "三线城市"),
    ("安阳市" -> "三线城市"),
    ("株洲市" -> "三线城市"),
    ("镇江市" -> "三线城市"),
    ("淄博市" -> "三线城市"),
    ("郴州市" -> "三线城市"),
    ("南平市" -> "三线城市"),
    ("齐齐哈尔市" -> "三线城市"),
    ("常德市" -> "三线城市"),
    ("柳州市" -> "三线城市"),
    ("咸阳市" -> "三线城市"),
    ("南充市" -> "三线城市"),
    ("泸州市" -> "三线城市"),
    ("蚌埠市" -> "三线城市"),
    ("邢台市" -> "三线城市"),
    ("舟山市" -> "三线城市"),
    ("宝鸡市" -> "三线城市"),
    ("常德市" -> "三线城市"),
    ("抚顺市" -> "三线城市"),
    ("宜宾市" -> "三线城市"),
    ("宜春市" -> "三线城市"),
    ("怀化市" -> "三线城市"),
    ("榆林市" -> "三线城市"),
    ("梅州市" -> "三线城市"),
    ("呼伦贝尔市" -> "三线城市")
  )

  /**
    * 根据城市获取城市级别
    *
    * @param code
    * @return
    */
  def getCl(code: String): String = {
    cityLevel.get(code).getOrElse("其他")
  }


  val epgChannelMapping = mutable.Map(
    ("CCTV-1" -> "CCTV-1"),
    ("cctv-1" -> "CCTV-1"),
    ("CCTV-1 综合" -> "CCTV-1"),
    ("cctv-1 综合" -> "CCTV-1"),
    ("CCTV-2" -> "CCTV-2"),
    ("cctv-2" -> "CCTV-2"),
    ("CCTV-3" -> "CCTV-3"),
    ("cctv-3" -> "CCTV-3"),
    ("CCTV-4" -> "CCTV-4"),
    ("cctv-4" -> "CCTV-4"),
    ("CCTV-5" -> "CCTV-5"),
    ("cctv-5" -> "CCTV-5"),
    ("CCTV-5+" -> "CCTV-5+"),
    ("cctv-5+" -> "CCTV-5+"),
    ("CCTV-6" -> "CCTV-6"),
    ("cctv-6" -> "CCTV-6"),
    ("CCTV-7" -> "CCTV-7"),
    ("cctv-7" -> "CCTV-7"),
    ("CCTV-8" -> "CCTV-8"),
    ("cctv-8" -> "CCTV-8"),
    ("CCTV-纪录" -> "CCTV-9"),
    ("cctv-纪录" -> "CCTV-9"),
    ("CCTV-9" -> "CCTV-9"),
    ("cctv-9" -> "CCTV-9"),
    ("CCTV-10" -> "CCTV-10"),
    ("cctv-10" -> "CCTV-10"),
    ("CCTV-11" -> "CCTV-11"),
    ("cctv-11" -> "CCTV-11"),
    ("CCTV-12" -> "CCTV-12"),
    ("cctv-12" -> "CCTV-12"),
    ("CCTV-13" -> "CCTV-13"),
    ("cctv-13" -> "CCTV-13"),
    ("CCTV-新闻" -> "CCTV-13"),
    ("cctv-新闻" -> "CCTV-13"),
    ("CCTV-少儿" -> "CCTV-14"),
    ("cctv-少儿" -> "CCTV-14"),
    ("CCTV-14" -> "CCTV-14"),
    ("cctv-14" -> "CCTV-14"),
    ("CCTV-15" -> "CCTV-15"),
    ("cctv-15" -> "CCTV-15"),
    ("CCTV-音乐" -> "CCTV-15"),
    ("cctv-音乐" -> "CCTV-15"),
    ("黑龙江卫视" -> "黑龙江卫视"),
    ("北京卫视" -> "北京卫视"),
    ("贵州卫视" -> "贵州卫视"),
    ("东方卫视" -> "上海东方卫视"),
    ("上海卫视" -> "上海东方卫视"),
    ("上海东方卫视" -> "上海东方卫视"),
    ("湖北卫视" -> "湖北卫视"),
    ("东南卫视" -> "东南卫视"),
    ("广东卫视" -> "广东卫视"),
    ("湖南卫视" -> "湖南卫视"),
    ("山东卫视" -> "山东卫视"),
    ("BTV北京卫视" -> "北京卫视"),
    ("青海卫视" -> "青海卫视"),
    ("旅游卫视" -> "旅游卫视"),
    ("甘肃卫视" -> "甘肃卫视"),
    ("重庆卫视" -> "重庆卫视"),
    ("新疆卫视" -> "新疆卫视"),
    ("厦门卫视" -> "厦门卫视"),
    ("辽宁卫视" -> "辽宁卫视"),
    ("山西卫视" -> "山西卫视"),
    ("宁夏卫视" -> "宁夏卫视"),
    ("安徽卫视" -> "安徽卫视"),
    ("河北卫视" -> "河北卫视"),
    ("浙江卫视" -> "浙江卫视"),
    ("江西卫视" -> "江西卫视"),
    ("河南卫视" -> "河南卫视"),
    ("广西卫视" -> "广西卫视"),
    ("江苏卫视" -> "江苏卫视"),
    ("四川卫视" -> "四川卫视"),
    ("云南卫视" -> "云南卫视"),
    ("内蒙古卫视" -> "内蒙古卫视"),
    ("西藏卫视" -> "西藏卫视"),
    ("深圳卫视" -> "深圳卫视"),
    ("天津卫视" -> "天津卫视"),
    ("陕西卫视" -> "陕西卫视"),
    ("吉林卫视" -> "吉林卫视")
  )
//  val epgChannelMapping = mutable.Map(
//    ("CCTV-1" -> "CCTV-1 综合"),
//    ("CCTV-1 综合" -> "CCTV-1 综合"),
//    ("CCTV-2" -> "CCTV-2 财经"),
//    ("CCTV-3" -> "CCTV-3 综艺"),
//    ("CCTV-4" -> "CCTV-4 中文国际"),
//    ("CCTV-8" -> "CCTV-8 电视剧"),
//    ("CCTV-纪录" -> "CCTV-9 纪录"),
//    ("CCTV-9" -> "CCTV-9 纪录"),
//    ("CCTV-5" -> "CCTV-5 体育"),
//    ("CCTV-6" -> "CCTV-6 电影"),
//    ("CCTV-7" -> "CCTV-7 军事·农业"),
//    ("CCTV-10" -> "CCTV-10 科教"),
//    ("CCTV-11" -> "CCTV-11 戏曲"),
//    ("CCTV-12" -> "CCTV-12 社会与法"),
//    ("CCTV-13" -> "CCTV-13 新闻"),
//    ("CCTV-新闻" -> "CCTV-13 新闻"),
//    ("CCTV-少儿" -> "CCTV-14 少儿"),
//    ("CCTV-15" -> "CCTV-15 音乐"),
//    ("CCTV-音乐" -> "CCTV-15 音乐"),
//    ("黑龙江卫视" -> "黑龙江卫视"),
//    ("贵州卫视" -> "贵州卫视"),
//    ("东方卫视" -> "上海东方卫视"),
//    ("湖北卫视" -> "湖北卫视"),
//    ("东南卫视" -> "东南卫视"),
//    ("广东卫视" -> "广东卫视"),
//    ("湖南卫视" -> "湖南卫视"),
//    ("山东卫视" -> "山东卫视"),
//    ("BTV北京卫视" -> "北京卫视"),
//    ("青海卫视" -> "青海卫视"),
//    ("旅游卫视" -> "旅游卫视"),
//    ("甘肃卫视" -> "甘肃卫视"),
//    ("重庆卫视" -> "重庆卫视"),
//    ("新疆卫视" -> "新疆卫视"),
//    ("厦门卫视" -> "厦门卫视"),
//    ("辽宁卫视" -> "辽宁卫视"),
//    ("山西卫视" -> "山西卫视"),
//    ("宁夏卫视" -> "宁夏卫视"),
//    ("安徽卫视" -> "安徽卫视"),
//    ("河北卫视" -> "河北卫视"),
//    ("浙江卫视" -> "浙江卫视"),
//    ("江西卫视" -> "江西卫视"),
//    ("河南卫视" -> "河南电视台卫星频道"),
//    ("广西卫视" -> "广西电视台卫星频道"),
//    ("江苏卫视" -> "江苏卫视"),
//    ("四川卫视" -> "四川卫视"),
//    ("云南卫视" -> "云南广播电视台卫视频道"),
//    ("内蒙古卫视" -> "内蒙古卫视"),
//    ("西藏卫视" -> "西藏二套"),
//    ("深圳卫视" -> "深圳卫视"),
//    ("天津卫视" -> "天津卫视"),
//    ("陕西卫视" -> "陕西卫视"),
//    ("吉林卫视" -> "吉林卫视")
//  )

  def epgChannel2Stand(name: String): String = {
    epgChannelMapping.get(name.trim).getOrElse("其他")
  }


}
package com.avcdata.vbox.util

import scala.collection.mutable

/**
  * @author zhangyongtian
  * @define 映射工具类
  */
object MapingUtils {

  val provinceMap = Map(
    ("北京市" -> "华北"),
    ("天津市" -> "华北"),
    ("上海市" -> "华东"),
    ("重庆市" -> "西南"),
    ("香港特别行政区" -> "港澳台"),
    ("河北省" -> "华北"),
    ("山西省" -> "华北"),
    ("内蒙古自治区" -> "华北"),
    ("辽宁省" -> "华北"),
    ("吉林省" -> "东北"),
    ("黑龙江省" -> "东北"),
    ("江苏省" -> "华东"),
    ("浙江省" -> "华东"),
    ("安徽省" -> "华东"),
    ("福建省" -> "华东"),
    ("江西省" -> "华东"),
    ("山东省" -> "华东"),
    ("河南省" -> "华中"),
    ("湖北省" -> "华中"),
    ("湖南省" -> "华中"),
    ("广东省" -> "华南"),
    ("广西壮族自治区" -> "华南"),
    ("海南省" -> "西南"),
    ("四川省" -> "西南"),
    ("贵州省" -> "西南"),
    ("云南省" -> "西南"),
    ("西藏自治区" -> "西南"),
    ("陕西省" -> "西北"),
    ("甘肃省" -> "西北"),
    ("青海省" -> "西北"),
    ("宁夏回族自治区" -> "西北"),
    ("新疆维吾尔自治区" -> "西北"),
    ("台湾省" -> "港澳台")
  )

  def getArea(province: String): String = {
    return provinceMap.get(province).getOrElse("其他")
  }

  val cityLevel = Map(
    ("北京市" -> "特级城市"),
    ("上海市" -> "特级城市"),
    ("广州市" -> "特级城市"),
    ("深圳市" -> "特级城市"),
    ("成都市" -> "一线城市"),
    ("杭州市" -> "一线城市"),
    ("武汉市" -> "一线城市"),
    ("天津市" -> "一线城市"),
    ("南京市" -> "一线城市"),
    ("重庆市" -> "一线城市"),
    ("西安市" -> "一线城市"),
    ("长沙市" -> "一线城市"),
    ("青岛市" -> "一线城市"),
    ("沈阳市" -> "一线城市"),
    ("大连市" -> "一线城市"),
    ("厦门市" -> "一线城市"),
    ("苏州市" -> "一线城市"),
    ("宁波市" -> "一线城市"),
    ("无锡市" -> "一线城市"),
    ("福州市" -> "二线城市"),
    ("合肥市" -> "二线城市"),
    ("郑州市" -> "二线城市"),
    ("哈尔滨市" -> "二线城市"),
    ("佛山市" -> "二线城市"),
    ("济南市" -> "二线城市"),
    ("东莞市" -> "二线城市"),
    ("昆明市" -> "二线城市"),
    ("太原市" -> "二线城市"),
    ("南昌市" -> "二线城市"),
    ("南宁市" -> "二线城市"),
    ("温州市" -> "二线城市"),
    ("石家庄市" -> "二线城市"),
    ("长春市" -> "二线城市"),
    ("泉州市" -> "二线城市"),
    ("贵阳市" -> "二线城市"),
    ("常州市" -> "二线城市"),
    ("珠海市" -> "二线城市"),
    ("金华市" -> "二线城市"),
    ("烟台市" -> "二线城市"),
    ("海口市" -> "二线城市"),
    ("惠州市" -> "二线城市"),
    ("乌鲁木齐市" -> "二线城市"),
    ("徐州市" -> "二线城市"),
    ("嘉兴市" -> "二线城市"),
    ("潍坊市" -> "二线城市"),
    ("洛阳市" -> "二线城市"),
    ("南通市" -> "二线城市"),
    ("扬州市" -> "二线城市"),
    ("汕头市" -> "二线城市"),
    ("兰州市" -> "三线城市"),
    ("桂林市" -> "三线城市"),
    ("三亚市" -> "三线城市"),
    ("呼和浩特市" -> "三线城市"),
    ("绍兴市" -> "三线城市"),
    ("泰州市" -> "三线城市"),
    ("银川市" -> "三线城市"),
    ("中山市" -> "三线城市"),
    ("保定市" -> "三线城市"),
    ("西宁市" -> "三线城市"),
    ("芜湖市" -> "三线城市"),
    ("赣州市" -> "三线城市"),
    ("绵阳市" -> "三线城市"),
    ("漳州市" -> "三线城市"),
    ("莆田市" -> "三线城市"),
    ("威海市" -> "三线城市"),
    ("邯郸市" -> "三线城市"),
    ("临沂市" -> "三线城市"),
    ("唐山市" -> "三线城市"),
    ("台州市" -> "三线城市"),
    ("宜昌市" -> "三线城市"),
    ("湖州市" -> "三线城市"),
    ("包头市" -> "三线城市"),
    ("济宁市" -> "三线城市"),
    ("盐城市" -> "三线城市"),
    ("鞍山市" -> "三线城市"),
    ("廊坊市" -> "三线城市"),
    ("衡阳市" -> "三线城市"),
    ("秦皇岛市" -> "三线城市"),
    ("吉林市" -> "三线城市"),
    ("大庆市" -> "三线城市"),
    ("淮安市" -> "三线城市"),
    ("丽江市" -> "三线城市"),
    ("揭阳市" -> "三线城市"),
    ("荆州市" -> "三线城市"),
    ("连云港市" -> "三线城市"),
    ("张家口市" -> "三线城市"),
    ("遵义市" -> "三线城市"),
    ("上饶市" -> "三线城市"),
    ("龙岩市" -> "三线城市"),
    ("衢州市" -> "三线城市"),
    ("赤峰市" -> "三线城市"),
    ("湛江市" -> "三线城市"),
    ("运城市" -> "三线城市"),
    ("鄂尔多斯市" -> "三线城市"),
    ("岳阳市" -> "三线城市"),
    ("安阳市" -> "三线城市"),
    ("株洲市" -> "三线城市"),
    ("镇江市" -> "三线城市"),
    ("淄博市" -> "三线城市"),
    ("郴州市" -> "三线城市"),
    ("南平市" -> "三线城市"),
    ("齐齐哈尔市" -> "三线城市"),
    ("常德市" -> "三线城市"),
    ("柳州市" -> "三线城市"),
    ("咸阳市" -> "三线城市"),
    ("南充市" -> "三线城市"),
    ("泸州市" -> "三线城市"),
    ("蚌埠市" -> "三线城市"),
    ("邢台市" -> "三线城市"),
    ("舟山市" -> "三线城市"),
    ("宝鸡市" -> "三线城市"),
    ("常德市" -> "三线城市"),
    ("抚顺市" -> "三线城市"),
    ("宜宾市" -> "三线城市"),
    ("宜春市" -> "三线城市"),
    ("怀化市" -> "三线城市"),
    ("榆林市" -> "三线城市"),
    ("梅州市" -> "三线城市"),
    ("呼伦贝尔市" -> "三线城市")
  )

  /**
    * 根据城市获取城市级别
    *
    * @param code
    * @return
    */
  def getCl(code: String): String = {
    cityLevel.get(code).getOrElse("其他")
  }


  val epgChannelMapping = mutable.Map(
    ("CCTV-1" -> "CCTV-1"),
    ("cctv-1" -> "CCTV-1"),
    ("CCTV-1 综合" -> "CCTV-1"),
    ("cctv-1 综合" -> "CCTV-1"),
    ("CCTV-2" -> "CCTV-2"),
    ("cctv-2" -> "CCTV-2"),
    ("CCTV-3" -> "CCTV-3"),
    ("cctv-3" -> "CCTV-3"),
    ("CCTV-4" -> "CCTV-4"),
    ("cctv-4" -> "CCTV-4"),
    ("CCTV-5" -> "CCTV-5"),
    ("cctv-5" -> "CCTV-5"),
    ("CCTV-5+" -> "CCTV-5+"),
    ("cctv-5+" -> "CCTV-5+"),
    ("CCTV-6" -> "CCTV-6"),
    ("cctv-6" -> "CCTV-6"),
    ("CCTV-7" -> "CCTV-7"),
    ("cctv-7" -> "CCTV-7"),
    ("CCTV-8" -> "CCTV-8"),
    ("cctv-8" -> "CCTV-8"),
    ("CCTV-纪录" -> "CCTV-9"),
    ("cctv-纪录" -> "CCTV-9"),
    ("CCTV-9" -> "CCTV-9"),
    ("cctv-9" -> "CCTV-9"),
    ("CCTV-10" -> "CCTV-10"),
    ("cctv-10" -> "CCTV-10"),
    ("CCTV-11" -> "CCTV-11"),
    ("cctv-11" -> "CCTV-11"),
    ("CCTV-12" -> "CCTV-12"),
    ("cctv-12" -> "CCTV-12"),
    ("CCTV-13" -> "CCTV-13"),
    ("cctv-13" -> "CCTV-13"),
    ("CCTV-新闻" -> "CCTV-13"),
    ("cctv-新闻" -> "CCTV-13"),
    ("CCTV-少儿" -> "CCTV-14"),
    ("cctv-少儿" -> "CCTV-14"),
    ("CCTV-14" -> "CCTV-14"),
    ("cctv-14" -> "CCTV-14"),
    ("CCTV-15" -> "CCTV-15"),
    ("cctv-15" -> "CCTV-15"),
    ("CCTV-音乐" -> "CCTV-15"),
    ("cctv-音乐" -> "CCTV-15"),
    ("黑龙江卫视" -> "黑龙江卫视"),
    ("北京卫视" -> "北京卫视"),
    ("贵州卫视" -> "贵州卫视"),
    ("东方卫视" -> "上海东方卫视"),
    ("上海卫视" -> "上海东方卫视"),
    ("上海东方卫视" -> "上海东方卫视"),
    ("湖北卫视" -> "湖北卫视"),
    ("东南卫视" -> "东南卫视"),
    ("广东卫视" -> "广东卫视"),
    ("湖南卫视" -> "湖南卫视"),
    ("山东卫视" -> "山东卫视"),
    ("BTV北京卫视" -> "北京卫视"),
    ("青海卫视" -> "青海卫视"),
    ("旅游卫视" -> "旅游卫视"),
    ("甘肃卫视" -> "甘肃卫视"),
    ("重庆卫视" -> "重庆卫视"),
    ("新疆卫视" -> "新疆卫视"),
    ("厦门卫视" -> "厦门卫视"),
    ("辽宁卫视" -> "辽宁卫视"),
    ("山西卫视" -> "山西卫视"),
    ("宁夏卫视" -> "宁夏卫视"),
    ("安徽卫视" -> "安徽卫视"),
    ("河北卫视" -> "河北卫视"),
    ("浙江卫视" -> "浙江卫视"),
    ("江西卫视" -> "江西卫视"),
    ("河南卫视" -> "河南卫视"),
    ("广西卫视" -> "广西卫视"),
    ("江苏卫视" -> "江苏卫视"),
    ("四川卫视" -> "四川卫视"),
    ("云南卫视" -> "云南卫视"),
    ("内蒙古卫视" -> "内蒙古卫视"),
    ("西藏卫视" -> "西藏卫视"),
    ("深圳卫视" -> "深圳卫视"),
    ("天津卫视" -> "天津卫视"),
    ("陕西卫视" -> "陕西卫视"),
    ("吉林卫视" -> "吉林卫视")
  )
//  val epgChannelMapping = mutable.Map(
//    ("CCTV-1" -> "CCTV-1 综合"),
//    ("CCTV-1 综合" -> "CCTV-1 综合"),
//    ("CCTV-2" -> "CCTV-2 财经"),
//    ("CCTV-3" -> "CCTV-3 综艺"),
//    ("CCTV-4" -> "CCTV-4 中文国际"),
//    ("CCTV-8" -> "CCTV-8 电视剧"),
//    ("CCTV-纪录" -> "CCTV-9 纪录"),
//    ("CCTV-9" -> "CCTV-9 纪录"),
//    ("CCTV-5" -> "CCTV-5 体育"),
//    ("CCTV-6" -> "CCTV-6 电影"),
//    ("CCTV-7" -> "CCTV-7 军事·农业"),
//    ("CCTV-10" -> "CCTV-10 科教"),
//    ("CCTV-11" -> "CCTV-11 戏曲"),
//    ("CCTV-12" -> "CCTV-12 社会与法"),
//    ("CCTV-13" -> "CCTV-13 新闻"),
//    ("CCTV-新闻" -> "CCTV-13 新闻"),
//    ("CCTV-少儿" -> "CCTV-14 少儿"),
//    ("CCTV-15" -> "CCTV-15 音乐"),
//    ("CCTV-音乐" -> "CCTV-15 音乐"),
//    ("黑龙江卫视" -> "黑龙江卫视"),
//    ("贵州卫视" -> "贵州卫视"),
//    ("东方卫视" -> "上海东方卫视"),
//    ("湖北卫视" -> "湖北卫视"),
//    ("东南卫视" -> "东南卫视"),
//    ("广东卫视" -> "广东卫视"),
//    ("湖南卫视" -> "湖南卫视"),
//    ("山东卫视" -> "山东卫视"),
//    ("BTV北京卫视" -> "北京卫视"),
//    ("青海卫视" -> "青海卫视"),
//    ("旅游卫视" -> "旅游卫视"),
//    ("甘肃卫视" -> "甘肃卫视"),
//    ("重庆卫视" -> "重庆卫视"),
//    ("新疆卫视" -> "新疆卫视"),
//    ("厦门卫视" -> "厦门卫视"),
//    ("辽宁卫视" -> "辽宁卫视"),
//    ("山西卫视" -> "山西卫视"),
//    ("宁夏卫视" -> "宁夏卫视"),
//    ("安徽卫视" -> "安徽卫视"),
//    ("河北卫视" -> "河北卫视"),
//    ("浙江卫视" -> "浙江卫视"),
//    ("江西卫视" -> "江西卫视"),
//    ("河南卫视" -> "河南电视台卫星频道"),
//    ("广西卫视" -> "广西电视台卫星频道"),
//    ("江苏卫视" -> "江苏卫视"),
//    ("四川卫视" -> "四川卫视"),
//    ("云南卫视" -> "云南广播电视台卫视频道"),
//    ("内蒙古卫视" -> "内蒙古卫视"),
//    ("西藏卫视" -> "西藏二套"),
//    ("深圳卫视" -> "深圳卫视"),
//    ("天津卫视" -> "天津卫视"),
//    ("陕西卫视" -> "陕西卫视"),
//    ("吉林卫视" -> "吉林卫视")
//  )

  def epgChannel2Stand(name: String): String = {
    epgChannelMapping.get(name.trim).getOrElse("其他")
  }


}
package com.avcdata.vbox.util

import java.sql.Connection
import com.mchange.v2.c3p0.ComboPooledDataSource

class MDBManager(isLocal: Boolean) extends Serializable {
  private val cpds: ComboPooledDataSource = new ComboPooledDataSource(true);
//  private val prop = new Properties()
//  private var in: InputStream = _
//  isLocal match {
//    case true => in = getClass().getResourceAsStream("/c3p0.properties");
//    case false => in = new FileInputStream(new File(SparkFiles.get("c3p0.properties")))
//  }
  try {
//    prop.load(in);
//    cpds.setJdbcUrl(prop.getProperty("jdbcUrl").toString());
//    cpds.setDriverClass(prop.getProperty("driverClass").toString());
//    cpds.setUser(prop.getProperty("user").toString());
//    cpds.setPassword(prop.getProperty("password").toString());
//    cpds.setMaxPoolSize(Integer.valueOf(prop.getProperty("maxPoolSize").toString()));
//    cpds.setMinPoolSize(Integer.valueOf(prop.getProperty("minPoolSize").toString()));
//    cpds.setAcquireIncrement(Integer.valueOf(prop.getProperty("acquireIncrement").toString()));
//    cpds.setInitialPoolSize(Integer.valueOf(prop.getProperty("initialPoolSize").toString()));
//    cpds.setMaxIdleTime(Integer.valueOf(prop.getProperty("maxIdleTime").toString()));
    cpds.setUser("root");
    cpds.setPassword("new.1234");
    cpds.setJdbcUrl("jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true&failOverReadOnly=false");
    cpds.setDriverClass("com.mysql.jdbc.Driver");
    cpds.setInitialPoolSize(10);
    cpds.setMinPoolSize(1);
    cpds.setMaxPoolSize(10);
    cpds.setMaxStatements(50);
    cpds.setMaxIdleTime(25000);
    cpds.setTestConnectionOnCheckin(true)
    cpds.setIdleConnectionTestPeriod(18000)
    cpds.setTestConnectionOnCheckout(true)


  } catch {
    case ex: Exception => ex.printStackTrace()
  }

  def getConnection: Connection = {
    try {
      return cpds.getConnection();
    } catch {
      case ex: Exception => ex.printStackTrace()
        null
    }
  }
}

object MDBManager {
  var mdbManager: MDBManager = _

  def getMDBManager(isLocal: Boolean): MDBManager = {
    synchronized {
      if (mdbManager == null) {
        mdbManager = new MDBManager(isLocal)
      }
    }
    mdbManager
  }
}
package com.avcdata.etl.export.pool.mongo

import com.mongodb.{MongoClient, MongoClientURI}
import org.apache.commons.pool2.impl.{DefaultPooledObject, GenericObjectPool}
import org.apache.commons.pool2.{BasePooledObjectFactory, ObjectPool, PooledObject}


/**
  * Mongo数据库连接池
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/7/16 08:18
  */
object MongoClientPool
{
  //客户端连接池
  private var clientPool: Map[String, ObjectPool[MongoClient]] = Map()

  sys.addShutdownHook
  {
    clientPool.values.foreach
    { pool => pool.close() }
  }

  //获取受管连接对象
  def apply(clientURI: String): ManagedConnection =
  {
    val pool = clientPool.getOrElse(clientURI,
      {
        MongoClientPool.synchronized[ObjectPool[MongoClient]]
          {
            clientPool.getOrElse(clientURI,
              {
                val p = new GenericObjectPool[MongoClient](new ConnectionFactory(clientURI))
                clientPool += clientURI -> p

                p
              })
          }
      })

    new ManagedConnection(pool, pool.borrowObject())
  }
}

/**
  * 受管连接对象
  *
  * @param pool   池对象
  * @param client 连接对象
  */
class ManagedConnection(private val pool: ObjectPool[MongoClient], val client: MongoClient)
{
  def close() = pool.returnObject(client)
}

/**
  * 连接池创建工场
  */
private class ConnectionFactory(clientURI: String)
  extends BasePooledObjectFactory[MongoClient]
{
  override def create() = new MongoClient(new MongoClientURI(clientURI))

  override def wrap(client: MongoClient) = new DefaultPooledObject[MongoClient](client)

  override def destroyObject(po: PooledObject[MongoClient]) = po.getObject.close()
}package com.avcdata.etl.export

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.export.pool.mongo.MongoClientPool
import com.mongodb.MongoClientURI
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import org.bson.Document
import org.slf4j.LoggerFactory

/**
  * Mongo的操作
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/22 10:03
  */
trait MongoOperation
{
  def deleteDataByKeys(df: DataFrame, mongoClientURI: String, collectionName: String, deleteKeys: Seq[String], parallelWrites: Int): Unit =
  {
    if (deleteKeys.nonEmpty)
    {
      LoggerFactory.getLogger(classOf[MongoOperation]).info(s"Mongo delete key(s) is <${deleteKeys.mkString(",")}>.")

      df.select(deleteKeys.map(col(_)): _ *).distinct().toJSON.map(Document.parse).coalesce(parallelWrites, shuffle = true).foreachPartition(docIt =>
      {
        LoanPattern.using(MongoClientPool(mongoClientURI))
        { clientWrapper =>

          //获取数据集名称
          val docCollection = clientWrapper.client.getDatabase(new MongoClientURI(mongoClientURI).getDatabase).getCollection(collectionName)

          //删除据
          //val bulkData = docIt.map(new DeleteManyModel(_)).toSeq
          //if (bulkData.nonEmpty) docCollection.bulkWrite(bulkData)
          docIt.foreach(deleteCon =>
          {
            val deleteResult = docCollection.deleteMany(deleteCon)

            LoggerFactory.getLogger(classOf[MongoOperation]).info(s"The delete condition josn is <${deleteCon.toJson}>. " +
              s"Delete ${deleteResult.getDeletedCount} docs, " +
              s"the acknowledge result is ${deleteResult.wasAcknowledged()}.")
          })
        }
      })
    }
  }
}
package com.avcdata.core;

import com.avcdata.config.ConfigInfo;
import lombok.extern.slf4j.Slf4j;
import org.apache.http.HttpResponse;
import org.apache.http.NameValuePair;
import org.apache.http.client.entity.UrlEncodedFormEntity;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.impl.client.HttpClientBuilder;
import org.apache.http.message.BasicNameValuePair;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Configuration;
import org.springframework.stereotype.Component;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/**
 * Created by dev on 15-10-16.
 */
@Slf4j
@Configuration
@Component
public class MsgToUser {

    @Autowired
    ConfigInfo configInfo;

    private int sendMessage(String sendURL, List<NameValuePair> params) {
        int code;
        HttpPost smsPost = new HttpPost(sendURL);
        try {
            smsPost.setEntity(new UrlEncodedFormEntity(params, "utf-8"));
            HttpResponse response = HttpClientBuilder.create().build().execute(smsPost);
            code = response.getStatusLine().getStatusCode();
            if (response.getStatusLine().getStatusCode() != 200)
                log.debug("sendMessage>> result code:" + response.getStatusLine().getStatusCode());
        } catch (IOException e) {
            code = 99;
            log.error(99+e.getMessage());
        }
        return code;
    }

    public void sendEmail(String loginName, String toEmail, String subject, String context) {

        log.debug("email send:loginName[{}] to[{}]", loginName, toEmail);
        List<NameValuePair> params = new ArrayList<>();
        params.add(new BasicNameValuePair("to", toEmail));
        params.add(new BasicNameValuePair("subject", subject));
        params.add(new BasicNameValuePair("content", context));
        sendMessage("http://em.avcdata.com/send?", params);
    }
}package com.avcdata.spark.job.mllib

import java.sql.{Connection, DriverManager}

object mysqlDB {
  /**
    * 'host': "192.168.1.201",
    * 'user': "root",
    * 'password': 'new.1234',
    * 'database': "personas",
    * 'charset': 'utf8'
    **/

  //模式匹配
  val url = "jdbc:MySQL://192.168.1.201:3306/personas"

  //所有数据的聚类结果写入数据库[sn,特征,聚类类别ID]
  def cluster_resultToDB(iterator: Iterator[(String, String, String, String, String, String, String, String, String, String, String, String, String, String, Int)]): Unit = {
    var conn: Connection = null
    var ps: java.sql.PreparedStatement = null
    val sql = "insert into device_feature_cluster(sn,stat_date,period,brand,province,price,size,workday_oc_dist,restday_oc_dist,workday_channel_dist,restday_channel_dist,pg_subject_dist,pg_year_dist,pg_region_dist,cluster_id) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    conn = DriverManager.getConnection(url, "root", "new.1234")
    ps = conn.prepareStatement(sql)
    iterator.foreach(data => {
      ps.setString(1, data._1)
      ps.setString(2, data._2)
      ps.setString(3, data._3)
      ps.setString(4, data._4)
      ps.setString(5, data._5)
      ps.setString(6, data._6)
      ps.setString(7, data._7)
      ps.setString(8, data._8)
      ps.setString(9, data._9)
      ps.setString(10, data._10)
      ps.setString(11, data._11)
      ps.setString(12, data._12)
      ps.setString(13, data._13)
      ps.setString(14, data._14)
      ps.setInt(15, data._15)
      ps.executeUpdate() //执行Sql语句
    }
    )
  }

  //聚类评价指标[聚类个数：聚合度值]
  def index_resultTODB(k: Int, index: Double): Unit = {
    var conn: Connection = null
    var ps: java.sql.PreparedStatement = null
    val sql = "insert into cluster_result_index1(cluster_number,index_value) values (?,?)"
    conn = DriverManager.getConnection(url, "root", "new.1234")
    ps = conn.prepareStatement(sql)
    //TODO 清空表中的数据
//    ps.executeUpdate("TRUNCATE " + "cluster_result_index1")
    ps.setInt(1, k)
    ps.setDouble(2, index)
    ps.executeUpdate() //执行sql语句
  }

  //问卷调查数据写入数据库
  def family_composeTODB(iterator: Iterator[(String, String, String, String)]): Unit = {
    var conn: Connection = null
    var ps: java.sql.PreparedStatement = null
    val sql = "insert into family_compose(sn,member_num,has_child,has_old) values (?,?,?,?)"
    conn = DriverManager.getConnection(url, "root", "new.1234")
    ps = conn.prepareStatement(sql)
    iterator.foreach(data => {
      ps.setString(1, data._1)
      ps.setString(2, data._2)
      ps.setString(3, data._3)
      ps.setString(4, data._4)
      ps.executeUpdate() //执行Sql语句
    }
    )
  }

}package com.avc.util

import java.sql.{Connection, DriverManager}

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object mysqlDB{
  /**
    *'host': "192.168.1.201",
    *'user': "root",
    *'password': 'new.1234',
    *'database': "personas",
    *'charset': 'utf8'
    * */

  //模式匹配
  val url="jdbc:MySQL://192.168.1.201:3306/test_kmeans"

  //所有数据的聚类结果写入数据库[sn,特征,聚类类别ID]
  def cluster_resultToDB(iterator: Iterator[(String,String,String,String,String,String,String,String,String,String,String,String,String,String,Int)]): Unit = {
    var conn: Connection= null
    var ps:java.sql.PreparedStatement=null
    val sql = "insert into device_feature_cluster(sn,stat_date,period,brand,province,price,size,workday_oc_dist,restday_oc_dist,workday_channel_dist,restday_channel_dist,pg_subject_dist,pg_year_dist,pg_region_dist,cluster_id) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    conn = DriverManager.getConnection(url,"root","new.1234")
    ps = conn.prepareStatement(sql)
    iterator.foreach(data => {
      ps.setString(1, data._1)
      ps.setString(2, data._2)
      ps.setString(3, data._3)
      ps.setString(4, data._4)
      ps.setString(5, data._5)
      ps.setString(6, data._6)
      ps.setString(7, data._7)
      ps.setString(8, data._8)
      ps.setString(9, data._9)
      ps.setString(10, data._10)
      ps.setString(11, data._11)
      ps.setString(12, data._12)
      ps.setString(13,data._13)
      ps.setString(14,data._14)
      ps.setInt(15, data._15)
      ps.executeUpdate() //执行Sql语句
    }
    )
  }
  //聚类评价指标[聚类个数：聚合度值]
  def index_resultTODB(k:Int,index:Double): Unit ={
    var conn: Connection= null
    var ps:java.sql.PreparedStatement=null
    val sql = "insert into cluster_result_index1(cluster_number,index_value) values (?,?)"
    conn = DriverManager.getConnection(url,"root","new.1234")
    ps = conn.prepareStatement(sql)
    ps.setInt(1,k)
    ps.setDouble(2,index)
    ps.executeUpdate()  //执行sql语句
  }
  //问卷调查数据写入数据库
  def family_composeTODB(iterator: Iterator[(String,String,String,String)]): Unit ={
    var conn: Connection= null
    var ps:java.sql.PreparedStatement=null
    val sql = "insert into family_compose(sn,member_num,has_child,has_old) values (?,?,?,?)"
    conn = DriverManager.getConnection(url,"root","new.1234")
    ps = conn.prepareStatement(sql)
    iterator.foreach(data => {
      ps.setString(1, data._1)
      ps.setString(2, data._2)
      ps.setString(3, data._3)
      ps.setString(4, data._4)
      ps.executeUpdate() //执行Sql语句
    }
    )
  }

}package com.avcdata.spark.job.common

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * Created by avc on 2017/4/5.
  * 新增终端
  * 近三个月内，第一次使用指定应用的终端数
  */
object NewAddTerminal {
    val digitRegex = """^\d+$""".r
    val dateRegex = """\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}""".r

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("new")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimApkCol = Bytes.toBytes("dim_apk")
        val dimDateCol = Bytes.toBytes("dim_date")

        val date = analysisDate.substring(0, 7) + "-01"
        val currDateStart = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd") //统计月开始时间
        val currDateEnd = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd") //统计月结束时间

        //历史参考数据
        // |where dim_date >= '"""+ hisMinDate +"""' and dim_date <= '"""+ hisMaxDate +"""'
        val baseSql =
            """
              |select dim_apk,dim_sn,dim_date from hr.tracker_new_terminal
            """

        println("baseSql : " + baseSql.stripMargin)
        val hiveContext = new HiveContext(sc)
        val baseData = hiveContext.sql(baseSql.stripMargin).mapPartitions(items =>{
            items.map(item => {
                val apk = item(0).toString
                val sn = item(1).toString
                (sn+"0x01"+apk, 1)
            })
        }).distinct()

        //统计月的数据
        val statisticSql =
            """
              |select appname,dim_sn,dim_date from
              |(select dim_apk,dim_sn,min(dim_date) dim_date from hr.tracker_apk_fact_partition
              |where date>='"""+ currDateStart +"""' and date<='"""+ currDateEnd +"""' group by dim_apk,dim_sn) a
              |join
              |(select packagename,appname from hr.apkinfo) b
              |on b.packagename=a.dim_apk
            """

        println("statisticSql : " + statisticSql.stripMargin)
        val staData = hiveContext.sql(statisticSql.stripMargin).mapPartitions(items =>{
            items.map(item => {
                val sn = item(1).toString
                val apk = item(0).toString
                val date = item(2).toString
                (sn+"0x01"+apk, date)
            })
        }).distinct()

        val reRdd = staData.leftOuterJoin(baseData).filter(x => x._2.toString().contains("None"))
            .filter(x => x._1.split("0x01").length==2).mapPartitions(items =>{
                items.map(item => {
                    val sn = item._1.split("0x01")(0)
                    val apk = item._1.split("0x01")(1)
                    (sn, apk, currDateStart)
                })
            })

        //reRdd.take(1000).foreach(println)

        /*val baseSql =
            """
              |select appname,dim_sn,concat(year(dim_date),if(month(dim_date) in (11,12,10),'-','-0'),month(dim_date),'-01') as dim_date from
              |(select dim_apk,dim_sn,min(dim_date) dim_date from hr.tracker_apk_fact_partition
              |where date>='2016-10-01' and date<='2016-12-31' group by dim_apk,dim_sn) a
              |join
              |(select packagename,appname from hr.apkinfo) b
              |on b.packagename=a.dim_apk
            """

        println("baseSql : " + baseSql.stripMargin)
        val hiveContext = new HiveContext(sc)
        val baseData = hiveContext.sql(baseSql.stripMargin).mapPartitions(items =>{
            items.map(item => {
                val sn = item(1).toString
                val apk = item(0).toString
                val date = item(2).toString
                (sn, apk, date)
            })
        })*/

        //baseData.foreachPartition(items => {
        reRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_new_terminal")) //tracker_new_terminal

            try {
                items.foreach(line => {
                    val sn = line._1
                    val apk = line._2
                    val date = line._3

                    val put = new Put(Bytes.toBytes(apk + date + sn))
                    put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apk))
                    put.addColumn(dimFamilyCol, dimDateCol, Bytes.toBytes(date))

                    mutator.mutate(put)
                })

                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })

    }

    def main(args: Array[String]): Unit = {
        val analysisDate = "2017-02-16"
        val date = analysisDate.substring(0, 7) + "-01"
        val currDateStart = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")
        val currDateEnd = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd")
        val hisMaxDate = DateTime.parse(currDateStart).plusMonths(-1).toString("yyyyMM")
        val hisMinDate = DateTime.parse(currDateStart).plusMonths(-3).toString("yyyyMM")
        println(currDateStart + ", " + currDateEnd + ", " + hisMaxDate + ", " + hisMinDate)
    }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OCData2Partition {

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)


    val sql = "INSERT OVERWRITE TABLE hr.tracker_oc_fact_partition partition(date='" + currentDate + "')  select key,sn,power_on_day,power_on_time,power_on_length,cnt,power_on_day from hr.tracker_oc_fact from hr.tracker_apk_fact where power_on_day='" + currentDate + "'"


    sqlContext.sql(sql)

    println(sql)

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")


  }


}
package com.avcdata.vbox.clean.oc

import com.avcdata.vbox.util.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OCData2Partition {

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)


    val sql = "INSERT OVERWRITE TABLE hr.tracker_oc_fact_partition partition(date='" + twoDaysAgoDate + "')   select key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_oc_fact  where power_on_day='" + twoDaysAgoDate + "'"+ " and key not like '%TCL%'"


    sqlContext.sql(sql)

    println(sql)

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")


  }


}
package com.avcdata.vbox.clean.oc

import com.avcdata.vbox.util.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OCData2PartitionTCL {

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")



    //加载四天前的数据
//    val daysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -3)
//
//    val daysAgoDate = TimeUtils.convertTimeStamp2DateStr(daysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)
//
//
//    val sql = "INSERT OVERWRITE TABLE hr.tracker_oc_fact_partition_tcl partition(date='" + daysAgoDate + "')  select key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_oc_fact   where power_on_day='" + daysAgoDate +
//      "'"+ " and key like '%TCL%'"
//
//    sqlContext.sql(sql)
//
//    println(sql)


    //加载三天前的数据
    val daysAgoTime01 = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)

    val daysAgoDate01 = TimeUtils.convertTimeStamp2DateStr(daysAgoTime01, TimeUtils.DAY_DATE_FORMAT_ONE)


    val sql01 = "INSERT OVERWRITE TABLE hr.tracker_oc_fact_partition_tcl2 partition(date='" + daysAgoDate01 + "')  select key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_oc_fact   where power_on_day='" + daysAgoDate01 +
      "'"+ " and key like '%TCL%'"

    sqlContext.sql(sql01)

    //    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_apk_fact_partition PARTITION (date) select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration,dim_date as date from hr.tracker_apk_fact")


  }


}
package stores

import kafka.common.TopicAndPartition
import org.apache.spark.rdd.RDD

trait OffsetsStore {

  def readOffsets(topic: String): Option[Map[TopicAndPartition, Long]]

  def saveOffsets(topic: String, rdd: RDD[_]): Unit

}package com.avcdata.etl.util.db

/**
  * 操作类型
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/28 16:07
  */
case class Operation(dbType: String, opType: String)package com.avcdata.etl.util.hive

import java.util.UUID

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.hive.ql.io.orc.{OrcFile, Writer}
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector
import org.slf4j.LoggerFactory

/**
  * ORC小文件合并,
  * 需要加"-classpath /opt/cloudera/parcels/CDH/lib/hive/lib"参数。
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/9/8 10:05
  */
object OrcFileCombiner
{
  private val logger = LoggerFactory.getLogger(OrcFileCombiner.getClass)

  val TEMPLATE_FILE_SUFFIX = ".template"

  def main(args: Array[String])
  {
    logger.info(s"OrcFileCombiner input file path -> ${args(0)}")

    //输入文件夹路径
    val inputPath = new Path(args(0))

    //获取文件系统
    val fs = FileSystem.get(new Configuration())

    //获取写入文件
    val destFilePath = new Path(s"${args(0)}/${UUID.randomUUID()}.orc")
    val destTemplateFilePath = new Path(s"${args(0)}/${UUID.randomUUID()}.orc$TEMPLATE_FILE_SUFFIX")

    var writer: OrcRecordWriter = null

    //获取输入文件夹下的文件列表
    val filesIterator = fs.listFiles(inputPath, false)
    while (filesIterator.hasNext)
    {
      val srcLocatedFileStatus = filesIterator.next()
      val srcLocatedFilePath = srcLocatedFileStatus.getPath

      //若是以.orc结尾的文件则进行处理
      if (srcLocatedFileStatus.isFile && srcLocatedFilePath.getName.toUpperCase.endsWith(".ORC"))
      {
        logger.info(s"Begin to handle orc file -> $srcLocatedFilePath")

        //获取文件的Reader
        val reader = OrcFile.createReader(fs, srcLocatedFilePath)

        logger.info(s"$srcLocatedFilePath metadata -> ${reader.getMetadata}")

        val inspector = reader.getObjectInspector.asInstanceOf[StructObjectInspector]

        logInspectorMetadata(inspector)

        //创建写入对象
        if (null == writer)
        {
          val writerOptions = OrcFile.writerOptions(fs.getConf)
          writerOptions.fileSystem(fs)
          writerOptions.inspector(inspector)

          writer = new OrcRecordWriter(destTemplateFilePath, writerOptions)
        }

        //循环写入记录
        val records = reader.rows()
        val row: Object = null
        while (records.hasNext)
        {
          writer.write(records.next(row))
        }

        fs.rename(srcLocatedFilePath, new Path(args(0), s"${srcLocatedFilePath.getName}$TEMPLATE_FILE_SUFFIX"))

        logger.info(s"End to handle orc file -> $srcLocatedFilePath")
      }
    }

    //关闭写入对象
    writer.close()

    //将临时文件重命名为正式文件
    fs.rename(destTemplateFilePath, destFilePath)

    deleteTemplateFiles(inputPath, fs)
  }

  def logInspectorMetadata(inspector: StructObjectInspector): Unit =
  {
    //记录的字段信息
    if (logger.isDebugEnabled)
    {
      import scala.collection.JavaConversions._
      val fields = inspector.getAllStructFieldRefs
      fields.foreach(field =>
      {
        logger.debug(s"${field.getFieldName} -> ${field.getFieldObjectInspector.getTypeName}")
      })
    }
  }

  private def deleteTemplateFiles(inputPath: Path, fs: FileSystem): Unit =
  {
    val lastFilesItrator = fs.listFiles(inputPath, false)
    while (lastFilesItrator.hasNext)
    {
      val lastLocatedFileStatus = lastFilesItrator.next()
      val lastLocatedFilePath = lastLocatedFileStatus.getPath

      if (lastLocatedFileStatus.isFile && lastLocatedFilePath.getName.endsWith(TEMPLATE_FILE_SUFFIX))
      {
        fs.delete(lastLocatedFilePath, false)
      }
    }
  }
}

class OrcRecordWriter
{
  private var writer: Writer = null
  private var path: Path = null
  private var options: OrcFile.WriterOptions = null

  def this(path: Path, options: OrcFile.WriterOptions)
  {
    this()
    this.path = path
    this.options = options
    this.writer = OrcFile.createWriter(path, options)
  }

  def write(row: Object): Unit =
  {
    writer.addRow(row)
  }

  def close(): Unit =
  {
    if (null != writer) writer.close()
  }
}package com.avcdata.etl.launcher.util

import java.util.UUID

import com.avcdata.etl.common.util.HdfsFileUtil
import org.apache.spark.{SparkConf, SparkContext}
import org.slf4j.LoggerFactory

/**
  * 使用Spark合并ORC小文件
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/9/8 15:18
  */
object OrcFileCombinerWithSpark
{
  val logger = LoggerFactory.getLogger(OrcFileCombinerWithSpark.getClass)

  def main(args: Array[String])
  {
    //由命令行设置/覆盖应用名称及相关参数
    val conf = new SparkConf().setIfMissing("spark.app.name", "OrcFileCombinerWithSpark-" + UUID.randomUUID())
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.logConf", "true")

    val sc = new SparkContext(conf)

    val inputPath = args(0)
    val numPartitions = args(1).toInt
    val outputPath = s"$inputPath.${UUID.randomUUID()}"

    logger.info(
      s"""Input path -> $inputPath
          |Num Partitions -> $numPartitions
       """.stripMargin)

    if (!HdfsFileUtil.existsChildFile(inputPath))
    {
      logger.error(s"There is no files in direcoty -> $inputPath, please check your configs.")
    }
    else
    {
      HdfsFileUtil.delete(outputPath)

      new org.apache.spark.sql.hive.HiveContext(sc).read.orc(inputPath).repartition(numPartitions).write.orc(outputPath)

      HdfsFileUtil.delete(inputPath)
      HdfsFileUtil.rename(outputPath, inputPath)
    }

    sc.stop()
  }
}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewApkTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt-1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt -1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" +"'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt-1).toString
            }

            println("-------------- week statis : " + analysisDate + ", " + week)
        }

        var month = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            if (weeknum._3.equals("1")) {
                month = "12"
            } else {
                month = (weeknum._3.toInt - 1).toString
            }
        }

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ottsql_daily(analysisDate), HiveSql.tracker_tv_overviewtable, 20)

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ottsql_weekly(analysisDate), HiveSql.tracker_tv_overviewtable, 20)
        }

        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ottsql_monthly(month, analysisDate), HiveSql.tracker_tv_overviewtable, 20)
        }

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ottsql_30days(analysisDate), HiveSql.tracker_tv_overviewtable, 20)

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ottsql_7days(analysisDate), HiveSql.tracker_tv_overviewtable, 20)


        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +
                    """', 32) and tv_date<='""" + analysisDate +
                    """'
                    """
        }
        else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +
                    """', 7) and tv_date<='""" + analysisDate +
                    """'
                    """
        }
        else {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
                  |WHERE tv_date='""" + analysisDate +
                    """'"""
        }

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewHourApkTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_hoursql_ott(analysisDate), HiveSql.tracker_tv_overview_hourtable, 21)
    }

}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewHourLiveTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_hoursql_live(analysisDate), HiveSql.tracker_tv_overview_hourtable, 21)
    }

}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewHourOCTnumPreTotalJob {
  def run(sc: SparkContext, analysisDate: String) = {
    val sqlc: HiveContext = new HiveContext(sc)
    sqlc.sql("SET hive.exec.dynamic.partition=true")
    sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

    HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_hoursql_oc(analysisDate), HiveSql.tracker_tv_overview_hourtable, 21)

  }

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewHourTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum

import com.avcdata.spark.job.common.{HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewHourTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        HiveToHbase.write(sqlc, HiveSql.getTracker_total_tv_overview_hoursql(analysisDate), HiveSql.tracker_total_tv_overview_hourtable, 27)

        val parsql =
            """
              |insert overwrite table hr.tracker_total_tv_overview_hour_partition partition(date)
              |select key,brand,license,province,city,behavior_type,tv_date,tv_hour,terminal_cnt,tv_date from hr.tracker_total_tv_overview_hour
              |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
            """

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewLiveTnumPreTotalJob {

  def run(sc: SparkContext, analysisDate: String) = {
    val sqlc: HiveContext = new HiveContext(sc)
    sqlc.sql("SET hive.exec.dynamic.partition=true")
    sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

    val datesql = "select * from hr.dateinfo where today = '" + analysisDate +"'"
    val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

    if (weeknum._2 != "" && weeknum._2.toInt == 1) {
      var week = "0"
      var y = ""
      if (weeknum._1.toInt-1 == 0 && weeknum._3.equals("1")) {
        y = (analysisDate.substring(0, 4).toInt -1).toString
        week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" +"'", analysisDate.contains("*"))._1
      } else {
        y = analysisDate.substring(0, 4)
        week = (weeknum._1.toInt-1).toString
      }

      println("-------------- week statis : " + analysisDate + ", " + week)
    }

    var month = ""
    if (weeknum._4 != "" && weeknum._4.toInt == 1) {
      if (weeknum._3.equals("1")) {
        month = "12"
      } else {
        month = (weeknum._3.toInt-1).toString
      }
    }


    HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_livesql_daily(analysisDate), HiveSql.tracker_tv_overviewtable, 20)
    

    if (weeknum._2 != "" && weeknum._2.toInt == 1) {
      HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_livesql_weekly(analysisDate), HiveSql.tracker_tv_overviewtable, 20)
      
    }

    if (weeknum._4 != "" && weeknum._4.toInt == 1) {
      HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_livesql_monthly(month, analysisDate), HiveSql.tracker_tv_overviewtable, 20)
      
    }

    HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_livesql_30days(analysisDate), HiveSql.tracker_tv_overviewtable, 20)
    

    HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_livesql_7days(analysisDate), HiveSql.tracker_tv_overviewtable, 20)

    var parsql = ""
    if (weeknum._4 != "" && weeknum._4.toInt == 1){
      parsql =
        """
          |insert overwrite table hr.tracker_tv_overview_partition partition(date)
          |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
          |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<='""" + analysisDate +"""'
                                                                                                    """
    }else if (weeknum._2 != "" && weeknum._2.toInt == 1){
      parsql =
        """
          |insert overwrite table hr.tracker_tv_overview_partition partition(date)
          |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
          |WHERE tv_date>=DATE_SUB('""" + analysisDate + """', 7) and tv_date<='""" + analysisDate +"""'
                                                                                                    """
    } else {
      parsql =
        """
          |insert overwrite table hr.tracker_tv_overview_partition partition(date)
          |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
          |WHERE tv_date='""" + analysisDate + """'"""
    }

    sqlc.sql(parsql.stripMargin)
  }



}
package com.avcdata.spark.job.total.tnumpre

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewOCTnumPreTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt-1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt -1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" +"'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt-1).toString
            }

            println("-------------- week statis : " + analysisDate + ", " + week)
        }

        var month = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            if (weeknum._3.equals("1")) {
                month = "12"
            } else {
                month = (weeknum._3.toInt - 1).toString
            }
        }

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ocsql_daily(analysisDate), HiveSql.tracker_tv_overviewtable, 20)


        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ocsql_weekly(analysisDate), HiveSql.tracker_tv_overviewtable, 20)

        }

        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ocsql_monthly(month, analysisDate), HiveSql.tracker_tv_overviewtable, 20)

        }

        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ocsql_30days(analysisDate), HiveSql.tracker_tv_overviewtable, 20)


        HiveToHbase.write(sqlc, HiveSql.getTracker_tv_overview_ocsql_7days(analysisDate), HiveSql.tracker_tv_overviewtable, 20)

        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<='""" + analysisDate +"""'
                """
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 7) and tv_date<='""" + analysisDate +"""'
                """
        } else {
            parsql =
                """
                  |insert overwrite table hr.tracker_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_tv_overview
                  |WHERE tv_date='""" + analysisDate +
                """'"""
        }

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum

import com.avcdata.spark.job.common.{DateInfo, HiveSql, HiveToHbase}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/2/14.
  */
object OverviewTnumTotalJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val sqlc: HiveContext = new HiveContext(sc)
        sqlc.sql("SET hive.exec.dynamic.partition=true")
        sqlc.sql("SET hive.exec.dynamic.partition.mode=nonstrict")

        val datesql = "select * from hr.dateinfo where today = '" + analysisDate + "'"
        val weeknum = DateInfo.getWeek(sc, datesql, analysisDate.contains("*"))

        if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            var week = "0"
            var y = ""
            if (weeknum._1.toInt - 1 == 0 && weeknum._3.equals("1")) {
                y = (analysisDate.substring(0, 4).toInt - 1).toString
                week = DateInfo.getWeek(sc, "select * from hr.dateinfo where today = '" + y + "-12-31" + "'", analysisDate.contains("*"))._1
            } else {
                y = analysisDate.substring(0, 4)
                week = (weeknum._1.toInt - 1).toString
            }

        }

        var subDate = "0"
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            subDate = "32"
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            subDate = "7"
        }

//        HiveToHbase.write(sqlc, HiveSql.getTracker_total_tv_overviewsql(analysisDate, subDate), HiveSql.tracker_total_tv_overviewtable, 26)

        var parsql = ""
        if (weeknum._4 != "" && weeknum._4.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_overview
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 32) and tv_date<=DATE_SUB('""" + analysisDate +"""', 0)
                """
        } else if (weeknum._2 != "" && weeknum._2.toInt == 1) {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_overview
                  |WHERE tv_date>=DATE_SUB('""" + analysisDate +"""', 7) and tv_date<=DATE_SUB('""" + analysisDate +"""', 0)
                """
        } else {
            parsql =
                """
                  |insert overwrite table hr.tracker_total_tv_overview_partition partition(date)
                  |select key,brand,license,province,city,behavior_type,period,tv_date,terminal_cnt,tv_date from hr.tracker_total_tv_overview
                  |WHERE tv_date=DATE_SUB('""" + analysisDate +"""', 0)
                """
        }

        sqlc.sql(parsql.stripMargin)
    }

}
package com.avcdata.etl.util.yarn

import java.sql.Timestamp

import com.avcdata.etl.common.pattern.LoanPattern
import com.avcdata.etl.common.pool.jdbc.JDBCConnectionPool
import com.avcdata.etl.common.util.DeSerializationUtil
import org.slf4j.LoggerFactory

import scala.io.Source
import scala.util.Try

/**
  * 解析APP任务运行信息
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2017/2/9 20:19
  */
object ParseAppsInfo
{
  private val logger = LoggerFactory.getLogger(ParseAppsInfo.getClass)

  private val yarnAppsInfoTblName = "yarn_apps_info"

  private val appsStartMark =
    """<script type="text/javascript">
      |              var appsTableData="""
      .stripMargin

  private val appsEndMark =
    """</script>
      |            <tbody>
      |            </tbody>""".stripMargin

  def main(args: Array[String])
  {
    logger.info("ParseAppsInfo begin.")

    val serverAddr = args(0)
    val dbConnectURI = args(1)
    val dbUsername = args(2)
    val dbPassword = args(3)

    //获取网页内容
    val htmlContent = xHtmlContent(serverAddr, "/cluster/apps")

    //获取JSON内容
    val dataStartIdx = htmlContent.indexOf(appsStartMark)
    val dataEndIdx = htmlContent.indexOf(appsEndMark)

    //序列化数组内容
    val items = DeSerializationUtil.deserialize[Array[Array[String]]](htmlContent.substring(dataStartIdx + appsStartMark.length, dataEndIdx))

    //处理每条记录
    val totalItems = items.length

    logger.info(s"The total app items is $totalItems.")

    var successedCount = 0
    var skipedCount = 0
    var failedCount = 0
    items.foreach(item =>
    {
      val allURLInfo = item(0)
      val urlStartMark = "<a href='"
      val urlEndMark = "'>application_"
      val relativeURL = allURLInfo.substring(allURLInfo.indexOf(urlStartMark) + urlStartMark.length, allURLInfo.indexOf(urlEndMark))

      //判断当前relativeURL是否需要处理
      LoanPattern.using(JDBCConnectionPool(dbConnectURI, dbUsername, dbPassword))
      { conn =>

        LoanPattern.using(conn.prepareStatement(s"SELECT 1 FROM $yarnAppsInfoTblName WHERE relative_url = ? AND state IN ('FINISHED', 'KILLED', 'FAILED')"))
        { selectPs =>

          selectPs.setString(1, relativeURL)
          LoanPattern.using(selectPs.executeQuery())
          { rs =>

            //若数据库中存在对应APPID且状态不为"FINISHED",则更新数据
            if (!rs.next())
            {
              val username = item(1)
              val appName = item(2)
              val appType = item(3)
              val queueName = item(4)
              val startTime = Try(new Timestamp(item(5).toLong)).getOrElse(null.asInstanceOf[Timestamp])
              val endTime = Try(new Timestamp(item(6).toLong)).getOrElse(null.asInstanceOf[Timestamp])
              val state = item(7)
              val finalStatus = item(8)

              //获取详情网页内容
              val detailHtmlContent = xHtmlContent(serverAddr, relativeURL)
              val secondsStartMark =
                """<th>
                  |                  Aggregate Resource Allocation:
                  |                </th>
                  |                <td>
                  |                  """.stripMargin
              val secondsEndMark =
                """
                  |                </td>
                  |              </tr>
                  |            </table>
                  |          </div>
                  |          <div class="info-wrap ui-widget-content ui-corner-bottom">
                  |            <table id="app">""".stripMargin

              val aggResAllocResult = Try(detailHtmlContent.substring(detailHtmlContent.indexOf(secondsStartMark) + secondsStartMark.length, detailHtmlContent.indexOf(secondsEndMark)))

              //若详情获取失败,则跳过
              if (aggResAllocResult.isFailure)
              {
                failedCount += 1

                logger.error(s"Failed to process relative URL <$relativeURL>")
              }
              else
              {
                val aggResAlloc = aggResAllocResult.get

                val resAlloc = aggResAlloc.replace("MB-seconds", "").replace("vcore-seconds", "").split(",").map(_.trim.toLong)

                //删除旧数据
                LoanPattern.using(conn.prepareStatement(s"DELETE FROM $yarnAppsInfoTblName WHERE relative_url = ?"))
                { delPs =>

                  delPs.setString(1, relativeURL)
                  delPs.execute()
                }

                //插入新数据
                LoanPattern.using(conn.prepareStatement(s"INSERT INTO `$yarnAppsInfoTblName` ( `relative_url`, `username`, `app_name`, `app_type`, `queue_name`, `start_time`, `end_time`, `mb_seconds`, `vcores_seconds`, `state`, `final_status`) " +
                  "VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"))
                { insertPs =>

                  insertPs.setString(1, relativeURL)
                  insertPs.setString(2, username)
                  insertPs.setString(3, appName)
                  insertPs.setString(4, appType)
                  insertPs.setString(5, queueName)
                  insertPs.setTimestamp(6, startTime)
                  insertPs.setTimestamp(7, endTime)
                  insertPs.setLong(8, resAlloc(0))
                  insertPs.setLong(9, resAlloc(1))
                  insertPs.setString(10, state)
                  insertPs.setString(11, finalStatus)

                  insertPs.execute()
                }

                successedCount += 1
              }
            }
            else
            {
              skipedCount += 1

              logger.info(s"Has skiped relative URI -> $relativeURL")
            }
          }
        }

        logger.info(s"Has handled relative URI -> $relativeURL [($successedCount + $skipedCount + $failedCount)/$totalItems]")
      }
    })

    logger.info("ParseAppsInfo end.")
  }

  private def xHtmlContent(serverAddr: String, relativePath: String): String =
  {
    val absPath = s"$serverAddr$relativePath"

    val htmlContent = LoanPattern.using(Source.fromURL(absPath))
    { bs =>

      bs.getLines().mkString("\n")
    }

    htmlContent
  }
}
package com.avcdata.vbox.clean.common


case class OCFactPartition(
                            key: String,
                            sn: String,
                            power_on_day: String,
                            power_on_time: String,
                            power_on_length: String,
                            cnt: String,
                            date: String
                          ) extends Serializable

case class LiveFactPartition(
                              key: String,
                              dim_sn: String,
                              dim_channel: String,
                              dim_date: String,
                              dim_hour: String,
                              dim_min: String,
                              fact_time_length: String,
                              fact_cnt: String,
                              date: String
                            ) extends Serializable

case class ApkFactPartition(key: String,
                            dim_sn: String,
                            dim_apk: String,
                            dim_date: String,
                            dim_hour: String,
                            fact_cnt: String,
                            fact_duration: String,
                            date: String) extends Serializable


//|insert overwrite table hr.tracker_oc_fact_partition partition(date='"""+analysisDate+"""')
//|select key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_oc_fact
//|where power_on_day='"""+analysisDate+"""'


//|insert overwrite table hr.tracker_live_fact_partition partition(date='"""+analysisDate+"""')
//|select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_time_length,fact_cnt from hr.tracker_live_fact02
//|where dim_date='"""+analysisDate+"""'

//|insert overwrite table hr.tracker_apk_fact_partition partition(date='"""+analysisDate+"""')
//|select key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact
//| where dim_date='"""+analysisDate+"""' and fact_duration>'0'package com.avcdata.spark.job.konka

import java.text.SimpleDateFormat

import org.json4s._
import org.json4s.JsonDSL._
import org.json4s.jackson.JsonMethods._


class PartitionHelper {

  implicit val formats = new DefaultFormats {
    override def dateFormatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
  }

  def toJson(obj: JValue):String = {
    compact(render(obj))
  }

  def toPrettyJson(obj: JValue):String = {
    pretty(render(obj))
  }

  def parse(json: String): org.json4s.JValue = {
    org.json4s.jackson.JsonMethods.parse(json)
  }


}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.playcrawler.SearchIndexDataLoadJob
import org.apache.log4j.Logger

/**
  * Created by avc on 2017/2/13.
  */
object PlaycrawlerExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //TODO 搜索指数清洗
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob start....")
      SearchIndexDataLoadJob.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-SearchIndexDataLoadJob end....")
    }


  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.TimeUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将到剧的数据导入到apk分区表
  */
object PlayData2Partition {
  def main(args: Array[String]) {
    val arr = "#,#".split(",")
    println(arr.length)
  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    //加载三天前的数据
    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate,TimeUtils.DAY_DATE_FORMAT_ONE,-2)

    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime,TimeUtils.DAY_DATE_FORMAT_ONE)

    //加载当天的数据
    val sql = "INSERT OVERWRITE table hr.tracker_player_fact_partition PARTITION (date='" + currentDate + "') select key, dim_sn,dim_name,dim_title,dim_awcid,dim_part,dim_date,dim_hour,fact_vv,fact_duration, dim_year,dim_model,dim_crowd,dim_region,dim_date from hr.tracker_player_fact where dim_date='" + currentDate + "'"
//
    sqlContext.sql(sql)

    println(sql)

//    sqlContext.sql("INSERT OVERWRITE table hr.tracker_player_fact_partition partition(date) select key, dim_sn,dim_name,dim_title,dim_awcid,dim_part,dim_date,dim_hour,fact_vv,fact_duration, dim_year,dim_model,dim_crowd,dim_region,dim_date from hr.tracker_player_fact")



  }

}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 到剧搜索指数数据清洗
  */
object PlayerTerminalLoadJob {
  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("SearchIndexDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-01")
    sc.stop()
  }

  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate

    val sqlContext = new HiveContext(sc)

    //TODO 获取Play中的SN
    val playPairRDD = sc.textFile(hdfsPath).distinct
      .map(line => {
        val cols = line.split("\t")
        cols(2).trim
      }).distinct
      .map(line => {
        val sn = line.toString
        (sn, "")
      })



    //TODO 关联sample_terminal_two
    val terminalPairRDD = sqlContext.sql("select sn,area,brand,city,citylevel,last_poweron,license,model,province,size from hr.terminal").rdd.map(line => {
      var i = 0
      val sn = line(i).toString
      i = i + 1
      val area = line(i)
      i = i + 1
      val brand = line(i)
      i = i + 1
      val city = line(i)
      i = i + 1
      val citylevel = line(i)
      i = i + 1
      val last_poweron = line(i)
      i = i + 1
      val license = line(i)
      i = i + 1
      val model = line(i)
      i = i + 1
      val province = line(i)
      i = i + 1
      val size = line(i)

      val otherInfo = area + "\t" + brand + "\t" + city + "\t" + citylevel + "\t" + last_poweron + "\t" + license + "\t" + model + "\t" + province+"\t"+size

      (sn, otherInfo)
    })


   val resultRDD =  playPairRDD.join(terminalPairRDD)
      .map(line => {
        val sn = line._1
        val otherInfo = line._2._2.toString()
        val cols = otherInfo.split("\t")

        var i = 0
        val area = cols(i)
        i = i + 1
        val brand = cols(i)
        i = i + 1
        val city = cols(i)
        i = i + 1
        val citylevel = cols(i)
        i = i + 1
        val last_poweron = cols(i)
        i = i + 1
        val license = cols(i)
        i = i + 1
        val model = cols(i)
        i = i + 1
        val province = cols(i)
        i = i + 1
        val size = cols(i)

        sn + "\t" + area + "\t" + brand + "\t" + city + "\t" + citylevel + "\t" + last_poweron + "\t" + license + "\t" +
          model + "\t" + province+ "\t" + size

      })



    resultRDD
      //TODO    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("player_terminal")

      try {

        lines.foreach(line => {
          var i = 0
          val cols = line.split("\t")

          val sn = cols(i)
          i = i + 1
          val area = cols(i)
          i = i + 1
          val brand = cols(i)
          i = i + 1
          val city = cols(i)
          i = i + 1
          val citylevel = cols(i)
          i = i + 1
          val last_poweron = cols(i)
          i = i + 1
          val license = cols(i)
          i = i + 1
          val model = cols(i)
          i = i + 1
          val province = cols(i)
          i = i + 1
          val size = cols(i)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          //          println(sortedLine)

          mutator.mutate(HBaseUtils.getPut_Plays_Terminal(brand,sortedLine))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

  }

}
package com.avcdata.vbox.other

import com.avcdata.vbox.util.{HBaseUtils, TimeUtils}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 到剧终端信息清洗
  */
object PlayerTerminalLoadJob {
  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("SearchIndexDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-01")
    sc.stop()
  }

  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate

    val sqlContext = new HiveContext(sc)

    //TODO 获取Play中的SN
    val playPairRDD = sc.textFile(hdfsPath).distinct
      .map(line => {
        val cols = line.split("\t")
        cols(2).trim
      }).distinct
      .map(line => {
        val sn = line.toString
        (sn, "")
      })



    //TODO 关联sample_terminal_two
    val terminalPairRDD = sqlContext.sql("select sn,area,brand,city,citylevel,last_poweron,license,model,province,size from hr.terminal").rdd.map(line => {
      var i = 0
      val sn = line(i).toString
      i = i + 1
      val area = line(i)
      i = i + 1
      val brand = line(i)
      i = i + 1
      val city = line(i)
      i = i + 1
      val citylevel = line(i)
      i = i + 1
      val last_poweron = line(i)
      i = i + 1
      val license = line(i)
      i = i + 1
      val model = line(i)
      i = i + 1
      val province = line(i)
      i = i + 1
      val size = line(i)

      val otherInfo = area + "\t" + brand + "\t" + city + "\t" + citylevel + "\t" + last_poweron + "\t" + license + "\t" + model + "\t" + province+"\t"+size

      (sn, otherInfo)
    })


   val resultRDD =  playPairRDD.join(terminalPairRDD)
      .map(line => {
        val sn = line._1
        val otherInfo = line._2._2.toString()
        val cols = otherInfo.split("\t")

        var i = 0
        val area = cols(i)
        i = i + 1
        val brand = cols(i)
        i = i + 1
        val city = cols(i)
        i = i + 1
        val citylevel = cols(i)
        i = i + 1
        val last_poweron = cols(i)
        i = i + 1
        val license = cols(i)
        i = i + 1
        val model = cols(i)
        i = i + 1
        val province = cols(i)
        i = i + 1
        val size = cols(i)

        sn + "\t" + area + "\t" + brand + "\t" + city + "\t" + citylevel + "\t" + last_poweron + "\t" + license + "\t" +
          model + "\t" + province+ "\t" + size

      })



    resultRDD
      //TODO    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("player_terminal")

      try {

        lines.foreach(line => {
          var i = 0
          val cols = line.split("\t")

          val sn = cols(i)
          i = i + 1
          val area = cols(i)
          i = i + 1
          val brand = cols(i)
          i = i + 1
          val city = cols(i)
          i = i + 1
          val citylevel = cols(i)
          i = i + 1
          val last_poweron = cols(i)
          i = i + 1
          val license = cols(i)
          i = i + 1
          val model = cols(i)
          i = i + 1
          val province = cols(i)
          i = i + 1
          val size = cols(i)

          val sortedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          //          println(sortedLine)

          mutator.mutate(HBaseUtils.getPut_Plays_Terminal(brand,sortedLine))

        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

  }

}
package com.avcdata.spark.job.total.tnumpre

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object PlayHourTnumPreTotalJob {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object PlayHourTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object PlayPgRegionDistCnt {
//  val log = Logger.getLogger(getClass.getName)

  case class PlayPgRegionDistCnt(
                                  sn: String,
                                  stat_date: String,
                                  period: String,
                                  region: String,
                                  cnt: String,
                                  cluster_id: String
                                )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("PlayPgRegionDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_year_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_region_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_region_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_REGION_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_region_dist_map)

    })

    val playPgRegionDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_region_dist_map = line._2._2

      val resultArr = new Array[PlayPgRegionDistCnt](pg_region_dist_map.size)

      val region_arr = pg_region_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgRegionDistCnt(
          sn,
          stat_date,
          period,
          region_arr(i),
          pg_region_dist_map.get(region_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgRegionDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgRegionDistCntDF.registerTempTable("playPgRegionDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.region,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,region,sum(cnt) as cnt  from playPgRegionDistCnt group by stat_date,period,cluster_id,region
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgRegionDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_play_pg_region_dist_cnt_k"+ clusterNum, false,
      SaveMode.Append)

  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object PlayPgSubjectDistCnt {

  case class PlayPgSubjectDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   subject: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("PlayPgSubjectDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_subject_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_subject_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_SUBJECT_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_subject_dist_map)

    })

    val playPgSubjectDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_subject_dist_map = line._2._2

      val resultArr = new Array[PlayPgSubjectDistCnt](pg_subject_dist_map.size)

      val subject_arr = pg_subject_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgSubjectDistCnt(
          sn,
          stat_date,
          period,
          subject_arr(i),
          pg_subject_dist_map.get(subject_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgSubjectDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgSubjectDistCntDF.registerTempTable("playPgSubjectDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.subject,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,subject,sum(cnt) as cnt  from playPgSubjectDistCnt group by stat_date,period,cluster_id,subject
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgSubjectDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_play_pg_subject_dist_cnt_k"+ clusterNum, false,
      SaveMode.Append)

  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object PlayPgYearDistCnt {

  case class PlayPgYearDistCnt(
                                sn: String,
                                stat_date: String,
                                period: String,
                                year: String,
                                cnt: String,
                                cluster_id: String
                              )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("PlayPgYearDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {

    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD
      //////////////test//////////////////
      //      .filter(line => {
      //      val cols = line.split("\t")
      //      val sn = cols(0)
      //      val stat_date = cols(1)
      //      val period = cols(2)
      //      val brand = cols(3)
      //      val province = cols(4)
      //      val price = cols(5)
      //      val size = cols(6)
      //      val workday_oc_dist = cols(7)
      //      val restday_oc_dist = cols(8)
      //      val workday_channel_dist = cols(9)
      //      val restday_channel_dist = cols(10)
      //      val pg_subject_dist = cols(11)
      //      val pg_yeay_dist = cols(12)
      //      val pg_region_dist = cols(13)
      //      val cluster_id = cols(14)
      //
      //      cluster_id.toString.equals("2")
      //    })

      .map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    //    snCidRDD.saveAsTextFile("/tmp/snCidRDD" + System.currentTimeMillis())

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,pg_year_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val pg_year_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_PG_YEAR_ARR)

      (sn + "\t" + stat_date + "\t" + period, pg_year_dist_map)

    })


    val playPgYearDistCntPair = snCidRDD.join(snWorkdayOCDistRDD)

    val playPgYearDistCntDF = playPgYearDistCntPair.flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val pg_year_dist_map = line._2._2

      val resultArr = new Array[PlayPgYearDistCnt](pg_year_dist_map.size)

      val year_arr = pg_year_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = PlayPgYearDistCnt(
          sn,
          stat_date,
          period,
          year_arr(i),
          pg_year_dist_map.get(year_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    playPgYearDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    playPgYearDistCntDF.registerTempTable("playPgYearDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.year,a.cnt,IF(b.c_cnt=0, 0, a.cnt/b.c_cnt)  as percent
        from
        (
        select stat_date,period,cluster_id,year,sum(cnt) as cnt  from playPgYearDistCnt group by stat_date,period,cluster_id,year
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from playPgYearDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_play_pg_year_dist_cnt_k" + clusterNum, false,
      SaveMode.Append)

  }
}
package com.avcdata.spark.job.konka

import java.util.regex.Pattern

import com.avcdata.spark.job.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object PlaysDataLoadHelper {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)


      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】 //不包含集数和第

      if (module.equals("电影") && "(第.+集)|(\\d集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFMovie(titleArr(i))

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") && !"([^0-9]+[0-9]+)|([ ]\\d*)|(_\\d*)|(-第\\d*集)|(第\\d*集)|(\\(第\\d*集\\))|(大结局)|(先导集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length if i != 3) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFPlays(titleArr(i), standard_name)
          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      if (module.equals("动画片")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFCartoon(titleArr(i), standard_name)
          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

        }

      }


      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////
      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFVariety(titleArr(i), standard_name)
          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


    })

    return "#"
  }

  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第"), result.indexOf("季") + 1)
    }else{
      result=""
    }
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.spark.job.coocaa

import java.util.regex.Pattern

import com.avcdata.spark.job.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object PlaysDataLoadHelper {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)


      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】 //不包含集数和第
      if (module.equals("电影") && "(第.+集)|(\\d集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFMovie(titleArr(i))

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") && !"([^0-9]+[0-9]+)|([ ]\\d*)|(_\\d*)|(-第\\d*集)|(第\\d*集)|(\\(第\\d*集\\))|(大结局)|(先导集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length if i != 3) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFPlays(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      if (module.equals("动画片")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFCartoon(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

        }

      }


      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////
      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFVariety(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


    })

    return "#"
  }

  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result = result.replaceAll("\\s","")
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim

    //判断标准名称是否包含 季
    if("[0-9]{1,2}$".r.findFirstIn(standard_name)!=None){
      result = result + getSeason(name)
    }

    result = result.replaceAll("\\s","")

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第")+1, result.indexOf("季"))
    }else{
      result=""
    }
    result= result.replaceAll("\\s","")
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    //去掉以0开头的数字
    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    //判断标准名称是否包含 季
    if("[0-9]{1,2}$".r.findFirstIn(standard_name)!=None){
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s","")
    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    //判断标准名称是否包含 季
    if("[0-9]{1,2}$".r.findFirstIn(standard_name)!=None){
      result = result + getSeason(name)
    }
    result = result.replaceAll("\\s","")
    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("^0","").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.spark.job.coocaa

import java.util.regex.Pattern

import com.avcdata.spark.job.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object PlaysDataLoadHelper01 {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)


      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】 //不包含集数和第

      if (module.equals("电影") && "(第.+集)|(\\d集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFMovie(titleArr(i))

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") && !"([^0-9]+[0-9]+)|([ ]\\d*)|(_\\d*)|(-第\\d*集)|(第\\d*集)|(\\(第\\d*集\\))|(大结局)|(先导集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length if i != 3) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFPlays(titleArr(i), standard_name)
          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      if (module.equals("动画片")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFCartoon(titleArr(i), standard_name)
          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

        }

      }


      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////
      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFVariety(titleArr(i), standard_name)
          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


    })

    return "#"
  }

  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第"), result.indexOf("季") + 1)
    }else{
      result=""
    }
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.spark.job.coocaa

import java.util.regex.Pattern

import com.avcdata.spark.job.util.Constant
import org.apache.spark.sql.Row

import scala.collection.mutable.ArrayBuffer

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗帮助类
  */
object PlaysDataLoadHelper02 {

  def main(args: Array[String]) {
    // println(getSimpleVideoNameOFMovieORCartoon("20160426贝乐虎儿歌 038 找朋友"))
    println(getSeason("季第季4第4"))
  }


  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractInfoFromVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      var dim_part = "unknow"

      //原始日志名称

      //数字转换匹配名称
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去掉版本匹配名称
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容名称
      val delBookMarkTitle = extractByBookMark(log_dim_title)


      //去特殊符号匹配名称
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      val titleArr = Array[String](log_dim_title, delVersionTitle, changeNumTitle,
        delBookMarkTitle, log_dim_title_spec)


      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】 //不包含集数和第

      if (module.equals("电影") && "(第.+集)|(\\d集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFMovie(titleArr(i))

          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFMovie(titleArr(i))
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if (module.equals("电视剧") && !"([^0-9]+[0-9]+)|([ ]\\d*)|(_\\d*)|(-第\\d*集)|(第\\d*集)|(\\(第\\d*集\\))|(大结局)|(先导集)".r.findFirstMatchIn(log_dim_title).isEmpty) {
        for (i <- 0 until titleArr.length if i != 3) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFPlays(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFPlays(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      if (module.equals("动画片")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFCartoon(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFCartoon(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }

        }

      }


      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////
      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if (module.equals("综艺")) {
        for (i <- 0 until titleArr.length) {
          if (original_name.length > 10 && titleArr(i).contains(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" +
              dim_part
          }
          val simpleName = getSimpleVideoNameOFVariety(titleArr(i), standard_name)
          //匹配原始名称
          if (simpleName.equals(original_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

          //匹配标准名称
          if (simpleName.equals(standard_name)) {
            dim_part = extractVideoPartOFVariety(log_dim_title)
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
          }

        }

      }


    })

    return "#"
  }

  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }


  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }

  /**
    * 提取电影名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFMovie(name: String): String = {
    var result = name
    result.trim
  }


  /**
    * 提取电视剧名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFPlays(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }



    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }

  def getSeason(log_title: String): String = {
    var result = log_title
    if (result.contains("第") && result.contains("季") && result.indexOf("第") < result.indexOf("季")) {
      result = result.substring(result.indexOf("第"), result.indexOf("季") + 1)
    }else{
      result=""
    }
    result
  }


  /**
    * 提取动画片名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFCartoon(name: String, standard_name: String): String = {
    var result = name

    if (result.startsWith("0")) {
      result = result.replaceAll("^0\\d*", "")
    }

    //TODO 第几季的匹配补上

    //去集数
    val partArr = Array[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.lastIndexOf(ele))
      }
    }

    //去集数0后面的无关信息
    //    if (result.contains("0")) {
    //      result = result.substring(0, result.lastIndexOf("0"))
    //    }

    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }


  /**
    * 提取综艺名称去影视库匹配
    *
    * @param name
    * @return
    */
  def getSimpleVideoNameOFVariety(name: String, standard_name: String): String = {
    var result = name

    //去之字  6(包含)个字以上的删除
    if (result.contains("之") && result.substring(result.indexOf("之")).length >= 6) {
      result = result.substring(0, result.indexOf("之"))
    }

    //去数字
    //    result = result.replaceAll("^[0-9]{8}", "")
    result = result.replaceAll("[0-9]{8}|[0-9]{2}|[0-9]{6}|0[0-9]+", "")

    //如果是八位提取  年份
    //    val matched = "[0-9]{8}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]",""))
    //    if (!result.isEmpty)
    //      result = matched.get.toString().trim.replaceAll("0","")


    //去集数
    val partArr = ArrayBuffer[String](
      " ", "：", "_", "第", "-第", "(第", "大结局", "先导集"
    )

    for (ele <- partArr) {
      if (result.contains(ele)) {
        result = result.substring(0, result.indexOf(ele))
      }
    }


    //    if("^[0-9]{8}".r.findFirstMatchIn(result).isEmpty){
    //      result = result.replaceAll("0[0-9]+", "")
    //    }

    //去空格
    result = result.trim
    if (standard_name.contains("第") && standard_name.contains("季"))
      result = result + getSeason(name)

    result
  }


  /**
    * 提取酷开视频名称中的集数（电影）
    */
  def extractVideoPartOFMovie(videoName: String): String = {
    var result = "unknow"
    result
  }


  /**
    * 提取酷开视频名称中的集数 (电视剧)
    */
  def extractVideoPartOFPlays(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,3}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim

    //////////////////////////////////////////////////////////////////
    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim

    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <100
    matched = "[0-9]{1,3}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("^0", "")

    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（动画片）
    */
  def extractVideoPartOFCartoon(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    //以数字结尾 位数<4 的为集数
    var matched = "[0-9]{1,4}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_|\\s", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )

    //电视剧的集数 三位数 <1000
    matched = "[0-9]{1,5}\\s+".r.findFirstMatchIn(result.replaceAll("[^0-9\\s]", ""))
    if (!matched.isEmpty)
      result = matched.get.toString().trim.replaceAll("0", "")


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")

    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 提取酷开视频名称中的集数（综艺）
    */
  def extractVideoPartOFVariety(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)


    //以数字结尾 位数<3 的为集数
    var matched = "[0-9]{1,2}$".r.findFirstMatchIn(videoName)
    if (!matched.isEmpty)
      return matched.get.toString().trim.replaceAll("[^0-9]", "")


    matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_|\\s", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }

    result = result.trim.replaceAll("^0", "").replaceAll("[^0-9]", "")
    if (result.trim.isEmpty) {
      result = "unknow"
    }
    result
  }


}
package com.avcdata.spark.job.konka

import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }


  def run(sc: SparkContext, currentDate: String) = {


    /////////////////////////test//////////////////////////
    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\plays\\KONKA\\livelauncher.log.2017-02-28"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    LZX1441W9036763D7YH1|112.103.100.198|LED55R6610U_1BOM|rtd2995d|55|三生三世十里桃花|CIBN|2017-02-27 08:05:44


    //TODO 读取数据文件
    //    val hdfsPath = "/user/hdfs/rsync/KONKA/" + currentDate+"/livelauncher.log." + currentDate

    //TODO 提取 过滤
    val initRDD = sc.textFile(hdfsPath).distinct()
//      .repartition(1).saveAsTextFile("livelauncher.log.2017-02-28-2")

    //      .map(line => {
    //        val cols = line.split("\\|")
    //        //        val cols  = line.split("[|]")
    //        val dim_sn = cols(0)
    //        val dim_name = cols(5)
    //        val dim_date = cols(7).split(" ")(0)
    //        val dim_hour = cols(7).split(" ")(1).substring(0, 2)
    //
    //        dim_sn + "\t" + dim_name + "\t" + dim_date + "\t" + dim_hour
    //      })
    //
    //      //TODO 补上集数、次数和时长  dim_title dim_awcid dim_year dim_model dim_crowd dim_region
    //      .map(line => {
    //      val cols = line.split("\t")
    //      val dim_sn = cols(0)
    //      val dim_name = cols(0)
    //      val dim_date = cols(0)
    //      val dim_hour = cols(0)
    //    })

    //TODO 取当天热度最高的类型的剧名

    //TODO 电影 电视剧 动画 综艺

    //TODO 关联


  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable


  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)



    /////////////////////////test//////////////////////////
    //        val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121     \\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    //TODO 读取数据文件
    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate + "/aowei_PlayerControl" + analysisDate + "_2.txt"
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate
    println(hdfsPath)
    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取 过滤
    val filterRdd = initRDD.distinct().map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct()

      //TODO 过滤
      .filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //      val timeStamp = cols(1).toLong
      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val isTimestampRight = ValidateUtils.isNumber(cols(1)) && !(cols(1).toLong < checkTime)


      val dim_title = cols(2)
      val dim_titleIsRight = !cols(2).isEmpty
      //      && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(3).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(4)
      val isDurationRight = !cols(4).isEmpty && ValidateUtils.isNumber(cols(4)) && cols(4).toLong >= 0

      isTimestampRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )



    //TODO 配对 合并
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp)

    })

    //    startRDD.foreach(println(_))

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val pairRDD = startRDD.join(stopRDD)

    //笛卡尔积配对并合并 =>阶段一完成
    val stageOneRDD = pairRDD
      //过滤startTime大于stopTime的
      .filter(tuple => {
      val valueLeftCols = tuple._2._1.split("\t")
      val starTime = valueLeftCols(0)

      val valueRightCols = tuple._2._2.split("\t")
      val stopTime = valueRightCols(0)

      starTime <= stopTime
    })
      .map(
        tuple => {
          val keyCols = tuple._1.split("\t")
          val sn = keyCols(0)
          val dim_title = keyCols(1)

          val valueLeftCols = tuple._2._1.split("\t")
          val starTime = valueLeftCols(0)

          val valueRightCols = tuple._2._2.split("\t")
          val stopTime = valueRightCols(0)

          val fact_duration = valueRightCols(1)

          (sn + "\t" + dim_title + "\t" + starTime, starTime + "\t" + stopTime + "\t" + fact_duration)
        }
      ).reduceByKey((pre, after) => {
      val preCols = pre.split("\t")
      val preDiff = preCols(1).toString.toLong - preCols(0).toString.toLong

      val afterCols = after.split("\t")
      val afterDiff = afterCols(1).toString.toLong - afterCols(0).toString.toLong

      var min = ""
      if (preDiff <= afterDiff) {
        min = pre
      } else {
        min = after
      }
      min
    }).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_title = keyCols(1)

        val valueCols = tuple._2.split("\t")
        val starTime = valueCols(0)

        val stopTime = valueCols(1)

        val fact_duration = valueCols(2)

        (sn + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    )
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //    “剧透”，“预告”，“抢先看”,“片花”，“片段”，“花絮”，“插曲”，“片尾曲”，“主题曲”，“广场舞”，“图书”，“独家策划”，“原创”，“搞笑”，“海报”“将映”，“剧照”

    //TODO 广播 film_properties表

    //////////////////////////////////test//////////////////////////////////////
    //    val sqlContext = new SQLContext(sc)
    //////////////////////////////////test//////////////////////////////////////

    ////////////////////////test/////////////////////////////
    //    val filmInfoArr = sc.textFile("E:\\aowei\\tracker-job\\doc\\data\\plays\\film_properties.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val id = cols(0)
    //        val original_name = cols(1)
    //        val standard_name = cols(2)
    //        val year = cols(3)
    //        val model = cols(4)
    //        val crowd = cols(5)
    //        val region = cols(6)
    //        Row(original_name, standard_name, model, id, year, crowd, region)
    //      }).collect
    ////////////////////////test/////////////////////////////

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect

    //    println("filmInfoArr.size=" + filmInfoArr.size)
    //    for(ele<-filmInfoArr){
    //      println(ele)
    //    }
//    val filmInfoArrBV = sc.broadcast(filmInfoArr)

    //fca3864d91ad		锦绣未央 第19集预告	1479700448481	1479700448481	65530
    val stageTwoRDD = stageOneRDD

      //TODO 过滤特定关键词
      .filter(line => {
      val cols = line.split("\t")
      !ValidateUtils.isContainsSpecWords(cols(1), keywordArr)
    }).distinct()
      .coalesce(1000, shuffle = false)

      .mapPartitions(lines => {

        val filmInfo = filmInfoArr
        lines
          //TODO 提取集数和日期
          //    fca3864d91ad	_yinhe_203364601	锦绣未央 第19集预告	日期  开始时间  70368(时长)
          .map(line => {
          val cols = line.split("\t")

          //终端sn
          val dim_sn = cols(0)

          //日志的视频名称
          val dim_name = cols(1)

          //TODO 通过资源名称获取其他信息
          var dim_title = "#"
          var dim_model = "#"
          var dim_awcid = "#"
          var dim_year = "#"
          var dim_crowd = "#"
          var dim_region = "#"
          var dim_part = "unknow"

          val filmProp = PlaysDataLoadHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfo)
          if (!filmProp.equals("#")) {
            val filmPropCols = filmProp.split("\t")
            //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
            dim_title = filmPropCols(0)
            dim_model = filmPropCols(1)
            dim_awcid = filmPropCols(2)
            dim_year = filmPropCols(3)
            dim_crowd = filmPropCols(4)
            dim_region = filmPropCols(5)
            dim_part = filmPropCols(6)
          }


          //集数
          //      val dim_part = PlaysDataLoadHelper.extractVideoPartOFCooCaa(cols(1))

          //日期
          val dim_date = currentDate

          //开始时间
          val startTime = cols(2).toLong

          //时长
          val fact_duration = cols(4).toLong

          //结束时间=开始时间+时长
          val stopTime = startTime + fact_duration

          dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + startTime + "\t" + stopTime + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

        })

      })


    //TODO 过滤掉不匹配的数据
    val stageTwoPassRDD = stageTwoRDD.filter(line => {
      val cols = line.split("\t")
      !cols(1).equals("#")
    })

    //    //TODO 不匹配的数据统计
//    val stageTwoUnPassRDD = stageTwoRDD.filter(line => {
//      val cols = line.split("\t")
//      cols(1).equals("#")
//    }).map(line => {
//      val cols = line.split("\t")
//      val dim_name = cols(11)
//      (dim_name, 1)
//    }).reduceByKey(_ + _).repartition(1).saveAsTextFile("/user/hdfs/rsync/playunpass/" + currentDate)

    // TODO 其他（提取资源名称 单独存放)

    //        stageTwoRDD.foreach(println(_))
    //
    //    println("清洗前记录数：" + initRDD.count())
    //    println("清洗后记录数：：" + stageTwoRDD.count())


    //TODO 获得Awcid
    //    val keyword2awcidArr = sc.textFile("/user/hive/warehouse/hr.db/tracker_plays_tag/tracker_plays_tag.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        (cols(4), cols(0))
    //      })
    //      .collect()

    //TODO 分时
    val splitTimeRdd = stageTwoPassRDD.flatMap(line => {
      val cols = line.split("\t")

      val dim_sn = cols(0)

      val dim_title = cols(1)

      //      val keyword = ValidateUtils.convertTitle2Keyword(cols(1))
      //
      //      for (keword2cid <- keyword2awcidArr) {
      //        if (keyword.equals(keword2cid._1)) {
      //          dim_awcid = keword2cid._2
      //        }
      //      }

      val dim_part = cols(2)
      val dim_date = cols(3)
      val startTime = cols(4).toLong
      val stopTime = cols(5).toLong
      val dim_model = cols(6)
      val dim_awcid = cols(7)
      val dim_year = cols(8)
      val dim_crowd = cols(9)
      val dim_region = cols(10)

      val dim_name = cols(11)

      //+ "\t" + dim_model+ "\t" + dim_awcid+ "\t" + dim_year+ "\t" + dim_crowd+ "\t" + dim_region


      val tmpArrayBuffer = TimeUtils.splitTimeByHour(dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year ++ "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name + "\t", startTime, stopTime)

      tmpArrayBuffer

    })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val dim_sn = cols(i)

        i = i + 1

        //市
        val dim_title = cols(i)
        i = i + 1


        val dim_part = cols(i)
        i = i + 1

        val dim_date = cols(i)
        i = i + 1
        val dim_model = cols(i)
        i = i + 1
        val dim_awcid = cols(i)
        i = i + 1
        val dim_year = cols(i)
        i = i + 1
        val dim_crowd = cols(i)
        i = i + 1
        val dim_region = cols(i)
        i = i + 1

        val dim_name = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        val other = dim_awcid + ";" + dim_model + ";" + dim_year + ";" + dim_crowd + ";" + dim_region + ";" + dim_name

        Row(dim_sn, dim_title, dim_part, dim_date, hour, launchCnt, duration, other)

      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("dim_sn", StringType, false),
        StructField("dim_title", StringType, false),
        StructField("dim_part", StringType, false),
        StructField("dim_date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false),
        StructField("other", StringType, false)
      )
    )


    //TODO 使用sql统计次数和整点时长
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_tmp")
    sqlContext.cacheTable("tb_tmp")

    val hTableName = "tracker_player_active_fact"
    //      val hTableName ="tracker_player_active_fact_test"

    //统计分析
    sqlContext.sql(
      """
       SELECT
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
           other,
         SUM (launchCnt),
         SUM (duration)
         FROM
         tb_tmp
         GROUP BY
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
          other
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      val duration = line(7).toString().toLong
      duration > 0 && duration <= 3600
    })

      /////////////////////////////////////////test///////////////////
      //      .foreach(println(_))

      /////////////////////////////////////////test///////////////////
      //TODO 写入到Hbase
      .foreachPartition(lines => {


      val mutator = HBaseUtils.getMutator(hTableName)


      try {

        lines.foreach(line => {
          var i = 0

          val dim_sn = line(i).toString
          i = i + 1

          val dim_title = line(i).toString
          i = i + 1

          val dim_part = line(i).toString
          i = i + 1

          val dim_date = line(i).toString
          i = i + 1

          val dim_hour = line(i).toString
          i = i + 1

          val other = line(i).toString
          val cols = other.split(";")
          val dim_awcid = cols(0)
          val dim_model = cols(1)
          var dim_year = cols(2)
          if (dim_year.equals("unknow")) dim_year = ""
          val dim_crowd = cols(3)
          var dim_region = cols(4)
          if (dim_region.equals("unknow")) dim_region = ""
          var dim_name = cols(5)

          i = i + 1

          val fact_vv = line(i).toString
          i = i + 1

          //dim_model,dim_year,dim_crowd,dim_region
          val fact_duration = line(i).toString


          //关联terminal 加上brand last_poweron area province city siz model  license citylevel

          val sortedLine = dim_sn + "\t" + dim_title + "\t" + dim_awcid + "\t" + dim_part + "\t" + dim_date + "\t" + dim_hour + "\t" + fact_vv + "\t" + fact_duration + "\t" + dim_model + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

          //          println(sortedLine)
          val brand = "CC"
          mutator.mutate(HBaseUtils.getPut_Plays(brand, sortedLine))
        })
        mutator.flush()
      } finally {
        mutator.close()
      }
    }
    )
    println("#####写入到Hbase######hbase table" + hTableName)
    sqlContext.uncacheTable("tb_tmp")
  }
}
package com.avcdata.spark.job.konka

import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysDataLoadJob01 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }



  def run(sc: SparkContext, currentDate: String) = {


    /////////////////////////test//////////////////////////
    val hdfsPath = "E:\\aowei\\tracker-job\\doc\\data\\plays\\livelauncher.log.2017-02-27"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    LZX1441W9036763D7YH1|112.103.100.198|LED55R6610U_1BOM|rtd2995d|55|三生三世十里桃花|CIBN|2017-02-27 08:05:44


    //TODO 读取数据文件
    //    val hdfsPath = "/user/hdfs/rsync/KONKA/" + currentDate+"/livelauncher.log." + currentDate

    //TODO 提取 过滤
    val initRDD = sc.textFile(hdfsPath).distinct()
      .map(line => {
        val cols = line.split("\\|")
        //        val cols  = line.split("[|]")
        val dim_sn = cols(0)
        val dim_name = cols(5)
        val dim_date = cols(7).split(" ")(0)
        val dim_hour = cols(7).split(" ")(1).substring(0, 2)

         dim_name
      }).distinct().repartition(1).saveAsTextFile("out.txt")

    //TODO 取当天热度最高的类型的剧名

    //TODO 电影 电视剧 动画 综艺

    //TODO 关联


  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysDataLoadJob01 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-12-01")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable

  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/ aowei_PlayerControl" + analysisDate

    println(hdfsPath)



    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()
    val sqlContext = new SQLContext(sc);
    import sqlContext.implicits._

    /////////////////////////test//////////////////////////

    //    val initRDD = sc.textFile("S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161018\\aowei_appStatus20161018.txt").distinct()
    /////////////////////////test//////////////////////////////

    //过滤
    val filterRdd = initRDD.filter(line => {
      val cols = line.split("\t")
      ValidateUtils.isContainsSpeciChar(cols(5))
    }
    ).map(line => {
      val cols = line.split("\t")
      cols(5)
    }).distinct()


    //转换成dataFrame
    var df = filterRdd.map(line => {
      val cols = line.split("\t")
      PlayName("1", cols(5).trim().toString)
    }
    ).toDF()


    //写入到Mysql
    JdbcUtils.writeDF2Mysql(sc, df, Helper.mysqlConf, "wanghongran_auth", "plays", false)

  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysDataLoadJob02 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable

  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/ aowei_PlayerControl" + analysisDate

    /////////////////////////test//////////////////////////
    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121\\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////


    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()
    val sqlContext = new SQLContext(sc);


    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯
    //////////////////////////////提取 过滤/////////////////////////////////////////////////
    val filterRdd = initRDD.map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(5).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct().filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val timeStamp = cols(1).toString.toLong
      val isTimestampRight = !(timeStamp < checkTime)


      //资源ID 可能为空
      val dim_awcid = cols(2)
      val isAwcidRight = !cols(2).isEmpty

      val dim_title = cols(3)
      val dim_titleIsRight = !cols(3).isEmpty && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(4).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(5)

      isTimestampRight && isAwcidRight && dim_titleIsRight && operateIsRight
    }
    )

    //      .sortBy(x => x).foreach(println(_))

    ////////////////////////////////////////////////////配对 合并//////////////////////////////////////
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(4).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_awcid = cols(2)
      val dim_title = cols(3)
      val operate = cols(4).toString
      val fact_duration = cols(5)

      (sn + "\t" + dim_awcid + "\t" + dim_title, timeStamp + "\t" + operate + "\t" + fact_duration)

    })

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(4).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_awcid = cols(2)
      val dim_title = cols(3)
      val operate = cols(4).toString
      val fact_duration = cols(5)

      (sn + "\t" + dim_awcid + "\t" + dim_title, timeStamp + "\t" + operate + "\t" + fact_duration)

    })

    startRDD.join(stopRDD).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_awcid = keyCols(1)
        val dim_title = keyCols(2)

        val valueLeftCols = tuple._2._1.split("\t")
        val starTime = valueLeftCols(0)

        val valueRightCols = tuple._2._2.split("\t")
        val stopTime = valueRightCols(0)

        val fact_duration = valueRightCols(2)

        (sn + "\t" + dim_awcid + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    ).foreach(println(_))

    //      .foreach(println(_))


    //    排序
    //
    //    配对
    //
    //    合并
    //
    //    时长
    //
    //    次数
    //
    //    分时
    //
    //    累加


    //分时
    //    filterRdd.map(line => {
    //      val cols = line.split("\t")
    //
    //      val dim_sn = ""
    //
    //      //资源名称
    //      val dim_title = ""
    //
    //      //标准ID
    //      val dim_awcid = ""
    //
    //      //集数
    //      val dim_part = ""
    //
    //      //日期
    //      val dim_date = ""
    //
    //
    //      //整点
    //      val dim_hour = ""
    //
    //      //vv
    //      val fact_vv = ""
    //
    //      //时长
    //      val fact_duration = ""
    //
    //
    //
    //      cols(5)
    //    })


    //存入到Hbase

  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.log4j.Logger
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysDataLoadJob03 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable

  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)

    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/ aowei_PlayerControl" + analysisDate

    /////////////////////////test//////////////////////////
    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121\\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////


    //合并文件 略
    val initRDD = sc.textFile(hdfsPath).distinct()
    val sqlContext = new SQLContext(sc);


    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯
    //////////////////////////////提取 过滤/////////////////////////////////////////////////
    val filterRdd = initRDD.map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(5).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct().filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val timeStamp = cols(1).toString.toLong
      val isTimestampRight = !(timeStamp < checkTime)


      //资源ID 可能为空
      val dim_awcid = cols(2)
      val isAwcidRight = !cols(2).isEmpty

      val dim_title = cols(3)
      val dim_titleIsRight = !cols(3).isEmpty && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(4).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(5)
      val isDurationRight = !cols(5).isEmpty

      isTimestampRight && isAwcidRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )

    //      .sortBy(x => x).foreach(println(_))

    ////////////////////////////////////////////////////配对 合并//////////////////////////////////////
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(4).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_awcid = cols(2)
      val dim_title = cols(3)
      val operate = cols(4).toString
      val fact_duration = cols(5)

      (sn + "\t" + dim_awcid + "\t" + dim_title, timeStamp)

    })

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(4).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_awcid = cols(2)
      val dim_title = cols(3)
      val operate = cols(4).toString
      val fact_duration = cols(5)

      (sn + "\t" + dim_awcid + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val joinRDD = startRDD.join(stopRDD)


    //取出start信息 去重 排序
    val startFiltedRDD = joinRDD.map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_awcid = keyCols(1)
        val dim_title = keyCols(2)

        val valueLeftCols = tuple._2._1.split("\t")
        val starTime = valueLeftCols(0)

        sn + "\t" + dim_awcid + "\t" + dim_title + "\t" + starTime + "\t" + "Start"
      }
    ).distinct()


    //取出stop信息 去重 排序
    // stopTime + "\t" + fact_duration
    val stopFiltedRDD = joinRDD.map(
      tuple => {

        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_awcid = keyCols(1)
        val dim_title = keyCols(2)

        val valueRightCols = tuple._2._2.split("\t")
        val stopTime = valueRightCols(0)

        val fact_duration = valueRightCols(1)

        sn + "\t" + dim_awcid + "\t" + dim_title + "\t" + stopTime + "\t" + fact_duration + "\t" + "Stop"
      }
    ).distinct()

    //      .foreach(println(_))
    startFiltedRDD.union(stopFiltedRDD).sortBy(x => x)

      .foreach(println(_))


    //    排序

    //    配对
    //
    //    合并
    //
    //    时长
    //
    //    次数
    //
    //    分时
    //
    //    累加


    //分时
    //    filterRdd.map(line => {
    //      val cols = line.split("\t")
    //
    //      val dim_sn = ""
    //
    //      //资源名称
    //      val dim_title = ""
    //
    //      //标准ID
    //      val dim_awcid = ""
    //
    //      //集数
    //      val dim_part = ""
    //
    //      //日期
    //      val dim_date = ""
    //
    //
    //      //整点
    //      val dim_hour = ""
    //
    //      //vv
    //      val fact_vv = ""
    //
    //      //时长
    //      val fact_duration = ""
    //
    //
    //
    //      cols(5)
    //    })


    //存入到Hbase

  }
}
package com.avcdata.spark.job.konka

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysUnpassDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable


  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)



    /////////////////////////test//////////////////////////
    //        val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121     \\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    //TODO 读取数据文件
    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate + "/aowei_PlayerControl" + analysisDate + "_2.txt"
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate
    println(hdfsPath)
    val initRDD = sc.textFile(hdfsPath)

    //TODO 提取 过滤
    val filterRdd = initRDD.distinct().map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct()

      //TODO 过滤
      .filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //      val timeStamp = cols(1).toLong
      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val isTimestampRight = ValidateUtils.isNumber(cols(1)) && !(cols(1).toLong < checkTime)


      val dim_title = cols(2)
      val dim_titleIsRight = !cols(2).isEmpty
      //      && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(3).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(4)
      val isDurationRight = !cols(4).isEmpty && ValidateUtils.isNumber(cols(4)) && cols(4).toLong >= 0

      isTimestampRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )



    //TODO 配对 合并
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp)

    })

    //    startRDD.foreach(println(_))

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val pairRDD = startRDD.join(stopRDD)

    //笛卡尔积配对并合并 =>阶段一完成
    val stageOneRDD = pairRDD
      //过滤startTime大于stopTime的
      .filter(tuple => {
      val valueLeftCols = tuple._2._1.split("\t")
      val starTime = valueLeftCols(0)

      val valueRightCols = tuple._2._2.split("\t")
      val stopTime = valueRightCols(0)

      starTime <= stopTime
    })
      .map(
        tuple => {
          val keyCols = tuple._1.split("\t")
          val sn = keyCols(0)
          val dim_title = keyCols(1)

          val valueLeftCols = tuple._2._1.split("\t")
          val starTime = valueLeftCols(0)

          val valueRightCols = tuple._2._2.split("\t")
          val stopTime = valueRightCols(0)

          val fact_duration = valueRightCols(1)

          (sn + "\t" + dim_title + "\t" + starTime, starTime + "\t" + stopTime + "\t" + fact_duration)
        }
      ).reduceByKey((pre, after) => {
      val preCols = pre.split("\t")
      val preDiff = preCols(1).toString.toLong - preCols(0).toString.toLong

      val afterCols = after.split("\t")
      val afterDiff = afterCols(1).toString.toLong - afterCols(0).toString.toLong

      var min = ""
      if (preDiff <= afterDiff) {
        min = pre
      } else {
        min = after
      }
      min
    }).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_title = keyCols(1)

        val valueCols = tuple._2.split("\t")
        val starTime = valueCols(0)

        val stopTime = valueCols(1)

        val fact_duration = valueCols(2)

        (sn + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    )
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //    “剧透”，“预告”，“抢先看”,“片花”，“片段”，“花絮”，“插曲”，“片尾曲”，“主题曲”，“广场舞”，“图书”，“独家策划”，“原创”，“搞笑”，“海报”“将映”，“剧照”

    //TODO 广播 film_properties表

    //////////////////////////////////test//////////////////////////////////////
    //    val sqlContext = new SQLContext(sc)
    //////////////////////////////////test//////////////////////////////////////

    ////////////////////////test/////////////////////////////
    //    val filmInfoArr = sc.textFile("E:\\aowei\\tracker-job\\doc\\data\\plays\\film_properties.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val id = cols(0)
    //        val original_name = cols(1)
    //        val standard_name = cols(2)
    //        val year = cols(3)
    //        val model = cols(4)
    //        val crowd = cols(5)
    //        val region = cols(6)
    //        Row(original_name, standard_name, model, id, year, crowd, region)
    //      }).collect
    ////////////////////////test/////////////////////////////

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect
    //    println("filmInfoArr.size=" + filmInfoArr.size)
    //    for(ele<-filmInfoArr){
    //      println(ele)
    //    }
    val filmInfoArrBV = sc.broadcast(filmInfoArr)

    //fca3864d91ad		锦绣未央 第19集预告	1479700448481	1479700448481	65530
    val stageTwoRDD = stageOneRDD

      //TODO 过滤特定关键词
      .filter(line => {
      val cols = line.split("\t")
      !ValidateUtils.isContainsSpecWords(cols(1), keywordArr)
    }).distinct()

      //TODO 提取集数和日期
      //    fca3864d91ad	_yinhe_203364601	锦绣未央 第19集预告	日期  开始时间  70368(时长)
      .map(line => {
      val cols = line.split("\t")

      //终端sn
      val dim_sn = cols(0)

      //日志的视频名称
      var dim_name = cols(1)

      //TODO 通过资源名称获取其他信息
      var dim_title = "#"
      var dim_model = "#"
      var dim_awcid = "#"
      var dim_year = "#"
      var dim_crowd = "#"
      var dim_region = "#"
      var dim_part = "unknow"

      val filmProp = PlaysDataLoadHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfoArrBV.value)
      if (!filmProp.equals("#")) {
        val filmPropCols = filmProp.split("\t")
        //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
        dim_title = filmPropCols(0)
        dim_model = filmPropCols(1)
        dim_awcid = filmPropCols(2)
        dim_year = filmPropCols(3)
        dim_crowd = filmPropCols(4)
        dim_region = filmPropCols(5)
        dim_part = filmPropCols(6)
      }


      //集数
      //      val dim_part = PlaysDataLoadHelper.extractVideoPartOFCooCaa(cols(1))

      //日期
      val dim_date = currentDate

      //开始时间
      val startTime = cols(2).toLong

      //时长
      val fact_duration = cols(4).toLong

      //结束时间=开始时间+时长
      val stopTime = startTime + fact_duration

      dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + startTime + "\t" + stopTime + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

    })

    //    //TODO 过滤掉不匹配的数据
    //    val stageTwoPassRDD = stageTwoRDD.filter(line => {
    //      val cols = line.split("\t")
    //      !cols(1).equals("#")
    //    })

    //    //TODO 不匹配的数据统计
    val stageTwoUnPassRDD = stageTwoRDD.filter(line => {
      val cols = line.split("\t")
      cols(1).equals("#")
    })

    // TODO 其他（提取资源名称 单独存放)

    //        stageTwoRDD.foreach(println(_))
    //
    //    println("清洗前记录数：" + initRDD.count())
    //    println("清洗后记录数：：" + stageTwoRDD.count())


    //TODO 获得Awcid
    //    val keyword2awcidArr = sc.textFile("/user/hive/warehouse/hr.db/tracker_plays_tag/tracker_plays_tag.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        (cols(4), cols(0))
    //      })
    //      .collect()

    //TODO 分时
    val splitTimeRdd = stageTwoUnPassRDD.flatMap(line => {
      val cols = line.split("\t")

      val dim_sn = cols(0)

      val dim_title = cols(1)

      //      val keyword = ValidateUtils.convertTitle2Keyword(cols(1))
      //
      //      for (keword2cid <- keyword2awcidArr) {
      //        if (keyword.equals(keword2cid._1)) {
      //          dim_awcid = keword2cid._2
      //        }
      //      }

      val dim_part = cols(2)
      val dim_date = cols(3)
      val startTime = cols(4).toLong
      val stopTime = cols(5).toLong
      val dim_model = cols(6)
      val dim_awcid = cols(7)
      val dim_year = cols(8)
      val dim_crowd = cols(9)
      val dim_region = cols(10)

      val dim_name = cols(11)


      //+ "\t" + dim_model+ "\t" + dim_awcid+ "\t" + dim_year+ "\t" + dim_crowd+ "\t" + dim_region


      val tmpArrayBuffer = TimeUtils.splitTimeByHour(dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year ++ "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name + "\t", startTime, stopTime)

      tmpArrayBuffer

    })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val dim_sn = cols(i)

        i = i + 1

        //市
        val dim_title = cols(i)
        i = i + 1


        val dim_part = cols(i)
        i = i + 1

        val dim_date = cols(i)
        i = i + 1
        val dim_model = cols(i)
        i = i + 1
        val dim_awcid = cols(i)
        i = i + 1
        val dim_year = cols(i)
        i = i + 1
        val dim_crowd = cols(i)
        i = i + 1
        val dim_region = cols(i)
        i = i + 1

        val dim_name = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        val other = dim_awcid + ";" + dim_model + ";" + dim_year + ";" + dim_crowd + ";" + dim_region + ";" + dim_name

        Row(dim_sn, dim_title, dim_part, dim_date, hour, launchCnt, duration, other)

      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("dim_sn", StringType, false),
        StructField("dim_title", StringType, false),
        StructField("dim_part", StringType, false),
        StructField("dim_date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false),
        StructField("other", StringType, false)
      )
    )


    //TODO 使用sql统计次数和整点时长
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_tmp")
    sqlContext.cacheTable("tb_tmp")


    //统计分析
    sqlContext.sql(
      """
       SELECT
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
           other,
         SUM (launchCnt),
         SUM (duration)
         FROM
         tb_tmp
         GROUP BY
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
          other
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(7).toString().toLong <= 3600
    })

      /////////////////////////////////////////test///////////////////
      //      .foreach(println(_))

      /////////////////////////////////////////test///////////////////
      //TODO 写入到Hbase
      .foreachPartition(lines => {

      //                val mutator = HBaseUtils.getMutator("tracker_player_active_fact")
      val mutator = HBaseUtils.getMutator("tracker_player_active_fact_unpass")

      try {

        lines.foreach(line => {
          var i = 0

          val dim_sn = line(i).toString
          i = i + 1

          val dim_title = line(i).toString
          i = i + 1

          val dim_part = line(i).toString
          i = i + 1

          val dim_date = line(i).toString
          i = i + 1

          val dim_hour = line(i).toString
          i = i + 1

          val other = line(i).toString
          val cols = other.split(";")
          val dim_awcid = cols(0)
          val dim_model = cols(1)
          var dim_year = cols(2)
          if (dim_year.equals("unknow")) dim_year = ""
          val dim_crowd = cols(3)
          var dim_region = cols(4)
          if (dim_region.equals("unknow")) dim_region = ""
          var dim_name = cols(5)

          i = i + 1

          val fact_vv = line(i).toString
          i = i + 1

          //dim_model,dim_year,dim_crowd,dim_region
          val fact_duration = line(i).toString


          //关联terminal 加上brand last_poweron area province city siz model  license citylevel

          val sortedLine = dim_sn + "\t" + dim_title + "\t" + dim_awcid + "\t" + dim_part + "\t" + dim_date + "\t" + dim_hour + "\t" + fact_vv + "\t" + fact_duration + "\t" + dim_model + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

          //          println(sortedLine)
          val brand = "KO"
          mutator.mutate(HBaseUtils.getPut_Plays(brand,sortedLine))
        })
        mutator.flush()
      } finally {
        mutator.close()
      }
    }
    )
    sqlContext.uncacheTable("tb_tmp")
  }
}

package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysUnpassDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable


  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)



    /////////////////////////test//////////////////////////
    //        val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121     \\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    //TODO 读取数据文件
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate + "/aowei_PlayerControl" + analysisDate + "_1.txt"
    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate
    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取 过滤
    val filterRdd = initRDD.distinct().map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct()

      //TODO 过滤
      .filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //      val timeStamp = cols(1).toLong
      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val isTimestampRight = ValidateUtils.isNumber(cols(1)) && !(cols(1).toLong < checkTime)


      val dim_title = cols(2)
      val dim_titleIsRight = !cols(2).isEmpty
      //      && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(3).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(4)
      val isDurationRight = !cols(4).isEmpty && ValidateUtils.isNumber(cols(4)) && cols(4).toLong >= 0

      isTimestampRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )



    //TODO 配对 合并
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp)

    })

    //    startRDD.foreach(println(_))

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val pairRDD = startRDD.join(stopRDD)

    //笛卡尔积配对并合并 =>阶段一完成
    val stageOneRDD = pairRDD
      //过滤startTime大于stopTime的
      .filter(tuple => {
      val valueLeftCols = tuple._2._1.split("\t")
      val starTime = valueLeftCols(0)

      val valueRightCols = tuple._2._2.split("\t")
      val stopTime = valueRightCols(0)

      starTime <= stopTime
    })
      .map(
        tuple => {
          val keyCols = tuple._1.split("\t")
          val sn = keyCols(0)
          val dim_title = keyCols(1)

          val valueLeftCols = tuple._2._1.split("\t")
          val starTime = valueLeftCols(0)

          val valueRightCols = tuple._2._2.split("\t")
          val stopTime = valueRightCols(0)

          val fact_duration = valueRightCols(1)

          (sn + "\t" + dim_title + "\t" + starTime, starTime + "\t" + stopTime + "\t" + fact_duration)
        }
      ).reduceByKey((pre, after) => {
      val preCols = pre.split("\t")
      val preDiff = preCols(1).toString.toLong - preCols(0).toString.toLong

      val afterCols = after.split("\t")
      val afterDiff = afterCols(1).toString.toLong - afterCols(0).toString.toLong

      var min = ""
      if (preDiff <= afterDiff) {
        min = pre
      } else {
        min = after
      }
      min
    }).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_title = keyCols(1)

        val valueCols = tuple._2.split("\t")
        val starTime = valueCols(0)

        val stopTime = valueCols(1)

        val fact_duration = valueCols(2)

        (sn + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    )
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //    “剧透”，“预告”，“抢先看”,“片花”，“片段”，“花絮”，“插曲”，“片尾曲”，“主题曲”，“广场舞”，“图书”，“独家策划”，“原创”，“搞笑”，“海报”“将映”，“剧照”

    //TODO 广播 film_properties表

    //////////////////////////////////test//////////////////////////////////////
    //    val sqlContext = new SQLContext(sc)
    //////////////////////////////////test//////////////////////////////////////

    ////////////////////////test/////////////////////////////
    //    val filmInfoArr = sc.textFile("E:\\aowei\\tracker-job\\doc\\data\\plays\\film_properties.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val id = cols(0)
    //        val original_name = cols(1)
    //        val standard_name = cols(2)
    //        val year = cols(3)
    //        val model = cols(4)
    //        val crowd = cols(5)
    //        val region = cols(6)
    //        Row(original_name, standard_name, model, id, year, crowd, region)
    //      }).collect
    ////////////////////////test/////////////////////////////

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect
    //    println("filmInfoArr.size=" + filmInfoArr.size)
    //    for(ele<-filmInfoArr){
    //      println(ele)
    //    }
    val filmInfoArrBV = sc.broadcast(filmInfoArr)

    //fca3864d91ad		锦绣未央 第19集预告	1479700448481	1479700448481	65530
    val stageTwoRDD = stageOneRDD

      //TODO 过滤特定关键词
      .filter(line => {
      val cols = line.split("\t")
      !ValidateUtils.isContainsSpecWords(cols(1), keywordArr)
    }).distinct()

      //TODO 提取集数和日期
      //    fca3864d91ad	_yinhe_203364601	锦绣未央 第19集预告	日期  开始时间  70368(时长)
      .map(line => {
      val cols = line.split("\t")

      //终端sn
      val dim_sn = cols(0)

      //日志的视频名称
      var dim_name = cols(1)

      //TODO 通过资源名称获取其他信息
      var dim_title = "#"
      var dim_model = "#"
      var dim_awcid = "#"
      var dim_year = "#"
      var dim_crowd = "#"
      var dim_region = "#"
      var dim_part = "unknow"

      val filmProp = PlaysDataLoadHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfoArrBV.value)
      if (!filmProp.equals("#")) {
        val filmPropCols = filmProp.split("\t")
        //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
        dim_title = filmPropCols(0)
        dim_model = filmPropCols(1)
        dim_awcid = filmPropCols(2)
        dim_year = filmPropCols(3)
        dim_crowd = filmPropCols(4)
        dim_region = filmPropCols(5)
        dim_part = filmPropCols(6)
      }


      //集数
      //      val dim_part = PlaysDataLoadHelper.extractVideoPartOFCooCaa(cols(1))

      //日期
      val dim_date = currentDate

      //开始时间
      val startTime = cols(2).toLong

      //时长
      val fact_duration = cols(4).toLong

      //结束时间=开始时间+时长
      val stopTime = startTime + fact_duration

      dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + startTime + "\t" + stopTime + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

    })

    //    //TODO 过滤掉不匹配的数据
    //    val stageTwoPassRDD = stageTwoRDD.filter(line => {
    //      val cols = line.split("\t")
    //      !cols(1).equals("#")
    //    })

    //    //TODO 不匹配的数据统计
    val stageTwoUnPassRDD = stageTwoRDD.filter(line => {
      val cols = line.split("\t")
      cols(1).equals("#")
    })
  }
}

package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysUnpassDataLoadJob01 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable


  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)



    /////////////////////////test//////////////////////////
    //    val hdfsPath = "S:\\奥维云网\\code\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121\\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    //TODO 读取数据文件
    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate + "/aowei_PlayerControl" + analysisDate + "_1.txt"
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate
    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取 过滤
    val filterRdd = initRDD.distinct().map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct()

      //TODO 过滤
      .filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //      val timeStamp = cols(1).toLong
      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val isTimestampRight = ValidateUtils.isNumber(cols(1)) && !(cols(1).toLong < checkTime)


      val dim_title = cols(2)
      val dim_titleIsRight = !cols(2).isEmpty
      //      && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(3).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(4)
      val isDurationRight = !cols(4).isEmpty && ValidateUtils.isNumber(cols(4)) && cols(4).toLong >= 0

      isTimestampRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )



    //TODO 配对 合并
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp)

    })

    //    startRDD.foreach(println(_))

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val pairRDD = startRDD.join(stopRDD)

    //笛卡尔积配对并合并 =>阶段一完成
    val stageOneRDD = pairRDD
      //过滤startTime大于stopTime的
      .filter(tuple => {
      val valueLeftCols = tuple._2._1.split("\t")
      val starTime = valueLeftCols(0)

      val valueRightCols = tuple._2._2.split("\t")
      val stopTime = valueRightCols(0)

      starTime <= stopTime
    })
      .map(
        tuple => {
          val keyCols = tuple._1.split("\t")
          val sn = keyCols(0)
          val dim_title = keyCols(1)

          val valueLeftCols = tuple._2._1.split("\t")
          val starTime = valueLeftCols(0)

          val valueRightCols = tuple._2._2.split("\t")
          val stopTime = valueRightCols(0)

          val fact_duration = valueRightCols(1)

          (sn + "\t" + dim_title + "\t" + starTime, starTime + "\t" + stopTime + "\t" + fact_duration)
        }
      ).reduceByKey((pre, after) => {
      val preCols = pre.split("\t")
      val preDiff = preCols(1).toString.toLong - preCols(0).toString.toLong

      val afterCols = after.split("\t")
      val afterDiff = afterCols(1).toString.toLong - afterCols(0).toString.toLong

      var min = ""
      if (preDiff <= afterDiff) {
        min = pre
      } else {
        min = after
      }
      min
    }).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_title = keyCols(1)

        val valueCols = tuple._2.split("\t")
        val starTime = valueCols(0)

        val stopTime = valueCols(1)

        val fact_duration = valueCols(2)

        (sn + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    )
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //    “剧透”，“预告”，“抢先看”,“片花”，“片段”，“花絮”，“插曲”，“片尾曲”，“主题曲”，“广场舞”，“图书”，“独家策划”，“原创”，“搞笑”，“海报”“将映”，“剧照”

    //TODO 广播 film_properties表

    //////////////////////////////////test//////////////////////////////////////
    //    val sqlContext = new SQLContext(sc)
    //////////////////////////////////test//////////////////////////////////////

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr" +
      ".film_properties").collect
    println("filmInfoArr.size=" + filmInfoArr.size)
    val filmInfoArrBV = sc.broadcast(filmInfoArr)

    //fca3864d91ad		锦绣未央 第19集预告	1479700448481	1479700448481	65530
    val stageTwoRDD = stageOneRDD
      .filter(line => {
        val cols = line.split("\t")
        !ValidateUtils.isContainsSpecWords(cols(1), keywordArr)
      })

      //TODO 提取集数和日期
      //    fca3864d91ad	_yinhe_203364601	锦绣未央 第19集预告	日期  开始时间  70368(时长)
      .map(line => {
      val cols = line.split("\t")

      //终端sn
      val dim_sn = cols(0)

      //TODO 通过资源名称获取其他信息
      var dim_title = "#"
      var dim_model = "#"
      var dim_awcid = "#"
      var dim_year = "#"
      var dim_crowd = "#"
      var dim_region = "#"
      var dim_part = "#"

      val filmProp = PlaysDataLoadHelper.extractInfoFromVideoNameOFCooCaa(cols(1), filmInfoArrBV.value)
      if (!filmProp.equals("#")) {
        val filmPropCols = filmProp.split("\t")
        //original_name+"\t"+model+"\t"+id+"\t"+year+"\t"+crowd+"\t"+region
        dim_title = filmPropCols(0)
        dim_model = filmPropCols(1)
        dim_awcid = filmPropCols(2)
        dim_year = filmPropCols(3)
        dim_crowd = filmPropCols(4)
        dim_region = filmPropCols(5)
        dim_part=filmPropCols(6)
      }


      //集数
//      val dim_part = PlaysDataLoadHelper.extractVideoPartOFCooCaa(cols(1))

      //日期
      val dim_date = currentDate

      //开始时间
      val startTime = cols(2).toLong

      //时长
      val fact_duration = cols(4).toLong

      //结束时间=开始时间+时长
      val stopTime = startTime + fact_duration

      dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + startTime + "\t" + stopTime + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region

    })


        //TODO 不匹配的数据统计
        val stageTwoUnPassRDD = stageTwoRDD
          .filter(line => {
          val cols = line.split("\t")
          cols(1).equals("#")
        })

    // TODO 其他（提取资源名称 单独存放)

    //        stageTwoRDD.foreach(println(_))
    //
    //    println("清洗前记录数：" + initRDD.count())
    //    println("清洗后记录数：：" + stageTwoRDD.count())


    //TODO 获得Awcid
    //    val keyword2awcidArr = sc.textFile("/user/hive/warehouse/hr.db/tracker_plays_tag/tracker_plays_tag.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        (cols(4), cols(0))
    //      })
    //      .collect()

    //TODO 分时
    val splitTimeRdd = stageTwoUnPassRDD.flatMap(line => {
      val cols = line.split("\t")

      val dim_sn = cols(0)

      val dim_title = cols(1)

      //      val keyword = ValidateUtils.convertTitle2Keyword(cols(1))
      //
      //      for (keword2cid <- keyword2awcidArr) {
      //        if (keyword.equals(keword2cid._1)) {
      //          dim_awcid = keword2cid._2
      //        }
      //      }

      val dim_part = cols(2)
      val dim_date = cols(3)
      val startTime = cols(4).toLong
      val stopTime = cols(5).toLong
      val dim_model = cols(6)
      val dim_awcid = cols(7)
      val dim_year = cols(8)
      val dim_crowd = cols(9)
      val dim_region = cols(10)

      //+ "\t" + dim_model+ "\t" + dim_awcid+ "\t" + dim_year+ "\t" + dim_crowd+ "\t" + dim_region


      val tmpArrayBuffer = TimeUtils.splitTimeByHour(dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year ++ "\t" + dim_crowd + "\t" + dim_region + "\t", startTime, stopTime)

      tmpArrayBuffer

    })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val dim_sn = cols(i)

        i = i + 1

        //市
        val dim_title = cols(i)
        i = i + 1


        val dim_part = cols(i)
        i = i + 1

        val dim_date = cols(i)
        i = i + 1
        val dim_model = cols(i)
        i = i + 1
        val dim_awcid = cols(i)
        i = i + 1
        val dim_year = cols(i)
        i = i + 1
        val dim_crowd = cols(i)
        i = i + 1
        val dim_region = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        val other = dim_awcid + ";" + dim_model + ";" + dim_year + ";" + dim_crowd + ";" + dim_region

        Row(dim_sn, dim_title, dim_part, dim_date, hour, launchCnt, duration, other)

      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("dim_sn", StringType, false),
        StructField("dim_title", StringType, false),
        StructField("dim_part", StringType, false),
        StructField("dim_date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false),
        StructField("other", StringType, false)
      )
    )


    //TODO 使用sql统计次数和整点时长
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_tmp")
    sqlContext.cacheTable("tb_tmp")


    //统计分析
    sqlContext.sql(
      """
       SELECT
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
           other,
         SUM (launchCnt),
         SUM (duration)
         FROM
         tb_tmp
         GROUP BY
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
          other
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(7).toString().toLong <= 3600
    })

      /////////////////////////////////////////test///////////////////
//      .foreach(println(_))
    /////////////////////////////////////////test///////////////////


    //TODO 写入到Hbase
      .foreachPartition(lines => {

      //      val mutator = HBaseUtils.getMutator("tracker_player_active_fact")
//      val mutator = HBaseUtils.getMutator("tracker_player_active_fact_test")
      val mutator = HBaseUtils.getMutator("tracker_player_active_fact_unpass")

      try {

        lines.foreach(line => {
          var i = 0

          val dim_sn = line(i).toString
          i = i + 1

          val dim_title = line(i).toString
          i = i + 1

          val dim_part = line(i).toString
          i = i + 1

          val dim_date = line(i).toString
          i = i + 1

          val dim_hour = line(i).toString
          i = i + 1

          val other = line(i).toString
          val cols = other.split(";")
          val dim_awcid = cols(0)
          val dim_model = cols(1)
          var dim_year = cols(2)
          if (dim_year.equals("unknow")) dim_year = ""
          val dim_crowd = cols(3)
          var dim_region = cols(4)
          if (dim_region.equals("unknow")) dim_region = ""

          i = i + 1

          val fact_vv = line(i).toString
          i = i + 1

          //dim_model,dim_year,dim_crowd,dim_region
          val fact_duration = line(i).toString


          //关联terminal 加上brand last_poweron area province city siz model  license citylevel

          val sortedLine = dim_sn + "\t" + dim_title + "\t" + dim_awcid + "\t" + dim_part + "\t" + dim_date + "\t" + dim_hour + "\t" + fact_vv + "\t" + fact_duration + "\t" + dim_model + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region

          //          println(sortedLine)
          val brand = "CC"
          mutator.mutate(HBaseUtils.getPut_Plays(brand,sortedLine))
        })
        mutator.flush()
      } finally {
        mutator.close()
      }
    }
    )
    sqlContext.uncacheTable("tb_tmp")
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开到剧数据清洗
  */
object PlaysUnpassDataLoadJob02 {

  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[4]")
      .setAppName("coocaa-PlaysDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-21")
    sc.stop()
  }

  case class PlayName(id: String, name: String) extends Serializable


  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)

    val checkTime = TimeUtils.convertDateStr2TimeStamp(analysisDate, TimeUtils.DAY_DATE_FORMAT_TWO)



    /////////////////////////test//////////////////////////
    //        val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\COOCAA\\20161121\\aowei_PlayerControl20161121     \\aowei_PlayerControl20161121_1.txt"
    /////////////////////////test//////////////////////////////

    //TODO 原始日志格式
    //    终端ip,日志包,mac,日志名,时间戳,资源id,资源名称,操作方式,time(如果action是stop 则time为播放时长,否则没有time),logtime（日志产生时间，部分老机器没有这个字段）,省,市,机型,机芯

    //TODO 读取数据文件
    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate + "/aowei_PlayerControl" + analysisDate + "_1.txt"
    //    val hdfsPath = "/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_PlayerControl" + analysisDate
    val initRDD = sc.textFile(hdfsPath)


    //TODO 提取 过滤
    val filterRdd = initRDD.distinct().map(line => {
      val cols = line.split("\t")

      var fact_duration = cols(8).trim
      if (fact_duration.isEmpty) {
        fact_duration = "0"
      }
      cols(2).trim + "\t" + cols(4).trim + "\t" + cols(6).trim + "\t" + cols(7).trim + "\t" + fact_duration
    }).distinct()

      //TODO 过滤
      .filter(line => {
      val cols = line.split("\t")
      val sn = cols(0)

      //      val timeStamp = cols(1).toLong
      //清洗数据，暂定不是日志当天的数据，暂为无效数据
      val isTimestampRight = ValidateUtils.isNumber(cols(1)) && !(cols(1).toLong < checkTime)


      val dim_title = cols(2)
      val dim_titleIsRight = !cols(2).isEmpty
      //      && !ValidateUtils.isContainsSpeciChar(cols(3))


      //II．酷开根据sn和资源名称，start和stop时间排序配对，配对不成功的，当成无效资源。
      //      III．Start和stop不在一个整体的情况，以配对的start记录整点时间。
      //      IV．Start出现的次数记录VV，stop的时长记录为时长。
      val operate = cols(3).toString
      val operateIsRight = (operate.equals("Start") || operate.equals("Stop"))

      //如果action是stop 则为播放时长 否则
      val fact_duration = cols(4)
      val isDurationRight = !cols(4).isEmpty && ValidateUtils.isNumber(cols(4)) && cols(4).toLong >= 0

      isTimestampRight && dim_titleIsRight && operateIsRight && isDurationRight
    }
    )



    //TODO 配对 合并
    val startRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Start")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp)

    })

    //    startRDD.foreach(println(_))

    val stopRDD = filterRdd.filter(line => {
      val cols = line.split("\t")
      cols(3).equals("Stop")
    }
    ).map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val timeStamp = cols(1)
      val dim_title = cols(2)
      val operate = cols(3).toString
      val fact_duration = cols(4)

      (sn + "\t" + dim_title, timeStamp + "\t" + fact_duration)

    })


    //join过滤 匹配不上的数据
    val pairRDD = startRDD.join(stopRDD)

    //笛卡尔积配对并合并 =>阶段一完成
    val stageOneRDD = pairRDD
      //过滤startTime大于stopTime的
      .filter(tuple => {
      val valueLeftCols = tuple._2._1.split("\t")
      val starTime = valueLeftCols(0)

      val valueRightCols = tuple._2._2.split("\t")
      val stopTime = valueRightCols(0)

      starTime <= stopTime
    })
      .map(
        tuple => {
          val keyCols = tuple._1.split("\t")
          val sn = keyCols(0)
          val dim_title = keyCols(1)

          val valueLeftCols = tuple._2._1.split("\t")
          val starTime = valueLeftCols(0)

          val valueRightCols = tuple._2._2.split("\t")
          val stopTime = valueRightCols(0)

          val fact_duration = valueRightCols(1)

          (sn + "\t" + dim_title + "\t" + starTime, starTime + "\t" + stopTime + "\t" + fact_duration)
        }
      ).reduceByKey((pre, after) => {
      val preCols = pre.split("\t")
      val preDiff = preCols(1).toString.toLong - preCols(0).toString.toLong

      val afterCols = after.split("\t")
      val afterDiff = afterCols(1).toString.toLong - afterCols(0).toString.toLong

      var min = ""
      if (preDiff <= afterDiff) {
        min = pre
      } else {
        min = after
      }
      min
    }).map(
      tuple => {
        val keyCols = tuple._1.split("\t")
        val sn = keyCols(0)
        val dim_title = keyCols(1)

        val valueCols = tuple._2.split("\t")
        val starTime = valueCols(0)

        val stopTime = valueCols(1)

        val fact_duration = valueCols(2)

        (sn + "\t" + dim_title + "\t" + starTime + "\t" + stopTime + "\t" + fact_duration)
      }
    )
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO  清理剧名
    val keywordArr = Array[String]("XXX", "剧透", "预告", "抢先看", "片花", "特辑", "片段", "花絮", "插曲", "片尾曲", "主题曲", "广场舞", "图书", "独家策划", "原创", "搞笑", "搜库", "海报", "将映", "剧照", "优酷网", "土豆", "乐视网", "搜狐视频",
      ".rmvb", ".mp4", ".flv", ".mkv", ".mpg", "mov", "720p", "1080p"
    )

    //    “剧透”，“预告”，“抢先看”,“片花”，“片段”，“花絮”，“插曲”，“片尾曲”，“主题曲”，“广场舞”，“图书”，“独家策划”，“原创”，“搞笑”，“海报”“将映”，“剧照”

    //TODO 广播 film_properties表

    //////////////////////////////////test//////////////////////////////////////
    //    val sqlContext = new SQLContext(sc)
    //////////////////////////////////test//////////////////////////////////////

    ////////////////////////test/////////////////////////////
    //    val filmInfoArr = sc.textFile("E:\\aowei\\tracker-job\\doc\\data\\plays\\film_properties.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val id = cols(0)
    //        val original_name = cols(1)
    //        val standard_name = cols(2)
    //        val year = cols(3)
    //        val model = cols(4)
    //        val crowd = cols(5)
    //        val region = cols(6)
    //        Row(original_name, standard_name, model, id, year, crowd, region)
    //      }).collect
    ////////////////////////test/////////////////////////////

    val sqlContext = new HiveContext(sc)

    val filmInfoArr = sqlContext.sql("select original_name,standard_name,model,id,year,crowd,region from hr.film_properties").collect
    //    println("filmInfoArr.size=" + filmInfoArr.size)
    //    for(ele<-filmInfoArr){
    //      println(ele)
    //    }
    val filmInfoArrBV = sc.broadcast(filmInfoArr)

    //fca3864d91ad		锦绣未央 第19集预告	1479700448481	1479700448481	65530
    val stageTwoRDD = stageOneRDD

      //TODO 过滤特定关键词
      .filter(line => {
      val cols = line.split("\t")
      !ValidateUtils.isContainsSpecWords(cols(1), keywordArr)
    }).distinct()

      //TODO 提取集数和日期
      //    fca3864d91ad	_yinhe_203364601	锦绣未央 第19集预告	日期  开始时间  70368(时长)
      .map(line => {
      val cols = line.split("\t")

      //终端sn
      val dim_sn = cols(0)

      //日志的视频名称
      var dim_name = cols(1)

      //TODO 通过资源名称获取其他信息
      var dim_title = "#"
      var dim_model = "#"
      var dim_awcid = "#"
      var dim_year = "#"
      var dim_crowd = "#"
      var dim_region = "#"
      var dim_part = "unknow"

      val filmProp = PlaysDataLoadHelper.extractInfoFromVideoNameOFCooCaa(dim_name, filmInfoArrBV.value)
      if (!filmProp.equals("#")) {
        val filmPropCols = filmProp.split("\t")
        //standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region + "\t" + dim_part
        dim_title = filmPropCols(0)
        dim_model = filmPropCols(1)
        dim_awcid = filmPropCols(2)
        dim_year = filmPropCols(3)
        dim_crowd = filmPropCols(4)
        dim_region = filmPropCols(5)
        dim_part = filmPropCols(6)
      }


      //集数
      //      val dim_part = PlaysDataLoadHelper.extractVideoPartOFCooCaa(cols(1))

      //日期
      val dim_date = currentDate

      //开始时间
      val startTime = cols(2).toLong

      //时长
      val fact_duration = cols(4).toLong

      //结束时间=开始时间+时长
      val stopTime = startTime + fact_duration

      dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + startTime + "\t" + stopTime + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

    })

//    //TODO 过滤掉不匹配的数据
//    val stageTwoPassRDD = stageTwoRDD.filter(line => {
//      val cols = line.split("\t")
//      !cols(1).equals("#")
//    })

    //    //TODO 不匹配的数据统计
        val stageTwoUnPassRDD = stageTwoRDD.filter(line => {
          val cols = line.split("\t")
          cols(1).equals("#")
        })

    // TODO 其他（提取资源名称 单独存放)

    //        stageTwoRDD.foreach(println(_))
    //
    //    println("清洗前记录数：" + initRDD.count())
    //    println("清洗后记录数：：" + stageTwoRDD.count())


    //TODO 获得Awcid
    //    val keyword2awcidArr = sc.textFile("/user/hive/warehouse/hr.db/tracker_plays_tag/tracker_plays_tag.csv")
    //      .map(line => {
    //        val cols = line.split(",")
    //        (cols(4), cols(0))
    //      })
    //      .collect()

    //TODO 分时
    val splitTimeRdd = stageTwoUnPassRDD.flatMap(line => {
      val cols = line.split("\t")

      val dim_sn = cols(0)

      val dim_title = cols(1)

      //      val keyword = ValidateUtils.convertTitle2Keyword(cols(1))
      //
      //      for (keword2cid <- keyword2awcidArr) {
      //        if (keyword.equals(keword2cid._1)) {
      //          dim_awcid = keword2cid._2
      //        }
      //      }

      val dim_part = cols(2)
      val dim_date = cols(3)
      val startTime = cols(4).toLong
      val stopTime = cols(5).toLong
      val dim_model = cols(6)
      val dim_awcid = cols(7)
      val dim_year = cols(8)
      val dim_crowd = cols(9)
      val dim_region = cols(10)

      val dim_name = cols(11)


      //+ "\t" + dim_model+ "\t" + dim_awcid+ "\t" + dim_year+ "\t" + dim_crowd+ "\t" + dim_region


      val tmpArrayBuffer = TimeUtils.splitTimeByHour(dim_sn + "\t" + dim_title + "\t" + dim_part + "\t" + dim_date + "\t" + dim_model + "\t" + dim_awcid + "\t" + dim_year ++ "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name + "\t", startTime, stopTime)

      tmpArrayBuffer

    })

    //TODO 映射成临时表中的行
    val cleanedRDDRows = splitTimeRdd
      .map(line => {
        val cols = line.split('\t')

        var i = 0
        val dim_sn = cols(i)

        i = i + 1

        //市
        val dim_title = cols(i)
        i = i + 1


        val dim_part = cols(i)
        i = i + 1

        val dim_date = cols(i)
        i = i + 1
        val dim_model = cols(i)
        i = i + 1
        val dim_awcid = cols(i)
        i = i + 1
        val dim_year = cols(i)
        i = i + 1
        val dim_crowd = cols(i)
        i = i + 1
        val dim_region = cols(i)
        i = i + 1

        val dim_name = cols(i)
        i = i + 1

        //启动时间的小时数
        val hour = cols(i)
        i = i + 1

        //启动次数
        val launchCnt = cols(i).toInt
        i = i + 1

        //启动时长 单位：秒
        val duration = cols(i).toLong

        val other = dim_awcid + ";" + dim_model + ";" + dim_year + ";" + dim_crowd + ";" + dim_region + ";" + dim_name

        Row(dim_sn, dim_title, dim_part, dim_date, hour, launchCnt, duration, other)

      })

    //TODO 映射成临时表
    val schema = StructType(
      Seq(
        StructField("dim_sn", StringType, false),
        StructField("dim_title", StringType, false),
        StructField("dim_part", StringType, false),
        StructField("dim_date", StringType, false),
        StructField("hour", StringType, false),
        StructField("launchCnt", IntegerType, false),
        StructField("duration", LongType, false),
        StructField("other", StringType, false)
      )
    )


    //TODO 使用sql统计次数和整点时长
    println("sql on temptable ...")

    sqlContext.createDataFrame(cleanedRDDRows, schema).registerTempTable("tb_tmp")
    sqlContext.cacheTable("tb_tmp")


    //统计分析
    sqlContext.sql(
      """
       SELECT
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
           other,
         SUM (launchCnt),
         SUM (duration)
         FROM
         tb_tmp
         GROUP BY
           dim_sn,
           dim_title,
           dim_part,
           dim_date,
           hour,
          other
      """.stripMargin).rdd
      //再次过滤
      .filter(line => {
      line(7).toString().toLong <= 3600
    })

      /////////////////////////////////////////test///////////////////
      //      .foreach(println(_))

      /////////////////////////////////////////test///////////////////
      //TODO 写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_player_active_fact_unpass")

      try {

        lines.foreach(line => {
          var i = 0

          val dim_sn = line(i).toString
          i = i + 1

          val dim_title = line(i).toString
          i = i + 1

          val dim_part = line(i).toString
          i = i + 1

          val dim_date = line(i).toString
          i = i + 1

          val dim_hour = line(i).toString
          i = i + 1

          val other = line(i).toString
          val cols = other.split(";")
          val dim_awcid = cols(0)
          val dim_model = cols(1)
          var dim_year = cols(2)
          if (dim_year.equals("unknow")) dim_year = ""
          val dim_crowd = cols(3)
          var dim_region = cols(4)
          if (dim_region.equals("unknow")) dim_region = ""
          var dim_name = cols(5)

          i = i + 1

          val fact_vv = line(i).toString
          i = i + 1

          //dim_model,dim_year,dim_crowd,dim_region
          val fact_duration = line(i).toString


          //关联terminal 加上brand last_poweron area province city siz model  license citylevel

          val sortedLine = dim_sn + "\t" + dim_title + "\t" + dim_awcid + "\t" + dim_part + "\t" + dim_date + "\t" + dim_hour + "\t" + fact_vv + "\t" + fact_duration + "\t" + dim_model + "\t" + dim_year + "\t" + dim_crowd + "\t" + dim_region + "\t" + dim_name

          //          println(sortedLine)
          val brand = "CC"
          mutator.mutate(HBaseUtils.getPut_Plays(brand,sortedLine))
        })
        mutator.flush()
      } finally {
        mutator.close()
      }
    }
    )
    sqlContext.uncacheTable("tb_tmp")
  }
}

package com.avcdata.spark.job.total.tnumpre

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object PlayTnumPreTotalJob {
  def run(sc: SparkContext, analysisDate: String) = ???

}
package com.avcdata.spark.job.total.tnum2partition

import org.apache.spark.SparkContext

/**
  * Created by avc on 2017/2/14.
  */
object PlayTnumTotal2Partition {
  def run(sc: SparkContext, analysisDate: String) = ???

}
import java.util.Properties

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import utils.Tools

import scala.util.Random

object Producer extends App {
  val events = 1
  val topic = "vboxtopictest"
  //val brokers = "192.168.1.11:2181,192.168.1.12:2181,192.168.1.13:2181"
  //val brokers="117.122.192.50"
  val rnd = new Random()
  val props = new Properties()
  //props.put("bootstrap.servers", "srv3.avcdata.com:9092")
  //props.put("bootstrap.servers", kafkaUtility.getBrokerList(brokers))
  props.put("client.id", "ScalaProducerExample")
  props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
  props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")
  //props.put("bootstrap.servers", "192.168.1.11:9092,192.168.1.12:9092,192.168.1.13:9092")
  props.put("bootstrap.servers", "117.122.192.50:19092")

  println("正在连接服务器...")
  val producer = new KafkaProducer[String, String](props)
  println("连接服务器成功!压入数据ing...")


  val t = System.currentTimeMillis()
  for (nEvents <- Range(0, events)) {
    //val map = mutable.Map("sn" -> sn, "date" -> date, "time" -> time, "length" -> length, "cnt" -> cnt)

    var msg =
      """{
      "Data": [
      {
        "Num": 33,
        "IP": "118.113.146.191",
        "Start_Time": "20170426151951",
        "Program_provider": "腾讯",
        "Play_protocol": "HLS",
        "Date": "00000000000000149%s",
        "Program_type": "电视剧",
        "Stop_Time": "20170426152022",
        "MessageTpye": 3,
        "Program_name": "上海王",
        "URL": "http://182.140.219.13/moviets.tc.qq.com/sl5stw5cnSJBQtXTKN6u_febxhvJLp7IRWy7YnqvZXfLrS7qWcg_cy64x8v9gN77zHG_br6yDsELwaNxDR706V0Xqw-GwYVhq8ViChyFrZI7eUPnwghSAOVvVkb-UGTteIdDBN0xC01fbaocP0-ZDg/f0016jbizxo.320086.ts.m3u8?ver=4&hlskey=&sdtfrom=v8041",
        "Total_Time": 3500
      },
      {
        "Package_Nmae": "package:com.avc_mr.datacollectionandroid",
        "Date": "00000000000000149%s",
        "MessageTpye": 1
      },
      {
        "Package_Nmae": "package:com.android.systemuitest",
        "Date": "00000000000000149%s",
        "MessageTpye": 2
      },
      {
        "Package_Nmae": "package:com.android.providers.userdictionarytest",
        "Date": "00000000000000149%s",
        "MessageTpye": 4
      },
      {
        "Package_Nmae": "package:com.android.shelltest",
        "Date": "00000000000000149%s",
        "MessageTpye": 5
      },
      {
        "Date": "00000000000000149%s",
        "MessageTpye": 6
      }
      ],
      "Manufacturer": "酷开adasdsafsfsa",
      "Mac": "%s",
      "Date": "000000000000001490000008"
    }""".stripMargin
    val rm = new Random()

    val temp1 = rm.nextDouble().formatted("%.7f").toString.substring(2, 9)
    val temp2 = rm.nextDouble().formatted("%.7f").toString.substring(2, 9)

    val temp3 = rm.nextDouble().formatted("%.7f").toString.substring(2, 9)

    val temp4 = rm.nextDouble().formatted("%.7f").toString.substring(2, 9)

    val temp5 = rm.nextDouble().formatted("%.7f").toString.substring(2, 9)

    val temp6 = rm.nextDouble().formatted("%.7f").toString.substring(2, 9)


    val sn = Tools.getRandomSN()
    msg = String.format(msg, temp1, temp2, temp3, temp4, temp5, temp6, sn)

    val data = new ProducerRecord[String, String](topic, msg)

    //async
    //producer.send(data, (m,e) => {})
    //sync
    println("准备发送数据:" + data)
    producer.send(data)
    println("数据发送成功!")
  }

  System.out.println("sent per second: " + events * 1000 / (System.currentTimeMillis() - t))
  producer.close()
}package com.avcdata.etl.common.util

import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern


/**
  * 属性文件加载
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/13 18:22
  */
object PropertiesLoader
{
  /**
    * 从类加载路径加载配置文件
    *
    * @param propertiesFileName 类路径下的文件名
    * @return 加载的属性文件
    */
  def load(propertiesFileName: String): Properties =
  {
    LoanPattern.using(getClass.getClassLoader.getResourceAsStream(propertiesFileName))
    { in =>
      val prop = new Properties()
      prop.load(in)

      prop
    }
  }
}
package com.avcdata.etl.common.util

import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern


/**
  * 属性文件加载
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/13 18:22
  */
object PropertiesLoader
{
  /**
    * 从类加载路径加载配置文件
    *
    * @param propertiesFileName 类路径下的文件名
    * @return 加载的属性文件
    */
  def load(propertiesFileName: String): Properties =
  {
    LoanPattern.using(getClass.getClassLoader.getResourceAsStream(propertiesFileName))
    { in =>
      val prop = new Properties()
      prop.load(in)

      prop
    }
  }
}
package com.avcdata.etl.common.util

import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern


/**
  * 属性文件加载
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/13 18:22
  */
object PropertiesLoader
{
  /**
    * 从类加载路径加载配置文件
    *
    * @param propertiesFileName 类路径下的文件名
    * @return 加载的属性文件
    */
  def load(propertiesFileName: String): Properties =
  {
    LoanPattern.using(getClass.getClassLoader.getResourceAsStream(propertiesFileName))
    { in =>
      val prop = new Properties()
      prop.load(in)

      prop
    }
  }
}
package com.avcdata.etl.common.util

import java.util.Properties

import com.avcdata.etl.common.pattern.LoanPattern


/**
  * 属性文件加载
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/13 18:22
  */
object PropertiesLoader
{
  /**
    * 从类加载路径加载配置文件
    *
    * @param propertiesFileName 类路径下的文件名
    * @return 加载的属性文件
    */
  def load(propertiesFileName: String): Properties =
  {
    LoanPattern.using(getClass.getClassLoader.getResourceAsStream(propertiesFileName))
    { in =>
      val prop = new Properties()
      prop.load(in)

      prop
    }
  }
}
package com.avcdata.spark.job.mock

import scala.util.Random
import scala.util.control.Breaks._

object RandomUtil {

  case class CategoryWeight(category: String, weight: Int)

  def main(args: Array[String]) {
    val arr =
      Array[CategoryWeight](
        CategoryWeight("单身", 2700),
        CategoryWeight("二人世界", 3500),
        CategoryWeight("夫妻和小孩", 5000),
        CategoryWeight("夫妻和老人", 2000),
        CategoryWeight("夫妻、老人和小孩", 1900))

    for(i <-Range(0,100)){
      println(getRandomByWeight(arr))
    }
  }

  def getRandomByWeight(categoryWeights: Array[CategoryWeight]): String = {

    var res = "#"

    var weightSum = 0;

    for (cw <- categoryWeights) {
      weightSum = weightSum + (cw.weight)
    }

    if (weightSum <= 0) {
      throw new RuntimeException("Error: weightSum<0")
    }

    val n = Random.nextInt(weightSum) // n in [0, weightSum)

    var m = 0

    breakable(
      for (cw <- categoryWeights) {
        if (m <= n && n < m + cw.weight) {
          res = cw.category
          break
        }
        m = m + cw.weight;
      })

    res
  }
}
package com.avcdata.spark.job.common

import org.apache.spark.sql.hive.HiveContext
import org.joda.time.DateTime

case class Licenseration(brand: String, license: String, province: String, apklicense: String, apkration: String, dim_date: String)
case class Adjust(apk: String, daily_sn: String, weekly_sn: String, monthly_sn: String, days7_sn: String, days30_sn: String,
daily_length: String, weekly_length: String, monthly_length: String, usage_daily: String, dim_date: String)
case class AdjustLicense(apk: String, daily_sn: String, weekly_sn: String, monthly_sn: String, days7_sn: String, days30_sn: String,
daily_length: String, weekly_length: String, monthly_length: String, dim_date: String)
case class Fst(brand: String, province: String, ratio: String, dim_date: String)
case class OverviewOtt(dim_date: String, lenratio: String, ratio: String)
case class Sec(brand: String, license: String, province: String, ratio: String, dim_date: String)

/**
  * Created by avc on 2017/5/16.
  * 系数表的更新，当系数表不变的情况下复制前一天的存为当天
  */
object RatioTableUpdate {
    def run(hiveContext: HiveContext, analysisDate: String) = {
        import hiveContext.implicits._

        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")

        //1、licenseration
        println("----licenseration")
        /*val licenseRation = hiveContext.sql("select brand,license,province,apklicense,apkration" +
            " from hr.licenseration where date= '" + preDate +"'")
                .mapPartitions(items => {
                items.map(item =>{
                    val brand = item(0).toString
                    val license	= item(1).toString
                    val province = item(2).toString
                    val apklicense = item(3).toString
                    val apkration = item(4).toString
                    val dim_date = analysisDate
                    val lr = new Licenseration(brand, license, province, apklicense, apkration, dim_date)

                    lr
                })
            })

        licenseRation.toDF.registerTempTable("lr")
        hiveContext.sql(
            """insert overwrite table hr.licenseration partition(date='"""+ analysisDate +"""')
            |select brand,license,province,apklicense,apkration,dim_date from lr""".stripMargin)*/

        val licenseRationsql =
            """
              |insert overwrite table hr.licenseration partition(date='"""+ analysisDate +"""')
              |select brand,license,province,apklicense,apkration,""""+ analysisDate +"""" dim_date from hr.licenseration where date= '""" + preDate +"""'
            """
        println(licenseRationsql.stripMargin)
        hiveContext.sql(licenseRationsql.stripMargin)

        //2、tracker_total_dim_ratio_adjust_license
        println("----tracker_total_dim_ratio_adjust_license")
        /*val adjustLicense = hiveContext.sql("select apk,daily_sn,weekly_sn,monthly_sn," +
            "7days_sn,30days_sn,daily_length,weekly_length,monthly_length  " +
            "from hr.tracker_total_dim_ratio_adjust_license where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val apk = item(0).toString
                    val daily_sn	= item(1).toString
                    val weekly_sn = item(2).toString
                    val monthly_sn = item(3).toString
                    val days7_sn = item(4).toString
                    val days30_sn	= item(5).toString
                    val daily_length = item(6).toString
                    val weekly_length = item(7).toString
                    val monthly_length = item(8).toString
                    val dim_date = analysisDate

                    val al = new AdjustLicense(apk, daily_sn, weekly_sn, monthly_sn, days7_sn, days30_sn, daily_length,
                        weekly_length, monthly_length,dim_date)

                    al
                })
            })

        //adjustLicense.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_adjust_license")
        adjustLicense.toDF.registerTempTable("al")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_adjust_license partition(date='"""+ analysisDate +"""')
            |select apk,daily_sn,weekly_sn,monthly_sn,days7_sn,days30_sn,daily_length,weekly_length,monthly_length,dim_date from al""".stripMargin)*/
        val adjustLicensesql =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_adjust_license partition(date='"""+ analysisDate +"""')
              |select apk,daily_sn,weekly_sn,monthly_sn,7days_sn,30days_sn,daily_length,weekly_length,monthly_length,""""+ analysisDate +"""" dim_date
              |from hr.tracker_total_dim_ratio_adjust_license where date= '""" + preDate +"""'
            """
        println(adjustLicensesql.stripMargin)
        hiveContext.sql(adjustLicensesql.stripMargin)

        //3、tracker_total_dim_ratio_fst
        println("----tracker_total_dim_ratio_fst")
        /*val ratioFst = hiveContext.sql("select brand,province,ratio from hr.tracker_total_dim_ratio_fst where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val brand = item(0).toString
                    val province = item(1).toString
                    val ratio = item(2).toString
                    val dim_date = analysisDate

                    val fst = new Fst(brand, province, ratio, dim_date)

                    fst
                })
            })

        //ratioFst.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_fst")
        ratioFst.toDF.registerTempTable("fst")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_fst partition(date='"""+ analysisDate +"""')
            |select dim_date,brand,province,ratio from fst""".stripMargin)*/
        val ratioFstsql =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_fst partition(date='"""+ analysisDate +"""')
              |select """"+ analysisDate +"""" dim_date,brand,province,ratio from hr.tracker_total_dim_ratio_fst where date= '""" + preDate +"""'
            """
        println(ratioFstsql.stripMargin)
        hiveContext.sql(ratioFstsql.stripMargin)

        //4、tracker_total_dim_ratio_oc_fst
        println("----tracker_total_dim_ratio_oc_fst")
        /*val ocFst = hiveContext.sql("select brand,province,ratio from hr.tracker_total_dim_ratio_oc_fst where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val brand = item(0).toString
                    val province = item(1).toString
                    val ratio = item(2).toString
                    val dim_date = analysisDate

                    val fst = new Fst(brand, province, ratio, dim_date)

                    fst
                })
            })

        //ocFst.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_oc_fst")
        ocFst.toDF.registerTempTable("ocfst")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_oc_fst partition(date='"""+ analysisDate +"""')
            |select dim_date,brand,province,ratio from ocfst""".stripMargin)*/
        val ocFstsql =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_oc_fst partition(date='"""+ analysisDate +"""')
              |select """"+ analysisDate +"""" dim_date,brand,province,ratio from hr.tracker_total_dim_ratio_oc_fst where date= '""" + preDate +"""'
            """
        println(ocFstsql.stripMargin)
        hiveContext.sql(ocFstsql.stripMargin)

        //5、tracker_total_dim_ratio_overviewott
        println("----tracker_total_dim_ratio_overviewott")
        /*val overviewott = hiveContext.sql("select lenratio,ratio from hr.tracker_total_dim_ratio_overviewott where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val lenratio = item(0).toString
                    val ratio = item(1).toString
                    val dim_date = analysisDate

                    val oo = new OverviewOtt(dim_date, lenratio, ratio)

                    oo
                })
            })

        //overviewott.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_overviewott")
        overviewott.toDF.registerTempTable("oo")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_overviewott partition(date='"""+ analysisDate +"""')
            |select dim_date,ratio,lenratio from oo""".stripMargin)*/
        val overviewottsql =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_overviewott partition(date='"""+ analysisDate +"""')
              |select """"+ analysisDate +"""" dim_date,ratio,lenratio from hr.tracker_total_dim_ratio_overviewott where date= '""" + preDate +"""'
            """
        println(overviewottsql.stripMargin)
        hiveContext.sql(overviewottsql.stripMargin)

        //6、tracker_total_dim_ratio_sec
        println("----tracker_total_dim_ratio_sec")
        /*val sec = hiveContext.sql("select brand,license,province,ratio from hr.tracker_total_dim_ratio_sec where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val brand = item(0).toString
                    val license = item(1).toString
                    val province = item(2).toString
                    val ratio = item(3).toString
                    val dim_date = analysisDate

                    val sec = new Sec(brand, license, province, ratio, dim_date)

                    sec
                })
            })

        //sec.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_sec")
        sec.toDF.registerTempTable("sec")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_sec partition(date='"""+ analysisDate +"""')
            |select brand,license,province,ratio,dim_date from sec""".stripMargin)*/
        val selsql =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_sec partition(date='"""+ analysisDate +"""')
              |select brand,license,province,ratio,""""+ analysisDate +"""" dim_date from hr.tracker_total_dim_ratio_sec where date= '""" + preDate +"""'
            """
        println(selsql.stripMargin)
        hiveContext.sql(selsql.stripMargin)

        //7、tracker_total_dim_ratio_sec2
        println("----tracker_total_dim_ratio_sec2")
        /*val sec2 = hiveContext.sql("select brand,license,province,ratio from hr.tracker_total_dim_ratio_sec2 where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val brand = item(0).toString
                    val license = item(1).toString
                    val province = item(2).toString
                    val ratio = item(3).toString
                    val dim_date = analysisDate

                    val sec2 = new Sec(brand, license, province, ratio, dim_date)

                    sec2
                })
            })

        //sec2.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_sec2")
        sec2.toDF.registerTempTable("sec2")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_sec2 partition(date='"""+ analysisDate +"""')
            |select brand,license,province,ratio,dim_date from sec2""".stripMargin)*/
        val selsql2 =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_sec2 partition(date='"""+ analysisDate +"""')
              |select brand,license,province,ratio,""""+ analysisDate +"""" dim_date from hr.tracker_total_dim_ratio_sec2 where date= '""" + preDate +"""'
            """
        println(selsql2.stripMargin)
        hiveContext.sql(selsql2.stripMargin)

        //8、tracker_total_dim_ratio_adjust
        println("----tracker_total_dim_ratio_adjust")
        /*val adjust = hiveContext.sql("select apk,daily_sn,weekly_sn,monthly_sn,7days_sn," +
            "30days_sn,daily_length, weekly_length,monthly_length,usage_daily " +
            "from hr.tracker_total_dim_ratio_adjust where date= '" + preDate +"'")
            .mapPartitions(items => {
                items.map(item =>{
                    val apk = item(0).toString
                    val daily_sn	= item(1).toString
                    val weekly_sn = item(2).toString
                    val monthly_sn = item(3).toString
                    val days7_sn = item(4).toString
                    val days30_sn	= item(5).toString
                    val daily_length = item(6).toString
                    val weekly_length = item(7).toString
                    val monthly_length = item(8).toString
                    val usage_daily = item(9).toString
                    val dim_date = analysisDate

                    val al = new Adjust(apk, daily_sn, weekly_sn, monthly_sn, days7_sn, days30_sn, daily_length,
                        weekly_length, monthly_length,usage_daily,dim_date)

                    al
                })
            })

        //adjust.toDF.write.saveAsTable("hr.tracker_total_dim_ratio_adjust")
        adjust.toDF.registerTempTable("al1")
        hiveContext.sql(
            """insert overwrite table hr.tracker_total_dim_ratio_adjust partition(date='"""+ analysisDate +"""')
            |select apk,daily_sn,weekly_sn,monthly_sn,days7_sn,days30_sn,daily_length, weekly_length,monthly_length,usage_daily,dim_date from al1""".stripMargin)*/
        val adjustsql =
            """
              |insert overwrite table hr.tracker_total_dim_ratio_adjust partition(date='"""+ analysisDate +"""')
              |select apk,daily_sn,weekly_sn,monthly_sn,7days_sn,30days_sn,daily_length,weekly_length,monthly_length,usage_daily,""""+ analysisDate +"""" dim_date
              |from hr.tracker_total_dim_ratio_adjust where date= '""" + preDate +"""'
            """
        println(adjustsql.stripMargin)
        hiveContext.sql(adjustsql.stripMargin)
    }
}
package com.avcdata.spark.job.mllib

import com.avcdata.spark.job.clean.UserVectorTerminalETL
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkConf, SparkContext}

object ReadTest {

//  val log = Logger.getLogger(getClass.getName)


  def main(args: Array[String]) {
        val conf = new SparkConf()
          .setMaster("local[1]")
          .setAppName("UserVectorBehaviorETL")
        val sc = new SparkContext(conf)
        run(sc, "2017-03-31","30")
        sc.stop()

  }

  def run(sc: SparkContext, s: String, s1: String) = {

   val initRDD =  sc.textFile("E:\\workspace\\TestKmeans\\query_result.csv")

//user_vector_terminal.key,user_vector_terminal.sn,user_vector_terminal.brand,user_vector_terminal.province,user_vector_terminal.price,user_vector_terminal.size,user_vector_terminal.pro_year

    val tmp= initRDD.map(line=>{

      val li = line.replaceAll("\"","")

      val arr = li.split("\t")

      val sn = arr(0)
      val vectorStr = li.substring(line.indexOf(sn)+sn.length+1)

      val vector = Vectors.dense(vectorStr.split("\t")
        .filter(ele=>{
          !ele.isEmpty && !ele.contains(",")
        }) .map(_.toDouble))

      (sn,vector.toString)

    })
//      .foreach(println(_))
//      .repartition(1)
//    tmp.saveAsTextFile("D:\\test")

    ////////////////////////////////////////////////////////////////////////////////////////
//    tmp.

  }

}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

object RegexUtils {

  def getPlayModelByLogTitle(title: String): String = {


    ////电视剧：8为数字/空 书名号 数字 去符号 版本 集数
    val dsjRegex = "^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季)|第\\d(集|季)|\\(第\\d(集|季)\\)|大结局|先导集){1})([^集季]*)$".r
    if(!dsjRegex.findFirstMatchIn(title).isEmpty){
         return "电视剧"
    }

    //数字 名称  版本  集数
    val dhRegex = "^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r
    if(!dhRegex.findFirstMatchIn(title).isEmpty){
      return "动画片"
    }

    //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
    val zyRegex = "^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季|期)|第\\d(集|季|期)|\\(第\\d(集|季|期)\\)|大结局|先导集){1}).*$".r
    if(!zyRegex.findFirstMatchIn(title).isEmpty){
      return "综艺"
    }

    return "未知"

  }

  def main(args: Array[String]) {
      var res = getPlayModelByLogTitle("20160724幻城未删减版第51集")
      println(res)
  }

  def test01(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test02(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test03(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test04(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

object RegexUtils {

  def getPlayModelByLogTitle(title: String): String = {


    ////电视剧：8为数字/空 书名号 数字 去符号 版本 集数
    val dsjRegex = "^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季)|第\\d(集|季)|\\(第\\d(集|季)\\)|大结局|先导集){1})([^集季]*)$".r
    if (!dsjRegex.findFirstMatchIn(title).isEmpty) {
      return "电视剧"
    }

    //数字 名称  版本  集数
    val dhRegex = "^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r
    if (!dhRegex.findFirstMatchIn(title).isEmpty) {
      return "动画片"
    }

    //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
    val zyRegex = "^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季|期)|第\\d(集|季|期)|\\(第\\d(集|季|期)\\)|大结局|先导集){1}).*$".r
    if (!zyRegex.findFirstMatchIn(title).isEmpty) {
      return "综艺"
    }

    return "未知"

  }

  def main(args: Array[String]) {
    var res = getPlayModelByLogTitle("20160724幻城未删减版第51集")
    println(res)
  }


  def test01(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test02(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test03(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test04(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  /**
    *
    * 转义正则特殊字符 （$()*+.[]?\^{},|）
    * @param keyword
    * @return
    */
  def escapeExprSpecialWord(keyword: String): String = {

    var result = keyword

    if (!result.isEmpty) {
      val fbsArr = Array("\\", "$", "(", ")", "*", "+", ".", "[", "]", "?", "^", "{", "}", "|")
      for (key <- fbsArr) {
        if (result.contains(key)) {
          result = result.replace(key, "\\" + key)
        }
      }
    }

    result
  }


}
package com.avcdata.vbox.util

import java.util.regex.Pattern

object RegexUtils {




  def getPlayModelByLogTitle(title: String): String = {


    ////电视剧：8为数字/空 书名号 数字 去符号 版本 集数
    val dsjRegex = "^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季)|第\\d(集|季)|\\(第\\d(集|季)\\)|大结局|先导集){1})([^集季]*)$".r
    if (!dsjRegex.findFirstMatchIn(title).isEmpty) {
      return "电视剧"
    }

    //数字 名称  版本  集数
    val dhRegex = "^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r
    if (!dhRegex.findFirstMatchIn(title).isEmpty) {
      return "动画片"
    }

    //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
    val zyRegex = "^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季|期)|第\\d(集|季|期)|\\(第\\d(集|季|期)\\)|大结局|先导集){1}).*$".r
    if (!zyRegex.findFirstMatchIn(title).isEmpty) {
      return "综艺"
    }

    return "未知"

  }

  def main(args: Array[String]) {
//    var res = getPlayModelByLogTitle("20160724幻城未删减版第51集")
//    println(res)
    val haha = "哈哈123211"
    val zyRegex = "[0-9]{6,8}".r
    if (!zyRegex.findFirstMatchIn(haha).isEmpty) {
      println("综艺")
    }

  }

  def getIP(text: String): Option[String] = {
    val reg = "((?:(?:25[0-5]|2[0-4]\\d|((1\\d{2})|([1-9]?\\d)))\\.){3}(?:25[0-5]|2[0-4]\\d|((1\\d{2})|([1-9]?\\d))))".r

    reg.findFirstIn(text)

  }

  def getTimeStamp(text: String): Option[String] = {
    val reg = "\\d{9,}".r

    reg.findFirstIn(text)

  }


  def getMac(text: String): Option[String] = {
    val reg = "([A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2}".r

    reg.findFirstIn(text)

  }

  def test01(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test02(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test03(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

  def test04(str: String): String = {
    //查找以Java开头,任意结尾的字符串
    val pattern = Pattern.compile("^Java.*");
    val matcher = pattern.matcher("Java不是人");
    val b = matcher.matches();
    //当条件满足时，将返回true，否则返回false
    System.out.println(b);
    ""
  }

}
package com.avcdata.spark.job.common

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/4/11.
  * 沉默终端
  * 针对装机的上个月新增终端，统计一个月中无使用行为的数量:3月1日来统计2月相对1月新增的留存
  */
object RetainedTerminal {
    def run(sc: SparkContext, baseDate: String, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("retain")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimApkCol = Bytes.toBytes("dim_apk")
        //val dimDuraCol = Bytes.toBytes("duration")
        //val dimCntCol = Bytes.toBytes("cnt")
        val dimNewdateCol = Bytes.toBytes("new_date")
        //val dimBasedateCol = Bytes.toBytes("base_date")
        val dimRetaindateCol = Bytes.toBytes("retain_date")

        val date = analysisDate.substring(0, 7) + "-01"
        val currDateStart = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd") //统计月开始时间
        val currDateEnd = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd") //统计月结束时间

        val hiveContext = new HiveContext(sc)
        //统计月的数据
        val statisticSql =
            """
              |select appname,dim_sn from
              |(select dim_apk,dim_sn,dim_date from hr.tracker_apk_fact_partition
              |where date>='"""+ currDateStart +"""' and date<='"""+ currDateEnd +"""') a
              |join
              |(select packagename,appname from hr.apkinfo) b
              |on b.packagename=a.dim_apk
            """

        println("statisticSql : " + statisticSql.stripMargin)
        val staData = hiveContext.sql(statisticSql.stripMargin).mapPartitions(items =>{
            items.map(item => {
                val sn = item(1).toString
                val apk = item(0).toString
                //val dura = item(2).toString
                //val cnt = item(3).toString
                (sn+"0x01"+apk, 1)
            })
        }).distinct()

        //获取基础月份
        val list = getMonths(baseDate, DateTime.parse(date).plusMonths(-2).toString("yyyy-MM-dd"))
        println(list)

        for(l <- list) {
            val preStaDate = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd") //统计月是3
            val newmon = DateTime.parse(l).toString("yyyy-MM-dd") //新增终端作为基础月份 2017-01-01
            val differMons = getDifferMonths(l, preStaDate) //统计月和最开始统计的基础月 相差几个月 1
            val basemon = DateTime.parse(l).plusMonths(differMons).toString("yyyy-MM-dd") //留存参考月份 (2017-01+1)-01-->2017-02-01
            println(newmon + ", " +basemon + ", " + currDateStart)
            var baseSql = ""
            if (differMons == 0) { //第一次统计留存参考数据从 new_terminal中拿
                baseSql =
                    """
                      |select dim_apk,dim_sn from hr.tracker_new_terminal where dim_date='"""+ basemon + """'
                    """
            } else if (differMons > 0) { //第N+1次从retain_terminal中拿数据
                baseSql =
                    """
                      |select dim_apk,dim_sn from hr.tracker_retain_terminal
                      |where new_date='"""+ newmon +"""' and retain_date='"""+ basemon + """'
                    """
            }

            println("baseSql : " + baseSql.stripMargin)
            val baseData = hiveContext.sql(baseSql.stripMargin).repartition(200).mapPartitions(items =>{
                items.map(item => {
                    val apk = item(0).toString
                    val sn = item(1).toString
                    (sn+"0x01"+apk, 1)
                })
            })/*.collect()
            val baseBro = sc.broadcast(baseData)
            val reRdd = staData.mapPartitions(items=>{
                items.map(item => {
                    val re = retain(item, baseBro.value)
                    re
                })
            }).filter(x => x._1.split("0x01").length==2).mapPartitions(items =>{
                items.map(item => {
                    val sn = item._1.split("0x01")(0)
                    val apk = item._1.split("0x01")(1)
                    val dura = item._2.split("0x01")(0)
                    val cnt = item._2.split("0x01")(1)
                    (sn, apk, dura, cnt)
                })
            })*/

            val reRdd = staData.leftOuterJoin(baseData).filter(x => !x._2.toString().contains("None"))
                .filter(x => x._1.split("0x01").length==2).mapPartitions(items =>{
                items.map(item => {
                    val sn = item._1.split("0x01")(0)
                    val apk = item._1.split("0x01")(1)
                    //val dura = item._2._1.split("0x01")(0)
                    //val cnt = item._2._1.split("0x01")(1)
                    (sn, apk)
                })
            })

            reRdd.foreachPartition(items => {
                val myConf = HBaseConfiguration.create()
                myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
                myConf.set("hbase.zookeeper.property.clientPort", "2181")
                val hbaseConn = ConnectionFactory.createConnection(myConf)
                val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_retain_terminal"))

                try {
                    items.foreach(line => {
                        val sn = line._1
                        val apk = line._2
                        val newDate = newmon
                        //val baseDate = basemon
                        val retainDate = currDateStart
                        //val dura = line._3
                        //val cnt = line._4

                        val put = new Put(Bytes.toBytes(apk + sn + newDate + retainDate))
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apk))
                        put.addColumn(dimFamilyCol, dimNewdateCol, Bytes.toBytes(newDate))
                        put.addColumn(dimFamilyCol, dimRetaindateCol, Bytes.toBytes(retainDate))
                        //put.addColumn(dimFamilyCol, dimDuraCol, Bytes.toBytes(dura))
                        //put.addColumn(dimFamilyCol, dimCntCol, Bytes.toBytes(cnt))

                        mutator.mutate(put)
                    })

                    mutator.flush()
                } finally {
                    mutator.close()
                    hbaseConn.close()
                }
            })
        }
    }

    /**
      * 获取统计月前一个月和基础月 中间包含哪几个月
      * @param baseDate baseDate 2017-01-01
      * @param preStaticsDate 统计月前月份
      * @return
      */
    def getMonths(baseDate: String, preStaticsDate : String) : List[String]={
        var i = 0
        var list:List[String] =List()

        for (i<-0 to 20) {
            if (DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd").equals(preStaticsDate)) {
                list = list :+ DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd")
                return list
            }
            list = list :+ DateTime.parse(baseDate).plusMonths(i).toString("yyyy-MM-dd")
        }

        list
    }

    /**
      * 获取统计月和需要统计留存的某个基础月相差几个月
      * @param currBaseMon 需要统计留存的某个基础月
      * @param preStaticsDate 统计月月份
      * @return
      */
    def getDifferMonths(currBaseMon: String, preStaticsDate: String) : Int={
        var i = 0

        for (i<-0 to 20) {
            if (DateTime.parse(currBaseMon).plusMonths(i+1).toString("yyyy-MM-dd").equals(preStaticsDate)) {
                return i
            }
        }

        i
    }

    def retain(item: (String, String), vu: Array[(String, Int)]) : (String, String) ={
        var key = ""
        var duracnt = ""
        vu.foreach(ele => {
            if (item._1.equals(ele._1)) {
                key = item._1
                duracnt = item._2
            }
        })

        (key, duracnt)
    }

    def main(args: Array[String]): Unit = {
        val analysisDate = "2017-04-05" //2017-04-05 来统计上一个月 3月的存留终端，3月是统计月
        val baseDate = "2017-01-01"
        val date = analysisDate.substring(0, 7) + "-01"
        val currDateMonth = DateTime.parse(date).plusMonths(-1).toString("yyyyMM") //统计月月份
        val currDateStart = DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd") //统计月开始时间
        val currDateEnd = DateTime.parse(date).plusDays(-1).toString("yyyy-MM-dd") //统计月结束时间
        val baseReMonth = DateTime.parse(currDateStart).plusMonths(-1).toString("yyyyMM") //留存参照月月份

        println(DateTime.parse(date).plusMonths(-2).toString("yyyy-MM-dd"))
        val list = getMonths(baseDate, DateTime.parse(date).plusMonths(-2).toString("yyyy-MM-dd"))
        println(list)
        for(l <- list) {
            println(l + ", " + getDifferMonths(l, DateTime.parse(date).plusMonths(-1).toString("yyyy-MM-dd")))
        }

        println(DateTime.parse(analysisDate).plusDays(1).toString("yyyy-MM-dd"))
    }
}
package com.avcdata.spark.job.common

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by avc on 2017/5/15.
  * 现修改为某一天的连续N天的留存数据
  */
object RetainedTerminalDaily {
    def run(sc: SparkContext, analysisDate: String, nextDate: String, flag: Int) = {
        val dimFamilyCol = Bytes.toBytes("next")

        val dimSnCol = Bytes.toBytes("dim_sn")
        val dimApkCol = Bytes.toBytes("appname")
        val dimCurrCol = Bytes.toBytes("curr_date")
        val dimNextCol = Bytes.toBytes("next_date")
        val dimBrCol = Bytes.toBytes("brand")
        val dimProCol = Bytes.toBytes("province")
        val dimLicCol = Bytes.toBytes("license")

        val preDate = DateTime.parse(nextDate).plusDays(-1).toString("yyyy-MM-dd")

        val hiveContext = new HiveContext(sc)
        var statisticSql = ""
        var staData:RDD[(String, String)] = null
        if (flag == 1) {
            //统计日的历史数据
            statisticSql =
                """
                  |select dim_sn,appname from
                  |(select dim_sn,dim_apk from hr.tracker_apk_fact_partition where date < '"""+ analysisDate +"""') a
                  |join
                  |(select packagename,appname from hr.apkinfo where onelevel='视频'
                  |and appname in('CIBN环球影视','银河·奇异果','腾讯视频TV端','芒果TV','电视猫视频','CIBN微视听')) b
                  |on a.dim_apk=b.packagename
                """

            println("statisticSql : " + statisticSql.stripMargin)
            val hisRdd = hiveContext.sql(statisticSql.stripMargin).mapPartitions(items =>{
                items.map(item => {
                    val sn = item(0).toString

                    (sn, 1)
                })
            }).distinct()

            //统计日的sn
            val newaddSql =
                """
                  |select dim_sn,appname,brand,province,license from
                  |(select dim_sn,dim_apk from hr.tracker_apk_fact_partition
                  |where date='"""+ analysisDate +"""') a
                  |join
                  |(select packagename,appname from hr.apkinfo where onelevel='视频'
                  |and appname in('CIBN环球影视','银河·奇异果','腾讯视频TV端','芒果TV','电视猫视频','CIBN微视听')) b
                  |join
                  |(select brand,province,sn,license from hr.terminal where license <> 'null') c
                  |on a.dim_apk=b.packagename and a.dim_sn=c.sn
                """

            println("newaddSql : " + newaddSql.stripMargin)

            //得到新增的数据，次日留存是根据统计日的新增得到的
            val newaddRdd = hiveContext.sql(newaddSql.stripMargin).mapPartitions(items =>{
                items.map(item => {
                    val sn = item(0).toString
                    val apk = item(1).toString
                    val brand = item(2).toString
                    val province = item(3).toString
                    val license = item(4).toString

                    (sn, apk+"0x01"+brand+"0x01"+province+"0x01"+license)
                })
            }).distinct()

            staData = newaddRdd.leftOuterJoin(hisRdd).filter(x => !x._2.toString().contains("None"))
                .mapPartitions(items =>{
                    items.map(item => {
                        val sn = item._1
                        (sn, item._2._1)
                    })
                })

            staData.foreachPartition(items => {
                val myConf = HBaseConfiguration.create()
                myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
                myConf.set("hbase.zookeeper.property.clientPort", "2181")
                val hbaseConn = ConnectionFactory.createConnection(myConf)
                val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_redaily_terminal"))

                try {
                    items.foreach(line => {
                        val sn = line._1
                        val apk = line._2.split("0x01")(0)
                        val nextDat = "2017-05-25"
                        val currDate = "2017-05-25"
                        val brand = line._2.split("0x01")(1)
                        val province = line._2.split("0x01")(2)
                        val license = line._2.split("0x01")(3)

                        val put = new Put(Bytes.toBytes(apk + sn + brand + province + license + currDate))
                        put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apk))
                        put.addColumn(dimFamilyCol, dimNextCol, Bytes.toBytes(nextDat))
                        put.addColumn(dimFamilyCol, dimCurrCol, Bytes.toBytes(currDate))
                        put.addColumn(dimFamilyCol, dimBrCol, Bytes.toBytes(brand))
                        put.addColumn(dimFamilyCol, dimLicCol, Bytes.toBytes(license))
                        put.addColumn(dimFamilyCol, dimProCol, Bytes.toBytes(province))

                        mutator.mutate(put)
                    })

                    mutator.flush()
                } finally {
                    mutator.close()
                    hbaseConn.close()
                }
            })

        } else {
            //3日留存是根据次日留存得到的
            statisticSql =
                """
                  |select dim_sn,appname,brand,province,license from hr.tracker_redaily_terminal where curr_date='"""+ analysisDate +"""'
                  |and next_date='"""+ preDate +"""'
                """

            println("statisticSql : " + statisticSql.stripMargin)
            staData = hiveContext.sql(statisticSql.stripMargin).mapPartitions(items =>{
                items.map(item => {
                    val sn = item(0).toString
                    val apk = item(1).toString
                    val brand = item(2).toString
                    val province = item(3).toString
                    val license = item(4).toString

                    (sn, apk+"0x01"+brand+"0x01"+province+"0x01"+license)
                })
            }).distinct()
        }

        //次日
        val nextDaySql =
            """
              |select dim_sn,appname from
              |(select dim_sn,dim_apk from hr.tracker_apk_fact_partition
              |where date='"""+ nextDate +"""') a
              |join
              |(select packagename,appname from hr.apkinfo where onelevel='视频'
              |and appname in('CIBN环球影视','银河·奇异果','腾讯视频TV端','芒果TV','电视猫视频','CIBN微视听')) b
              |on a.dim_apk=b.packagename
            """
        println("nextDaySql : " + nextDaySql.stripMargin)
        /*val nextData = hiveContext.sql(nextDaySql.stripMargin).mapPartitions(items =>{
            items.map(item => {
                val sn = item(0).toString

                (sn, 1)
            })
        }).distinct()

        val reRdd = nextData.leftOuterJoin(staData).filter(x => !x._2.toString().contains("None"))
            .mapPartitions(items =>{
            items.map(item => {
                val sn = item._1
                val str = item._2._2.get
                (sn, str)
            })
        })

        reRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_redaily_terminal"))

            try {
                items.foreach(line => {
                    val sn = line._1
                    val apk = line._2.split("0x01")(0)
                    val nextDat = nextDate
                    val currDate = analysisDate
                    val brand = line._2.split("0x01")(1)
                    val province = line._2.split("0x01")(2)
                    val license = line._2.split("0x01")(3)

                    val put = new Put(Bytes.toBytes(apk + sn + nextDate + currDate + brand + province + license))
                    put.addColumn(dimFamilyCol, dimSnCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimApkCol, Bytes.toBytes(apk))
                    put.addColumn(dimFamilyCol, dimNextCol, Bytes.toBytes(nextDat))
                    put.addColumn(dimFamilyCol, dimCurrCol, Bytes.toBytes(currDate))
                    put.addColumn(dimFamilyCol, dimBrCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicCol, Bytes.toBytes(license))
                    put.addColumn(dimFamilyCol, dimProCol, Bytes.toBytes(province))

                    mutator.mutate(put)
                })

                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })*/
    }
}
package com.avcdata.etl.common.util.encryption.rsa

import java.io.ByteArrayOutputStream
import java.security.Key
import java.security.KeyFactory
import java.security.KeyPair
import java.security.KeyPairGenerator
import java.security.PrivateKey
import java.security.PublicKey
import java.security.Signature
import java.security.interfaces.RSAPrivateKey
import java.security.interfaces.RSAPublicKey
import java.security.spec.PKCS8EncodedKeySpec
import java.security.spec.X509EncodedKeySpec
import java.util
import javax.crypto.Cipher

object RSAUtil
{
  val KEY_ALGORITHM: String = "RSA"
  val SIGNATURE_ALGORITHM: String = "MD5withRSA"
  private val PUBLIC_KEY: String = "RSAPublicKey"
  private val PRIVATE_KEY: String = "RSAPrivateKey"
  private val MAX_ENCRYPT_BLOCK: Int = 117
  private val MAX_DECRYPT_BLOCK: Int = 128

  @throws[Exception]
  def genKeyPair: util.Map[String, AnyRef] =
  {
    val keyPairGen: KeyPairGenerator = KeyPairGenerator.getInstance(KEY_ALGORITHM)
    keyPairGen.initialize(1024)
    val keyPair: KeyPair = keyPairGen.generateKeyPair
    val publicKey: RSAPublicKey = keyPair.getPublic.asInstanceOf[RSAPublicKey]
    val privateKey: RSAPrivateKey = keyPair.getPrivate.asInstanceOf[RSAPrivateKey]
    val keyMap: util.Map[String, AnyRef] = new util.HashMap[String, AnyRef](2)
    keyMap.put(PUBLIC_KEY, publicKey)
    keyMap.put(PRIVATE_KEY, privateKey)
    keyMap
  }

  @throws[Exception]
  def sign(data: Array[Byte], privateKey: String): String =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(privateKey)
    val pkcs8KeySpec: PKCS8EncodedKeySpec = new PKCS8EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val privateK: PrivateKey = keyFactory.generatePrivate(pkcs8KeySpec)
    val signature: Signature = Signature.getInstance(SIGNATURE_ALGORITHM)
    signature.initSign(privateK)
    signature.update(data)
    Base64Util.encode(signature.sign)
  }

  @throws[Exception]
  def verify(data: Array[Byte], publicKey: String, sign: String): Boolean =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(publicKey)
    val keySpec: X509EncodedKeySpec = new X509EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val publicK: PublicKey = keyFactory.generatePublic(keySpec)
    val signature: Signature = Signature.getInstance(SIGNATURE_ALGORITHM)
    signature.initVerify(publicK)
    signature.update(data)
    signature.verify(Base64Util.decode(sign))
  }

  @throws[Exception]
  def decryptByPrivateKey(encryptedData: Array[Byte], privateKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(privateKey)
    val pkcs8KeySpec: PKCS8EncodedKeySpec = new PKCS8EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val privateK: Key = keyFactory.generatePrivate(pkcs8KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.DECRYPT_MODE, privateK)
    val inputLen: Int = encryptedData.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_DECRYPT_BLOCK)
        {
          cache = cipher.doFinal(encryptedData, offSet, MAX_DECRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(encryptedData, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_DECRYPT_BLOCK
      }
    }
    val decryptedData: Array[Byte] = out.toByteArray
    out.close()
    decryptedData
  }

  @throws[Exception]
  def decryptByPublicKey(encryptedData: Array[Byte], publicKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(publicKey)
    val x509KeySpec: X509EncodedKeySpec = new X509EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val publicK: Key = keyFactory.generatePublic(x509KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.DECRYPT_MODE, publicK)
    val inputLen: Int = encryptedData.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_DECRYPT_BLOCK)
        {
          cache = cipher.doFinal(encryptedData, offSet, MAX_DECRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(encryptedData, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_DECRYPT_BLOCK
      }
    }
    val decryptedData: Array[Byte] = out.toByteArray
    out.close()
    decryptedData
  }

  @throws[Exception]
  def encryptByPublicKey(data: Array[Byte], publicKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(publicKey)
    val x509KeySpec: X509EncodedKeySpec = new X509EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val publicK: Key = keyFactory.generatePublic(x509KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.ENCRYPT_MODE, publicK)
    val inputLen: Int = data.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_ENCRYPT_BLOCK)
        {
          cache = cipher.doFinal(data, offSet, MAX_ENCRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(data, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_ENCRYPT_BLOCK
      }
    }
    val encryptedData: Array[Byte] = out.toByteArray
    out.close()
    encryptedData
  }

  @throws[Exception]
  def encryptByPrivateKey(data: Array[Byte], privateKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(privateKey)
    val pkcs8KeySpec: PKCS8EncodedKeySpec = new PKCS8EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val privateK: Key = keyFactory.generatePrivate(pkcs8KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.ENCRYPT_MODE, privateK)
    val inputLen: Int = data.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_ENCRYPT_BLOCK)
        {
          cache = cipher.doFinal(data, offSet, MAX_ENCRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(data, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_ENCRYPT_BLOCK
      }
    }
    val encryptedData: Array[Byte] = out.toByteArray
    out.close()
    encryptedData
  }

  @throws[Exception]
  def getPrivateKey(keyMap: util.Map[String, AnyRef]): String =
  {
    val key: Key = keyMap.get(PRIVATE_KEY).asInstanceOf[Key]
    Base64Util.encode(key.getEncoded)
  }

  @throws[Exception]
  def getPublicKey(keyMap: util.Map[String, AnyRef]): String =
  {
    val key: Key = keyMap.get(PUBLIC_KEY).asInstanceOf[Key]
    Base64Util.encode(key.getEncoded)
  }
}package com.avcdata.etl.common.util.encryption.rsa

import java.io.ByteArrayOutputStream
import java.security.Key
import java.security.KeyFactory
import java.security.KeyPair
import java.security.KeyPairGenerator
import java.security.PrivateKey
import java.security.PublicKey
import java.security.Signature
import java.security.interfaces.RSAPrivateKey
import java.security.interfaces.RSAPublicKey
import java.security.spec.PKCS8EncodedKeySpec
import java.security.spec.X509EncodedKeySpec
import java.util
import javax.crypto.Cipher

object RSAUtil
{
  val KEY_ALGORITHM: String = "RSA"
  val SIGNATURE_ALGORITHM: String = "MD5withRSA"
  private val PUBLIC_KEY: String = "RSAPublicKey"
  private val PRIVATE_KEY: String = "RSAPrivateKey"
  private val MAX_ENCRYPT_BLOCK: Int = 117
  private val MAX_DECRYPT_BLOCK: Int = 128

  @throws[Exception]
  def genKeyPair: util.Map[String, AnyRef] =
  {
    val keyPairGen: KeyPairGenerator = KeyPairGenerator.getInstance(KEY_ALGORITHM)
    keyPairGen.initialize(1024)
    val keyPair: KeyPair = keyPairGen.generateKeyPair
    val publicKey: RSAPublicKey = keyPair.getPublic.asInstanceOf[RSAPublicKey]
    val privateKey: RSAPrivateKey = keyPair.getPrivate.asInstanceOf[RSAPrivateKey]
    val keyMap: util.Map[String, AnyRef] = new util.HashMap[String, AnyRef](2)
    keyMap.put(PUBLIC_KEY, publicKey)
    keyMap.put(PRIVATE_KEY, privateKey)
    keyMap
  }

  @throws[Exception]
  def sign(data: Array[Byte], privateKey: String): String =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(privateKey)
    val pkcs8KeySpec: PKCS8EncodedKeySpec = new PKCS8EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val privateK: PrivateKey = keyFactory.generatePrivate(pkcs8KeySpec)
    val signature: Signature = Signature.getInstance(SIGNATURE_ALGORITHM)
    signature.initSign(privateK)
    signature.update(data)
    Base64Util.encode(signature.sign)
  }

  @throws[Exception]
  def verify(data: Array[Byte], publicKey: String, sign: String): Boolean =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(publicKey)
    val keySpec: X509EncodedKeySpec = new X509EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val publicK: PublicKey = keyFactory.generatePublic(keySpec)
    val signature: Signature = Signature.getInstance(SIGNATURE_ALGORITHM)
    signature.initVerify(publicK)
    signature.update(data)
    signature.verify(Base64Util.decode(sign))
  }

  @throws[Exception]
  def decryptByPrivateKey(encryptedData: Array[Byte], privateKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(privateKey)
    val pkcs8KeySpec: PKCS8EncodedKeySpec = new PKCS8EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val privateK: Key = keyFactory.generatePrivate(pkcs8KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.DECRYPT_MODE, privateK)
    val inputLen: Int = encryptedData.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_DECRYPT_BLOCK)
        {
          cache = cipher.doFinal(encryptedData, offSet, MAX_DECRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(encryptedData, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_DECRYPT_BLOCK
      }
    }
    val decryptedData: Array[Byte] = out.toByteArray
    out.close()
    decryptedData
  }

  @throws[Exception]
  def decryptByPublicKey(encryptedData: Array[Byte], publicKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(publicKey)
    val x509KeySpec: X509EncodedKeySpec = new X509EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val publicK: Key = keyFactory.generatePublic(x509KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.DECRYPT_MODE, publicK)
    val inputLen: Int = encryptedData.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_DECRYPT_BLOCK)
        {
          cache = cipher.doFinal(encryptedData, offSet, MAX_DECRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(encryptedData, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_DECRYPT_BLOCK
      }
    }
    val decryptedData: Array[Byte] = out.toByteArray
    out.close()
    decryptedData
  }

  @throws[Exception]
  def encryptByPublicKey(data: Array[Byte], publicKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(publicKey)
    val x509KeySpec: X509EncodedKeySpec = new X509EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val publicK: Key = keyFactory.generatePublic(x509KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.ENCRYPT_MODE, publicK)
    val inputLen: Int = data.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_ENCRYPT_BLOCK)
        {
          cache = cipher.doFinal(data, offSet, MAX_ENCRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(data, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_ENCRYPT_BLOCK
      }
    }
    val encryptedData: Array[Byte] = out.toByteArray
    out.close()
    encryptedData
  }

  @throws[Exception]
  def encryptByPrivateKey(data: Array[Byte], privateKey: String): Array[Byte] =
  {
    val keyBytes: Array[Byte] = Base64Util.decode(privateKey)
    val pkcs8KeySpec: PKCS8EncodedKeySpec = new PKCS8EncodedKeySpec(keyBytes)
    val keyFactory: KeyFactory = KeyFactory.getInstance(KEY_ALGORITHM)
    val privateK: Key = keyFactory.generatePrivate(pkcs8KeySpec)
    val cipher: Cipher = Cipher.getInstance(keyFactory.getAlgorithm)
    cipher.init(Cipher.ENCRYPT_MODE, privateK)
    val inputLen: Int = data.length
    val out: ByteArrayOutputStream = new ByteArrayOutputStream
    var offSet: Int = 0
    var cache: Array[Byte] = null
    var i: Int = 0
    while (inputLen - offSet > 0)
    {
      {
        if (inputLen - offSet > MAX_ENCRYPT_BLOCK)
        {
          cache = cipher.doFinal(data, offSet, MAX_ENCRYPT_BLOCK)
        }
        else
        {
          cache = cipher.doFinal(data, offSet, inputLen - offSet)
        }
        out.write(cache, 0, cache.length)
        i += 1
        offSet = i * MAX_ENCRYPT_BLOCK
      }
    }
    val encryptedData: Array[Byte] = out.toByteArray
    out.close()
    encryptedData
  }

  @throws[Exception]
  def getPrivateKey(keyMap: util.Map[String, AnyRef]): String =
  {
    val key: Key = keyMap.get(PRIVATE_KEY).asInstanceOf[Key]
    Base64Util.encode(key.getEncoded)
  }

  @throws[Exception]
  def getPublicKey(keyMap: util.Map[String, AnyRef]): String =
  {
    val key: Key = keyMap.get(PUBLIC_KEY).asInstanceOf[Key]
    Base64Util.encode(key.getEncoded)
  }
}package com.avcdata.etl.launcher.jd

import java.util.UUID

import com.avcdata.etl.common.util.HdfsFileUtil
import org.apache.spark.{SparkConf, SparkContext}
import org.slf4j.LoggerFactory

/**
  * 京东->销售品类数据匹配标记
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/21 上午10:57
  */
object SalesCategoryMatch
{
  val logger = LoggerFactory.getLogger(SalesCategoryMatch.getClass)

  def main(args: Array[String])
  {
    if (args.length < 3)
    {
      println("Usage: com.avcdata.etl.jd.SalesCategoryMatch config-file-path data-root-dir batch-size")

      return
    }

    //获取配置信息
    val categoryConfigs = HdfsFileUtil.read(args(0))

    //    categoryConfigs.map(categoryConfig => )

    //由命令行设置/覆盖应用名称及相关参数
    val conf = new SparkConf().setIfMissing("spark.app.name", "SalesCategoryMatch-" + UUID.randomUUID())
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.logConf", "true")

    //创建SQL执行环境
    val sc = new SparkContext(conf)
    //    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

    //并行处理数据文件?
    val sliceStep = args(2).toInt
    val handleCount = if (categoryConfigs.length % sliceStep == 0) categoryConfigs.length / sliceStep else categoryConfigs.length + 1
    (0 until handleCount).par.foreach(handleIndex =>
    {
      val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

      categoryConfigs.slice(handleIndex * sliceStep, (handleIndex + 1) * sliceStep)
        .foreach(SqlMannerMatcher.handle(_, args(1), hiveContext))
    })
    //    categoryConfigs.par.foreach(SqlMannerMatcher.handle(_, args(1), hiveContext))

//    sc.stop()
  }
}
package com.avcdata.spark.job.konka

import com.avcdata.spark.job.konka.TerminalPowerOnDataLoadJob.getEndTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by wxc on 10/27/16.
  * 样本采集，给推总用
  */

object SampleTerminal {

    def run(sc: SparkContext, analysisDate: String) = {
        val digitRegex = """^\d+$""".r
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")

        //apk
        //val apkrdd = sc.textFile("F:/avc/docs/changhong/data/apk.txt")
        val apkrdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/activity.log." + analysisDate)
        //val apkrdd = sc.textFile("/user/hdfs/rsync/KONKA/history/activity.log.since_10-01")
            .mapPartitions(items => {

                items.map(line => {
                    val cols = line.split('|')
                    val sn = cols(0)
                    val model = cols(2)
                    ("KO"+"0x01"+model, sn)
                })
            }).distinct()

        //直播
        val terRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/tv_logo.log." + analysisDate)
        //val terRdd = sc.textFile("/user/hdfs/rsync/KONKA/history/tv_logo.log.since_10-01")
            .filter(x =>{
                val cols = x.split('|')
                cols.length == 7 && cols(6) != "" && cols(6).length == 19
            })
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split('|')
                    val sn = cols(0)
                    val model = cols(2)
                    ("KO"+"0x01"+model, sn)
                })
            })

        val apkocRdd = apkrdd.union(terRdd).distinct()

        //oc
        /*val apkocRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/duration.log." + analysisDate)
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split('|')
                    val sn = cols(0)
                    (sn, 1)
                })
            })*/

        //ko_sn_brand
        /*val sqlc = new HiveContext(sc)
        val snBrand = sqlc.sql("select * from hr.ko_sn_brand").mapPartitions(items => {
            items.map(line => {
                (line(1).toString, line(2).toString)
            })
        })*/

        val sqlc = new HiveContext(sc)
        val snBrand = sqlc.sql("select brand,model,license from hr.ko_mp_brand").mapPartitions(items => {
            items.map(line => {
                (line(0).toString+"0x01"+line(1).toString, line(2).toString)
            })
        })

        //println("count : " + apkocRdd.leftOuterJoin(snBrand).count())

        //获取license
        val apkBrand = apkocRdd.leftOuterJoin(snBrand).filter(x => !x._2.toString.contains("None")).mapPartitions(items => {
            items.map(line => {

                val sn = line._2._1
                val license = line._2._2.get

                (sn,license)
            })
        })

        val ter = sqlc.sql("select * from hr.terminal where brand = 'KO'")
            .mapPartitions(items => {
                items.map(line => {
                    (line(1).toString, line)
                })
            })

        val sampleRdd = apkBrand.leftOuterJoin(ter).filter(x => !x._2.toString().contains("None"))

        //println("sampleRdd count : " + sampleRdd.count())
        sampleRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("sample_terminal_four_test")) //sample_terminal_four

            try {
                items.foreach(item => {
                    val sn = item._1
                    val cols = item._2._2.get.toString().split(",")
                    val brand = "KO"
                    val license = item._2._1
                    val model = cols(9)
                    val lastPowerOn = cols(3)
                    val size = cols(8)
                    val area = cols(4)
                    val province = cols(5)
                    val city = cols(6)
                    val clevel = cols(7)

                    val put = new Put(Bytes.toBytes(sn + "KO"))
                    put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                    put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes(license))
                    put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                    put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                    put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                    put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                    put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                    put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                    put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                    put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))

                    mutator.mutate(put)
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }

}

object testsa {
    def main(args: Array[String]): Unit = {
        val x = (""""d8:47:10:6d:3c:a5CH","d8:47:10:6d:3c:a5","CH",,"华北","辽宁省","丹东市","其他",,"55A1U","tencent","CH"""")
        println(x.split(",")(9))
    }
}
package com.avcdata.etl.launcher.util

import com.avcdata.etl.common.util.{ScriptFormatter, VariableSubstitution}
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext
import org.slf4j.LoggerFactory

/**
  * 脚本行执行
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 下午12:52
  */
object ScriptExecutor
{
  val logger = LoggerFactory.getLogger(ScriptExecutor.getClass)

  /**
    * 指定需要执行的脚本行,返回最后一行脚本执行结果集
    *
    * @param scriptLines  需要执行的脚本行
    * @param hiveContext  HIVE上下文对象
    * @param mergedParams 执行参数
    * @return 执行结果集
    */
  def executeScriptLines(scriptLines: Seq[String], hiveContext: HiveContext, mergedParams: Map[String, String]): DataFrame =
  {
    logger.info(s"The original script content is => ${scriptLines.mkString("\n")}")

    //变量替换
    val varSubstitution = new VariableSubstitution(mergedParams)
    val parsedScripts = scriptLines.map(varSubstitution.substitute)

    logger.info(s"Parsed script content is => ${parsedScripts.mkString("\n")}")

    //格式化脚本
    val formatedScripts = ScriptFormatter.groupingScripts(parsedScripts)

    if (formatedScripts.isEmpty) throw new IllegalArgumentException(s"There is no executable lines in $scriptLines file.")

    logger.info(s"Formated script content is => ${formatedScripts.mkString("\n")}")

    //执行除最后一行脚本之前的脚本
    formatedScripts.reverse.tail.reverse.foreach(script =>
    {
      logger.info(s"Begin to execute before the last script -> $script")
      hiveContext.sql(script)
    })

    //执行最后一行脚本
    val lastScript = formatedScripts.last
    logger.info(s"Begin to execute last script -> $lastScript")
    val df = hiveContext.sql(lastScript)

    //打印执行计划
    if (logger.isDebugEnabled) df.explain(true)

    df
  }
}
package com.avcdata.etl.common.util

/**
  * 脚本格式化处理相关
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 上午10:40
  */
object ScriptFormatter
{
  /**
    * 通过原始脚本字符串行返回可逐条执行的脚本
    *
    * @param srcScriptLines 原始脚本行
    * @return 可执行的脚本行
    */
  def groupingScripts(srcScriptLines: Seq[String]): Seq[String] =
  {
    var executableSqlSeq = 1

    //过滤空行
    srcScriptLines.filter(line => !"""^\s*$""".r.findAllIn(line).hasNext)
      //过滤注释
      .filter(line => !"""^\s*[#|\-\-].*""".r.findAllIn(line).hasNext)
      //对脚本进行分组
      .map(line => if (line.trim.endsWith(";"))
    {
      val currentScriptSeq = executableSqlSeq
      executableSqlSeq += 1

      (line.substring(0, line.lastIndexOf(";")), currentScriptSeq)
    }
    else (line, executableSqlSeq))
      //以";"为分隔符对脚本进行分组
      .groupBy(_._2).toList.sortBy(_._1)
      //将分组脚本行合并成一条脚本
      .map(_._2.map(_._1)).map(_.mkString("\n"))
  }
}
package com.avcdata.etl.common.util

/**
  * 脚本格式化处理相关
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 上午10:40
  */
object ScriptFormatter
{
  /**
    * 通过原始脚本字符串行返回可逐条执行的脚本
    *
    * @param srcScriptLines 原始脚本行
    * @return 可执行的脚本行
    */
  def groupingScripts(srcScriptLines: Seq[String]): Seq[String] =
  {
    var executableSqlSeq = 1

    //过滤空行
    srcScriptLines.filter(line => !"""^\s*$""".r.findAllIn(line).hasNext)
      //过滤注释
      .filter(line => !"""^\s*[#|\-\-].*""".r.findAllIn(line).hasNext)
      //对脚本进行分组
      .map(line => if (line.trim.endsWith(";"))
    {
      val currentScriptSeq = executableSqlSeq
      executableSqlSeq += 1

      (line.substring(0, line.lastIndexOf(";")), currentScriptSeq)
    }
    else (line, executableSqlSeq))
      //以";"为分隔符对脚本进行分组
      .groupBy(_._2).toList.sortBy(_._1)
      //将分组脚本行合并成一条脚本
      .map(_._2.map(_._1)).map(_.mkString("\n"))
  }
}
package com.avcdata.etl.common.util

/**
  * 脚本格式化处理相关
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 上午10:40
  */
object ScriptFormatter
{
  /**
    * 通过原始脚本字符串行返回可逐条执行的脚本
    *
    * @param srcScriptLines 原始脚本行
    * @return 可执行的脚本行
    */
  def groupingScripts(srcScriptLines: Seq[String]): Seq[String] =
  {
    var executableSqlSeq = 1

    //过滤空行
    srcScriptLines.filter(line => !"""^\s*$""".r.findAllIn(line).hasNext)
      //过滤注释
      .filter(line => !"""^\s*[#|\-\-].*""".r.findAllIn(line).hasNext)
      //对脚本进行分组
      .map(line => if (line.trim.endsWith(";"))
    {
      val currentScriptSeq = executableSqlSeq
      executableSqlSeq += 1

      (line.substring(0, line.lastIndexOf(";")), currentScriptSeq)
    }
    else (line, executableSqlSeq))
      //以";"为分隔符对脚本进行分组
      .groupBy(_._2).toList.sortBy(_._1)
      //将分组脚本行合并成一条脚本
      .map(_._2.map(_._1)).map(_.mkString("\n"))
  }
}
package com.avcdata.etl.common.util

/**
  * 脚本格式化处理相关
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 上午10:40
  */
object ScriptFormatter
{
  /**
    * 通过原始脚本字符串行返回可逐条执行的脚本
    *
    * @param srcScriptLines 原始脚本行
    * @return 可执行的脚本行
    */
  def groupingScripts(srcScriptLines: Seq[String]): Seq[String] =
  {
    var executableSqlSeq = 1

    //过滤空行
    srcScriptLines.filter(line => !"""^\s*$""".r.findAllIn(line).hasNext)
      //过滤注释
      .filter(line => !"""^\s*[#|\-\-].*""".r.findAllIn(line).hasNext)
      //对脚本进行分组
      .map(line => if (line.trim.endsWith(";"))
    {
      val currentScriptSeq = executableSqlSeq
      executableSqlSeq += 1

      (line.substring(0, line.lastIndexOf(";")), currentScriptSeq)
    }
    else (line, executableSqlSeq))
      //以";"为分隔符对脚本进行分组
      .groupBy(_._2).toList.sortBy(_._1)
      //将分组脚本行合并成一条脚本
      .map(_._2.map(_._1)).map(_.mkString("\n"))
  }
}
package pojo

import com.alibaba.fastjson.{JSON, JSONObject}
import utils.{DBHelper, JDBCConnectionPool, LoanPattern, Tools}

import scala.collection.JavaConversions._
import scala.collection.mutable.ListBuffer

/**
  * Created by guxiaoyang on 2017/6/16.
  */
case class SDKStreamBean(sn: String, manufacturer: String, data: ListBuffer[DataDetail]) extends Serializable

case class DataDetail(
                       date: String,
                       hour: String,
                       messagetype: Int,
                       build_date: String,
                       product: String,
                       version_release: String,
                       name: String,
                       description: String,
                       model: String,
                       restarttime: String,
                       duration: String,
                       programstart: String,
                       programtype: String,
                       provider: String,
                       programend: String,
                       programname: String,
                       package_name: String,
                       order: String,
                       ipmac: String,
                       province: String,
                       city: String
                     )

object SDKStreamToBean
{
  val mysqlurl = "jdbc:mysql://192.168.1.201:3306/vboxDB?useUnicode=true&characterEncoding=utf-8&useSSL=false"
  val mysqlusername = "root"
  val mysqlpassword = "new.1234"


  def ToBean(item: JSONObject): SDKStreamBean =
  {
    try
    {
      val sn = if (item.getString("mac") == null || item.getString("mac").isEmpty) "00:00:00:00:00" else item.getString("mac")

      val manufacturer = if (item.getString("manufacturer") == null || item.getString("manufacturer").isEmpty) "-1" else item.getString("manufacturer")

      val jsonDataArr = if (item.getJSONArray("data") == null || item.getJSONArray("data").isEmpty) null else item.getJSONArray("data")

      val list = new ListBuffer[DataDetail]()
      //ipmac列表是单独的格式
      val messageType = if (item.getString("messagetype") != null) item.getString("messagetype").toInt else -1
      var ipMacList = new StringBuilder
      if (messageType == 5)
      {
        val dateTime = if (item.getString("date") == null || item.getString("date").isEmpty) "1970-01-01 08:00:00" else Tools.getTimeFromUnix(item.getString("date"))

        val date = dateTime.split(" ")(0)
        val hour = dateTime.split(" ")(1)
        val ipMacArr = if (item.getJSONArray("data") == null || item.getJSONArray("data").isEmpty) null else item.getJSONArray("data")
        if (ipMacArr != null)
        {
          ipMacArr.foreach(a =>
          {
            val jsonData = JSON.parseObject(a.toString)
            val ip = if (jsonData.getString("ip") != null) jsonData.getString("ip") else "-1"
            val mac = if (jsonData.getString("mac") != null) jsonData.getString("mac") else "-1"
            ipMacList.append(ip + "," + mac + "#")
          }
          )
        }
        //println("messageType == 5:" + dateTime)
        val dataDetail = new DataDetail(date, hour, messageType, "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", ipMacList.toString(), "-1", "-1")
        list += dataDetail
        SDKStreamBean(sn, manufacturer, list)
      }
      else if (messageType == 20 || messageType == 21)
      {
        //20,21和ipmac格式相同
        val time = item.getString("date")
        val dateTime = if (time == null || time.isEmpty) "1970-01-01 08:00:00" else Tools.getTimeFromUnix(time)
        //println("messageType == 20,21:" + dateTime)
        val date = dateTime.split(" ")(0)
        val hour = dateTime.split(" ")(1)

        val dataDetail = new DataDetail(date, hour, messageType, "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1", "-1")
        list += dataDetail
        SDKStreamBean(sn, manufacturer, list)
      }
      else
      {
        if (jsonDataArr != null)
        {
          jsonDataArr.foreach(a =>
          {
            val jsonData = JSON.parseObject(a.toString)

            val messageType = if (jsonData.getString("messagetype") != null) jsonData.getString("messagetype").toInt else -1
            //暂时不取行为为8的数据
            if (messageType != 8)
            {
              val time = jsonData.getString("date")
              var dateTime = ""
              //messageType == 20,21的数据也可能存在在data里
              if (messageType == 20 || messageType == 21 || messageType == 13)
              {
                dateTime = if (time == null || time.isEmpty) "1970-01-01 08:00:00" else Tools.getTimeFromUnix(time.substring(0, time.length - 3))
              }
              else
              {
                dateTime = if (time == null || time.isEmpty) "1970-01-01 08:00:00" else Tools.getTimeFromUnix(time)
              }
              val date = dateTime.split(" ")(0)
              val hour = dateTime.split(" ")(1)

              //编译日期
              val build_date = if (jsonData.getString("build_date") == null || jsonData.getString("build_date").isEmpty) "-1" else jsonData.getString("build_date")
              //产品?
              val product = if (jsonData.getString("product") == null || jsonData.getString("product").isEmpty) "-1" else jsonData.getString("product")
              //发布的版本号
              val version_release = if (jsonData.getString("version_release") == null || jsonData.getString("version_release").isEmpty) "-1" else jsonData.getString("version_release")
              //设备名称
              val name = if (jsonData.getString("name") == null || jsonData.getString("name").isEmpty) "-1" else jsonData.getString("name")
              //设备描述
              val description = if (jsonData.getString("description") == null || jsonData.getString("description").isEmpty) "-1" else jsonData.getString("description")
              //无?
              val model = if (jsonData.getString("model") == null || jsonData.getString("model").isEmpty) "-1" else jsonData.getString("model")

              val restarttime = if (jsonData.getString("restarttime") == null || jsonData.getString("restarttime").isEmpty) "-1" else jsonData.getString("restarttime")

              //节目播放时长
              val duration = if (jsonData.getString("duration") == null || jsonData.getString("duration").isEmpty) "-1" else jsonData.getString("duration")
              //播放开始时间
              val programstart = if (jsonData.getString("programstart") == null || jsonData.getString("programstart").isEmpty) "1970-01-01 08:00:00" else Tools.getTimeFromUnix(jsonData.getString("programstart"))
              //节目类型 0 非影视类 1电影 3电视剧 4综艺
              val programtype = if (jsonData.getString("programtype") == null || jsonData.getString("programtype").isEmpty) "-1" else jsonData.getString("programtype")
              //节目来源 0:taotv,1:huasu,2:taotvjuhe,4:sohu,5:qiyi,7:youku,9:youku_charge,10:golive(全球播),11:mango(芒果)
              val provider = if (jsonData.getString("provider") == null || jsonData.getString("provider").isEmpty) "-1" else jsonData.getString("provider")
              //节目结束时间
              val programend = if (jsonData.getString("programend") == null || jsonData.getString("programend").isEmpty) "1970-01-01 08:00:00" else Tools.getTimeFromUnix(jsonData.getString("programend"))
              //节目名称
              val programname = if (jsonData.getString("programname") == null || jsonData.getString("programname").isEmpty) "-1" else jsonData.getString("programname")
              //包名
              val package_name = if (jsonData.getString("package_name") == null || jsonData.getString("package_name").isEmpty) if (jsonData.getString("packagename") == null || jsonData.getString("packagename").isEmpty) "-1" else jsonData.getString("packagename") else jsonData.getString("package_name")
              //节目集数
              val order = if (jsonData.getString("order") == null || jsonData.getString("order").isEmpty) "-1" else jsonData.getString("order")

              val province = if (jsonData.getString("province") == null || jsonData.getString("province").isEmpty) "-1" else jsonData.getString("province")
              val city = if (jsonData.getString("city") == null || jsonData.getString("city").isEmpty) "-1" else jsonData.getString("city")

              val dataDetail = new DataDetail(date, hour, messageType, build_date, product, version_release, name, description, model, restarttime, duration, programstart, programtype, provider, programend, programname, package_name, order, "", province, city)

              list += dataDetail
            }
          }
          )
        }

        SDKStreamBean(sn, manufacturer, list)
      }
    }
    catch
    {
      case e: Exception =>
      {
        LoanPattern.using(JDBCConnectionPool(mysqlurl, mysqlusername, mysqlpassword))
        { conn =>
          DBHelper.insertException(e.toString, conn)
        }
        null
      }
    }
  }
}package com.avcdata.spark.job.playcrawler

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 到剧搜索指数数据清洗
  */
object SearchIndexDataLoadJob {
  /////////////////////////////////////////////test//////////////////////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SearchIndexDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-01")
    sc.stop()
  }

  def run(sc: SparkContext, analysisDate: String) = {

    //////////////test/////////////////////////////////
    //    val hdfsPath = "S:\\aowei\\tracker-job\\doc\\data\\爬虫\\11月\\"+ analysisDate + "_index_360.csv"
    //////////////test/////////////////////////////////

    val hdfsPath360 = "/user/hdfs/rsync/playcrawler/" + analysisDate + "/" + analysisDate + "_index_360.csv"
    val hdfsPathBaidu = "/user/hdfs/rsync/playcrawler/" + analysisDate + "/" + analysisDate + "_index_baidu.csv"

    val result360RDD = sc.textFile(hdfsPath360).distinct().filter(line => {
      !line.contains("category")
    })

      .map(line => {
        val cols = line.split(",")
        val category = cols(0)
        val index = cols(1)
        val name = cols(2)
        val time = cols(3)
        val platform = hdfsPath360.substring(hdfsPath360.lastIndexOf("_") + 1, hdfsPath360.lastIndexOf("."))
        platform + "\t" + category + "\t" + index + "\t" + name + "\t" + time
      })

    val resultBaiduRDD = sc.textFile(hdfsPathBaidu).distinct().filter(line => {
      !line.contains("category")
    })
      .map(line => {
        val cols = line.split(",")
        val category = cols(0)
        val index = cols(1)
        val name = cols(2)
        val time = cols(3)
        val platform = hdfsPathBaidu.substring(hdfsPathBaidu.lastIndexOf("_") + 1, hdfsPathBaidu.lastIndexOf("."))
        platform + "\t" + category + "\t" + index + "\t" + name + "\t" + time
      })

    val resultRDD = result360RDD.union(resultBaiduRDD)
    resultRDD
      //    写入到Hbase
      .foreachPartition(lines => {

      val mutator = HBaseUtils.getMutator("tracker_plays_active_index")

      try {

        lines.foreach(line => {
          var i = 0

          val cols = line.split("\t")

          val platform = cols(i)
          i = i + 1

          val category = cols(i)
          i = i + 1

          val index = cols(i)
          i = i + 1

          val name = cols(i)
          i = i + 1

          val time = cols(i)

          val sortedLine = platform + "\t" + category + "\t" + index + "\t" + name + "\t" + time

          //          println(sortedLine)

          mutator.mutate(HBaseUtils.getPut_Plays_SearchIndex(sortedLine))


        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

  }

}
package com.avcdata.spark.job.stat

import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  *
  * hbase
  * disable 'tracker_silent_terminal';
  * drop 'tracker_silent_terminal';
  * create 'tracker_silent_terminal', 'info'

  * drop 'tracker_silent_terminal'
  * create 'tracker_silent_terminal', 'info'

  * hive
  * Drop table  hr.tracker_silent_terminal;
  * CREATE EXTERNAL TABLE hr.tracker_silent_terminal(
  * key string,
  * dim_date string,
  * dim_apk string,
  * brand string,
  * license string,
  * province string,
  * terminal_cnt string
  * )
  * STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  * WITH SERDEPROPERTIES ("hbase.columns.mapping" =
  * "info:dim_date,
  * info:dim_apk ,
  * info:brand,
  * info:license,
  * info:province,
  * info:terminal_cnt")
  * TBLPROPERTIES("hbase.table.name" = "tracker_silent_terminal");
  *
  * @author zhangyongtian
  * @define 2017新增终端相对上月的沉默终端数
  */
object SilentTerminal {
  val log = Logger.getLogger(getClass.getName)

  case class TerminalAPK(sn: String, dim_apk: String, brand: String, license: String, province: String)

  def main(args: Array[String]) {

    println(isLauncherApk("haha","youku"))

//    val conf = new SparkConf()
//      .setMaster("local[1]")
//      .setAppName("SilentTerminal")
//    val sc = new SparkContext(conf)
//    run(sc, "2017-01-04")
//
//    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date


    val year = currentDate.substring(0, 4)
    val month = currentDate.substring(5, 7)
    //    val yearmonth = year + month
    val yearmonth2 = year + "-" + month
    val firstDayOfMonth = yearmonth2 + "-01"

    val firstDayOfNextMonth = TimeUtils.getPerFirstDayOfMonth(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    //TODO 获取数据
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    sqlContext.sql("use hr")

    ////////////////////////////////////////////////计算沉默终端数
    // ////////////////////////////////////////////////////////////////////////////////

    //TODO 截止到当月第一次使用apk的sn
    val beforeSql = "SELECT DISTINCT dim_sn, dim_apk FROM hr.tracker_new_terminal join hr.sample_terminal_three st on" +
      " (dim_sn=st.sn) " +
      "JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (dim_apk = ai.appname) " +
      " WHERE dim_date < '" +
      firstDayOfNextMonth + "'"
    val beforeSnAppName = sqlContext.sql(beforeSql).distinct.rdd.map(line => {
      val sn = line(0)
      val appname = line(1)
      sn + "\t" + appname
    })
    println("beforeSql:" + beforeSql)
    println("beforePair" + beforeSnAppName.count)


    //TODO 当月的apk行为的sn
    val currentMonthSql = "SELECT DISTINCT dim_sn, ai.appname   FROM hr.tracker_apk_fact_partition af   JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (af.dim_apk = ai.packagename) join hr.sample_terminal_three st on(af.dim_sn=st.sn)  " +
      "WHERE date LIKE '" +
      yearmonth2 + "%'"


    val currentMonthSnAppName = sqlContext.sql(currentMonthSql).distinct.rdd.map(line => {
      val sn = line(0)
      val appname = line(1)
      sn + "\t" + appname
    })
    println("currentMonthPair：" + currentMonthSnAppName.count)


    //TODO 当月的沉默终端数= 截止到当月第一次使用apk的sn。补集（）
    val silentPair = beforeSnAppName.map((_, 1)).leftOuterJoin(currentMonthSnAppName.map((_, 1)))
      .filter(line => {
        line._2._2.isEmpty
      })
      .map(line => {
        val cols = line._1.split("\t")
        val sn = cols(0)
        val appname = cols(1)
        (sn, appname)
      })

    println("silentPair：" + silentPair.count)



    //TODO 获取终端信息
    val terminalPair = sqlContext.sql("select sn,brand,license,province from hr.sample_terminal_three").rdd.distinct
      .map(line => {
        val sn = line(0).toString
        val brand = line(1)
        val license = line(2)
        val province = line(3)

        (sn, brand + "\t" + license + "\t" + province)
      })

    val terminalApkSilentDF = silentPair.join(terminalPair).map(line => {

      val sn = line._1

      //应用名称
      val dim_apk = line._2._1.toString

      val cols = line._2._2.split("\t")
      val brand = cols(0)
      val license = cols(1)
      val province = cols(2)

      TerminalAPK(sn, dim_apk, brand, license, province)
    }).toDF


    //TODO 注册成临时表
    terminalApkSilentDF.registerTempTable("silent_terminal_apk")



    ////////////////////////////////////////计算装机量//////////////////////////////////////////////////////////////
    //TODO 截止当月的装机量
    val beforeSql2 = "SELECT DISTINCT dim_sn, dim_apk FROM hr.tracker_new_terminal " +
      " JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (dim_apk = ai.appname)" +
      " WHERE dim_date < '" +
      firstDayOfNextMonth + "'"

    val terminalApkAllDF =
      sqlContext.sql(beforeSql2).rdd.map(line => {
        val sn = line(0).toString
        val appname = line(1).toString
        (sn, appname)
      }).join(terminalPair).map(line => {

        val sn = line._1

        //应用名称
        val dim_apk = line._2._1.toString

        val cols = line._2._2.split("\t")
        val brand = cols(0)
        val license = cols(1)
        val province = cols(2)

        TerminalAPK(sn, dim_apk, brand, license, province)
      }).toDF

    //TODO 注册成临时表
    terminalApkAllDF.registerTempTable("all_terminal_apk")

    val resultRDD = sqlContext.sql(
      """
        select
        	silent_apk.dim_apk, silent_apk.brand, silent_apk.license,silent_apk.province,
        	silent_apk.cnt as slient_cnt,all_apk.cnt as all_cnt
        from
        (
          select dim_apk, brand, license, province,count(distinct sn) as cnt
          from silent_terminal_apk group by dim_apk, brand, license, province
        ) silent_apk
        join
        (
          select dim_apk, brand, license, province,count(distinct sn) as cnt
          from all_terminal_apk
          join
          (select distinct appname from hr.apkinfo where onelevel = '视频') ai
          ON (dim_apk = ai.appname)
          group by dim_apk, brand, license, province
        ) all_apk
        on
        (
        silent_apk.dim_apk = all_apk.dim_apk and
        silent_apk.brand = all_apk.brand and
        silent_apk.license = all_apk.license and
        silent_apk.province = all_apk.province
        )

      """.stripMargin).rdd


    ///////////////////////////////////////////test/////////////////
    //      println(resultRDD.count)
    //          .saveAsTextFile("/tmp/silent_terminal" + System.currentTimeMillis)


    //    // TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:terminal_cnt

          val dim_date = firstDayOfMonth

          //TODO 应用名称


          val brand = line(1)

          val license = line(2).toString

          val province = line(3)


          val dim_apk = line(0).toString

          //独立apk
          var apk_type = "apk"

          //launcher
          if (isLauncherApk(dim_apk, license)) {
            apk_type = "launcher"
          }


          val silent_cnt = line(4)

          val all_cnt = line(5)


          val orderedLine = dim_date + "\t" + apk_type + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + silent_cnt + "\t" + all_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


  def isLauncherApk(dim_apk: String, license: String): Boolean = {

    ((dim_apk.equals("银河·奇异果")) && license.equals("yinhe")) || ((dim_apk.equals("CIBN环球影视")) && license.equals("youku")) || ((dim_apk.equals("腾讯视频TV端")) && license.equals("tencent")) || ((dim_apk.equals("CIBN环球影视")) && license.equals("YUNOS"))

  }


}
package com.avcdata.vbox.stat

import com.avcdata.vbox.util.{HBaseUtils, TimeUtils}
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  *
  * hbase
  * disable 'tracker_silent_terminal';
  * drop 'tracker_silent_terminal';
  * create 'tracker_silent_terminal', 'info'

  * drop 'tracker_silent_terminal'
  * create 'tracker_silent_terminal', 'info'

  * hive
  * Drop table  hr.tracker_silent_terminal;
  * CREATE EXTERNAL TABLE hr.tracker_silent_terminal(
  * key string,
  * dim_date string,
  * dim_apk string,
  * brand string,
  * license string,
  * province string,
  * terminal_cnt string
  * )
  * STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  * WITH SERDEPROPERTIES ("hbase.columns.mapping" =
  * "info:dim_date,
  * info:dim_apk ,
  * info:brand,
  * info:license,
  * info:province,
  * info:terminal_cnt")
  * TBLPROPERTIES("hbase.table.name" = "tracker_silent_terminal");
  *
  * @author zhangyongtian
  * @define 2017新增终端相对上月的沉默终端数
  */
object SilentTerminal {
  val log = Logger.getLogger(getClass.getName)

  case class TerminalAPK(sn: String, dim_apk: String, brand: String, license: String, province: String)

  def main(args: Array[String]) {

    println(isLauncherApk("haha", "youku"))

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("SilentTerminal")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-01-04")
    //
    //    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date


    val year = currentDate.substring(0, 4)
    val month = currentDate.substring(5, 7)
    //    val yearmonth = year + month
    val yearmonth2 = year + "-" + month
    val firstDayOfMonth = yearmonth2 + "-01"

    val firstDayOfNextMonth = TimeUtils.getPerFirstDayOfMonth(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE)

    //TODO 获取数据
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    sqlContext.sql("use hr")

    ////////////////////////////////////////////////计算沉默终端数
    // ////////////////////////////////////////////////////////////////////////////////

    //TODO 截止到当月第一次使用apk的sn
    val beforeSql = "SELECT DISTINCT dim_sn, dim_apk FROM hr.tracker_new_terminal join hr.sample_terminal_three st on" +
      " (dim_sn=st.sn) " +
      "JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (dim_apk = ai.appname) " +
      " WHERE dim_date < '" +
      firstDayOfNextMonth + "'"
    val beforeSnAppName = sqlContext.sql(beforeSql).distinct.rdd.map(line => {
      val sn = line(0)
      val appname = line(1)
      sn + "\t" + appname
    })
    println("beforeSql:" + beforeSql)
    println("beforePair" + beforeSnAppName.count)


    //TODO 当月的apk行为的sn
    val currentMonthSql = "SELECT DISTINCT dim_sn, ai.appname   FROM hr.tracker_apk_fact_partition af   JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (af.dim_apk = ai.packagename) join hr.sample_terminal_three st on(af.dim_sn=st.sn)  " +
      "WHERE date LIKE '" +
      yearmonth2 + "%'"


    val currentMonthSnAppName = sqlContext.sql(currentMonthSql).distinct.rdd.map(line => {
      val sn = line(0)
      val appname = line(1)
      sn + "\t" + appname
    })
    println("currentMonthPair：" + currentMonthSnAppName.count)


    //TODO 当月的沉默终端数= 截止到当月第一次使用apk的sn。补集（）
    val silentPair = beforeSnAppName.map((_, 1)).leftOuterJoin(currentMonthSnAppName.map((_, 1)))
      .filter(line => {
        line._2._2.isEmpty
      })
      .map(line => {
        val cols = line._1.split("\t")
        val sn = cols(0)
        val appname = cols(1)
        (sn, appname)
      })

    println("silentPair：" + silentPair.count)



    //TODO 获取终端信息
    val terminalPair = sqlContext.sql("select sn,brand,license,province from hr.sample_terminal_three").rdd.distinct
      .map(line => {
        val sn = line(0).toString
        val brand = line(1)
        val license = line(2)
        val province = line(3)

        (sn, brand + "\t" + license + "\t" + province)
      })

    val terminalApkSilentDF = silentPair.join(terminalPair).map(line => {

      val sn = line._1

      //应用名称
      val dim_apk = line._2._1.toString

      val cols = line._2._2.split("\t")
      val brand = cols(0)
      val license = cols(1)
      val province = cols(2)

      TerminalAPK(sn, dim_apk, brand, license, province)
    }).toDF


    //TODO 注册成临时表
    terminalApkSilentDF.registerTempTable("silent_terminal_apk")



    ////////////////////////////////////////计算装机量//////////////////////////////////////////////////////////////
    //TODO 截止当月的装机量
    val beforeSql2 = "SELECT DISTINCT dim_sn, dim_apk FROM hr.tracker_new_terminal " +
      " JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (dim_apk = ai.appname)" +
      " WHERE dim_date < '" +
      firstDayOfNextMonth + "'"

    val terminalApkAllDF =
      sqlContext.sql(beforeSql2).rdd.map(line => {
        val sn = line(0).toString
        val appname = line(1).toString
        (sn, appname)
      }).join(terminalPair).map(line => {

        val sn = line._1

        //应用名称
        val dim_apk = line._2._1.toString

        val cols = line._2._2.split("\t")
        val brand = cols(0)
        val license = cols(1)
        val province = cols(2)

        TerminalAPK(sn, dim_apk, brand, license, province)
      }).toDF

    //TODO 注册成临时表
    terminalApkAllDF.registerTempTable("all_terminal_apk")

    val resultRDD = sqlContext.sql(
      """
        select
        	silent_apk.dim_apk, silent_apk.brand, silent_apk.license,silent_apk.province,
        	silent_apk.cnt as slient_cnt,all_apk.cnt as all_cnt
        from
        (
          select dim_apk, brand, license, province,count(distinct sn) as cnt
          from silent_terminal_apk group by dim_apk, brand, license, province
        ) silent_apk
        join
        (
          select dim_apk, brand, license, province,count(distinct sn) as cnt
          from all_terminal_apk
          join
          (select distinct appname from hr.apkinfo where onelevel = '视频') ai
          ON (dim_apk = ai.appname)
          group by dim_apk, brand, license, province
        ) all_apk
        on
        (
        silent_apk.dim_apk = all_apk.dim_apk and
        silent_apk.brand = all_apk.brand and
        silent_apk.license = all_apk.license and
        silent_apk.province = all_apk.province
        )

      """.stripMargin).rdd


    ///////////////////////////////////////////test/////////////////
    //      println(resultRDD.count)
    //          .saveAsTextFile("/tmp/silent_terminal" + System.currentTimeMillis)


    //    // TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:terminal_cnt

          val dim_date = firstDayOfMonth

          //TODO 应用名称


          val brand = line(1)

          val license = line(2).toString

          val province = line(3)


          val dim_apk = line(0).toString

          //独立apk
          var apk_type = "apk"

          //launcher
          if (isLauncherApk(dim_apk, license)) {
            apk_type = "launcher"
          }


          val silent_cnt = line(4)

          val all_cnt = line(5)


          val orderedLine = dim_date + "\t" + apk_type + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + silent_cnt + "\t" + all_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


  def isLauncherApk(dim_apk: String, license: String): Boolean = {

    ((dim_apk.equals("银河·奇异果")) && license.equals("yinhe")) || ((dim_apk.equals("CIBN环球影视")) && license.equals("youku")) || ((dim_apk.equals("腾讯视频TV端")) && license.equals("tencent")) || ((dim_apk.equals("CIBN环球影视")) && license.equals("YUNOS"))

  }


}
package com.avcdata.spark.job.stat

import com.avcdata.spark.job.until.TimeUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * hbase
  * disable 'tracker_silent_terminal';
  * drop 'tracker_silent_terminal';
  * create 'tracker_silent_terminal', 'info'

  * drop 'tracker_silent_terminal'
  * create 'tracker_silent_terminal', 'info'

  * hive
  * Drop table  hr.tracker_silent_terminal;
  * CREATE EXTERNAL TABLE hr.tracker_silent_terminal(
  * key string,
  * dim_date string,
  * dim_apk string,
  * brand string,
  * license string,
  * province string,
  * terminal_cnt string
  * )
  * STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  * WITH SERDEPROPERTIES ("hbase.columns.mapping" =
  * "info:dim_date,
  * info:dim_apk ,
  * info:brand,
  * info:license,
  * info:province,
  * info:terminal_cnt")
  * TBLPROPERTIES("hbase.table.name" = "tracker_silent_terminal");
  *
  * @author zhangyongtian
  * @define 2017新增终端相对上月的沉默终端数
  */
object SilentTerminal01 {
  val log = Logger.getLogger(getClass.getName)

  case class TerminalAPK(sn: String, dim_apk: String, brand: String, license: String, province: String)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-01-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date


    val year = currentDate.substring(0, 4)
    val month = currentDate.substring(5, 7)
    //    val yearmonth = year + month
    val yearmonth2 = year + "-" + month
    val firstDayOfMonth = yearmonth2 + "-01"

    val firstDayOfNextMonth = TimeUtils.getPerFirstDayOfMonth(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE)
    val currentMonthSql2 = "SELECT DISTINCT dim_sn, ai.appname   FROM hr.tracker_apk_fact_partition af   JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (af.dim_apk = ai.packagename) join hr.sample_terminal_three st on(af.dim_sn=st.sn)  " +
      "WHERE date LIKE '" +
      yearmonth2 + "%'"

    println(currentMonthSql2)

    //TODO 获取数据
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    sqlContext.sql("use hr")

    ////////////////////////////////////////////////计算沉默终端数
    // ////////////////////////////////////////////////////////////////////////////////

    //TODO 截止到当月第一次使用apk的sn
    val beforeSql = "SELECT DISTINCT dim_sn, dim_apk FROM hr.tracker_new_terminal join hr.sample_terminal_three st on" +
      " (dim_sn=st.sn) " +
      "JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (dim_apk = ai.appname) " +
      " WHERE dim_date < '" +
      firstDayOfNextMonth + "'"
    val beforeSnAppName = sqlContext.sql(beforeSql).distinct.rdd.map(line => {
      val sn = line(0)
      val appname = line(1)
      sn + "\t" + appname
    })
    println("beforeSql:" + beforeSql)
    println("beforePair" + beforeSnAppName.count)


    //TODO 当月的apk行为的sn
    val currentMonthSql = "SELECT DISTINCT dim_sn, ai.appname   FROM hr.tracker_apk_fact_partition af   JOIN (select distinct packagename,appname from hr.apkinfo where onelevel = '视频') ai ON (af.dim_apk = ai.packagename) join hr.sample_terminal_three st on(af.dim_sn=st.sn)  " +
      "WHERE date LIKE '" +
      yearmonth2 + "%'"


    val currentMonthSnAppName = sqlContext.sql(currentMonthSql).distinct.rdd.map(line => {
      val sn = line(0)
      val appname = line(1)
      sn + "\t" + appname
    })
    println("currentMonthPair：" + currentMonthSnAppName.count)


    //    val newPair = currentMonthSnAppName.map((_, 1)).leftOuterJoin(beforeSnAppName.map((_, 1)))
    //      .filter(line => {
    //        line._2._2.isEmpty
    //      })
    //      .map(line => {
    //        val cols = line._1.split("\t")
    //        val sn = cols(0)
    //        val appname = cols(1)
    //        (sn, appname)
    //      })
    //    newPair.saveAsTextFile("/tmp/newPair")
    //    println("newPair：" + newPair.count)


    //TODO 当月的沉默终端数= 截止到当月第一次使用apk的sn。补集（）
    val silentPair = beforeSnAppName.map((_, 1)).leftOuterJoin(currentMonthSnAppName.map((_, 1)))
      .filter(line => {
        line._2._2.isEmpty
      })
      .map(line => {
        val cols = line._1.split("\t")
        val sn = cols(0)
        val appname = cols(1)
        (sn, appname)
      })
    //      .subtract(newPair)

    println("silentPair：" + silentPair.count)



    //    //TODO 获取终端信息
    val terminalPair = sqlContext.sql("select sn,brand,license,province from hr.sample_terminal_three").rdd.distinct
      .map(line => {
        val sn = line(0).toString
        val brand = line(1)
        val license = line(2)
        val province = line(3)

        (sn, brand + "\t" + license + "\t" + province)
      })

    val terminalApkSilentDF = silentPair.join(terminalPair).map(line => {

      val sn = line._1

      //应用名称
      val dim_apk = line._2._1.toString

      val cols = line._2._2.split("\t")
      val brand = cols(0)
      val license = cols(1)
      val province = cols(2)

      TerminalAPK(sn, dim_apk, brand, license, province)
    }).toDF


    //TODO 注册成临时表
    terminalApkSilentDF.registerTempTable("silent_terminal_apk")



    ////////////////////////////////////////计算装机量//////////////////////////////////////////////////////////////
    //TODO 截止当月的装机量
    val terminalApkAllDF =
      beforeSnAppName.map(line => {
        val cols = line.split("\t")
        val sn = cols(0)
        val appname = cols(1)
        (sn, appname)
      }).join(terminalPair).map(line => {

        val sn = line._1

        //应用名称
        val dim_apk = line._2._1.toString

        val cols = line._2._2.split("\t")
        val brand = cols(0)
        val license = cols(1)
        val province = cols(2)

        TerminalAPK(sn, dim_apk, brand, license, province)
      }).toDF

    //TODO 注册成临时表
    terminalApkAllDF.registerTempTable("all_terminal_apk")

    val resultRDD = sqlContext.sql(
      """
        select
        	silent_apk.dim_apk, silent_apk.brand, silent_apk.license,silent_apk.province,
        	silent_apk.cnt as slient_cnt,all_apk.cnt as all_cnt
        from
        (
          select dim_apk, brand, license, province,count(distinct sn) as cnt
          from silent_terminal_apk group by dim_apk, brand, license, province
        ) silent_apk
        join
        (
          select dim_apk, brand, license, province,count(distinct sn) as cnt
          from all_terminal_apk
          join
          (select distinct appname from hr.apkinfo where onelevel = '视频') ai
          ON (dim_apk = ai.appname)
          group by dim_apk, brand, license, province
        ) all_apk
        on
        (
        silent_apk.dim_apk = all_apk.dim_apk and
        silent_apk.brand = all_apk.brand and
        silent_apk.license = all_apk.license and
        silent_apk.province = all_apk.province
        )

      """.stripMargin).rdd


    ///////////////////////////////////////////test/////////////////
    //      println(resultRDD.count)
    //          .saveAsTextFile("/tmp/silent_terminal" + System.currentTimeMillis)


    //    // TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:terminal_cnt

          val dim_date = firstDayOfMonth

          //TODO 应用名称
          val dim_apk = line(0)
          val brand = line(1)
          val license = line(2)
          val province = line(3)
          val silent_cnt = line(4)
          val all_cnt = line(5)


          val orderedLine = dim_date + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + silent_cnt + "\t" + all_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.test

import org.apache.spark.{SparkContext, SparkConf}

object SilentTerminalTest {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("ApkTimeTotalJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")
    sc.stop()

  }

  def run(sc: SparkContext, s: String) = {


  }

}
package com.avcdata.spark.job.stat

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  *
  * hbase
  * disable 'tracker_total_silent_terminal';
  * drop 'tracker_total_silent_terminal';
  * create 'tracker_total_silent_terminal', 'info'
  * hive
  * Drop table  hr.tracker_total_silent_terminal;
  * CREATE EXTERNAL TABLE hr.tracker_total_silent_terminal(
  * key string,
  * dim_date string,
  * dim_apk string,
  * brand string,
  * license string,
  * province string,
  * silent_cnt string
  * )
  * STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  * WITH SERDEPROPERTIES ("hbase.columns.mapping" =
  * "info:dim_date,
  * info:dim_apk ,
  * info:brand,
  * info:license,
  * info:province,
  * info:silent_cnt")
  * TBLPROPERTIES("hbase.table.name" = "tracker_total_silent_terminal");
  *

  * @author zhangyongtian
  * @define 2017年新增终端相对上月的沉默终端数
  */
object SilentTerminalTotal {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date

    //TODO 获取数据
    val sqlContext = new HiveContext(sc)

    sqlContext.sql("use hr")

    //TODO 1月
    //    val sql =
    //      """
    //        select
    //          st.dim_date,
    //        	st.dim_apk,
    //        	st.brand,
    //        	st.license,
    //        	st.province,
    //        	st.silent_cnt * ratio
    //        from
    //        	(select * from hr.tracker_silent_terminal where year(dim_date) = year('"""+currentDate+"""') and month(dim_date) = month('"""+currentDate+"""')) st
    //        join
    //        	(
    //        	select
    //        		distinct
    //        		packagename,
    //        		 appname
    //        	from  hr.apkinfo
    //        	where appname not in('CIBN微视听','云视听·泰捷')
    //        	) ai
    //        	on (st.dim_apk=packagename)
    //        join
    //        	hr.tracker_total_dim_ratio_sec ratio
    //        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
    //
    //        union all
    //
    //        select
    //          st.dim_date,
    //        	st.dim_apk,
    //        	st.brand,
    //        	st.license,
    //        	st.province,
    //        	st.silent_cnt * ratio
    //        from
    //        	hr.tracker_silent_terminal st
    //        join
    //        	(
    //        	select
    //        		distinct
    //        		packagename,
    //        		 appname
    //        	from  hr.apkinfo
    //        	where appname in('CIBN微视听','云视听·泰捷')
    //        	) ai
    //        	on (st.dim_apk=packagename)
    //        join
    //        	hr.tracker_total_dim_ratio_vsttj_sec ratio
    //        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
    //      """.stripMargin

    //TODO 4月
    val sql =
      """
        select
          st.dim_date,
        	st.dim_apk,
        	st.brand,
        	st.license,
        	st.province,
        	st.silent_cnt * ratio,
        	st.all_cnt * ratio
        from
        		(select * from hr.tracker_silent_terminal where year(dim_date) = year('""" + currentDate +
        """') and month(dim_date) = month('""" + currentDate +
        """') and dim_apk not in('银河·奇异果','腾讯视频TV端','CIBN环球影视')) st
        join
        	(select * from hr.tracker_total_dim_ratio_sec where date = '2017-05-01') ratio
        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)

        union all

        select
          st.dim_date,
        	st.dim_apk,
        	st.brand,
        	st.license,
        	st.province,
        	st.silent_cnt * ratio,
        	st.all_cnt * ratio
        from
        	(select * from hr.tracker_silent_terminal where year(dim_date) = year('""" + currentDate +
        """') and month(dim_date) = month('""" + currentDate +
        """') and dim_apk in('银河·奇异果','腾讯视频TV端','CIBN环球影视')
         ) st
        join
        	(select * from hr.tracker_total_dim_ratio_sec2 where date = '2017-05-01')  ratio
        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
        """.stripMargin

    println(sql)

    //date_sub('""" + currentDate + """',
    //    1))

    val resultDF = sqlContext.sql(sql)

      //TODO 去除launcher 保留独立APK
      .rdd.filter(line => {
      val dim_date = line(0)
      val dim_apk = line(1)
      val brand = line(2)
      val license = line(3)
      val province = line(4)
      val silent_cnt = line(5)
      val all_cnt = line(6)

      ((!dim_apk.equals("银河·奇异果")) && license.equals("yinhe")) || ((!dim_apk.equals("CIBN环球影视")) && license.equals("youku")) || ((!dim_apk.equals("腾讯视频TV端")) && license.equals("tencent"))



    })

    resultDF

      // TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_total_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:silent_cnt

          //          '"""+firstDayOfMonth+"""' AS dim_date, slientSN.dim_apk, terminal.brand, terminal.license, terminal.province
          //          	, COUNT(slientSN.dim_sn) AS silent_cnt

          val dim_date = line(0)
          val dim_apk = line(1)
          val brand = line(2)
          val license = line(3)
          val province = line(4)
          val silent_cnt = line(5)
          val all_cnt = line(6)

          val orderedLine = dim_date + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + silent_cnt + "\t" + all_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.vbox.stat

import com.avcdata.vbox.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  *
  * hbase
  * disable 'tracker_total_silent_terminal';
  * drop 'tracker_total_silent_terminal';
  * create 'tracker_total_silent_terminal', 'info'
  * hive
  * Drop table  hr.tracker_total_silent_terminal;
  * CREATE EXTERNAL TABLE hr.tracker_total_silent_terminal(
  * key string,
  * dim_date string,
  * dim_apk string,
  * brand string,
  * license string,
  * province string,
  * silent_cnt string
  * )
  * STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  * WITH SERDEPROPERTIES ("hbase.columns.mapping" =
  * "info:dim_date,
  * info:dim_apk ,
  * info:brand,
  * info:license,
  * info:province,
  * info:silent_cnt")
  * TBLPROPERTIES("hbase.table.name" = "tracker_total_silent_terminal");
  *

  * @author zhangyongtian
  * @define 2017年新增终端相对上月的沉默终端数
  */
object SilentTerminalTotal {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date

    //TODO 获取数据
    val sqlContext = new HiveContext(sc)

    sqlContext.sql("use hr")

    //TODO 1月
    //    val sql =
    //      """
    //        select
    //          st.dim_date,
    //        	st.dim_apk,
    //        	st.brand,
    //        	st.license,
    //        	st.province,
    //        	st.silent_cnt * ratio
    //        from
    //        	(select * from hr.tracker_silent_terminal where year(dim_date) = year('"""+currentDate+"""') and month(dim_date) = month('"""+currentDate+"""')) st
    //        join
    //        	(
    //        	select
    //        		distinct
    //        		packagename,
    //        		 appname
    //        	from  hr.apkinfo
    //        	where appname not in('CIBN微视听','云视听·泰捷')
    //        	) ai
    //        	on (st.dim_apk=packagename)
    //        join
    //        	hr.tracker_total_dim_ratio_sec ratio
    //        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
    //
    //        union all
    //
    //        select
    //          st.dim_date,
    //        	st.dim_apk,
    //        	st.brand,
    //        	st.license,
    //        	st.province,
    //        	st.silent_cnt * ratio
    //        from
    //        	hr.tracker_silent_terminal st
    //        join
    //        	(
    //        	select
    //        		distinct
    //        		packagename,
    //        		 appname
    //        	from  hr.apkinfo
    //        	where appname in('CIBN微视听','云视听·泰捷')
    //        	) ai
    //        	on (st.dim_apk=packagename)
    //        join
    //        	hr.tracker_total_dim_ratio_vsttj_sec ratio
    //        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
    //      """.stripMargin

    //TODO 4月
    val sql =
      """
        select
          st.dim_date,
        	st.dim_apk,
        	st.brand,
        	st.license,
        	st.province,
        	st.silent_cnt * ratio,
        	st.all_cnt * ratio
        from
        		(select * from hr.tracker_silent_terminal where year(dim_date) = year('""" + currentDate +
        """') and month(dim_date) = month('""" + currentDate +
        """') and dim_apk not in('银河·奇异果','腾讯视频TV端','CIBN环球影视')) st
        join
        	(select * from hr.tracker_total_dim_ratio_sec where date = '2017-05-01') ratio
        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)

        union all

        select
          st.dim_date,
        	st.dim_apk,
        	st.brand,
        	st.license,
        	st.province,
        	st.silent_cnt * ratio,
        	st.all_cnt * ratio
        from
        	(select * from hr.tracker_silent_terminal where year(dim_date) = year('""" + currentDate +
        """') and month(dim_date) = month('""" + currentDate +
        """') and dim_apk in('银河·奇异果','腾讯视频TV端','CIBN环球影视')
         ) st
        join
        	(select * from hr.tracker_total_dim_ratio_sec2 where date = '2017-05-01')  ratio
        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
        """.stripMargin

    println(sql)

    //date_sub('""" + currentDate + """',
    //    1))

    val resultDF = sqlContext.sql(sql)

      //TODO 去除launcher 保留独立APK
      .rdd.filter(line => {
      val dim_date = line(0)
      val dim_apk = line(1)
      val brand = line(2)
      val license = line(3)
      val province = line(4)
      val silent_cnt = line(5)
      val all_cnt = line(6)

      ((!dim_apk.equals("银河·奇异果")) && license.equals("yinhe")) || ((!dim_apk.equals("CIBN环球影视")) && license.equals("youku")) || ((!dim_apk.equals("腾讯视频TV端")) && license.equals("tencent"))



    })

    resultDF

      // TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_total_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:silent_cnt

          //          '"""+firstDayOfMonth+"""' AS dim_date, slientSN.dim_apk, terminal.brand, terminal.license, terminal.province
          //          	, COUNT(slientSN.dim_sn) AS silent_cnt

          val dim_date = line(0)
          val dim_apk = line(1)
          val brand = line(2)
          val license = line(3)
          val province = line(4)
          val silent_cnt = line(5)
          val all_cnt = line(6)

          val orderedLine = dim_date + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + silent_cnt + "\t" + all_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.stat

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 2017年新增终端相对上月的沉默终端数
  */
object SilentTerminalTotal_packagename {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date

    //TODO 获取数据
    val sqlContext = new HiveContext(sc)
    
    sqlContext.sql("use hr")

    //TODO 1月
//    val sql =
//      """
//        select
//          st.dim_date,
//        	st.dim_apk,
//        	st.brand,
//        	st.license,
//        	st.province,
//        	st.terminal_cnt * ratio
//        from
//        	(select * from hr.tracker_silent_terminal where year(dim_date) = year('"""+currentDate+"""') and month(dim_date) = month('"""+currentDate+"""')) st
//        join
//        	(
//        	select
//        		distinct
//        		packagename,
//        		 appname
//        	from  hr.apkinfo
//        	where appname not in('CIBN微视听','云视听·泰捷')
//        	) ai
//        	on (st.dim_apk=packagename)
//        join
//        	hr.tracker_total_dim_ratio_sec ratio
//        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
//
//        union all
//
//        select
//          st.dim_date,
//        	st.dim_apk,
//        	st.brand,
//        	st.license,
//        	st.province,
//        	st.terminal_cnt * ratio
//        from
//        	hr.tracker_silent_terminal st
//        join
//        	(
//        	select
//        		distinct
//        		packagename,
//        		 appname
//        	from  hr.apkinfo
//        	where appname in('CIBN微视听','云视听·泰捷')
//        	) ai
//        	on (st.dim_apk=packagename)
//        join
//        	hr.tracker_total_dim_ratio_vsttj_sec ratio
//        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
//      """.stripMargin

    //TODO 4月
    val sql =
      """
        select
          st.dim_date,
        	st.dim_apk,
        	st.brand,
        	st.license,
        	st.province,
        	st.terminal_cnt * ratio
        from
        		(select * from hr.tracker_silent_terminal where year(dim_date) = year('"""+currentDate+"""') and month(dim_date) = month('"""+currentDate+"""')) st
        join
        	(
        	select
        		distinct
        		packagename,
        		 appname
        	from  hr.apkinfo
        	where appname not in('银河·奇异果','腾讯视频TV端','CIBN环球影视')
        	) ai
        	on (st.dim_apk=packagename)
        join
        	hr.tracker_total_dim_ratio_sec ratio
        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)

        union all

        select
          st.dim_date,
        	st.dim_apk,
        	st.brand,
        	st.license,
        	st.province,
        	st.terminal_cnt * ratio
        from
        	hr.tracker_silent_terminal st
        join
        	(
        	select
        		distinct
        		packagename,
        		 appname
        	from  hr.apkinfo
        	where appname  in('银河·奇异果','腾讯视频TV端','CIBN环球影视')
        	) ai
        	on (st.dim_apk=packagename)
        join
        	hr.tracker_total_dim_ratio_sec2 ratio
        on (st.brand=ratio.brand and st.license=ratio.license and st.province = ratio.province)
        """.stripMargin

    println(sql)

    val resultDF = sqlContext.sql(sql)

      // TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_total_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:terminal_cnt

          //          '"""+firstDayOfMonth+"""' AS dim_date, slientSN.dim_apk, terminal.brand, terminal.license, terminal.province
          //          	, COUNT(slientSN.dim_sn) AS terminal_cnt

          val dim_date = line(0)
          val dim_apk = line(1)
          val brand = line(2)
          val license = line(3)
          val province = line(4)
          val terminal_cnt = line(5)

          val orderedLine = dim_date + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + terminal_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.stat

import org.apache.log4j.Logger
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * hbase
  * disable 'tracker_silent_terminal';
  * drop 'tracker_silent_terminal';
  * create 'tracker_silent_terminal', 'info'

  * drop 'tracker_silent_terminal'
  * create 'tracker_silent_terminal', 'info'

  * hive
  * Drop table  hr.tracker_silent_terminal;
  * CREATE EXTERNAL TABLE hr.tracker_silent_terminal(
  * key string,
  * dim_date string,
  * dim_apk string,
  * brand string,
  * license string,
  * province string,
  * terminal_cnt string
  * )
  * STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  * WITH SERDEPROPERTIES ("hbase.columns.mapping" =
  * "info:dim_date,
  * info:dim_apk ,
  * info:brand,
  * info:license,
  * info:province,
  * info:terminal_cnt")
  * TBLPROPERTIES("hbase.table.name" = "tracker_silent_terminal");
  *
  * @author zhangyongtian
  * @define 2017新增终端相对上月的沉默终端数
  */
object SilentTerminalValidateSQL {
  val log = Logger.getLogger(getClass.getName)

  case class TerminalAPK(sn: String, dim_apk: String, brand: String, license: String, province: String)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-01-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date


    val year = currentDate.substring(0, 4)
    val month = currentDate.substring(5, 7)
    //    val yearmonth = year + month
    val yearmonth2 = year + "-" + month
    val firstDayOfMonth = yearmonth2 + "-01"

    //TODO new terminal SQL
    /*
    *     |select appname,dim_sn,dim_date from
              |(select dim_apk,dim_sn,min(dim_date) dim_date from hr.tracker_apk_fact_partition
              |where date>='"""+ currDateStart +"""' and date<='"""+ currDateEnd +"""' group by dim_apk,dim_sn) a
              |join
              |(select packagename,appname from hr.apkinfo) b
              |on b.packagename=a.dim_apk
    * */

    //TODO 当月的apk行为的sn数
    println("SELECT  ai.appname,count(DISTINCT dim_sn)   FROM hr.tracker_apk_fact_partition af JOIN hr.apkinfo ai ON (af.dim_apk = ai.packagename) WHERE date LIKE '" + yearmonth2 + "%' group by ai.appname ")


    //TODO 当月新增apk行为的sn
    println("select dim_apk, count(distinct dim_sn) from hr.tracker_new_terminal where dim_date LIKE  '" + yearmonth2
      + "%' group by dim_apk")

    //TODO 截止到当月第一次使用apk的sn数
    println("SELECT dim_apk, count(DISTINCT dim_sn) FROM hr.tracker_new_terminal WHERE dim_date < '" +
      firstDayOfMonth + "' group by dim_apk ")


    //TODO  当月的沉默终端数
    println("select dim_apk,SUM(terminal_cnt) from hr.tracker_silent_terminal where dim_date LIKE '" + yearmonth2
      + "%' group by dim_apk")

    //TODO 验证： 当月的apk行为的sn数-当月新增apk行为的sn数=截止到当月第一次使用apk的sn数-当月的沉默终端数


  }


}
package com.avcdata.spark.job.stat

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 2017新增终端相对上月的沉默终端数
  */
object SilentTerminal_packagename {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date


    val year = currentDate.substring(0, 4)
    val month = currentDate.substring(5, 7)
    val yearmonth = year + month
    val yearmonth2 = year + "-" + month
    val firstDayOfMonth = yearmonth2 + "-01"


    //TODO 获取数据
    val sqlContext = new HiveContext(sc)
    sqlContext.sql("use hr")
    val sql =
      """
        SELECT '""" + firstDayOfMonth +
        """' AS dim_date, slientSN.dim_apk, terminal.brand, terminal.license, terminal.province, COUNT(DISTINCT slientSN.dim_sn) AS terminal_cnt
        FROM (SELECT newterminal.dim_sn, newterminal.dim_apk
        	FROM (SELECT DISTINCT dim_sn, dim_apk
        		FROM hr.tracker_new_terminal
        		WHERE dim_date < '""" + yearmonth +
        """'
        		) newterminal
        		LEFT JOIN (SELECT DISTINCT dim_sn, dim_apk
        			FROM hr.tracker_total_apk_fact_partition
        			WHERE dim_date LIKE '""" + yearmonth2 +
        """'
        			) ap ON (newterminal.dim_sn = ap.dim_sn and newterminal.dim_apk = ap.dim_apk)
        	WHERE ap.dim_sn IS NULL
        	) slientSN
        	JOIN hr.sample_terminal_three terminal ON (slientSN.dim_sn = terminal.sn)
        GROUP BY slientSN.dim_apk, terminal.brand, terminal.license, terminal.province
        """.stripMargin

    println(sql)

    val resultDF = sqlContext.sql(sql)

      // TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:terminal_cnt

          //          '"""+firstDayOfMonth+"""' AS dim_date, slientSN.dim_apk, terminal.brand, terminal.license, terminal.province
          //          	, COUNT(slientSN.dim_sn) AS terminal_cnt

          val dim_date = line(0)
          //TODO 应用名称
          val dim_apk = line(1)
          val brand = line(2)
          val license = line(3)
          val province = line(4)
          val terminal_cnt = line(5)



          val orderedLine = dim_date + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + terminal_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.stat

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  *
  * hbase
*disable 'tracker_silent_terminal';
	*drop 'tracker_silent_terminal';
	*create 'tracker_silent_terminal', 'info'

*drop 'tracker_silent_terminal'
*create 'tracker_silent_terminal', 'info'

*hive
*Drop table  hr.tracker_silent_terminal;
*CREATE EXTERNAL TABLE hr.tracker_silent_terminal(
*key string,
*dim_date string,
*dim_apk string,
*brand string,
*license string,
*province string,
*terminal_cnt string
*)
*STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
*WITH SERDEPROPERTIES ("hbase.columns.mapping" =
*"info:dim_date,
*info:dim_apk ,
*info:brand,
*info:license,
*info:province,
*info:terminal_cnt")
*TBLPROPERTIES("hbase.table.name" = "tracker_silent_terminal");
  *
  * @author zhangyongtian
  * @define 2017新增终端相对上月的沉默终端数
  */
object SilentTerminal_SQL {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SilentTerminal")
    val sc = new SparkContext(conf)
    run(sc, "2017-01-04")

    sc.stop()

  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    //key	dim_apk dim_sn dim_date


    val year = currentDate.substring(0, 4)
    val month = currentDate.substring(5, 7)
    //    val yearmonth = year + month
    val yearmonth2 = year + "-" + month
    val firstDayOfMonth = yearmonth2 + "-01"


    //TODO 获取数据
    val sqlContext = new HiveContext(sc)
    sqlContext.sql("use hr")
    val sql =
      """
        SELECT '""" + firstDayOfMonth +
        """' AS dim_date, slientSN.appname, terminal.brand, terminal.license, terminal.province, COUNT(DISTINCT slientSN.dim_sn) AS terminal_cnt
        FROM (SELECT newterminal.dim_sn, newterminal.dim_apk As appname
        	FROM (SELECT DISTINCT dim_sn, dim_apk
        		FROM hr.tracker_new_terminal
        		WHERE dim_date < '""" + firstDayOfMonth +
        """'
        		) newterminal
        		LEFT JOIN (SELECT DISTINCT dim_sn, ai.appname
        			FROM hr.tracker_total_apk_fact_partition af
              JOIN hr.apkinfo ai ON (af.dim_apk = ai.packagename)
        			WHERE date LIKE '""" + yearmonth2 +
        """'
        			) ap ON (newterminal.dim_sn = ap.dim_sn and newterminal.dim_apk = ap.appname)
        	WHERE ap.dim_sn IS NULL
        	) slientSN
        	JOIN hr.sample_terminal_three terminal ON (slientSN.dim_sn = terminal.sn)
        GROUP BY slientSN.appname, terminal.brand, terminal.license, terminal.province
        """.stripMargin

    println(sql)


    val resultDF = sqlContext.sql(sql)

      // TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_silent_terminal")

      try {

        items.foreach(line => {

          //          info:dim_date,
          //          info:dim_apk ,
          //          info:brand,
          //          info:license,
          //          info:province,
          //          info:terminal_cnt

          //          '"""+firstDayOfMonth+"""' AS dim_date, slientSN.dim_apk, terminal.brand, terminal.license, terminal.province
          //          	, COUNT(slientSN.dim_sn) AS terminal_cnt

          val dim_date = line(0)

          //TODO 应用名称
          val dim_apk = line(1)
          val brand = line(2)
          val license = line(3)
          val province = line(4)
          val terminal_cnt = line(5)



          val orderedLine = dim_date + "\t" + dim_apk + "\t" + brand + "\t" + license + "\t" + province + "\t" + terminal_cnt


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SilentTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.until.ValidateUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object SizePriceNoneETL {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("UserVectorTerminalETL")
    val sc = new SparkContext(conf)
    run(sc, "2017-01-04", "30")

    sc.stop()


  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    //TODO 从sample_termianl_three  &  live_termianl中获取终端信息
    val sqlContext = new HiveContext(sc)

    //TODO 获取终端数据
    sqlContext.sql("use hr")
//
//    val userTerminalCnt = sqlContext.sql("select distinct sn from hr.user_vector_terminal").rdd.map(line=>(line
//      .toString,0))
//    val tmpTerminalCnt = sc.textFile("/user/hdfs/rsync/userdata/user-tmp-sn.txt").map(line=>(line,0))
//    userTerminalCnt.join(tmpTerminalCnt).count
//
//    sqlContext.sql("select distinct sn from hr.user_vector_terminal").count
//    sqlContext.sql("select distinct sn from hr.user_vector_terminal").rdd.map(_.toString).intersection(sc.textFile("/user/hdfs/rsync/userdata/user-tmp-sn.txt")).count


//    userTerminalCnt.intersection(tmpTerminalCnt)

    val sampleThreeDF = sqlContext.sql("SELECT tr.sn, tr.brand, tr.province, tm.model, tm.size FROM hr.sample_terminal_three tr JOIN hr.terminal tm ON (tr.sn = tm.sn)")
    val liveTerminalDF = sqlContext.sql("SELECT tr.sn, tr.brand, tr.province, tm.model, tm.size FROM hr.live_terminal tr JOIN hr.terminal tm ON (tr.sn = tm.sn) ")

    val unionDF = sampleThreeDF.unionAll(liveTerminalDF).distinct

    val terminalPair = unionDF.rdd.map(line => {

      val sn = line(0)
      val brand = line(1).toString
      val province = line(2).toString

      var model = line(3).toString
      var size = line(4).toString

      if (model.trim.isEmpty) {
        model = "#"
      }

      if (size.trim.isEmpty || (!ValidateUtils.isNumber(size))) {
        size = "0"
      }

      (brand + "\t" + model, sn + "\t" + province + "\t" + size)
    })

    println(terminalPair.count)

    //TODO 获取价格匹配数据
    val pricePair = sc.textFile("/user/hdfs/rsync/userdata/terminal-price.csv").map(line => {
      val cols = line.split(",")
      val brand = cols(0)
      val model = cols(1)
      val size = cols(2)
      (brand + "\t" + model, size)
    })


    //TODO 关联
    val allRDD = terminalPair.leftOuterJoin(pricePair)
      .filter(line => {

        val rightTerminalCols = line._2._1.split("\t")

        val size = rightTerminalCols(2)

        //关联不上的
        line._2._2.isEmpty
      })
    println(allRDD.count())

//      .map(line => {
//        //      (k, (v, None))
//        val leftCols = line._1.split("\t")
//
//        val brand = leftCols(0)
//        val model = leftCols(1)
//
//        val rightTerminalCols = line._2._1.split("\t")
//        var size = rightTerminalCols(2)
//        var price = "0"
//
//        brand + "\t" + model
//      })

      //.distinct.saveAsTextFile("/tmp/SizePriceNone" + System.currentTimeMillis)


  }
}

package com.avcdata.spark.job.konka

import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext
import org.joda.time.DateTime
import scala.collection.mutable

case class Brand(sn: String, actions: mutable.MutableList[BrandItem]) {
}
case class BrandItem(month: String, appname: String, or_sdura: String)

/**
  * Created by avc on 2017/4/1.
  * 根据ko_brand_dura 计算得到每个sn的牌照
  */
object SnBrand {
    def run(sc: SparkContext, analysisDate: String) = {

        val snBrandCol = Bytes.toBytes("brinfo")
        val snCol = Bytes.toBytes("sn")
        val licenseCol = Bytes.toBytes("license")

        //每个sn每个月在三个apk中的时长并且进行排名
        val orderSql =
            """
              |select *,
              |rank() over(partition by sn,month order by sum_dura desc) or_sdura
              |from
              |(select sn,appname,month,sum(duration) as sum_dura
              | from hr.ko_brand_dura group by sn, appname, month) a
            """

        //比较最近三个月的app排名，只统计判断排名为1的数据
        var flag = 0 //当月是1、2月份的话，确保后面的排序是正确的（0012,01,02）
        val mon = analysisDate.substring(5, 7)
        val preDate = DateTime.parse(analysisDate).plusMonths(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusMonths(-2).toString("yyyy-MM-dd")
        val preMon = preDate.substring(5, 7)
        val yesBeMon = yesBeforeDate.substring(5, 7)
        if(preMon > mon || yesBeMon > mon) {
            flag = 1
        }

        val hiveCon = new HiveContext(sc)
        val orderBaseRdd = hiveCon.sql(orderSql.stripMargin)
        val orderReRdd = orderBaseRdd.mapPartitions(items => {
            items.map(line => {
                val sn = line(0).toString
                val appname = line(1).toString
                val month = line(2).toString
                val or_sdura = line(4).toString
                (sn, appname, month, or_sdura)
            })
        }).filter(_._4.equals("1")).filter(x => {x._3.equals(mon) || x._3.equals(preMon) || x._3.equals(yesBeMon)})

        //orderReRdd.saveAsTextFile("hdfs:///user/hdfs/player/order.txt")

        val snBrRdd = orderReRdd.mapPartitions(items => {
            items.map(line => {
                val sn = line._1
                val appname = line._2
                var month = line._3
                if(flag == 1 && month.equals("11") || month.equals("12")){
                    month = "00" + month
                }
                val or_sdura = line._4
                val brandItem = new BrandItem(month, appname, or_sdura)
                val brand = new Brand(sn, mutable.MutableList(brandItem))
                (sn, brand)
            })
        }).reduceByKey((left, right) => {
            left.actions ++= right.actions
            left
        }).filter(x => x._2.actions.size >= 2).mapPartitions(items => {
            items.map(item => {
                val sortedList = item._2.actions.sortBy(_.month).clone()
                item._2.actions.clear()
                item._2.actions ++= sortedList
                item._2
            })
        })

        //取最近三个月的sn：先过滤掉只有一个月数据的sn
        //然后如果只有两个月的就比较两个月的，如果有三个月的三个月的都要比较，如果三个月中有一个不同，不需要
        val snBrReRdd = snBrRdd.mapPartitions(items => {
            items.map(item => {
                val sn = item.sn
                val len = item.actions.size
                var appname = ""
                if(len == 2 && item.actions.get(0).get.appname.equals(item.actions.get(1).get.appname)){
                    appname = item.actions.get(0).get.appname
                } else if (len == 3 && item.actions.get(0).get.appname.equals(item.actions.get(1).get.appname)
                    && item.actions.get(0).get.appname.equals(item.actions.get(2).get.appname)
                    && item.actions.get(1).get.appname.equals(item.actions.get(2).get.appname)) {
                    appname = item.actions.get(0).get.appname
                }

                (sn, item, appname)
            })
        }).filter(_._3 != "")

        //snBrReRdd.take(200).foreach(println)
        //snBrReRdd.saveAsTextFile("hdfs:///user/hdfs/player/snBrand.txt")

        snBrReRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("ko_sn_brand"))
            try {

                items.foreach(line => {
                    val sn = line._1
                    val appname = line._3

                    var license = ""
                    if (appname.equals("CIBN环球影视")) {
                        license = "youku"
                    } else if (appname.equals("银河·奇异果")) {
                        license = "yinhe"
                    } else if (appname.equals("腾讯视频TV端")) {
                        license = "tencent"
                    }

                    val put = new Put(Bytes.toBytes(sn + "KO"))
                    put.addColumn(snBrandCol, snCol, Bytes.toBytes(sn))
                    put.addColumn(snBrandCol, licenseCol, Bytes.toBytes(license))

                    mutator.mutate(put)
                })
                mutator.flush()

            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}
package com.avcdata.etl.launcher

import java.util.UUID

import com.avcdata.etl.common.util.HdfsFileUtil
import com.avcdata.etl.export.{ExportDBHelper, ExportK2MHelper, ExportKafkaHelper, ExportMongoHelper}
import com.avcdata.etl.export.config._
import com.avcdata.etl.launcher.config.ExportType
import com.avcdata.etl.launcher.util.{CliParser, ScriptExecutor, SQLParamUtil, UDFUtil}
import org.apache.commons.cli.CommandLine
import org.apache.spark.sql.DataFrame
import org.apache.spark.{SparkConf, SparkContext}
import org.slf4j.LoggerFactory

/**
  * 通过SPARK执行HiveQL并将数据导入至数据库中
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 9:43
  */
object Spark4HiveQLExecutor
{
  val logger = LoggerFactory.getLogger(Spark4HiveQLExecutor.getClass)

  def main(args: Array[String])
  {
    val cl = CliParser.parse(args)

    if (cl.hasOption("help"))
    {
      CliParser.printHelp()
    }
    else
    {
      //由命令行设置/覆盖应用名称及相关参数
      val conf = new SparkConf().setIfMissing("spark.app.name", "Spark4HiveQLExecutor-" + UUID.randomUUID())
        .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        //.set("spark.kryoserializer.buffer.max", "128m")
        .set("spark.logConf", "true")

      val sc = new SparkContext(conf)

      //创建SQL执行环境
      val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

      //设置Hive默认参数
      //hiveContext.setConf("mapreduce.input.fileinputformat.split.minsize", "33554432") //33M
      //hiveContext.setConf("mapreduce.input.fileinputformat.split.maxsize", "134217728") //128M
      //hiveContext.setConf("mapreduce.input.fileinputformat.split.minsize.per.node", "33554432") //33M
      //hiveContext.setConf("mapreduce.input.fileinputformat.split.minsize.per.rack", "33554432") //33M
      //hiveContext.setConf("hive.input.format", "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat")

      //注册UDF
      UDFUtil.registerUDF(hiveContext)

      //合并系统默认参数和命令行参数
      val mergedParams = SQLParamUtil.getCliMergedParams(cl, conf)
      logger.info("Merged params =>\n" + mergedParams.mkString("\n"))

      //获取前置脚本文件内容和执行脚本内容
      val rcScriptLines = if (cl.hasOption("rc-file")) HdfsFileUtil.read(cl.getOptionValue("rc-file")) else Seq[String]()
      val rtScriptLines = if (cl.hasOption("sql-file")) HdfsFileUtil.read(cl.getOptionValue("sql-file")) else throw new IllegalArgumentException(s"Must specify the <sql-file> param value.")

      //执行合并后的脚本行,获取执行结果
      val dfResult = ScriptExecutor.executeScriptLines(rcScriptLines ++ rtScriptLines, hiveContext, mergedParams)

      //根据导出类型决定结果集的处理方式
      (if (cl.hasOption("export-type")) cl.getOptionValue("export-type") else ExportType.JDBC) match
      {
        case ExportType.NONE => logger.info(s"The export type is '${ExportType.NONE}', just execute sql file.")

        case ExportType.JDBC => exportToRdbms(dfResult, mergedParams)

        case ExportType.MONGO => exportToMongo(dfResult, mergedParams)

        case ExportType.KAFKA => exportToKafka(dfResult, mergedParams)

        case ExportType.K2M => exportToK2M(dfResult, mergedParams)

        case ExportType.CSV => exportToCsvFile(dfResult, cl)

        case ExportType.HIVE => logger.info(s"The export type is '${ExportType.HIVE}', just execute sql file.")

        case otherExportType => throw new IllegalArgumentException(s"Unsupported export type -> '$otherExportType'.")
      }

      sc.stop()
    }
  }

  /**
    * 将SQL执行结果保存至关系型数据库
    *
    * @param dfResult     执行结果
    * @param mergedParams 合并后的参数
    */
  private def exportToRdbms(dfResult: DataFrame, mergedParams: Map[String, String]): Unit =
  {
    //解析命令行配置参数
    val exportJDBCConfig = CliParser.readJDBCExportConfig(mergedParams)

    //将执行结果保存至指定数据库
    ExportDBHelper.export(dfResult, exportJDBCConfig)
  }

  /**
    * 将SQL执行结果保存至MONGO
    *
    * @param dfResult     执行结果
    * @param mergedParams 合并后的参数
    */
  private def exportToMongo(dfResult: DataFrame, mergedParams: Map[String, String]): Unit =
  {
    val clientURI = mergedParams("mongo-connect-uri")
    val collectionName = mergedParams("collection-name")
    val deleteKeys = mergedParams.getOrElse(ExportJDBCConfigOption.DELETE_KEY._2, "").split(",").map(_.trim).filter(_ != "").toSeq

    if (mergedParams.getOrElse(ExportJDBCConfigOption.VERBOSE._2, "false").toBoolean) dfResult.foreach(row => println(row.mkString(", ")))

    val config = ExportMongoConfig(clientURI, collectionName, deleteKeys)
    ExportMongoHelper.export(dfResult, config)
  }

  /**
    * 将SQL执行结果保存至Kafka
    *
    * @param dfResult     执行结果
    * @param mergedParams 合并后的参数
    */
  private def exportToKafka(dfResult: DataFrame, mergedParams: Map[String, String]): Unit =
  {
    val zookeeperList = mergedParams("kafka-zookeeper-hosts")
    val writeTopic = mergedParams("kafka-write-topic")

    if (mergedParams.getOrElse(ExportJDBCConfigOption.VERBOSE._2, "false").toBoolean) dfResult.foreach(row => println(row.mkString(", ")))

    val config = ExportKafkaConfig(zookeeperList, writeTopic)
    ExportKafkaHelper.export(dfResult, config)
  }

  /**
    * 删除Mongo中已经存在的数据,将SQL执行结果保存至Kafka
    *
    * @param dfResult     执行结果
    * @param mergedParams 合并后的参数
    */
  private def exportToK2M(dfResult: DataFrame, mergedParams: Map[String, String]): Unit =
  {
    val mongoClientURI = mergedParams("mongo-connect-uri")
    val collectionName = mergedParams("collection-name")
    val deleteKeys = mergedParams.getOrElse(ExportJDBCConfigOption.DELETE_KEY._2, "").split(",").map(_.trim).filter(_ != "").toSeq
    val zookeeperList = mergedParams("kafka-zookeeper-hosts")
    val writeTopic = mergedParams("kafka-write-topic")
    val commonConnectURI = mergedParams("common-db-connect-uri")
    val commonUsername = mergedParams("common-db-username")
    val commonPassword = mergedParams("common-db-password")

    if (mergedParams.getOrElse(ExportJDBCConfigOption.VERBOSE._2, "false").toBoolean) dfResult.foreach(row => println(row.mkString(", ")))

    val config = ExportK2MConfig(mongoClientURI, collectionName, deleteKeys, zookeeperList, writeTopic
      , commonConnectURI, commonUsername, commonPassword)

    ExportK2MHelper.export(dfResult, config)
  }

  /**
    * 将执行结果保存至CSV文件
    *
    * @param dfResult 执行结果
    * @param cl       命令行配置
    */
  private def exportToCsvFile(dfResult: DataFrame, cl: CommandLine): Unit =
  {
    import com.databricks.spark.csv._

    dfResult.saveAsCsvFile(cl.getOptionValue(ExportCsvConfigOption.SAVE_PATH._2), CliParser.readCsvExportConfig(cl))
  }
}
package com.avcdata.etl.launcher

import java.util.UUID

import com.avcdata.etl.common.util.HdfsFileUtil
import com.avcdata.etl.launcher.util.{CliParser, SQLParamUtil, ScriptExecutor, UDFUtil}
import org.apache.spark.{SparkConf, SparkContext}
import org.slf4j.LoggerFactory

/**
  * 通过SPARK执行HiveQL并将结果打印至控制台
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/4/14 9:43
  */
object Spark4HiveQLPrint
{
  val logger = LoggerFactory.getLogger(Spark4HiveQLPrint.getClass)

  def main(args: Array[String])
  {
    val cl = CliParser.parse(args)

    if (cl.hasOption("help"))
    {
      CliParser.printHelp()
    }
    else
    {
      //由命令行设置/覆盖应用名称及相关参数
      val conf = new SparkConf()
      val appName = conf.get("spark.app.name", "Spark4HiveQLExecutor-" + UUID.randomUUID())
      conf.setAppName(appName)
      conf.set("spark.logConf", "true")
      val sc = new SparkContext(conf)

      //创建SQL执行环境
      val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

      //注册UDF
      UDFUtil.registerUDF(hiveContext)

      //导入语句，可以隐式地将RDD转化成DataFrame
      //import hiveContext.implicits._

      //合并系统默认参数和命令行参数
      val mergedParams = SQLParamUtil.getCliMergedParams(cl, conf)
      logger.info("Merged params => " + mergedParams.mkString(", "))

      //获取前置脚本文件内容和执行脚本内容
      val rcScriptLines = if (cl.hasOption("rc-file")) HdfsFileUtil.read(cl.getOptionValue("rc-file")) else Seq[String]()
      val rtScriptLines = HdfsFileUtil.read(cl.getOptionValue("sql-file"))

      //执行脚本文件并返回最后一行的执行结果并打印
      ScriptExecutor.executeScriptLines(rcScriptLines ++ rtScriptLines, hiveContext, mergedParams)
        .collect().foreach(row => println(row.mkString(", ")))
    }
  }
}
package com.avcdata.spark.job.util

import org.apache.spark.SparkConf

/**
  * Created by avc on 2016/12/12.
  */
object SparkConfUtils {

  def optimize(conf: SparkConf): SparkConf = {
    conf.set("spark.sql.codegen", "false");
    conf.set("spark.sql.inMemoryColumnarStorage.compressed", "false");
    conf.set("spark.sql.inMemoryColumnarStorage.batchSize", "1000");
    conf.set("spark.sql.parquet.compression.codec", "snappy");
    conf.set("spark2.default.parallelism", "100")
    conf.set("spark2.storage.memoryFraction", "0.5")
    conf.set("spark2.shuffle.consolidateFiles", "true")
    conf.set("spark2.shuffle.file.buffer", "64")
    conf.set("spark2.shuffle.memoryFraction", "0.3")
    conf.set("spark2.reducer.maxSizeInFlight", "24")
    conf.set("spark2.shuffle.io.maxRetries", "60")
    conf.set("spark2.shuffle.io.retryWait", "60")
    conf.set("spark2.serializer", "org.apache.spark2.serializer.KryoSerializer")
    //    conf.registerKryoClasses(new Class[]{
    //        ClassOne.class,
    //        ClassTwo.class});
    conf
  }


}
package com.avcdata.spark.job.util

import org.apache.spark.SparkConf

/**
  * Created by avc on 2016/12/12.
  */
object SparkConfUtils {

  def optimize(conf: SparkConf): SparkConf = {
    conf.set("spark.sql.codegen", "false");
    conf.set("spark.sql.inMemoryColumnarStorage.compressed", "false");
    conf.set("spark.sql.inMemoryColumnarStorage.batchSize", "1000");
    conf.set("spark.sql.parquet.compression.codec", "snappy");
    conf.set("spark2.default.parallelism", "100")
    conf.set("spark2.storage.memoryFraction", "0.5")
    conf.set("spark2.shuffle.consolidateFiles", "true")
    conf.set("spark2.shuffle.file.buffer", "64")
    conf.set("spark2.shuffle.memoryFraction", "0.3")
    conf.set("spark2.reducer.maxSizeInFlight", "24")
    conf.set("spark2.shuffle.io.maxRetries", "60")
    conf.set("spark2.shuffle.io.retryWait", "60")
    conf.set("spark2.serializer", "org.apache.spark2.serializer.KryoSerializer")
    //    conf.registerKryoClasses(new Class[]{
    //        ClassOne.class,
    //        ClassTwo.class});
    conf
  }


}
package com.avcdata.vbox.util

import org.apache.spark.SparkConf

/**
  * Created by avc on 2016/12/12.
  */
object SparkConfUtils {

  def optimize(conf: SparkConf): SparkConf = {
    conf.set("spark.sql.codegen", "false");
    conf.set("spark.sql.inMemoryColumnarStorage.compressed", "false");
    conf.set("spark.sql.inMemoryColumnarStorage.batchSize", "1000");
    conf.set("spark.sql.parquet.compression.codec", "snappy");
    conf.set("spark2.default.parallelism", "100")
    conf.set("spark2.storage.memoryFraction", "0.5")
    conf.set("spark2.shuffle.consolidateFiles", "true")
    conf.set("spark2.shuffle.file.buffer", "64")
    conf.set("spark2.shuffle.memoryFraction", "0.3")
    conf.set("spark2.reducer.maxSizeInFlight", "24")
    conf.set("spark2.shuffle.io.maxRetries", "60")
    conf.set("spark2.shuffle.io.retryWait", "60")
    conf.set("spark2.serializer", "org.apache.spark2.serializer.KryoSerializer")
    //    conf.registerKryoClasses(new Class[]{
    //        ClassOne.class,
    //        ClassTwo.class});
    conf
  }


}
package com.avcdata.spark.job.mllib

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkContext, SparkConf}

object SparkTest {
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("SparkTest")
    val sc = new SparkContext(conf)
//    run(sc, "2017-01-04", "30")

    val film_RDD =
      sc.textFile("E:\\aowei\\tracker-user\\data\\to01.17.txt")
      .filter(!_.contains("原始名称"))
      .distinct
      .flatMap(line => {
        // ﻿原始名称标准名称ID模块年份人群产地剧情类型
        println(line)
        val cols = line.split("\\|")

        val original_name = cols(0).trim
//        println(original_name)
        val standard_name = cols(1).trim
        val id = cols(2).trim
        val model = cols(3).trim
        val year = cols(4).trim
        val crowd = cols(5).trim
        val region = cols(6).trim
        val plots = cols(7).split("/")
        println(plots(0))

        val arr = new Array[(String, String, String)](plots.length)

        for (i <- 0 until plots.length) {
          arr(i) = (id, model, plots(i))
        }

        arr

      })


    film_RDD.count











    sc.stop()
  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    //    val dataPath = "hdfs://192.168.1.15:8020/user/hdfs/rsync/uservector/2017-05-04-ClusterResult"

    //    sc.textFile(dataPath).foreach(println(_))

    //    val wo_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")
    //    println(None.getOrElse(wo_default))
    //
    //    UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0").toVector

    //    val tmpTerminalCnt_one = sc.textFile("E:\\aowei\\tracker-user\\doc\\user-tmp-sn.txt")
    //    val tmpTerminalCnt_two = sc.textFile("E:\\aowei\\tracker-user\\doc\\user-tmp-sn.txt").distinct
    //
    //    println(tmpTerminalCnt_one.distinct.count)
    //    val cnt = tmpTerminalCnt_one.intersection(tmpTerminalCnt_one).count
    //    println(cnt)
    //
    //    val sqlContext = new HiveContext(sc)
    //
    //    sqlContext.sql("select distinct sn from hr.user_vector_terminal").rdd.map(_(0))


    val samples =
      sc.textFile("2017-03-15-15-FamilyVectorDataExport.txt")
        .map(line => {
          val arr = line.split("\t")
          val sn = arr(0)

          val member_num = arr(12)

          //TODO 合并向量字符串
          val sb = new StringBuilder
          for (i <- 1 until arr.length - 1){
            if (i < (arr.length - 2)) {
              sb.append(arr(i) + ",")
            } else {
              sb.append(arr(i))
            }
          }

          //var sample_vectorStr = ""
          //          //向量字段
          //          for (i <- 1 until arr.length - 1)
          //            sample_vectorStr = sample_vectorStr + "," + arr(i)
          //
          //          val sample_vector = Vectors.dense(sample_vectorStr.split(",")
          //            .map(_.toDouble))

          //带有sn号和向量的字段，样本向量，样本中的家庭构成
          (sn, sb.toString, member_num)
        })
    //样本数据结果
    val sample_label_vec = samples.map(x => {
      val vector = Vectors.dense(x._2.split(",")
        .map(_.toDouble))
//      x._2.split(",")        .map(_.toDouble).foreach(println(_))
//        println(x._2)

      vector
    })

    //TODO 模型训练
    // 类簇的个数 number of clusters
    //    val k = 8
    // 设置最大迭代次数 maxIterations max number of iterations
    //    val dataModelTrainTimes = 30
    val dataModelTrainTimes = 1
    // 运行3次选出最优解 runs number of parallel runs, defaults to 1. The best model is returned
    //    val runs = 3
    val runs = 1
    // 初始聚类中心的选取为k-means++ initializationMode initialization model, either "random" or "k-means||" (default).
    val initMode = "k-means||"

    //TODO 生成模型
    val model_k = KMeans.train(sample_label_vec, 1, dataModelTrainTimes, runs, initMode)


    samples.map(x => {
      val v = Vectors.dense(x._2.split(",")
        .map(_.toDouble))
      //TODO 预测 生成类别ID
      val cluster_id = model_k.predict(v)
      (x._1, cluster_id, x._3)
    })

      .foreach(println(_))


  }

}
package com.avcdata.vbox.test

import com.avcdata.vbox.util.TimeUtils

import scala.collection.mutable.ArrayBuffer

object SparkTest {

  /**
    * 按照长虹的清洗规则 将HH:mm:ss数组转换成时间段数组
    */
  def getTimeRangArrByTimes(timeArr: Array[String]): Array[(String, String)] = {

    val timeStampArr = timeArr.map(x => TimeUtils.convertDateStr2TimeStamp(x, "HH:mm:ss"))

    //        for (i <- 0 until timeStampArr.length) {
    //          println(timeStampArr(i))
    //        }


    scala.util.Sorting.stableSort(timeStampArr)
    val timeRangArr = new scala.collection.mutable.ArrayBuffer[(String, String)]
    var startTime = "0"
    var endTime = "0"

    //用户行为超过间隔10分钟内的累加
    for (i <- 0 until timeStampArr.length - 1) {
      if (i == 0) {
        startTime = timeStampArr(0).toString
      }
      val diff = (timeStampArr(i + 1) - timeStampArr(i)) / 1000 / 60
      if (diff <= 10) {
        endTime = timeStampArr(i + 1).toString
      } else {
        if (endTime.toLong > startTime.toLong)
          timeRangArr.+=((TimeUtils.convertTimeStamp2DateStr(startTime.toLong, "HH:mm:ss"), TimeUtils.convertTimeStamp2DateStr
          (endTime.toLong, "HH:mm:ss")))
        //间隔超过10分钟 算新记录
        startTime = timeStampArr(i + 1).toString
      }
      if (i == (timeStampArr.length - 2) && endTime.toLong > startTime.toLong) {
        timeRangArr.+=((TimeUtils.convertTimeStamp2DateStr(startTime.toLong, "HH:mm:ss"), TimeUtils.convertTimeStamp2DateStr
        (endTime.toLong, "HH:mm:ss")))
      }
    }
    timeRangArr
      .filter(!_._2.equals("0"))
      .distinct.toArray
  }


  def main(args: Array[String]) {


    println("yyyy-MM-dd HH:mm:ss".length)

    val arr = new ArrayBuffer[Int]()

    for (i <- Range(0, 21)) {
      arr += 0
    }

    for (i <- 0 until arr.length) {
      arr(i) = 1
      println(arr)
      arr(i) = 0
    }




    //    val analysisDate = "2017-09-12"
    //    val fst_Day = DateTime.parse(analysisDate).plusDays(-3).toString("yyyy-MM-dd")
    //    val sec_Day = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")
    //    val td_Day = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
    //
    //    val daysArr = Array[String](
    //      fst_Day,
    //      sec_Day,
    //      td_Day,
    //      analysisDate
    //    )
    //
    //    for (ele <- daysArr) {
    //      println("------------" + ele)
    //    }


    //   val area = "\"[{\"\"w\"\":\"\"31.223487\"\",\"\"j\"\":\"\"121.48499\"\",\"\"p\"\":\"\"上海市\"\",\"\"c\"\":\"\"上海市\"\",\"\"d\"\":\"\"黄浦区\"\"}]\"\t\"Herbalife (China) Health Products Ltd.\"".replaceAll("\"","")
    //
    //    println(area)
    //    println(area.split(",")(2).substring(area.split(",")(2).indexOf(":")+1))
    //    println(area.split(",")(3).substring(area.split(",")(2).indexOf(":")+1))

    //    val str = "[{""w"":""-33.867900"",""j"":""151.207000"",""p"":"""",""c"":"""",""d"":""""}]"	""

    //    val timeArr = Array[String]("00:10:00", "00:11:00", "00:12:00", "00:13:00", "00:23:00", "00:24:00")

    //    val timeArr = Array[String]("00:10:00", "00:11:00", "00:12:00", "00:13:00", "00:33:00")

    //              val timeArr = Array[String]("00:10:00","00:11:00","00:12:00","00:13:00","00:33:00","00:35:00")

    //              val timeArr = Array[String]("00:10:00","00:11:00","00:12:00","00:13:00","00:33:00","00:35:00",
    //     "00:40:00")

    //              val timeArr = Array[String]("00:10:00","00:11:00","00:12:00","00:13:00","00:33:00","00:35:00",
    //     "00:40:00","01:25:00")

    //    val timeArr = Array[String]("00:10:00","00:11:00","00:12:00","00:13:00","00:33:00","00:35:00",
    //      "00:40:00","01:25:00","01:30:00")


    //    println(TimeUtils.convertDateStr2TimeStamp("00:10:10","HH:mm:ss"))
    //    println(TimeUtils.convertTimeStamp2DateStr(-28190000,"HH:mm:ss"))


    //    println("2017-08-29 00:00:00".length)

    //    val timeRangArr = getTimeRangArrByTimes(timeArr)
    //
    //    for (i <- 0 until timeRangArr.length) {
    //      println(timeRangArr(i)._1 + "-" + timeRangArr(i)._2)
    //    }
    //    println(getTimeRangArrByTimes(timeArr))


    ////////////////////////////////////////////
    //    val length = 5
    //    val array = new Array[String](length)
    //
    //    for (i <- 0 until length) {
    //      if (array(i) == null) {
    //        array(i) = "haha"
    //      }
    //    }

    //////////////////////////////////////
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("coocaa-ApkDataLoadJob")
    //    val sc = new SparkContext(conf)

    //    val arr = "aa|bb|".split('|')
    //    println(arr.length)
    //

    //    val test = new String"haha"
    //    println(test=="haha")

    //  sc.parallelize(Array(
    //    "bcec234338f2","20170320剃刀边缘第13集","剃刀边缘","32828","13","2017/4/25","0","1","602","2016","电视剧","非少儿","中国大陆"
    //  ))

    //    .map(line => {
    //      val dim_sn = line(0).toString
    //      val dim_title = line(1).toString
    //      val dim_part = line(2).toString
    //      val dim_date = line(3).toString
    //      val dim_hour = line(4).toString
    //      val other = line(5).toString
    //      val cols = other.split(";")
    //      val dim_awcid = cols(0)
    //      val dim_model = cols(1)
    //      var dim_year = cols(2)
    //      if (dim_year.equals("unknow")) dim_year = "haha"
    //      val dim_crowd = cols(3)
    //      var dim_region = cols(4)
    //      if (dim_region.equals("unknow")) dim_region = "haha"
    //      var dim_name = cols(5)
    //      val fact_vv = line(6).toString
    //      val fact_duration = line(7).toString
    //      dim_sn + dim_hour + dim_crowd + dim_date + "CC" + "|" + dim_sn + "|" + dim_name + "|" + dim_title + "|" + dim_awcid + "|" + dim_part + "|" + dim_date + "|" + dim_hour + "|" + fact_vv + "|" + fact_duration + "|" + dim_year + "|" + dim_model + "|" + dim_crowd + "|" + dim_region
    //    })
    //    .map(_.split('#'))
    //    .map(p=>Row(p(0),p(1),p(2),p(3),p(4),p(5),p(6),p(7),p(8),p(9),p(10),p(11),p(12),p(13)))
    //    .distinct()

    //    val fact_str="key,dim_sn,dim_name,dim_title,dim_awcid,dim_part,dim_date,dim_hour,fact_vv,fact_duration,dim_year,dim_model,dim_crowd,dim_region"
    //    val fact_schema = StructType (
    //      fact_str.split(",").map(fieldName => StructField(fieldName,StringType,true))
    //    )
    //

    //    sc.stop()
  }


}
package com.avcdata.spark.job.total

object Sql {

  //////////////////////////////////////////HiveSQL次数和时长推总//////////////////////////////////////////////////////////

  //////////////////////////////////开关机//////////////////////////////////////////////////////////////////
  val tracker_total_terminal_active_fact_HtableName = "tracker_total_terminal_active_fact"

  //TODO 4月
  def getTracker_total_terminal_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT
        tof2.key,
        tof2.sn,
        tof2.date,
        tof2.power_on_time,
        tof2.cnt * ratio.ratio    as  cnt,
        tof2.power_on_length * ratio.ratio as  power_on_length
      FROM
        (
          SELECT
            tof.key,
            tof.sn,
            tof.date,
            tof.power_on_time,
            tof.cnt,
            tof.power_on_length,
            terminal.brand,
            terminal.province
          FROM (select key,sn,date,power_on_time,cnt,power_on_length from hr.tracker_oc_fact_partition where date = '""" + analysisDate +
      """') tof
              JOIN (select sn,brand as brand,province from hr.sample_terminal_three) terminal
                ON (tof.sn = terminal.sn)
          ) tof2
          JOIN
          (select * from hr.tracker_total_dim_ratio_oc_fst where date = date_sub('""" + analysisDate + """',0)) ratio
            ON (tof2.brand = ratio.brand AND tof2.province = ratio.province)
      """.stripMargin
  }



  ///////////////////////////////直播////////////////////////////////////////////////////////////////////
  val tracker_total_live_active_fact_HtableName = "tracker_total_live_active_fact"

  //TODO 6月
  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT tlf2.key,
             tlf2.dim_sn,
             tlf2.dim_channel,
             tlf2.date,
             tlf2.dim_hour,
             tlf2.dim_min,
             tlf2.fact_cnt * sr_cache.ratio as fact_cnt ,
             tlf2.fact_time_length * sr_cache.ratio as fact_time_length
        FROM
        (SELECT KEY, dim_sn, dim_channel, date, dim_hour, dim_min, fact_cnt, fact_time_length
         FROM hr.tracker_live_fact_partition
         WHERE date = '""" + analysisDate +
      """' ) tlf2
        JOIN sr_cache ON (tlf2.dim_sn = sr_cache.sn)
      """.stripMargin
  }
//  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
//    """
//      SELECT tlf2.key,
//             tlf2.dim_sn,
//             tlf2.dim_channel,
//             tlf2.date,
//             tlf2.dim_hour,
//             tlf2.dim_min,
//             tlf2.fact_cnt * ratio_fst.ratio as fact_cnt ,
//             tlf2.fact_time_length * ratio_fst.ratio as fact_time_length
//        FROM
//        (SELECT KEY, dim_sn, dim_channel, date, dim_hour, dim_min, fact_cnt, fact_time_length
//         FROM hr.tracker_live_fact_partition
//         WHERE date = '""" + analysisDate +
//      """' ) tlf2
//        JOIN hr.live_terminal t
//            ON tlf2.dim_sn = t.sn
//          join
//          (select * from hr.tracker_total_dim_ratio_fst where date =date_sub('""" + analysisDate +
//    """',0)))  ratio_fst
//          on
//          (ratio_fst.brand = t .brand and ratio_fst.province=t .province)
//      """.stripMargin
//  }


  



  //TODO 截止到3月
  //  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
  //    """
  //      SELECT
  //        tlf2.key,
  //        tlf2.dim_sn,
  //        tlf2.dim_channel,
  //        tlf2.date,
  //        tlf2.dim_hour,
  //        tlf2.dim_min,
  //        tlf2.fact_cnt * ratio.ratio      fact_cnt,
  //        tlf2.fact_time_length * ratio.ratio fact_time_length
  //      FROM
  //        (
  //          SELECT
  //            tlf.key,
  //            tlf.dim_sn,
  //            tlf.dim_channel,
  //            tlf.date,
  //            tlf.dim_hour,
  //            tlf.dim_min,
  //            tlf.fact_cnt,
  //            tlf.fact_time_length,
  //            terminal.brand,
  //            terminal.province
  //          FROM (select key,dim_sn,dim_channel,date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_live_fact_partition where date = '""" + analysisDate +
  //      """') tlf
  //              JOIN
  //                (select sn,brand as brand,province from hr.live_terminal) terminal
  //                ON (tlf.dim_sn = terminal.sn)
  //          ) tlf2
  //          JOIN
  //          hr.tracker_total_dim_ratio_fst ratio
  //            ON (tlf2.brand = ratio.brand AND tlf2.province = ratio.province)
  //      """.stripMargin
  //  }

  ////////////////////////////////////APK///////////////////////////////////////////
  val tracker_total_apk_active_fact_HtableName = "tracker_total_apk_active_fact"

//  val tracker_total_apk_active_fact_HtableName = "tracker_total_apk_fact_zyt"

  //TODO 4月
  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
    """
          SELECT
                    taf2.key,
                    taf2.dim_sn,
                    taf2.dim_apk,
                    taf2.date,
                    taf2.dim_hour,
                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
                    taf2.fact_duration * ratio.ratio as fact_duration
                  FROM
                    (
                      SELECT
                        taf.key,
                        taf.dim_sn,
                        taf.dim_apk,
                        taf.date,
                        taf.dim_hour,
                        taf.fact_cnt,
                        taf.fact_duration,
                        terminal.brand,
                        terminal.license,
                        terminal.province
                      FROM
                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition ap
                        JOIN (select * from hr.apkinfo where onelevel = '视频')   ai
                        ON (ap.dim_apk = ai.packagename and ai.appname not in('银河·奇异果','腾讯视频TV端','CIBN环球影视'))
                       where date = '""" + analysisDate + """') taf
                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
                    ON (taf.dim_sn = terminal.sn)
              ) taf2
              JOIN
              ( select * from hr.tracker_total_dim_ratio_sec where date = date_sub('""" + analysisDate + """',0) )
               ratio
                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province )
      """.stripMargin
  }

  def getTracker_total_apk_active_fact_vsttj_HQL(analysisDate: String): String = {
    """
          SELECT
                    taf2.key,
                    taf2.dim_sn,
                    taf2.dim_apk,
                    taf2.date,
                    taf2.dim_hour,
                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
                    taf2.fact_duration * ratio.ratio as fact_duration
                  FROM
                    (
                      SELECT
                        taf.key,
                        taf.dim_sn,
                        taf.dim_apk,
                        taf.date,
                        taf.dim_hour,
                        taf.fact_cnt,
                        taf.fact_duration,
                        terminal.brand,
                        terminal.license,
                        terminal.province
                      FROM
                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr
                       .tracker_apk_fact_partition ap
                       JOIN (select * from hr.apkinfo where onelevel = '视频') ai
                       ON (ap.dim_apk = ai.packagename and ai.appname in('银河·奇异果','腾讯视频TV端','CIBN环球影视'))
                       where date = '""" + analysisDate +
      """') taf
                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
                    ON (taf.dim_sn = terminal.sn)
              ) taf2
              JOIN
               ( select * from hr.tracker_total_dim_ratio_sec2 where date = date_sub('""" + analysisDate + """',0) )
                                                                           ratio
                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
      """.stripMargin
  }




  //TODO 截止到3月
//  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
//    """
//          SELECT
//                    taf2.key,
//                    taf2.dim_sn,
//                    taf2.dim_apk,
//                    taf2.date,
//                    taf2.dim_hour,
//                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
//                    taf2.fact_duration * ratio.ratio as fact_duration
//                  FROM
//                    (
//                      SELECT
//                        taf.key,
//                        taf.dim_sn,
//                        taf.dim_apk,
//                        taf.date,
//                        taf.dim_hour,
//                        taf.fact_cnt,
//                        taf.fact_duration,
//                        terminal.brand,
//                        terminal.license,
//                        terminal.province
//                      FROM
//                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition ap
//                        JOIN (select * from hr.apkinfo where onelevel = '视频')   ai
//                        ON (ap.dim_apk = ai.packagename and ai.appname not in('CIBN微视听','云视听·泰捷'))
//                       where date = '""" + analysisDate + """') taf
//                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
//                    ON (taf.dim_sn = terminal.sn)
//              ) taf2
//              JOIN
//              hr.tracker_total_dim_ratio_sec ratio
//                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
//      """.stripMargin
//  }
//
//  def getTracker_total_apk_active_fact_vsttj_HQL(analysisDate: String): String = {
//    """
//          SELECT
//                    taf2.key,
//                    taf2.dim_sn,
//                    taf2.dim_apk,
//                    taf2.date,
//                    taf2.dim_hour,
//                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
//                    taf2.fact_duration * ratio.ratio as fact_duration
//                  FROM
//                    (
//                      SELECT
//                        taf.key,
//                        taf.dim_sn,
//                        taf.dim_apk,
//                        taf.date,
//                        taf.dim_hour,
//                        taf.fact_cnt,
//                        taf.fact_duration,
//                        terminal.brand,
//                        terminal.license,
//                        terminal.province
//                      FROM
//                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr
//                       .tracker_apk_fact_partition ap
//                       JOIN (select * from hr.apkinfo where onelevel = '视频') ai
//                       ON (ap.dim_apk = ai.packagename and ai.appname in('CIBN微视听','云视听·泰捷'))
//                       where date = '""" + analysisDate +
//      """') taf
//                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
//                    ON (taf.dim_sn = terminal.sn)
//              ) taf2
//              JOIN
//              hr.tracker_total_dim_ratio_vsttj_sec ratio
//                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
//      """.stripMargin
//  }


  ///////////////////////////////////////////HiveSQL TO  Mysql /////////////////////////////////////////////////////


  //TV总览--地区分布
  val vbox_tv_eara_tableName = "vbox_tv_eara"

  val vbox_tv_eara_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'weekly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 week
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.week

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
      )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'monthly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 month
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.month

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_daily_hql =
    """

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
          GROUP BY
            date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            '智能电视开关机'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            'OTT'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  val vbox_tv_eara_weekly_hql =
    """


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '直播'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '直播'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_monthly_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'monthly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  //TV总览-排行榜
  val vbox_tv_ranking_tableName = "vbox_tv_ranking"

  val vbox_tv_ranking_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all


      	SELECT
        alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                            onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            SUM(fact_duration)                        AS t_tcnt,
            SUM(fact_cnt)                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct
                         appname,
                         packagename
                       FROM hr.apkinfo
                        WHERE onelevel = '视频'
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
      	   group by dim_date,apkinfo2.appname
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)

      	union all


      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)


      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      	SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'weekly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 week
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.week

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)

      	 union all

      	 SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      		SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'monthly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 month
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.month

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_daily_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all

      SELECT
        Distinct alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all


      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                         Distinct onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                           Distinct onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      union all


      SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct  onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             Distinct dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            fact_duration                        AS t_tcnt,
            fact_cnt                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct  onelevel,
                         appname,
                         packagename
                       FROM hr.apkinfo
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
          WHERE apkinfo2.onelevel = '视频'
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)
    """.stripMargin


  val vbox_tv_ranking_weekly_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'weekly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.week

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_monthly_hql =
    """

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                         dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'monthly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.month

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)

    """.stripMargin
}
package com.avcdata.spark.job.konka

object Sql {
    //TV总览--整体趋势
    val vbox_tv_ratetable = "vbox_tv_rate"
    val vbox_tv_ratesql =
        """
          |select (dap.today) t_today, ("daily") t_datetype, ("OTT") t_name,dap.apk_count t_acnt, dap.apk_dura t_tcnt, dap.oc_cnt t_ucnt, dap.apk_avg t_tcntavgclient from
          |(select d.today, COALESCE(ap.apk_count,'0') apk_count, COALESCE(ap.apk_dura,'0') apk_dura, COALESCE(ap.oc_cnt,'0') oc_cnt,COALESCE(cast(ap.apk_dura/ap.apk_count as double), 0) apk_avg from
          |(select today from hr.dateinfo) d
          |left outer join
          |(select dim_date,count(distinct dim_sn) apk_count, cast(sum(fact_duration)/3600 as int) apk_dura, cast(sum(fact_cnt) as int) oc_cnt from hr.tracker_apk_fact group by dim_date) ap
          |on d.today=ap.dim_date) dap
          |
          |union all
          |
          |select (dtp.today) t_today, ("daily") t_datetype, ("智能电视开机") t_name,dtp.oc_count t_acnt, dtp.oc_dura t_tcnt, dtp.oc_cnt t_ucnt, dtp.oc_avg t_tcntavgclient from
          |(select d.today, COALESCE(tp.oc_count,'0') oc_count, COALESCE(tp.oc_dura,'0') oc_dura, COALESCE(tp.oc_cnt,'0') oc_cnt, COALESCE(cast(tp.oc_dura/tp.oc_count as double), 0) oc_avg from
          |(select today from hr.dateinfo) d
          |left outer join
          |(select power_on_day,count(distinct sn) oc_count, cast(sum(power_on_length)/60 as int) oc_dura, cast(sum(cnt) as int) oc_cnt from hr.tracker_oc_fact group by power_on_day) tp
          |on d.today=tp.power_on_day) dtp
          |
          |union all
          |
          |select (dlp.today) t_today, ("daily") t_datetype, ("直播") t_name,dlp.live_count t_acnt, dlp.live_dura t_tcnt, dlp.oc_cnt t_ucnt, dlp.live_avg t_tcntavgclient from
          |(select d.today, COALESCE(lp.live_count,'0') live_count, COALESCE(lp.live_dura,'0') live_dura, COALESCE(lp.oc_cnt,'0') oc_cnt, COALESCE(cast(lp.live_dura/lp.live_count as double), 0) live_avg from
          |(select today from hr.dateinfo) d
          |left outer join
          |(select dim_date,count(distinct dim_sn) live_count, cast(sum(fact_time_length)/3600 as int) live_dura, cast(sum(fact_cnt) as int) oc_cnt from hr.tracker_live_fact group by dim_date) lp
          |on d.today=lp.dim_date) dlp
          |
          |union all
          |
          |select (wap.today) t_today, ("weekly") t_datetype, ("OTT") t_name, wap.apk_count t_acnt, wap.apks t_tcnt, wap.oc_cnt t_ucnt, wap.apk_avg t_tcntavgclient from
          |(select dap.today, cast(dap.apk_count as int) apk_count, cast(dap.apks as int) apks, COALESCE(dap.oc_cnt,'0') oc_cnt, COALESCE(cast(dap.apks/dap.apk_count as double), 0) apk_avg from
          |(select min(d.today) today,d.year,d.week,count(distinct ap.dim_sn) apk_count, coalesce(sum(ap.snc)/3600,0) apks, cast(sum(ap.oc_cnt) as int) oc_cnt from
          |(select today,year,week from hr.dateinfo d)d
          |left join
          |(select dim_date,dim_sn,sum(fact_duration) snc, cast(sum(fact_cnt) as int) oc_cnt from hr.tracker_apk_fact group by dim_date,dim_sn) ap
          |on d.today=ap.dim_date group by d.year,d.week) dap)wap
          |
          |union all
          |
          |select (wtp.today) t_today, ("weekly") t_datetype, ("智能电视开机") t_name, wtp.oc_count t_acnt, wtp.apks t_tcnt, wtp.ocu t_ucnt, wtp.oc_avg t_tcntavgclient from
          |(select dtp.today, cast(dtp.oc_count as int) oc_count, cast(dtp.apks as int) apks, cast(dtp.ocu as int) ocu, COALESCE(cast(dtp.apks/dtp.oc_count as double), 0) oc_avg from
          |(select min(d.today) today,d.year,d.week, count(distinct tp.sn) oc_count, coalesce(sum(tp.snc)/60,0) apks, coalesce(sum(tp.oc_cnt), 0) ocu from
          |(select today,year,week from hr.dateinfo d)d
          |left join
          |(select power_on_day, sn, sum(power_on_length) snc, cast(sum(cnt) as int) oc_cnt from hr.tracker_oc_fact group by power_on_day, sn) tp
          |on d.today=tp.power_on_day group by d.year,d.week) dtp)wtp
          |
          |union all
          |
          |select (wlp.today) t_today, ("weekly") t_datetype, ("直播") t_name, wlp.live_count t_acnt, wlp.apks t_tcnt, wlp.oc_cnt t_ucnt, wlp.live_avg t_tcntavgclient from
          |(select dlp.today, cast(dlp.live_count as int) live_count, cast(dlp.apks as int) apks, COALESCE(dlp.oc_cnt,'0') oc_cnt, COALESCE(cast(dlp.apks/dlp.live_count as double), 0) live_avg from
          |(select min(d.today) today,d.year,d.week, count(distinct lp.dim_sn) live_count, coalesce(sum(lp.snc)/3600,0) apks, cast(sum(lp.oc_cnt) as int) oc_cnt from
          |(select today,year,week from hr.dateinfo d)d
          |left join
          |(select dim_date,dim_sn, sum(fact_time_length) snc, cast(sum(fact_cnt) as int) oc_cnt from hr.tracker_live_fact group by dim_date, dim_sn) lp
          |on d.today=lp.dim_date group by d.year,d.week) dlp)wlp
          |
          |union all
          |
          |select (moap.today) t_today, ("monthly") t_datetype, ("OTT") t_name, moap.apk_count t_acnt, moap.apks t_tcnt, moap.oc_cnt t_ucnt, moap.apk_avg t_tcntavgclient from
          |(select dap.today, cast(dap.apk_count as int) apk_count, cast(dap.apks as int) apks, COALESCE(dap.oc_cnt,'0') oc_cnt, COALESCE(cast(dap.apks/dap.apk_count as double), 0) apk_avg from
          |(select min(d.today) today,d.year,d.month,count(distinct ap.dim_sn) apk_count,coalesce(sum(ap.snc)/3600,0) apks, cast(sum(oc_cnt) as int) oc_cnt from
          |(select today,year,month from hr.dateinfo d)d
          |left join
          |(select dim_date,dim_sn,sum(fact_duration) snc, cast(sum(fact_cnt) as int) oc_cnt from hr.tracker_apk_fact group by dim_date, dim_sn) ap
          |on d.today=ap.dim_date group by d.year,d.month) dap)moap
          |
          |union all
          |
          |select (mtp.today) t_today, ("monthly") t_datetype, ("智能电视开机") t_name, mtp.oc_count t_acnt, mtp.apks t_tcnt, mtp.ocu t_ucnt, mtp.oc_avg t_tcntavgclient from
          |(select dtp.today, cast(dtp.oc_count as int) oc_count, cast(dtp.apks as int) apks, cast(dtp.ocu as int) ocu, COALESCE(cast(dtp.apks/dtp.oc_count as double), 0) oc_avg from
          |(select min(d.today) today,d.year,d.month,count(distinct tp.sn) oc_count,coalesce(sum(tp.snc)/60,0) apks, coalesce(sum(tp.oc_cnt),0) ocu from
          |(select today,year,month from hr.dateinfo d)d
          |left join
          |(select power_on_day,sn,sum(power_on_length) snc, cast(sum(cnt) as int) oc_cnt from hr.tracker_oc_fact group by power_on_day, sn) tp
          |on d.today=tp.power_on_day group by d.year,d.month) dtp)mtp
          |
          |union all
          |
          |select (mlp.today) t_today, ("monthly") t_datetype, ("直播") t_name, mlp.live_count t_acnt, mlp.apks t_tcnt, mlp.oc_cnt t_ucnt, mlp.live_avg t_tcntavgclient from
          |(select dlp.today, cast(dlp.live_count as int) live_count, cast(dlp.apks as int) apks, COALESCE(dlp.oc_cnt,'0') oc_cnt, COALESCE(cast(dlp.apks/dlp.live_count as double), 0) live_avg from
          |(select min(d.today) today,d.year,d.month,count(distinct lp.dim_sn) live_count,coalesce(sum(lp.snc)/60,0) apks, cast(sum(lp.oc_cnt) as int) oc_cnt from
          |(select today,year,month from hr.dateinfo d)d
          |left join
          |(select dim_date,dim_sn,sum(fact_time_length) snc, cast(sum(fact_cnt) as int) oc_cnt from hr.tracker_live_fact group by dim_date, dim_sn) lp
          |on d.today=lp.dim_date group by d.year,d.month) dlp)mlp
        """.stripMargin

    //TV总览--排行榜
    val vbox_tv_rankingtable = "vbox_tv_ranking"
    val vbox_tv_rankingsql =
        """
          |SELECT
          |  dayres.dim_date                                t_today,
          |  'daily'                                        t_datetype,
          |  dayres.dim_area                                t_eara,
          |  dayres.apkorchannel                            t_name,
          |  COUNT(DISTINCT dayres.dim_sn)                  t_acnt,
          |  ROUND(SUM(COALESCE(dayres.dura, 0)) / 3600, 2) t_tcnt,
          |  SUM(COALESCE(dayres.fact_cnt, 0))              t_ucnt
          |FROM
          |  (
          |    SELECT
          |      liveorapk.dim_sn,
          |      liveorapk.dim_date,
          |      liveorapk.apkorchannel,
          |      liveorapk.fact_cnt,
          |      liveorapk.dura,
          |      COALESCE(terminal_region.province, 'unknow') dim_area
          |    FROM
          |      (SELECT
          |         dim_sn,
          |         dim_date,
          |         dim_channel      apkorchannel,
          |         fact_cnt,
          |         fact_time_length dura
          |       FROM
          |         hr.tracker_live_fact tracker_live_fact
          |       WHERE tracker_live_fact.dim_channel <> '其他'
          |       UNION
          |       ALL
          |       SELECT
          |         dim_sn,
          |         dim_date,
          |         apkorchannel,
          |         fact_cnt,
          |         dura
          |       FROM (
          |              SELECT
          |                taf.dim_sn,
          |                taf.dim_date,
          |                COALESCE(apkinfo.appname, 'unknow') apkorchannel,
          |                taf.fact_cnt,
          |                taf.fact_duration                   dura
          |              FROM hr.tracker_apk_fact taf
          |                JOIN hr.apkinfo apkinfo
          |                  ON (taf.dim_apk = apkinfo.packagename)
          |              WHERE apkinfo.onelevel = '视频'
          |            ) taf2
          |
          |
          |      ) liveorapk
          |      JOIN
          |      (
          |        SELECT
          |          terminal.sn,
          |          region.province
          |        FROM hr.terminal
          |          JOIN hr.region
          |            ON (terminal.area = region.code)
          |      ) terminal_region
          |        ON (liveorapk.dim_sn = terminal_region.sn)
          |  ) dayres
          |GROUP BY dim_date,
          |  dim_area,
          |  apkorchannel
          |
          |UNION ALL
          |
          |SELECT
          |  MIN(dateinfo2.today)        t_today,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  ROUND(tmp.t_tcnt / 3600, 2) t_tcnt,
          |  tmp.t_ucnt
          |FROM
          |  (
          |    SELECT
          |      dayres.week_num,
          |      'weekly'                          t_datetype,
          |      dayres.dim_area                   t_eara,
          |      dayres.apkorchannel               t_name,
          |      COUNT(DISTINCT dayres.dim_sn)     t_acnt,
          |      SUM(COALESCE(dayres.dura, 0))     t_tcnt,
          |      SUM(COALESCE(dayres.fact_cnt, 0)) t_ucnt
          |    FROM
          |      (
          |        SELECT
          |          liveorapk.dim_sn,
          |          CONCAT(YEAR(liveorapk.dim_date), '-', WEEKOFYEAR(liveorapk.dim_date)) week_num,
          |          liveorapk.apkorchannel,
          |          liveorapk.fact_cnt,
          |          liveorapk.dura,
          |          COALESCE(terminal_region.province, 'unknow')                          dim_area
          |        FROM
          |          (SELECT
          |             dim_sn,
          |             dim_date,
          |             dim_channel      apkorchannel,
          |             fact_cnt,
          |             fact_time_length dura
          |           FROM
          |             hr.tracker_live_fact tracker_live_fact
          |           WHERE tracker_live_fact.dim_channel <> '其他'
          |           UNION
          |           ALL
          |           SELECT
          |             dim_sn,
          |             dim_date,
          |             apkorchannel,
          |             fact_cnt,
          |             dura
          |           FROM (
          |                  SELECT
          |                    taf.dim_sn,
          |                    taf.dim_date,
          |                    COALESCE(apkinfo.appname, 'unknow') apkorchannel,
          |                    taf.fact_cnt,
          |                    taf.fact_duration                   dura
          |                  FROM hr.tracker_apk_fact taf
          |                    JOIN hr.apkinfo apkinfo
          |                      ON (taf.dim_apk = apkinfo.packagename)
          |                  WHERE apkinfo.onelevel = '视频'
          |                ) taf2
          |
          |          ) liveorapk
          |          JOIN
          |          (
          |            SELECT
          |              terminal.sn,
          |              region.province
          |            FROM hr.terminal
          |              JOIN hr.region
          |                ON (terminal.area = region.code)
          |          ) terminal_region
          |            ON (liveorapk.dim_sn = terminal_region.sn)
          |      ) dayres
          |    GROUP BY week_num,
          |      dim_area,
          |      apkorchannel
          |  ) tmp
          |
          |  JOIN (
          |         SELECT
          |           concat(YEAR, '-', week) week_num,
          |           today
          |         FROM hr.dateinfo
          |       ) dateinfo2
          |
          |    ON (
          |    tmp.week_num = dateinfo2.week_num
          |    )
          |
          |GROUP BY
          |  tmp.week_num,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  tmp.t_tcnt,
          |  tmp.t_ucnt
          |
          |UNION ALL
          |
          |SELECT
          |  MIN(dateinfo2.today)        t_today,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  ROUND(tmp.t_tcnt / 3600, 2) t_tcnt,
          |  tmp.t_ucnt
          |FROM
          |  (
          |    SELECT
          |      dayres.month_num,
          |      'monthly'                         t_datetype,
          |      dayres.dim_area                   t_eara,
          |      dayres.apkorchannel               t_name,
          |      COUNT(DISTINCT dayres.dim_sn)     t_acnt,
          |      SUM(COALESCE(dayres.dura, 0))     t_tcnt,
          |      SUM(COALESCE(dayres.fact_cnt, 0)) t_ucnt
          |    FROM
          |      (
          |        SELECT
          |          liveorapk.dim_sn,
          |          CONCAT(YEAR(liveorapk.dim_date), '-', MONTH(liveorapk.dim_date)) month_num,
          |          liveorapk.apkorchannel,
          |          liveorapk.fact_cnt,
          |          liveorapk.dura,
          |          COALESCE(terminal_region.province, 'unknow')                     dim_area
          |        FROM
          |          (SELECT
          |             dim_sn,
          |             dim_date,
          |             dim_channel      apkorchannel,
          |             fact_cnt,
          |             fact_time_length dura
          |           FROM
          |             hr.tracker_live_fact tracker_live_fact
          |           WHERE tracker_live_fact.dim_channel <> '其他'
          |           UNION
          |           ALL
          |           SELECT
          |             dim_sn,
          |             dim_date,
          |             apkorchannel,
          |             fact_cnt,
          |             dura
          |           FROM (
          |                  SELECT
          |                    taf.dim_sn,
          |                    taf.dim_date,
          |                    COALESCE(apkinfo.appname, 'unknow') apkorchannel,
          |                    taf.fact_cnt,
          |                    taf.fact_duration                   dura
          |                  FROM hr.tracker_apk_fact taf
          |                    JOIN hr.apkinfo apkinfo
          |                      ON (taf.dim_apk = apkinfo.packagename)
          |                  WHERE apkinfo.onelevel = '视频'
          |                ) taf2
          |
          |          ) liveorapk
          |          JOIN
          |          (
          |            SELECT
          |              terminal.sn,
          |              region.province
          |            FROM hr.terminal
          |              JOIN hr.region
          |                ON (terminal.area = region.code)
          |          ) terminal_region
          |            ON (liveorapk.dim_sn = terminal_region.sn)
          |      ) dayres
          |    GROUP BY month_num,
          |      dim_area,
          |      apkorchannel
          |  ) tmp
          |
          |  JOIN (
          |         SELECT
          |           CONCAT(YEAR, '-', month) month_num,
          |           today
          |         FROM hr.dateinfo
          |       ) dateinfo2
          |
          |    ON (
          |    tmp.month_num = dateinfo2.month_num
          |    )
          |
          |GROUP BY
          |  tmp.month_num,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  tmp.t_tcnt,
          |  tmp.t_ucnt
        """.stripMargin

    //TV总览--小时
    val vbox_tv_hourtable = "vbox_tv_hour"
    val vbox_tv_hoursql =
        """
          |select (hap.adate) t_today, (hap.ahour) t_hour, ("OTT") t_name,hap.apk_count t_acnt, hap.apk_dura t_tcnt, (null) t_ucnt from
          |(select ap.adate, ap.ahour, COALESCE(ap.apk_count,'0') apk_count, COALESCE(ap.apk_dura,'0') apk_dura from
          |(select (dim_date) adate, (dim_hour) ahour, count(distinct dim_sn) apk_count, cast(sum(fact_duration)/3600 as int) apk_dura
          |from hr.tracker_apk_fact group by dim_date, dim_hour) ap)hap
          |
          |union all
          |
          |select (htp.power_on_day) t_today, (htp.power_on_time) t_hour, ("智能电视开机") t_name, htp.oc_count t_acnt, htp.oc_dura t_tcnt, htp.oc_cnt t_ucnt from
          |(select tp.power_on_day, tp.power_on_time, COALESCE(tp.oc_count,'0') oc_count, COALESCE(tp.oc_dura,'0') oc_dura, COALESCE(tp.oc_cnt,'0') oc_cnt from
          |(select power_on_day, power_on_time, count(distinct sn) oc_count, cast(sum(power_on_length)/60 as int) oc_dura, cast(sum(cnt) as int) oc_cnt
          |from hr.tracker_oc_fact group by power_on_day, power_on_time) tp)htp
          |
          |union all
          |
          |select (hlp.dim_date) t_today, hlp.dim_hour t_hour, ("直播") t_name,hlp.live_count t_acnt, hlp.live_dura t_tcnt, (null) t_ucnt from
          |(select lp.dim_date, lp.dim_hour, COALESCE(lp.live_count,'0') live_count, COALESCE(lp.live_dura,'0') live_dura from
          |(select dim_date,dim_hour,count(distinct dim_sn) live_count, cast(sum(fact_time_length)/3600 as int) live_dura
          |from hr.tracker_live_fact group by dim_date, dim_hour) lp)hlp
        """.stripMargin

    //OTT--分析首页
    val vbox_ott_infotable = "vbox_ott_info"
    val vbox_ott_infosql =
        """
          |select dim_date as t_today,'daily' as t_datetype,IF (GROUPING__ID = 3, '全国', province) AS t_eara,appname as t_name,
          |count(dim_sn) as t_acnt,sum(fact_duration/3600) as t_tcnt,sum(fact_cnt) as t_ucnt,
          |sum(fact_duration/3600)/count(dim_sn) as t_tcntavgclient,sum(fact_duration/3600)/sum(fact_cnt) as t_tcntc,sum(fact_cnt)/count(dim_sn) as t_ucntavgclient
          |from (select dim_sn,dim_apk,dim_date,sum(fact_duration) fact_duration,sum(fact_cnt) fact_cnt
          |from hr.tracker_apk_fact
          |group by dim_sn,dim_apk,dim_date) ai
          |join
          |(select distinct packagename,appname
          |from hr.apkinfo) an
          |join
          |(select distinct sn,province
          |from hr.terminal t
          |join
          |hr.region r
          |on t.area=r.code) pc
          |on ai.dim_apk=an.packagename and ai.dim_sn=pc.sn
          |group by appname,dim_date,province
          |GROUPING SETS((appname,dim_date),(appname,dim_date,province))
          |
          |union all
          |
          |select t_today,t_datetype,t_eara,t_name,t_acnt,t_tcnt,t_ucnt,t_tcntavgclient,t_tcntc,t_ucntavgclient from
          |(select yearly,weekly,min(dim_date) as t_today,'weekly' as t_datetype,
          |IF (GROUPING__ID = 7, '全国', province) AS t_eara,appname as t_name,
          |count(dim_sn) as t_acnt,sum(fact_duration/3600) as t_tcnt,sum(fact_cnt) as t_ucnt,
          |sum(fact_duration/3600)/count(dim_sn) as t_tcntavgclient,sum(fact_duration/3600)/sum(fact_cnt) as t_tcntc,sum(fact_cnt)/count(dim_sn) as t_ucntavgclient
          |from (select dim_sn,dim_apk,dim_date,sum(fact_duration) fact_duration,sum(fact_cnt) fact_cnt
          |from hr.tracker_apk_fact
          |group by dim_sn,dim_apk,dim_date) ai
          |join
          |(select distinct packagename,appname
          |from hr.apkinfo) an
          |join
          |(select distinct sn,province
          |from hr.terminal t
          |join
          |hr.region r
          |on t.area=r.code) pc
          |join
          |(select today,week weekly,year yearly from hr.dateinfo) d
          |on ai.dim_apk=an.packagename and ai.dim_sn=pc.sn and ai.dim_date=d.today
          |group by yearly,weekly,appname,province
          |GROUPING SETS((yearly,weekly,appname),(yearly,weekly,appname,province))
          |)A order by t_today,t_name
          |
          |union all
          |
          |select t_today,t_datetype,t_eara,t_name,t_acnt,t_tcnt,t_ucnt,t_tcntavgclient,t_tcntc,t_ucntavgclient from
          |(select yearly,monthly,min(dim_date) as t_today,'monthly' as t_datetype,
          |IF (GROUPING__ID = 7, '全国', province) AS t_eara,appname as t_name,
          |count(dim_sn) as t_acnt,sum(fact_duration/3600) as t_tcnt,sum(fact_cnt) as t_ucnt,
          |sum(fact_duration/3600)/count(dim_sn) as t_tcntavgclient,sum(fact_duration/3600)/sum(fact_cnt) as t_tcntc,sum(fact_cnt)/count(dim_sn) as t_ucntavgclient
          |from (select dim_sn,dim_apk,dim_date,sum(fact_duration) fact_duration,sum(fact_cnt) fact_cnt
          |from hr.tracker_apk_fact
          |group by dim_sn,dim_apk,dim_date) ai
          |join
          |(select distinct packagename,appname
          |from hr.apkinfo) an
          |join
          |(select distinct sn,province
          |from hr.terminal t
          |join
          |hr.region r
          |on t.area=r.code) pc
          |join
          |(select today,month monthly,year yearly from hr.dateinfo) d
          |on ai.dim_apk=an.packagename and ai.dim_sn=pc.sn and ai.dim_date=d.today
          |group by yearly,monthly,appname,province
          |GROUPING SETS((yearly,monthly,appname),(yearly,monthly,appname,province))
          |)A order by t_today,t_name;
          |
          |
        """.stripMargin

    //TV
    val vbox_tv_infotable = "vbox_tv_info"
    val vbox_tv_infosql =
        """
          |select
          |  dim_date AS t_today ,'daily' AS t_datetype, IF (GROUPING__ID = 3, '全国', province) AS t_eara,dim_channel AS t_name ,count(DISTINCT(dim_sn)) AS  t_acnt,SUM(fact_time_length/3600) AS t_tcnt,SUM(fact_time_length/3600)/SUM(fact_cnt) AS  t_tcntc,SUM(fact_time_length/3600)/count(DISTINCT(dim_sn)) AS t_tcntavgclient,SUM(fact_cnt)/count(DISTINCT(dim_sn)) AS t_ucntavgclient,SUM(fact_cnt) AS t_ucnt
          |  from hr.tracker_live_fact A
          |  left join hr.terminal B
          |  on A.dim_sn=B.sn
          |  left join hr.region D
          |  on B.area=D.code
          |  group  by dim_channel,province,dim_date
          | GROUPING SETS((dim_channel,dim_date),(dim_channel,dim_date,province))
          | UNION All
          |select
          |  MIN(today) AS t_today ,'weekly' AS t_datetype, IF (GROUPING__ID = 9, '全国', province) AS t_eara,dim_channel AS t_name ,count(DISTINCT(dim_sn)) AS  t_acnt,SUM(fact_time_length/3600) AS t_tcnt,SUM(fact_time_length/3600)/SUM(fact_cnt) AS  t_tcntc,SUM(fact_time_length/3600)/count(DISTINCT(dim_sn)) AS t_tcntavgclient,SUM(fact_cnt)/count(DISTINCT(dim_sn)) AS t_ucntavgclient,SUM(fact_cnt) AS t_ucnt
          |  from hr.tracker_live_fact A
          |  left join hr.dateinfo C
          |  on A.dim_date=C.today
          |  left join hr.terminal B
          |  on A.dim_sn=B.sn
          |  left join hr.region D
          |  on B.area=D.code
          |  group  by dim_channel,province,week
          | GROUPING SETS((dim_channel,week),(dim_channel,week,province))
          | UNION All
          |select
          |  MIN(today) AS t_today ,'monthly' AS t_datetype, IF (GROUPING__ID = 7, '全国', province) AS t_eara,dim_channel AS t_name ,count(DISTINCT(dim_sn)) AS  t_acnt,SUM(fact_time_length/3600) AS t_tcnt,SUM(fact_time_length/3600)/SUM(fact_cnt) AS  t_tcntc,SUM(fact_time_length/3600)/count(DISTINCT(dim_sn)) AS t_tcntavgclient,SUM(fact_cnt)/count(DISTINCT(dim_sn)) AS t_ucntavgclient,SUM(fact_cnt) AS t_ucnt
          |  from hr.tracker_live_fact A
          |  left join hr.dateinfo C
          |  on A.dim_date=C.today
          |  left join hr.terminal B
          |  on A.dim_sn=B.sn
          |  left join hr.region D
          |  on B.area=D.code
          |  group  by dim_channel,province,month
          | GROUPING SETS((dim_channel,month),(dim_channel,month,province))
          |
        """.stripMargin

    //TV总览——地区分布
    val vbox_tv_earatable = "vbox_tv_eara"
    val vbox_tv_earasql =
        """
          |SELECT
          |  dayres.dim_date                                                             t_today,
          |  'daily'                                                                     t_datetype,
          |  'null'                                                                      t_eara,
          |  dayres.dim_area                                                             t_province,
          |  dayres.t_name,
          |  COUNT(DISTINCT dayres.dim_sn)                                               t_acnt,
          |  SUM(COALESCE(dayres.dura, 0))                                               t_tcnt,
          |  ROUND(SUM(COALESCE(dayres.fact_cnt, 0)), 2)                                 t_ucnt,
          |  IF(COUNT(DISTINCT dayres.dim_sn) = 0, 0,
          |     ROUND(SUM(COALESCE(dayres.dura, 0)) / COUNT(DISTINCT dayres.dim_sn), 2)) t_tcntavgclient
          |
          |FROM
          |  (
          |    SELECT
          |      all_terminal.dim_date,
          |      all_terminal.t_name,
          |      all_terminal.dim_sn,
          |      all_terminal.fact_cnt,
          |      all_terminal.dura,
          |      COALESCE(terminal_region.province, 'unknow') dim_area
          |
          |    FROM (
          |
          |           SELECT
          |             power_on_day         dim_date,
          |             '智能电视开机'             t_name,
          |             sn                   dim_sn,
          |             cnt                  fact_cnt,
          |             power_on_length / 60 dura
          |           FROM
          |             hr.tracker_oc_fact
          |           UNION ALL
          |           SELECT
          |             dim_date,
          |             '直播端'                   t_name,
          |             dim_sn,
          |             0                       fact_cnt,
          |             fact_time_length / 3600 dura
          |           FROM
          |             hr.tracker_live_fact
          |           UNION ALL
          |           SELECT
          |             dim_date,
          |             'OTT端'               t_name,
          |             dim_sn,
          |             0                    fact_cnt,
          |             fact_duration / 3600 dura
          |           FROM
          |             hr.tracker_apk_fact
          |
          |         ) all_terminal
          |      LEFT OUTER JOIN
          |      (
          |        SELECT
          |          terminal.sn,
          |          region.province
          |        FROM hr.terminal
          |          JOIN hr.region
          |            ON (terminal.area = region.code)
          |      ) terminal_region
          |        ON (all_terminal.dim_sn = terminal_region.sn)
          |
          |  ) dayres
          |GROUP BY dim_date,
          |  dim_area,
          |  t_name
          |
          |UNION ALL
          |
          |SELECT
          |  MIN(dateinfo2.today) t_today,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_province,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  ROUND(tmp.t_tcnt, 2) t_tcnt,
          |  tmp.t_ucnt,
          |  tmp.t_tcntavgclient
          |FROM
          |  (
          |    SELECT
          |      dayres.week_num,
          |      'weekly'                                                                    t_datetype,
          |      'null'                                                                      t_eara,
          |      dayres.dim_area                                                             t_province,
          |      dayres.t_name,
          |      COUNT(DISTINCT dayres.dim_sn)                                               t_acnt,
          |      SUM(COALESCE(dayres.dura, 0))                                               t_tcnt,
          |      SUM(COALESCE(dayres.fact_cnt, 0))                                           t_ucnt,
          |      IF(COUNT(DISTINCT dayres.dim_sn) = 0, 0,
          |         ROUND(SUM(COALESCE(dayres.dura, 0)) / COUNT(DISTINCT dayres.dim_sn), 2)) t_tcntavgclient
          |
          |    FROM
          |      (
          |        SELECT
          |          concat(YEAR(all_terminal.dim_date), '-',WEEKOFYEAR(all_terminal.dim_date)) week_num,
          |          all_terminal.t_name,
          |          all_terminal.dim_sn,
          |          all_terminal.fact_cnt,
          |          all_terminal.dura,
          |          COALESCE(terminal_region.province, 'unknow')                               dim_area
          |
          |        FROM (
          |
          |               SELECT
          |                 power_on_day         dim_date,
          |                 '智能电视开机'             t_name,
          |                 sn                   dim_sn,
          |                 cnt                  fact_cnt,
          |                 power_on_length / 60 dura
          |               FROM
          |                 hr.tracker_oc_fact
          |               UNION ALL
          |               SELECT
          |                 dim_date,
          |                 '直播端'                   t_name,
          |                 dim_sn,
          |                 0                       fact_cnt,
          |                 fact_time_length / 3600 dura
          |               FROM
          |                 hr.tracker_live_fact
          |               UNION ALL
          |               SELECT
          |                 dim_date,
          |                 'OTT端'               t_name,
          |                 dim_sn,
          |                 0                    fact_cnt,
          |                 fact_duration / 3600 dura
          |               FROM
          |                 hr.tracker_apk_fact
          |
          |             ) all_terminal
          |          LEFT OUTER JOIN
          |          (
          |            SELECT
          |              terminal.sn,
          |              region.province
          |            FROM hr.terminal
          |              JOIN hr.region
          |                ON (terminal.area = region.code)
          |          ) terminal_region
          |            ON (all_terminal.dim_sn = terminal_region.sn)
          |
          |      ) dayres
          |    GROUP BY week_num,
          |      dim_area,
          |      t_name
          |
          |  ) tmp
          |
          |  JOIN (
          |         SELECT
          |           CONCAT(YEAR, '-', week) week_num,
          |           today
          |         FROM hr.dateinfo
          |       ) dateinfo2
          |
          |    ON (
          |    tmp.week_num = dateinfo2.week_num
          |    )
          |
          |GROUP BY
          |  tmp.week_num,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_province,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  tmp.t_tcnt,
          |  tmp.t_ucnt,
          |  tmp.t_tcntavgclient
          |
          |UNION ALL
          |
          |SELECT
          |  MIN(dateinfo2.today) t_today,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_province,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  ROUND(tmp.t_tcnt, 2) t_tcnt,
          |  tmp.t_ucnt,
          |  tmp.t_tcntavgclient
          |FROM
          |  (
          |    SELECT
          |      dayres.month_num,
          |      'monthly'                                                                   t_datetype,
          |      'null'                                                                      t_eara,
          |      dayres.dim_area                                                             t_province,
          |      dayres.t_name,
          |      COUNT(DISTINCT dayres.dim_sn)                                               t_acnt,
          |      SUM(COALESCE(dayres.dura, 0))                                               t_tcnt,
          |      SUM(COALESCE(dayres.fact_cnt, 0))                                           t_ucnt,
          |      IF(COUNT(DISTINCT dayres.dim_sn) = 0, 0,
          |         ROUND(SUM(COALESCE(dayres.dura, 0)) / COUNT(DISTINCT dayres.dim_sn), 2)) t_tcntavgclient
          |
          |    FROM
          |      (
          |        SELECT
          |          concat(YEAR(all_terminal.dim_date), '-',MONTH(all_terminal.dim_date)) month_num,
          |          all_terminal.t_name,
          |          all_terminal.dim_sn,
          |          all_terminal.fact_cnt,
          |          all_terminal.dura,
          |          COALESCE(terminal_region.province, 'unknow')                          dim_area
          |
          |        FROM (
          |
          |               SELECT
          |                 power_on_day         dim_date,
          |                 '智能电视开机'             t_name,
          |                 sn                   dim_sn,
          |                 cnt                  fact_cnt,
          |                 power_on_length / 60 dura
          |               FROM
          |                 hr.tracker_oc_fact
          |               UNION ALL
          |               SELECT
          |                 dim_date,
          |                 '直播端'                   t_name,
          |                 dim_sn,
          |                 0                       fact_cnt,
          |                 fact_time_length / 3600 dura
          |               FROM
          |                 hr.tracker_live_fact
          |               UNION ALL
          |               SELECT
          |                 dim_date,
          |                 'OTT端'               t_name,
          |                 dim_sn,
          |                 0                    fact_cnt,
          |                 fact_duration / 3600 dura
          |               FROM
          |                 hr.tracker_apk_fact
          |
          |             ) all_terminal
          |          LEFT OUTER JOIN
          |          (
          |            SELECT
          |              terminal.sn,
          |              region.province
          |            FROM hr.terminal
          |              JOIN hr.region
          |                ON (terminal.area = region.code)
          |          ) terminal_region
          |            ON (all_terminal.dim_sn = terminal_region.sn)
          |
          |      ) dayres
          |    GROUP BY month_num,
          |      dim_area,
          |      t_name
          |
          |  ) tmp
          |
          |  JOIN (
          |         SELECT
          |           CONCAT(YEAR, '-', month) month_num,
          |           today
          |         FROM hr.dateinfo
          |       ) dateinfo2
          |
          |    ON (
          |    tmp.month_num = dateinfo2.month_num
          |    )
          |
          |GROUP BY
          |  tmp.month_num,
          |  tmp.t_datetype,
          |  tmp.t_eara,
          |  tmp.t_province,
          |  tmp.t_name,
          |  tmp.t_acnt,
          |  tmp.t_tcnt,
          |  tmp.t_ucnt,
          |  tmp.t_tcntavgclient
        """.stripMargin
}
package com.avcdata.spark.job.total

object Sql01 {

  //////////////////////////////////////////HiveSQL次数和时长推总//////////////////////////////////////////////////////////

  //开关机
  val tracker_total_terminal_active_fact_HtableName = "tracker_total_terminal_active_fact"

  def getTracker_total_terminal_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT
        tof2.sn,
        tof2.power_on_day,
        tof2.power_on_time,
        tof2.cnt * ratio.ratio    as  cnt,
        tof2.power_on_length * ratio.ratio as  power_on_length
      FROM
        (
          SELECT
            tof.sn,
            tof.power_on_day,
            tof.power_on_time,
            tof.cnt,
            tof.power_on_length,
            terminal.brand,
            terminal.province
          FROM (select  sn,power_on_day,power_on_time,cnt,power_on_length from hr.tracker_oc_fact_partition where power_on_day = '""" + analysisDate +
      """') tof
              JOIN (select sn,brand,province from hr.terminal_partition) terminal
                ON (tof.sn = terminal.sn)
          ) tof2
          JOIN
          hr.tracker_total_dim_ratio_fst ratio
            ON (tof2.brand = ratio.brand AND tof2.province = ratio.province)
      """.stripMargin
  }


  val tracker_total_terminal_active_fact_bak =
    """
      SELECT
        CONCAT(tof2.sn ,"-",tof2.power_on_day) key,
        tof2.sn,
        tof2.power_on_day,
        tof2.power_on_time,
        CAST(ROUND(tof2.cnt * ratio.ratio,0) AS BIGINT)      cnt,
        CAST(ROUND(tof2.power_on_length * ratio.ratio,0) AS BIGINT)  power_on_length
      FROM
        (
          SELECT
            tof.sn,
            tof.power_on_day,
            tof.power_on_time,
            tof.cnt,
            tof.power_on_length,
            terminal.brand,
            terminal.province
          FROM (select sn,power_on_day,power_on_time,cnt,power_on_length from hr.tracker_oc_fact) tof
            JOIN (select sn,brand,province from hr.terminal) terminal
              ON (tof.sn = terminal.sn)
        ) tof2
        JOIN
        hr.tracker_total_dim_ratio_fst ratio
          ON (tof2.brand = ratio.brand AND tof2.province = ratio.province)
    """.stripMargin

  //直播
  val tracker_total_live_active_fact_HtableName = "tracker_total_live_active_fact"

  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT
        tlf2.dim_sn,
        tlf2.dim_channel,
        tlf2.date,
        tlf2.dim_hour,
        tlf2.dim_min,
        tlf2.fact_cnt * ratio.ratio      fact_cnt,
        tlf2.fact_time_length * ratio.ratio fact_time_length
      FROM
        (
          SELECT
            tlf.dim_sn,
            tlf.dim_channel,
            tlf.date,
            tlf.dim_hour,
            tlf.dim_min,
            tlf.fact_cnt,
            tlf.fact_time_length,
            terminal.brand,
            terminal.province
          FROM (select dim_sn,dim_channel,date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_live_fact_partition where date = '""" + analysisDate +
      """') tlf
              JOIN
                (select sn,brand,province from hr.terminal_partition) terminal
                ON (tlf.dim_sn = terminal.sn)
          ) tlf2
          JOIN
          hr.tracker_total_dim_ratio_fst ratio
            ON (tlf2.brand = ratio.brand AND tlf2.province = ratio.province)
      """.stripMargin
  }

  val tracker_total_live_active_fact_HQL_bak =
    """
      SELECT
        CONCAT(tlf2.dim_sn ,"-" ,tlf2.dim_channel ,"-",tlf2.dim_date)  key,
        tlf2.dim_sn,
        tlf2.dim_channel,
        tlf2.dim_date,
        tlf2.dim_hour,
        tlf2.dim_min,
        CAST(ROUND(tlf2.fact_cnt * ratio.ratio,0) AS BIGINT)      fact_cnt,
        CAST(ROUND(tlf2.fact_time_length * ratio.ratio,0) AS BIGINT)  fact_time_length
      FROM
        (
          SELECT
            tlf.dim_sn,
            tlf.dim_channel,
            tlf.dim_date,
            tlf.dim_hour,
            tlf.dim_min,
            tlf.fact_cnt,
            tlf.fact_time_length,
            terminal.brand,
            terminal.province
          FROM (select dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_live_fact) tlf
            JOIN
              (select sn,brand,province from hr.terminal) terminal
              ON (tlf.dim_sn = terminal.sn)
        ) tlf2
        JOIN
        hr.tracker_total_dim_ratio_fst ratio
          ON (tlf2.brand = ratio.brand AND tlf2.province = ratio.province)
    """.stripMargin

  def getTracker_total_live_active_fact_HQL_bak01(analysisDate: String): String = {
    """
      SELECT
        tlf2.key key,
        tlf2.dim_sn,
        tlf2.dim_channel,
        tlf2.dim_date,
        tlf2.dim_hour,
        tlf2.dim_min,
        tlf2.fact_cnt * ratio.ratio      fact_cnt,
        tlf2.fact_time_length * ratio.ratio fact_time_length
      FROM
        (
          SELECT
            tlf.key,
            tlf.dim_sn,
            tlf.dim_channel,
            tlf.dim_date,
            tlf.dim_hour,
            tlf.dim_min,
            tlf.fact_cnt,
            tlf.fact_time_length,
            terminal.brand,
            terminal.province
          FROM (select key,dim_sn,dim_channel,dim_date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_live_fact where dim_date = '""" + analysisDate +
      """') tlf
              JOIN
                (select sn,brand,province from hr.terminal) terminal
                ON (tlf.dim_sn = terminal.sn)
          ) tlf2
          JOIN
          hr.tracker_total_dim_ratio_fst ratio
            ON (tlf2.brand = ratio.brand AND tlf2.province = ratio.province)
      """.stripMargin

  }

  //APK
  val tracker_total_apk_active_fact_HtableName = "tracker_total_apk_active_fact"

  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
    """
      |   SELECT
      |        taf2.dim_sn,
      |        taf2.dim_apk,
      |        taf2.date,
      |        taf2.dim_hour,
      |        taf2.fact_cnt * ratio.ratio    as  fact_cnt,
      |        taf2.fact_duration * ratio.ratio as fact_duration
      |      FROM
      |        (
      |          SELECT
      |            taf.dim_sn,
      |            taf.dim_apk,
      |            taf.date,
      |            taf.dim_hour,
      |            taf.fact_cnt,
      |            taf.fact_duration,
      |            terminal.brand,
      |            terminal.license,
      |            terminal.province
      |          FROM
      |           (select dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition
      |           where date = '""" + analysisDate +
      """') taf
        |            JOIN (select distinct sn,brand,license,province from hr.terminal_partition) terminal
        |              ON (taf.dim_sn = terminal.sn)
        |        ) taf2
        |        JOIN
        |        hr.tracker_total_dim_ratio_sec ratio
        |          ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
      """.stripMargin
  }

  def getTracker_total_apk_active_fact_HQL_bak02(analysisDate: String): String = {
    """
      SELECT
        taf2.key as key,
        taf2.dim_sn,
        taf2.dim_apk,
        taf2.dim_date,
        taf2.dim_hour,
        taf2.fact_cnt * ratio.ratio    as  fact_cnt,
        taf2.fact_duration * ratio.ratio as fact_duration
      FROM
        (
          SELECT
            taf.key,
            taf.dim_sn,
            taf.dim_apk,
            taf.dim_date,
            taf.dim_hour,
            taf.fact_cnt,
            taf.fact_duration,
            terminal.brand,
            terminal.license,
            terminal.province
          FROM
           (select distinct key,dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact
           where dim_date = '""" + analysisDate +
      """') taf
            JOIN (select distinct sn,brand,license,province from hr.terminal) terminal
              ON (taf.dim_sn = terminal.sn)
        ) taf2
        JOIN
        hr.tracker_total_dim_ratio_sec ratio
          ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
      """.stripMargin
  }

  val tracker_total_apk_active_fact_HQL_Bak01 =
    """
      SELECT
        CONCAT(taf2.dim_sn ,"-" ,taf2.dim_apk ,"-",taf2.dim_date) as key,
        taf2.dim_sn,
        taf2.dim_apk,
        taf2.dim_date,
        taf2.dim_hour,
        CAST(ROUND(taf2.fact_cnt * ratio.ratio,0) AS BIGINT)    as  fact_cnt,
        CAST(ROUND(taf2.fact_duration * ratio.ratio,0) AS BIGINT) as fact_duration
      FROM
        (
          SELECT
            taf.dim_sn,
            taf.dim_apk,
            taf.dim_date,
            taf.dim_hour,
            taf.fact_cnt,
            taf.fact_duration,
            terminal.brand,
            terminal.license,
            terminal.province
          FROM
           (select distinct dim_sn,dim_apk,dim_date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact) taf
            JOIN (select distinct sn,brand,license,province from hr.terminal) terminal
              ON (taf.dim_sn = terminal.sn)
        ) taf2
        JOIN
        hr.tracker_total_dim_ratio_sec ratio
          ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
    """.stripMargin


  ///////////////////////////////////////////HiveSQL TO  Mysql /////////////////////////////////////////////////////


  //TV总览--地区分布
  val vbox_tv_eara_tableName = "vbox_tv_eara"

  val vbox_tv_eara_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            power_on_day                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            power_on_day, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(power_on_day)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.power_on_day = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'weekly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 week
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.week

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
      )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(power_on_day)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.power_on_day = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'monthly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 month
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.month

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_daily_hql =
    """

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            power_on_day                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            power_on_day, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            power_on_day                           AS t_today,
            'daily'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
          GROUP BY
            power_on_day
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            '智能电视开关机'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            'OTT'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  val vbox_tv_eara_weekly_hql =
    """


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(power_on_day)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.power_on_day = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(power_on_day)                      AS t_today,
            'weekly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              power_on_day = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '直播'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '直播'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_monthly_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(power_on_day)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.power_on_day = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(power_on_day)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              power_on_day = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(power_on_day)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              power_on_day = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'monthly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  //TV总览-排行榜
  val vbox_tv_ranking_tableName = "vbox_tv_ranking"

  val vbox_tv_ranking_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all


      	SELECT
        alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                            onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            SUM(fact_duration)                        AS t_tcnt,
            SUM(fact_cnt)                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct
                         appname,
                         packagename
                       FROM hr.apkinfo
                        WHERE onelevel = '视频'
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
      	   group by dim_date,apkinfo2.appname
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)

      	union all


      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)


      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      	SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'weekly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 week
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.week

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)

      	 union all

      	 SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      		SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'monthly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 month
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.month

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_daily_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all

      SELECT
        Distinct alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all


      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                         Distinct onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                           Distinct onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      union all


      SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct  onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             Distinct dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            fact_duration                        AS t_tcnt,
            fact_cnt                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct  onelevel,
                         appname,
                         packagename
                       FROM hr.apkinfo
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
          WHERE apkinfo2.onelevel = '视频'
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)
    """.stripMargin


  val vbox_tv_ranking_weekly_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'weekly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.week

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_monthly_hql =
    """

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                         dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'monthly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.month

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)

    """.stripMargin
}
package com.avcdata.spark.job.total

object Sql20170601 {

  //////////////////////////////////////////HiveSQL次数和时长推总//////////////////////////////////////////////////////////

  //////////////////////////////////开关机//////////////////////////////////////////////////////////////////
  val tracker_total_terminal_active_fact_HtableName = "tracker_total_terminal_active_fact"

  //TODO 4月
//  def getTracker_total_terminal_active_fact_HQL(analysisDate: String): String = {
//    """
//      SELECT
//        tof2.key,
//        tof2.sn,
//        tof2.date,
//        tof2.power_on_time,
//        tof2.cnt * ratio.ratio    as  cnt,
//        tof2.power_on_length * ratio.ratio as  power_on_length
//      FROM
//        (
//          SELECT
//            tof.key,
//            tof.sn,
//            tof.date,
//            tof.power_on_time,
//            tof.cnt,
//            tof.power_on_length,
//            terminal.brand,
//            terminal.province
//          FROM (select key,sn,date,power_on_time,cnt,power_on_length from hr.tracker_oc_fact_partition where date = '""" + analysisDate +
//      """') tof
//              JOIN (select sn,brand as brand,province from hr.sample_terminal_three) terminal
//                ON (tof.sn = terminal.sn)
//          ) tof2
//          JOIN
//          (select * from hr.tracker_total_dim_ratio_oc_fst where date = date_sub('""" + analysisDate + """',1)) ratio
//            ON (tof2.brand = ratio.brand AND tof2.province = ratio.province)
//      """.stripMargin
//  }


  //TODO 6月1号
  def getTracker_total_terminal_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT
        tof2.key,
        tof2.sn,
        tof2.date,
        tof2.power_on_time,
        tof2.cnt * ratio.ratio    as  cnt,
        tof2.power_on_length * ratio.ratio as  power_on_length
      FROM
        (
          SELECT
            tof.key,
            tof.sn,
            tof.date,
            tof.power_on_time,
            tof.cnt,
            tof.power_on_length,
            terminal.brand,
            terminal.province
          FROM (select key,sn,date,power_on_time,cnt,power_on_length from hr.tracker_oc_fact_partition where date = '""" + analysisDate +
      """') tof
              JOIN (select sn,brand as brand,province from hr.sample_terminal_three) terminal
                ON (tof.sn = terminal.sn)
          ) tof2
          JOIN
          (select * from hr.tracker_total_dim_ratio_oc_fst where date = '2017-06-01') ratio
            ON (tof2.brand = ratio.brand AND tof2.province = ratio.province)
                                                                                                       """.stripMargin
  }

  ///////////////////////////////直播////////////////////////////////////////////////////////////////////
  val tracker_total_live_active_fact_HtableName = "tracker_total_live_active_fact"

  //TODO 6月
  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT tlf2.key,
             tlf2.dim_sn,
             tlf2.dim_channel,
             tlf2.date,
             tlf2.dim_hour,
             tlf2.dim_min,
             tlf2.fact_cnt * sr_cache.ratio as fact_cnt ,
             tlf2.fact_time_length * sr_cache.ratio as fact_time_length
        FROM
        (SELECT KEY, dim_sn, dim_channel, date, dim_hour, dim_min, fact_cnt, fact_time_length
         FROM hr.tracker_live_fact_partition
         WHERE date = '""" + analysisDate +
      """' ) tlf2
        JOIN sr_cache ON (tlf2.dim_sn = sr_cache.sn)
      """.stripMargin
  }


  //  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
  //    """
  //      SELECT
  //        tlf2.key,
  //        tlf2.dim_sn,
  //        tlf2.dim_channel,
  //        tlf2.date,
  //        tlf2.dim_hour,
  //        tlf2.dim_min,
  //        tlf2.fact_cnt * ratio.ratio      fact_cnt,
  //        tlf2.fact_time_length * ratio.ratio fact_time_length
  //      FROM
  //        (
  //          SELECT
  //            tlf.key,
  //            tlf.dim_sn,
  //            tlf.dim_channel,
  //            tlf.date,
  //            tlf.dim_hour,
  //            tlf.dim_min,
  //            tlf.fact_cnt,
  //            tlf.fact_time_length,
  //            terminal.brand,
  //            terminal.province
  //          FROM (select key,dim_sn,dim_channel,date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_live_fact_partition where date = '""" + analysisDate +
  //      """') tlf
  //              JOIN
  //                (select sn,brand as brand,province from hr.live_terminal) terminal
  //                ON (tlf.dim_sn = terminal.sn)
  //          ) tlf2
  //          JOIN
  //          hr.tracker_total_dim_ratio_fst ratio
  //            ON (tlf2.brand = ratio.brand AND tlf2.province = ratio.province)
  //      """.stripMargin
  //  }

  //APK
  val tracker_total_apk_active_fact_HtableName = "tracker_total_apk_active_fact"

//  val tracker_total_apk_active_fact_HtableName = "tracker_total_apk_fact_zyt"

  //TODO 4月
//  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
//    """
//          SELECT
//                    taf2.key,
//                    taf2.dim_sn,
//                    taf2.dim_apk,
//                    taf2.date,
//                    taf2.dim_hour,
//                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
//                    taf2.fact_duration * ratio.ratio as fact_duration
//                  FROM
//                    (
//                      SELECT
//                        taf.key,
//                        taf.dim_sn,
//                        taf.dim_apk,
//                        taf.date,
//                        taf.dim_hour,
//                        taf.fact_cnt,
//                        taf.fact_duration,
//                        terminal.brand,
//                        terminal.license,
//                        terminal.province
//                      FROM
//                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition ap
//                        JOIN (select * from hr.apkinfo where onelevel = '视频')   ai
//                        ON (ap.dim_apk = ai.packagename and ai.appname not in('银河·奇异果','腾讯视频TV端','CIBN环球影视'))
//                       where date = '""" + analysisDate + """') taf
//                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
//                    ON (taf.dim_sn = terminal.sn)
//              ) taf2
//              JOIN
//              ( select * from hr.tracker_total_dim_ratio_sec where date = date_sub('""" + analysisDate + """',1) )
//               ratio
//                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province )
//      """.stripMargin
//  }
//
//  def getTracker_total_apk_active_fact_vsttj_HQL(analysisDate: String): String = {
//    """
//          SELECT
//                    taf2.key,
//                    taf2.dim_sn,
//                    taf2.dim_apk,
//                    taf2.date,
//                    taf2.dim_hour,
//                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
//                    taf2.fact_duration * ratio.ratio as fact_duration
//                  FROM
//                    (
//                      SELECT
//                        taf.key,
//                        taf.dim_sn,
//                        taf.dim_apk,
//                        taf.date,
//                        taf.dim_hour,
//                        taf.fact_cnt,
//                        taf.fact_duration,
//                        terminal.brand,
//                        terminal.license,
//                        terminal.province
//                      FROM
//                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr
//                       .tracker_apk_fact_partition ap
//                       JOIN (select * from hr.apkinfo where onelevel = '视频') ai
//                       ON (ap.dim_apk = ai.packagename and ai.appname in('银河·奇异果','腾讯视频TV端','CIBN环球影视'))
//                       where date = '""" + analysisDate +
//      """') taf
//                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
//                    ON (taf.dim_sn = terminal.sn)
//              ) taf2
//              JOIN
//               ( select * from hr.tracker_total_dim_ratio_sec2 where date = date_sub('""" + analysisDate + """',1) ) ratio
//                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
//      """.stripMargin
//  }


  //TODO 6月1号
  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
    """
          SELECT
                    taf2.key,
                    taf2.dim_sn,
                    taf2.dim_apk,
                    taf2.date,
                    taf2.dim_hour,
                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
                    taf2.fact_duration * ratio.ratio as fact_duration
                  FROM
                    (
                      SELECT
                        taf.key,
                        taf.dim_sn,
                        taf.dim_apk,
                        taf.date,
                        taf.dim_hour,
                        taf.fact_cnt,
                        taf.fact_duration,
                        terminal.brand,
                        terminal.license,
                        terminal.province
                      FROM
                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition ap
                        JOIN (select * from hr.apkinfo where onelevel = '视频')   ai
                        ON (ap.dim_apk = ai.packagename and ai.appname not in('银河·奇异果','腾讯视频TV端','CIBN环球影视'))
                       where date = '""" + analysisDate + """') taf
                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
                    ON (taf.dim_sn = terminal.sn)
              ) taf2
              JOIN
              ( select * from hr.tracker_total_dim_ratio_sec where date = '2017-06-01')  ratio
                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province )
      """.stripMargin
  }

  def getTracker_total_apk_active_fact_vsttj_HQL(analysisDate: String): String = {
    """
          SELECT
                    taf2.key,
                    taf2.dim_sn,
                    taf2.dim_apk,
                    taf2.date,
                    taf2.dim_hour,
                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
                    taf2.fact_duration * ratio.ratio as fact_duration
                  FROM
                    (
                      SELECT
                        taf.key,
                        taf.dim_sn,
                        taf.dim_apk,
                        taf.date,
                        taf.dim_hour,
                        taf.fact_cnt,
                        taf.fact_duration,
                        terminal.brand,
                        terminal.license,
                        terminal.province
                      FROM
                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr
                       .tracker_apk_fact_partition ap
                       JOIN (select * from hr.apkinfo where onelevel = '视频') ai
                       ON (ap.dim_apk = ai.packagename and ai.appname in('银河·奇异果','腾讯视频TV端','CIBN环球影视'))
                       where date = '""" + analysisDate +
      """') taf
                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
                    ON (taf.dim_sn = terminal.sn)
              ) taf2
              JOIN
               ( select * from hr.tracker_total_dim_ratio_sec2 where date = '2017-06-01' ) ratio
                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
      """.stripMargin
  }

  //TODO 截止到3月
//  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
//    """
//          SELECT
//                    taf2.key,
//                    taf2.dim_sn,
//                    taf2.dim_apk,
//                    taf2.date,
//                    taf2.dim_hour,
//                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
//                    taf2.fact_duration * ratio.ratio as fact_duration
//                  FROM
//                    (
//                      SELECT
//                        taf.key,
//                        taf.dim_sn,
//                        taf.dim_apk,
//                        taf.date,
//                        taf.dim_hour,
//                        taf.fact_cnt,
//                        taf.fact_duration,
//                        terminal.brand,
//                        terminal.license,
//                        terminal.province
//                      FROM
//                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition ap
//                        JOIN (select * from hr.apkinfo where onelevel = '视频')   ai
//                        ON (ap.dim_apk = ai.packagename and ai.appname not in('CIBN微视听','云视听·泰捷'))
//                       where date = '""" + analysisDate + """') taf
//                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
//                    ON (taf.dim_sn = terminal.sn)
//              ) taf2
//              JOIN
//              hr.tracker_total_dim_ratio_sec ratio
//                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
//      """.stripMargin
//  }
//
//  def getTracker_total_apk_active_fact_vsttj_HQL(analysisDate: String): String = {
//    """
//          SELECT
//                    taf2.key,
//                    taf2.dim_sn,
//                    taf2.dim_apk,
//                    taf2.date,
//                    taf2.dim_hour,
//                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
//                    taf2.fact_duration * ratio.ratio as fact_duration
//                  FROM
//                    (
//                      SELECT
//                        taf.key,
//                        taf.dim_sn,
//                        taf.dim_apk,
//                        taf.date,
//                        taf.dim_hour,
//                        taf.fact_cnt,
//                        taf.fact_duration,
//                        terminal.brand,
//                        terminal.license,
//                        terminal.province
//                      FROM
//                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr
//                       .tracker_apk_fact_partition ap
//                       JOIN (select * from hr.apkinfo where onelevel = '视频') ai
//                       ON (ap.dim_apk = ai.packagename and ai.appname in('CIBN微视听','云视听·泰捷'))
//                       where date = '""" + analysisDate +
//      """') taf
//                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
//                    ON (taf.dim_sn = terminal.sn)
//              ) taf2
//              JOIN
//              hr.tracker_total_dim_ratio_vsttj_sec ratio
//                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
//      """.stripMargin
//  }


  ///////////////////////////////////////////HiveSQL TO  Mysql /////////////////////////////////////////////////////


  //TV总览--地区分布
  val vbox_tv_eara_tableName = "vbox_tv_eara"

  val vbox_tv_eara_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'weekly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 week
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.week

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
      )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'monthly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 month
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.month

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_daily_hql =
    """

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
          GROUP BY
            date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            '智能电视开关机'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            'OTT'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  val vbox_tv_eara_weekly_hql =
    """


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '直播'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '直播'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_monthly_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'monthly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  //TV总览-排行榜
  val vbox_tv_ranking_tableName = "vbox_tv_ranking"

  val vbox_tv_ranking_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all


      	SELECT
        alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                            onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            SUM(fact_duration)                        AS t_tcnt,
            SUM(fact_cnt)                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct
                         appname,
                         packagename
                       FROM hr.apkinfo
                        WHERE onelevel = '视频'
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
      	   group by dim_date,apkinfo2.appname
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)

      	union all


      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)


      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      	SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'weekly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 week
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.week

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)

      	 union all

      	 SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      		SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'monthly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 month
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.month

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_daily_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all

      SELECT
        Distinct alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all


      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                         Distinct onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                           Distinct onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      union all


      SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct  onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             Distinct dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            fact_duration                        AS t_tcnt,
            fact_cnt                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct  onelevel,
                         appname,
                         packagename
                       FROM hr.apkinfo
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
          WHERE apkinfo2.onelevel = '视频'
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)
    """.stripMargin


  val vbox_tv_ranking_weekly_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'weekly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.week

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_monthly_hql =
    """

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                         dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'monthly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.month

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)

    """.stripMargin
}
package com.avcdata.etl.launcher.jd

import com.databricks.spark.csv._
import org.apache.spark.sql.hive.HiveContext
import org.slf4j.LoggerFactory

/**
  * 使用SQL的方式处理数据
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/21 13:36
  */
object SqlMannerMatcher
{
  val logger = LoggerFactory.getLogger(SqlMannerMatcher.getClass)

  /**
    * 使用SQL方式处理京东数据
    *
    * @param configCategory 配置字符串
    * @param hiveContext    HiveContext
    */
  def handle(configCategory: String, dataRootPath: String, hiveContext: HiveContext): Unit =
  {
    logger.info(s"Start to handle -> '$configCategory', the data root path -> '$dataRootPath'")

    try
    {
      configCategory.split("\\,").map(_.split("\\:").filter(_ != "")) match
      {
        case Array(mainMatchFields, secondMatchFields, fileNames) => doHandle(mainMatchFields, secondMatchFields, fileNames(0), dataRootPath, hiveContext)
        case _ => throw new IllegalArgumentException(s"Config $configCategory content is error")
      }
    }
    catch
    {
      case ex: Throwable => logger.error(s"Handle '$configCategory' config failed", ex)
        throw ex
    }

    logger.info(s"End to handle -> '$configCategory'")
  }

  /**
    * 处理单文件数据
    *
    * @param mainMatchFields   主匹配字段名
    * @param secondMatchFields 第二匹配字段名
    * @param fileName          文件名
    * @param dataRootPath      文件根目录路径
    * @param hiveContext       HiveContext
    */
  private def doHandle(mainMatchFields: Array[String], secondMatchFields: Array[String], fileName: String, dataRootPath: String, hiveContext: HiveContext): Unit =
  {
    //获取待处理文件的全路径
    val fileFullPath = (if (dataRootPath.endsWith("/")) dataRootPath else dataRootPath + "/") + fileName + ".csv"

    logger.info(s"Begin to handle file -> $fileFullPath")

    //加载数据文件并注册临时表
    val csvFilterData = hiveContext.csvFile(fileFullPath)//.dropDuplicates(Array("url"))

    val queryTableName = "tmp_tbl_" + fileName
    csvFilterData.registerTempTable(queryTableName)

    val dbName = "jd_etl_last"

    //执行SQL并将SQL结果导入至HIVE中
    hiveContext.sql(s"create database if not exists `$dbName`")
    hiveContext.sql(s"drop table if exists `$dbName`.`$fileName`")

    //获取查询SQL
    val srcFieldNames = csvFilterData.schema.fieldNames.map(fieldName => s"`$fieldName`").mkString(", ")
    val matchFieldNames = (mainMatchFields ++ secondMatchFields).map(fieldName => s"`$fieldName`").mkString(", ")
    val querySql =
      s"""create table `$dbName`.`$fileName`
          |as
          |select $srcFieldNames, brand as avcbrand, brandid as avcbrandid, concat('aowei', minSkuId) as avcid
          |from
          |(
          | select $srcFieldNames, min(skuId) over (distribute by $matchFieldNames) as minSkuId
          | from $queryTableName
          | where skuId <> 'def'
          |) as tmp
          |""".stripMargin

    hiveContext.sql(querySql)
  }
}
package com.avcdata.etl.launcher.util

import org.apache.commons.cli.CommandLine
import org.apache.spark.SparkConf

/**
  * SQL查询参数处理
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 下午1:14
  */
object SQLParamUtil
{
  /**
    * 合并系统默认参数和命令行参数,命令行参数将会覆盖系统同名参数
    *
    * @param cl   命令行对象
    * @param conf 系统配置对象
    * @return 两者合并后的参数映射
    */
  def getCliMergedParams(cl: CommandLine, conf: SparkConf): Map[String, String] =
  {
    //集合隐式转换
    import scala.collection.JavaConversions._

    //获取系统默认参数
    val confParams = conf.getAll.toMap

    //获取配置文件的参数
    val exportConfigParams = CliParser.readParamsFromExportConfig(cl)

    //获取命令行参数
    val cliParams = cl.getOptionProperties("sql-param").toMap

    //获取合并后的参数
    val mergedParams = confParams ++ exportConfigParams ++ cliParams

    mergedParams
  }
}
package com.avcdata.spark.job.test

import com.avcdata.spark.job.common.Helper
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext

object SQLTest {

  def main(args: Array[String]): Unit = {
    val sc = Helper.sparkContext
    run(sc)
    sc.stop()
  }


  /**
    * 名称	Type	备注
    * @param sc
    */

  def run(sc: SparkContext) = {

    val log = Logger.getLogger(getClass.getName)
    //    读取hive数据
    // 创建HiveContext 这里接收的是sparkContext作为参数
    val hiveContext: HiveContext = new HiveContext(sc)

    hiveContext.sql("set hbase.client.scanner.caching=100000")
    hiveContext.sql("set mapred.map.tasks.speculative.execution = false")
    hiveContext.sql("set mapred.reduce.tasks.speculative.execution = false")

    val sql = "select original_name,model,id,year,crowd,region from hr.film_properties"
    val df: DataFrame = hiveContext.sql(sql)

//    df.foreach(println(_))

    val totalRDD = df.rdd

    for (elem <- totalRDD.collect) {
      println(elem)
    }


    //输出到单个文件
    //    totalRDD.repartition(1).saveAsTextFile("test/test.txt")
    //    totalRDD.coalesce(1).saveAsTextFile("test/test.txt")

    //    totalRDD.saveAsTextFile("/test/qs.txt")
    //    df.printSchema()

    //    df.collect.foreach(println)
    //    df.rdd.foreach(println(_))
//    df.write.mode(SaveMode.Append).save("/test/qs.txt")
    //    df.write.mode(SaveMode.Overwrite).text("/test")
    //    df.save("/test/qs.txt", "json", SaveMode.Overwrite);
    //    totalRDD.foreach(println(_))

  }


}package com.avcdata.spark.job.total

object Sql_to3month {

  //////////////////////////////////////////HiveSQL次数和时长推总//////////////////////////////////////////////////////////

  //开关机
  val tracker_total_terminal_active_fact_HtableName = "tracker_total_terminal_active_fact"

  def getTracker_total_terminal_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT
        tof2.key,
        tof2.sn,
        tof2.date,
        tof2.power_on_time,
        tof2.cnt * ratio.ratio    as  cnt,
        tof2.power_on_length * ratio.ratio as  power_on_length
      FROM
        (
          SELECT
            tof.key,
            tof.sn,
            tof.date,
            tof.power_on_time,
            tof.cnt,
            tof.power_on_length,
            terminal.brand,
            terminal.province
          FROM (select key,sn,date,power_on_time,cnt,power_on_length from hr.tracker_oc_fact_partition where date = '""" + analysisDate +
      """') tof
              JOIN (select sn,brand as brand,province from hr.sample_terminal_three) terminal
                ON (tof.sn = terminal.sn)
          ) tof2
          JOIN
          hr.tracker_total_dim_ratio_oc_fst ratio
            ON (tof2.brand = ratio.brand AND tof2.province = ratio.province)
      """.stripMargin
  }


  //直播
  val tracker_total_live_active_fact_HtableName = "tracker_total_live_active_fact"

  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
    """
      SELECT tlf2.key,
             tlf2.dim_sn,
             tlf2.dim_channel,
             tlf2.date,
             tlf2.dim_hour,
             tlf2.dim_min,
             tlf2.fact_cnt * sr_cache.ratio as fact_cnt ,
             tlf2.fact_time_length * sr_cache.ratio as fact_time_length
        FROM
        (SELECT KEY, dim_sn, dim_channel, date, dim_hour, dim_min, fact_cnt, fact_time_length
         FROM hr.tracker_live_fact_partition
         WHERE date = '""" + analysisDate +
      """' ) tlf2
        JOIN sr_cache ON (tlf2.dim_sn = sr_cache.sn)
      """.stripMargin
  }

  //  def getTracker_total_live_active_fact_HQL(analysisDate: String): String = {
  //    """
  //      SELECT
  //        tlf2.key,
  //        tlf2.dim_sn,
  //        tlf2.dim_channel,
  //        tlf2.date,
  //        tlf2.dim_hour,
  //        tlf2.dim_min,
  //        tlf2.fact_cnt * ratio.ratio      fact_cnt,
  //        tlf2.fact_time_length * ratio.ratio fact_time_length
  //      FROM
  //        (
  //          SELECT
  //            tlf.key,
  //            tlf.dim_sn,
  //            tlf.dim_channel,
  //            tlf.date,
  //            tlf.dim_hour,
  //            tlf.dim_min,
  //            tlf.fact_cnt,
  //            tlf.fact_time_length,
  //            terminal.brand,
  //            terminal.province
  //          FROM (select key,dim_sn,dim_channel,date,dim_hour,dim_min,fact_cnt,fact_time_length from hr.tracker_live_fact_partition where date = '""" + analysisDate +
  //      """') tlf
  //              JOIN
  //                (select sn,brand as brand,province from hr.live_terminal) terminal
  //                ON (tlf.dim_sn = terminal.sn)
  //          ) tlf2
  //          JOIN
  //          hr.tracker_total_dim_ratio_fst ratio
  //            ON (tlf2.brand = ratio.brand AND tlf2.province = ratio.province)
  //      """.stripMargin
  //  }

  //APK
  val tracker_total_apk_active_fact_HtableName = "tracker_total_apk_active_fact"


  def getTracker_total_apk_active_fact_HQL(analysisDate: String): String = {
    """
          SELECT
                    taf2.key,
                    taf2.dim_sn,
                    taf2.dim_apk,
                    taf2.date,
                    taf2.dim_hour,
                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
                    taf2.fact_duration * ratio.ratio as fact_duration
                  FROM
                    (
                      SELECT
                        taf.key,
                        taf.dim_sn,
                        taf.dim_apk,
                        taf.date,
                        taf.dim_hour,
                        taf.fact_cnt,
                        taf.fact_duration,
                        terminal.brand,
                        terminal.license,
                        terminal.province
                      FROM
                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr.tracker_apk_fact_partition ap
                        JOIN (select * from hr.apkinfo where onelevel = '视频')   ai
                        ON (ap.dim_apk = ai.packagename and ai.appname not in('CIBN微视听','云视听·泰捷'))
                       where date = '""" + analysisDate + """') taf
                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
                    ON (taf.dim_sn = terminal.sn)
              ) taf2
              JOIN
              hr.tracker_total_dim_ratio_sec ratio
                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
      """.stripMargin
  }

  def getTracker_total_apk_active_fact_vsttj_HQL(analysisDate: String): String = {
    """
          SELECT
                    taf2.key,
                    taf2.dim_sn,
                    taf2.dim_apk,
                    taf2.date,
                    taf2.dim_hour,
                    taf2.fact_cnt * ratio.ratio    as  fact_cnt,
                    taf2.fact_duration * ratio.ratio as fact_duration
                  FROM
                    (
                      SELECT
                        taf.key,
                        taf.dim_sn,
                        taf.dim_apk,
                        taf.date,
                        taf.dim_hour,
                        taf.fact_cnt,
                        taf.fact_duration,
                        terminal.brand,
                        terminal.license,
                        terminal.province
                      FROM
                       (select key,dim_sn,dim_apk,date,dim_hour,fact_cnt,fact_duration from hr
                       .tracker_apk_fact_partition ap
                       JOIN (select * from hr.apkinfo where onelevel = '视频') ai
                       ON (ap.dim_apk = ai.packagename and ai.appname in('CIBN微视听','云视听·泰捷'))
                       where date = '""" + analysisDate +
      """') taf
                  JOIN (select distinct sn,brand as brand,license,province from hr.sample_terminal_three) terminal
                    ON (taf.dim_sn = terminal.sn)
              ) taf2
              JOIN
              hr.tracker_total_dim_ratio_vsttj_sec ratio
                ON (taf2.brand = ratio.brand AND taf2.license = ratio.license AND taf2.province = ratio.province)
      """.stripMargin
  }


  ///////////////////////////////////////////HiveSQL TO  Mysql /////////////////////////////////////////////////////


  //TV总览--地区分布
  val vbox_tv_eara_tableName = "vbox_tv_eara"

  val vbox_tv_eara_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'weekly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 week
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.week

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
      )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                inner JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            inner JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt/3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt/3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
            SELECT
        MIN(dim_date)                      AS t_today,
        'monthly'                           AS t_datetype,
        terminal.province  AS t_province,
        'OTT'                               AS t_name,
        SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
        SUM(COALESCE(fact_duration, 0)) AS t_tcnt
      FROM
        (select dim_sn,dim_date, SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact
         group by dim_sn,dim_date

      ) octime

      INNER JOIN
        (
        SELECT
      	 today,
      	 YEAR,
      	 month
         FROM hr.dateinfo) dinfo
      	ON (
      	octime.dim_date = dinfo.today
      	)

      INNER JOIN
        (SELECT
      	 sn,
      	 province
         FROM
      	 hr.terminal
         GROUP BY sn, province
        ) terminal
      	ON (octime.dim_sn = terminal.sn)

      GROUP BY
         terminal.province,dinfo.year, dinfo.month

        ) alltime

        JOIN
        (
         SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_daily_hql =
    """

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
          '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
          ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient

      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            terminal.province                      AS t_province,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact octime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            date                           AS t_today,
            'daily'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
          GROUP BY
            date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            '直播'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                  AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            '智能电视开关机'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
       'OTT端'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                           AS t_today,
            'daily'                            AS t_datetype,
            terminal.province                  AS t_province,
            'OTT'                               AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact livetime
            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (livetime.dim_sn = terminal.sn)
          GROUP BY
            dim_date, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            dim_date                       AS t_today,
            'daily'                            AS t_datetype,
            'OTT'                          AS t_name,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
          GROUP BY
            dim_date
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'daily'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'daily'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  val vbox_tv_eara_weekly_hql =
    """


      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'weekly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '直播端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            '直播'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
          FROM
            hr.tracker_total_live_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            '直播'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'weekly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'weekly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.week

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                       AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'weekly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.week
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'weekly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'weekly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin

  val vbox_tv_eara_monthly_hql =
    """
      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '智能电视开关机'          AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                sn,
                MIN(date)                      AS t_today,
                'monthly'                               AS t_datetype,
                '智能电视开关机'                              AS t_name,
                SUM(COALESCE(cnt, 0))                  AS t_ucnt,
                SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
              FROM
                hr.tracker_total_oc_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.date = dinfo.today
                  )
              GROUP BY sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
        '智能电视开机'                   AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            '直播'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                '直播'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt
              FROM
                hr.tracker_total_live_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            '直播'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '直播' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )


      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         '直播端'                     AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(date)                      AS t_today,
            'monthly'                                AS t_datetype,
            '智能电视开关机'                              AS t_name,
            SUM(COALESCE(cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(power_on_length * 60, 0)) AS t_tcnt
          FROM
            hr.tracker_total_oc_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            '智能电视开关机'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = '智能电视开关机' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        alltime.t_province              AS t_province,
         'OTT端'                      AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM
        (
          SELECT
            octime.t_today     AS t_today,
            'monthly'           AS t_datetype,
            terminal.province  AS t_province,
            'OTT'               AS t_name,
            SUM(octime.t_ucnt) AS t_ucnt,
            SUM(octime.t_tcnt) AS t_tcnt
          FROM
            (
              SELECT
                dim_sn,
                MIN(dim_date)                      AS t_today,
                'monthly'                           AS t_datetype,
                'OTT'                               AS t_name,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt,
                SUM(COALESCE(fact_duration, 0)) AS t_tcnt
              FROM
                hr.tracker_total_apk_fact octime
                LEFT JOIN
                (SELECT
                   today,
                   YEAR,
                   month
                 FROM hr.dateinfo) dinfo
                  ON (
                  octime.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dinfo.year, dinfo.month

            ) octime

            LEFT JOIN
            (SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )
            terminal
              ON (octime.dim_sn = terminal.sn)
          GROUP BY
            t_today, terminal.province
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'          AS t_datetype,
            province          AS t_province,
            'OTT'              AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date, province
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_province = allterminal.t_province AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )

      union all

      SELECT
        alltime.t_today                 AS t_today,
        alltime.t_datetype              AS t_datetype,
        '未查询'                           AS t_eara,
        '全国'                            AS t_province,
         'OTT端'                    AS t_name,
        allterminal.t_acnt,
        alltime.t_ucnt,
        ROUND(alltime.t_tcnt / 3600, 2) AS t_tcnt,
         ROUND(alltime.t_tcnt / 3600/allterminal.t_acnt, 2)       AS t_tcntavgclient
      FROM

        (
          SELECT
            MIN(dim_date)                      AS t_today,
            'monthly'                                AS t_datetype,
            'OTT'                              AS t_name,
            SUM(COALESCE(fact_cnt, 0))                  AS t_ucnt,
            SUM(COALESCE(fact_duration, 0)) AS t_tcnt
          FROM
            hr.tracker_total_apk_fact
            LEFT JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              dim_date = dinfo.today
              )
          GROUP BY dinfo.year, dinfo.month
        ) alltime

        JOIN
        (
          SELECT
            tv_date           AS t_today,
            'monthly'           AS t_datetype,
            'OTT'         AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM
            hr.tracker_total_tv_overview
          WHERE behavior_type = 'OTT' AND period = 'monthly'
          GROUP BY tv_date
        ) allterminal
          ON (
          alltime.t_today = allterminal.t_today AND
          alltime.t_name = allterminal.t_name AND
          alltime.t_datetype = allterminal.t_datetype
          )
    """.stripMargin


  //TV总览-排行榜
  val vbox_tv_ranking_tableName = "vbox_tv_ranking"

  val vbox_tv_ranking_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all


      	SELECT
        alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                            onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            SUM(fact_duration)                        AS t_tcnt,
            SUM(fact_cnt)                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct
                         appname,
                         packagename
                       FROM hr.apkinfo
                        WHERE onelevel = '视频'
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
      	   group by dim_date,apkinfo2.appname
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)

      	union all


      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)


      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      	SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'weekly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 week
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.week

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)

      	 union all

      	 SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      	union all

      	SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)


      	union all

      		SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      	union all

      	SELECT
        Distinct
        alltimeapk.t_today,
        'monthly' AS t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
      		SELECT
      		MIN(dim_date) AS t_today,
      		COALESCE(apkinfo2.appname, 'unknow') AS t_name,
      		SUM(fact_cnt) AS t_tcnt,
      		SUM(fact_duration) AS t_ucnt
      		FROM
      		(
      		select dim_sn,dim_apk,dim_date,SUM(fact_cnt) AS fact_cnt,SUM(fact_duration) AS fact_duration from  hr.tracker_total_apk_fact GROUP BY dim_sn,dim_apk,dim_date
      		)   apktime
      		INNER JOIN (SELECT
      				  Distinct
      					 appname,
      					 packagename
      				   FROM hr.apkinfo
      				   WHERE onelevel = '视频'
      		) apkinfo2
      		ON (apktime.dim_apk = apkinfo2.packagename)
      		INNER JOIN
      		(SELECT
      		 DISTINCT today,
      		 YEAR,
      		 month
      		FROM dateinfo) dinfo
      		ON (
      		apktime.dim_date = dinfo.today
      		)
      		GROUP BY apkinfo2.appname,dim_date, dinfo.year, dinfo.month

         ) alltimeapk

          ON
            (tttl.t_name = alltimeapk.t_name  AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_daily_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            period       AS t_datetype,
            province     AS t_eara,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
          GROUP BY tv_date,channel,province,period
        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               dim_channel      AS t_name,
               fact_time_length ,
               fact_cnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,t_name,terminal.province

        ) alltimelive

          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND  tttl.t_datetype = alltimelive.t_datetype)


      union all

      SELECT
        Distinct alltimelive.t_today,
        'daily' t_datetype,
        '全国' AS t_eara,
        alltimelive.t_name,
        tttl.t_acnt,
        ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt
      FROM
        (
          SELECT
            tv_date      AS t_today,
            channel      AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'daily' AND channel <> '其他'
      	GROUP BY tv_date,period,channel
        ) tttl
        JOIN
        (
            SELECT
               dim_date         AS t_today,
               dim_channel      AS t_name,
               SUM(COALESCE(fact_time_length, 0)) as t_tcnt ,
               SUM(COALESCE(fact_cnt, 0)) as t_ucnt
             FROM hr.tracker_total_live_fact
             WHERE dim_channel <> '其他'
          GROUP BY dim_date,dim_channel
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name)

      union all


      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara,
        alltimeapk.t_name,
        tttl.t_acnt,
        ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt
      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                         Distinct onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)

        ) tttl
        JOIN
        (
          SELECT
            t_today,
            'daily' t_datetype,
            terminal.province AS t_eara,
            t_name,
            SUM(COALESCE(fact_duration, 0)) as t_tcnt ,
            SUM(COALESCE(fact_cnt, 0)) as t_ucnt
          FROM
            (SELECT
               dim_sn,
               dim_date         AS t_today,
               COALESCE(apkinfo2.appname, 'unknow') AS t_name,
               fact_duration ,
               fact_cnt
             FROM hr.tracker_total_apk_fact apktime
               LEFT JOIN (SELECT
                           Distinct onelevel,
                            appname,
                            packagename
                          FROM hr.apkinfo
                         ) apkinfo2
                 ON (apktime.dim_apk = apkinfo2.packagename)
             WHERE apkinfo2.onelevel = '视频'
            )
            alltime
            JOIN
            (
             SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            )   terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_today,terminal.province,t_name

        ) alltimeapk

          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND  tttl.t_datetype = alltimeapk.t_datetype)

      union all


      SELECT
       Distinct alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
         ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt
      FROM
        (
          SELECT
            Distinct t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'daily'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                        Distinct  onelevel,
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl
        JOIN
        (
          SELECT
             Distinct dim_date                             AS t_today,
            'daily'                              AS t_datetype,
            COALESCE(apkinfo2.appname, 'unknow') AS t_name,
            fact_duration                        AS t_tcnt,
            fact_cnt                             AS t_ucnt
          FROM hr.tracker_total_apk_fact apktime
            INNER JOIN (SELECT
                       Distinct  onelevel,
                         appname,
                         packagename
                       FROM hr.apkinfo
                      ) apkinfo2
              ON (apktime.dim_apk = apkinfo2.packagename)
          WHERE apkinfo2.onelevel = '视频'
        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND
             tttl.t_today = alltimeapk.t_today)
    """.stripMargin


  val vbox_tv_ranking_weekly_hql =
    """
      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'weekly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   week
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.week
            ) ttlf
            JOIN
            (
              SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'weekly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'weekly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.week

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'weekly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                          dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      week
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.week
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'weekly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'weekly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               week
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.week

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)
    """.stripMargin

  val vbox_tv_ranking_monthly_hql =
    """

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        tttl.t_eara       AS t_eara,
        alltimelive.t_name,
           ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            period            AS t_datetype,
            province          AS t_eara,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, channel, province, period
        ) tttl

        JOIN
        (
          SELECT
            ttlf.t_today,
            'monthly'          AS t_datetype,
            ttlf.t_name,
            SUM(ttlf.t_tcnt)  AS t_tcnt,
            SUM(ttlf.t_ucnt)  AS t_ucnt,
            terminal.province AS t_eara
          FROM
            (
              SELECT
                dim_sn,
                MIN(dinfo.today)                   AS t_today,
                dim_channel                        AS t_name,
                SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
                SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
              FROM (SELECT
                      dim_sn,
                      dim_date,
                      dim_channel,
                      fact_cnt,
                      fact_time_length
                    FROM hr.tracker_total_live_fact
                    WHERE dim_channel <> '其他'
                   ) ttlf
                JOIN
                (SELECT
                   today,
                   YEAR,
                   MONTH
                 FROM hr.dateinfo) dinfo
                  ON (
                  ttlf.dim_date = dinfo.today
                  )
              GROUP BY dim_sn, dim_channel, dinfo.year, dinfo.month
            ) ttlf
            JOIN
            (
           SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (ttlf.dim_sn = terminal.sn)
          GROUP BY t_today, t_name, terminal.province
        ) alltimelive
          ON
            (tttl.t_today = alltimelive.t_today AND tttl.t_name = alltimelive.t_name AND tttl.t_eara = alltimelive.t_eara
             AND tttl.t_datetype = alltimelive.t_datetype)

      union all

      SELECT
        alltimelive.t_today,
        alltimelive.t_datetype,
        '全国'        AS t_eara,
        alltimelive.t_name,
          ROUND(alltimelive.t_tcnt/3600,2) AS t_tcnt,
        alltimelive.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            tv_date           AS t_today,
            channel           AS t_name,
            SUM(terminal_cnt) AS t_acnt
          FROM hr.tracker_total_tv_live
          WHERE period = 'monthly' AND channel <> '其他'
          GROUP BY tv_date, period, channel
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                   AS t_today,
            'monthly'                           AS t_datetype,
            dim_channel                        AS t_name,
            SUM(COALESCE(fact_time_length, 0)) AS t_tcnt,
            SUM(COALESCE(fact_cnt, 0))         AS t_ucnt
          FROM (SELECT
                  dim_date,
                  dim_channel,
                  fact_cnt,
                  fact_time_length
                FROM hr.tracker_total_live_fact
                WHERE dim_channel <> '其他'
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM hr.dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_channel, dinfo.year, dinfo.month

        ) alltimelive
          ON
            (tttl.t_name = alltimelive.t_name AND
             tttl.t_today = alltimelive.t_today)

      union all

      SELECT
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        tttl.t_eara AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_eara,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              province          AS t_eara,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, province, period
            )
            INNER JOIN (SELECT
                          DISTINCT appname,
                          onelevel
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (
          SELECT
            alltime.t_today,
            'monthly'            AS t_datetype,
            terminal.province   AS t_eara,
            alltime.t_name,
            SUM(alltime.t_tcnt) AS t_tcnt,
            SUM(alltime.t_ucnt) AS t_ucnt
          FROM (
                 SELECT
                   dim_sn,
                   MIN(dinfo.today)                AS t_today,
                   dim_apk                         AS t_name,
                   SUM(COALESCE(fact_duration, 0)) AS t_tcnt,
                   SUM(COALESCE(fact_cnt, 0))      AS t_ucnt
                 FROM (SELECT
                         dim_sn,
                         dim_date,
                         COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                         fact_cnt,
                         fact_duration
                       FROM hr.tracker_total_apk_fact apktime
                         LEFT JOIN (SELECT
                                      DISTINCT   packagename,
                                      appname
                                    FROM hr.apkinfo
                                    WHERE onelevel = '视频'
                                   ) apkinfo2
                           ON (apktime.dim_apk = apkinfo2.packagename)

                      ) ttlf
                   JOIN
                   (SELECT
                      today,
                      YEAR,
                      month
                    FROM hr.dateinfo) dinfo
                     ON (
                     ttlf.dim_date = dinfo.today
                     )
                 GROUP BY dim_sn, dim_apk, dinfo.year, dinfo.month
               ) alltime
            JOIN
            (
            SELECT
               sn,
               province
             FROM
               hr.terminal
             GROUP BY sn, province
            ) terminal
              ON
                (alltime.dim_sn = terminal.sn)
          GROUP BY t_name, t_today, terminal.province


        ) alltimeapk
          ON
            (tttl.t_today = alltimeapk.t_today AND tttl.t_name = alltimeapk.t_name AND tttl.t_eara = alltimeapk.t_eara
             AND tttl.t_datetype = alltimeapk.t_datetype)

      union all

      SELECT
        Distinct
        alltimeapk.t_today,
        alltimeapk.t_datetype,
        '全国'        AS t_eara,
        alltimeapk.t_name,
          ROUND(alltimeapk.t_tcnt/3600,2) AS t_tcnt,
        alltimeapk.t_ucnt,
        tttl.t_acnt AS t_acnt

      FROM
        (
          SELECT
            t_today,
            t_datetype,
            t_name,
            t_acnt
          FROM (
            SELECT
              tv_date           AS t_today,
              period            AS t_datetype,
              apk               AS t_name,
              SUM(terminal_cnt) AS t_acnt
            FROM hr.tracker_total_tv_apk
            WHERE period = 'monthly'
            GROUP BY tv_date, apk, period
            )
            INNER JOIN (SELECT
                      Distinct
                          appname
                        FROM hr.apkinfo
                        WHERE onelevel = '视频'
                       ) apkinfo2
              ON (t_name = apkinfo2.appname)
        ) tttl

        JOIN
        (

          SELECT
            MIN(dinfo.today)                AS t_today,
            'monthly'                        AS t_datetype,
            dim_apk                         AS t_name,
            SUM(COALESCE(t_tcnt, 0)) AS t_tcnt,
            SUM(COALESCE(t_ucnt, 0))      AS t_ucnt
          FROM (SELECT
                  dim_date,
                  COALESCE(apkinfo2.appname, 'unknow') dim_apk,
                  SUM(fact_cnt) AS t_tcnt,
                  SUM(fact_duration) AS t_ucnt
                FROM tracker_total_apk_fact apktime
                  INNER JOIN (SELECT
                            Distinct   appname,
                               packagename
                             FROM hr.apkinfo
                             WHERE onelevel = '视频'
                            ) apkinfo2
                    ON (apktime.dim_apk = apkinfo2.packagename)
      		  GROUP BY dim_date,apkinfo2.appname
               ) ttlf
            JOIN
            (SELECT
               today,
               YEAR,
               month
             FROM dateinfo) dinfo
              ON (
              ttlf.dim_date = dinfo.today
              )
          GROUP BY dim_apk, dinfo.year, dinfo.month

        ) alltimeapk
          ON
            (tttl.t_name = alltimeapk.t_name AND tttl.t_datetype = alltimeapk.t_datetype AND tttl.t_today = alltimeapk.t_today)

    """.stripMargin
}
package utils

class Stopwatch {

  private val start = System.currentTimeMillis()

  override def toString() = (System.currentTimeMillis() - start) + " ms"

}package com.avcdata.etl.common.util.encryption

import org.jasypt.encryption.pbe.StandardPBEStringEncryptor
import org.jasypt.encryption.pbe.config.EnvironmentStringPBEConfig

/**
  * 字符串加解密工具
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/11/17 11:35
  */
object StringEncryptorUtil
{
  val DEFAULT_PASSWORD = "http://www.mllearn.com/"

  def encrypt(message: String, passowrd: String = DEFAULT_PASSWORD): String =
  {
    //加密工具
    val encryptor = new StandardPBEStringEncryptor()

    //加密配置
    val config = new EnvironmentStringPBEConfig()
    config.setAlgorithm("PBEWithMD5AndDES")

    //设置加密密码
    config.setPassword(passowrd)

    //应用配置
    encryptor.setConfig(config)

    //加密
    encryptor.encrypt(message)
  }

  def decrypt(encryptedMessage: String, password: String = DEFAULT_PASSWORD): String =
  {
    //加密工具
    val encryptor = new StandardPBEStringEncryptor()

    //加密配置
    val config = new EnvironmentStringPBEConfig()
    config.setAlgorithm("PBEWithMD5AndDES")

    //设置加密密码
    config.setPassword(password)

    //应用配置
    encryptor.setConfig(config)

    //解密
    encryptor.decrypt(encryptedMessage)
  }

  def main(args: Array[String])
  {
    if (args.length < 1)
    {
      println("Usage: com.avcdata.etl.common.util.encryption.StringEncryptorUtil plain-text")

      return
    }

    println(encrypt(args(0)))
  }
}
package com.avcdata.etl.common.util.encryption

import org.jasypt.encryption.pbe.StandardPBEStringEncryptor
import org.jasypt.encryption.pbe.config.EnvironmentStringPBEConfig

/**
  * 字符串加解密工具
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/11/17 11:35
  */
object StringEncryptorUtil
{
  val DEFAULT_PASSWORD = "http://www.mllearn.com/"

  def encrypt(message: String, passowrd: String = DEFAULT_PASSWORD): String =
  {
    //加密工具
    val encryptor = new StandardPBEStringEncryptor()

    //加密配置
    val config = new EnvironmentStringPBEConfig()
    config.setAlgorithm("PBEWithMD5AndDES")

    //设置加密密码
    config.setPassword(passowrd)

    //应用配置
    encryptor.setConfig(config)

    //加密
    encryptor.encrypt(message)
  }

  def decrypt(encryptedMessage: String, password: String = DEFAULT_PASSWORD): String =
  {
    //加密工具
    val encryptor = new StandardPBEStringEncryptor()

    //加密配置
    val config = new EnvironmentStringPBEConfig()
    config.setAlgorithm("PBEWithMD5AndDES")

    //设置加密密码
    config.setPassword(password)

    //应用配置
    encryptor.setConfig(config)

    //解密
    encryptor.decrypt(encryptedMessage)
  }

  def main(args: Array[String])
  {
    if (args.length < 1)
    {
      println("Usage: com.avcdata.etl.common.util.encryption.StringEncryptorUtil plain-text")

      return
    }

    println(encrypt(args(0)))
  }
}
package com.avcdata.spark.job.common.util

import scala.collection.mutable.ArrayBuffer

object StringUtils {

  def getIndexsOfCharOnStr(str: String, char: Char): ArrayBuffer[Int] = {
    var indexsArr = new ArrayBuffer[Int]()
    for (i <- 0 until str.length) {
      if (char.equals(str.charAt(i))) {
        indexsArr += i
      }
    }
    //    indexsArr.foreach(println(_))
    indexsArr
  }

}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyCompose01 {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val sqlContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")


    //TODO 单身：每个自然月内凌晨开机2小时以上的次数>=4次，且4-6月每个月连续符合以上条件

    //    val singlerSNRDD = sqlContext.sql(
    //      """
    //        SELECT DISTINCT sn
    //        FROM (SELECT sn, concat(year(op.power_on_day), '-', month(op.power_on_day)) AS y_m, SUM(power_on_length) / 60 AS m_dura, SUM(cnt) AS m_cnt
    //        	FROM hr.tracker_oc_fact_partition op
    //        	WHERE year(op.power_on_day) = '2017'
    //        		AND month(op.power_on_day) IN (4, 5, 6)
    //        		AND power_on_time IN (0, 1, 2, 3, 4, 5, 6)
    //        	GROUP BY sn, year(op.power_on_day), month(op.power_on_day)
    //        	) t
    //        WHERE m_dura >= 2
    //        	AND m_cnt >= 4
    //      """.stripMargin).rdd.map(line => {
    //      (line.toString, "单身")
    //    })
    //
    //
    //    println("singlerSNRDD:" + singlerSNRDD.count)
    //1525678

    //TODO 二人世界: 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件
    /**
      * ･ 数据源：点播日志
      * ･ 傍晚：17-19点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日傍晚点播launcher占比=工作日傍晚点播launcher时长/工作日傍晚点播总时长
      * 周末傍晚点播launcher占比=周末傍晚点播launcher时长/周末傍晚点播总时长
      * 工作日比周末增长率=工作日傍晚点播launcher占比/周末傍晚点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件
      */

    //工作日傍晚点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = sqlContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())

    // 工作日傍晚点播总时长 非节假日 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = sqlContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (17, 18, 19)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())
    //    3114580


    //TODO  工作日傍晚点播launcher占比

    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    print("workdayApkRatio:" + workdayApkRatio.count())

    // 周末或节假日傍晚点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = sqlContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (17, 18, 19)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    // 周末或节假日傍晚点播总时长
    val restdayAllApkPairRDD = sqlContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })
    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    print("restdayApkRatio:" + restdayApkRatio.count())

    val twoPersonRDD = workdayApkRatio.join(restdayApkRatio)

    //          .filter(line => {
    //          //每个自然月内工作日比周末增长率>=30%
    //          val sn = line._1
    //          val workdayRatio = line._2._1
    //          val restdayRatio = line._2._1
    //
    //          workdayRatio / restdayRatio > 1.3
    //        }).map(line => {
    //          val sn = line._1
    //          (sn, "二人世界")
    //        })


    println("twoPersonRDD:" + twoPersonRDD.count)


    //TODO 夫妻和小孩
    /**
      * ･ 数据源：开关机日志
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 周末日均开机时长=周末累计使用时长/周末总天数
      * 工作日开机时长=工作日累计开机时长/工作日总天数
      * 周末比工作日开机时长增长率=周末日均开机时长/工作日开机时长-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日开机时长增长率>=18%，且4-6月每个月连续符合以上条件
      */


    //TODO 夫妻和老人
    /** ･ 数据源：点播日志
      * ･ 黄金时段：19-22点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日黄金点播launcher占比=工作日黄金点播launcher时长/工作日黄金点播总时长
      * 周末黄金点播launcher占比=周末黄金点播launcher时长/周末黄金点播总时长
      * 周末比工作日增长率=周末黄金点播launcher占比/工作日黄金点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日增长率>=50%，且4-6月每个月连续符合以上条件
      *
      *
      */


    //TODO 夫妻、老人和小孩
    /**
      * 黄金时段看直播央视频道大于省级卫视频道30%以上
      * ･ 数据源：直播日志
      * ･ 黄金时段：19-22点
      * ･ 指标：
      * 黄金时段直播央视占比=黄金时段直播央视观看时长/黄金时段直播总观看时长
      * 黄金时段直播卫视占比=黄金时段直播卫视观看时长/黄金时段直播总观看时长
      * 黄金时段央视比省级卫视增长率=黄金时段直播央视占比/黄金时段直播卫视占比-1
      * ･ 标签判断标准
      * 每个自然月内黄金时段央视比省级卫视增长率>=30%，且4-6月每个月连续符合以上条件
      *
      */


  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyCompose02 {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //TODO 单身：每个自然月内凌晨开机2小时以上的次数>=4次，且4-6月每个月连续符合以上条件

    //        val singlerSNRDD = sqlContext.sql(
    //          """
    //            SELECT DISTINCT sn
    //            FROM (SELECT sn, concat(year(op.power_on_day), '-', month(op.power_on_day)) AS y_m, SUM(power_on_length) / 60 AS m_dura, SUM(cnt) AS m_cnt
    //            	FROM hr.tracker_oc_fact_partition op
    //            	WHERE year(op.power_on_day) = '2017'
    //            		AND month(op.power_on_day) IN (4, 5, 6)
    //            		AND power_on_time IN (0, 1, 2, 3, 4, 5, 6)
    //            	GROUP BY sn, year(op.power_on_day), month(op.power_on_day)
    //            	) t
    //            WHERE m_dura >= 2
    //            	AND m_cnt >= 4
    //          """.stripMargin).rdd.map(line => {
    //          (line.toString, "单身")
    //        })
    //    //
    //    //
    //    //    println("singlerSNRDD:" + singlerSNRDD.count)
    //    //1525678
    //
    //    singlerSNRDD.saveAsTextFile("/tmp/tag-singler")

    //TODO 二人世界: 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件
    /**
      * ･ 数据源：点播日志
      * ･ 傍晚：17-19点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日傍晚点播launcher占比=工作日傍晚点播launcher时长/工作日傍晚点播总时长
      * 周末傍晚点播launcher占比=周末傍晚点播launcher时长/周末傍晚点播总时长
      * 工作日比周末增长率=工作日傍晚点播launcher占比/周末傍晚点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件
      */

    //工作日傍晚点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    // 工作日傍晚点播总时长 非节假日 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (17, 18, 19)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())



    //TODO  工作日傍晚点播launcher占比
    //
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    print("workdayApkRatio:" + workdayApkRatio.count())

    // 周末或节假日傍晚点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (17, 18, 19)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    // 周末或节假日傍晚点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })
    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    print("restdayApkRatio:" + restdayApkRatio.count())

    val twoPersonRDD = workdayApkRatio.join(restdayApkRatio) //2282620

      .filter(line => {
      //每个自然月内工作日比周末增长率>=20%
      val sn = line._1
      val workdayRatio = line._2._1
      val restdayRatio = line._2._1

      workdayRatio / restdayRatio > 1.2
    })

      .map(line => {
        val sn = line._1
        (sn, "二人世界")
      })


    println("twoPersonRDD:" + twoPersonRDD.count)

    twoPersonRDD.saveAsTextFile("/tmp/twoPerson")


    //TODO 夫妻和小孩
    /**
      * ･ 数据源：开关机日志
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 周末日均开机时长=周末累计使用时长/周末总天数
      * 工作日开机时长=工作日累计开机时长/工作日总天数
      *
      * 周末比工作日开机时长增长率=周末日均开机时长/工作日开机时长-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日开机时长增长率>=18%，且4-6月每个月连续符合以上条件
      */





    //TODO 夫妻和老人
    /** ･ 数据源：点播日志
      * ･ 黄金时段：19-22点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日黄金点播launcher占比=工作日黄金点播launcher时长/工作日黄金点播总时长
      * 周末黄金点播launcher占比=周末黄金点播launcher时长/周末黄金点播总时长
      * 周末比工作日增长率=周末黄金点播launcher占比/工作日黄金点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日增长率>=50%，且4-6月每个月连续符合以上条件
      *
      *
      */


    //TODO 夫妻、老人和小孩
    /**
      * 黄金时段看直播央视频道大于省级卫视频道30%以上
      * ･ 数据源：直播日志
      * ･ 黄金时段：19-22点
      * ･ 指标：
      * 黄金时段直播央视占比=黄金时段直播央视观看时长/黄金时段直播总观看时长
      * 黄金时段直播卫视占比=黄金时段直播卫视观看时长/黄金时段直播总观看时长
      * 黄金时段央视比省级卫视增长率=黄金时段直播央视占比/黄金时段直播卫视占比-1
      * ･ 标签判断标准
      * 每个自然月内黄金时段央视比省级卫视增长率>=30%，且4-6月每个月连续符合以上条件
      *
      */


  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyCompose03 {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //TODO 单身：每个自然月内凌晨开机2小时以上的次数>=4次，且4-6月每个月连续符合以上条件====================

      val singlerSNRDD = hiveContext.sql(
        """
          SELECT DISTINCT sn
          FROM (SELECT sn, concat(year(op.power_on_day), '-', month(op.power_on_day)) AS y_m, SUM(power_on_length) / 60 AS m_dura, SUM(cnt) AS m_cnt
            FROM hr.tracker_oc_fact_partition op
            WHERE year(op.power_on_day) = '2017'
              AND month(op.power_on_day) IN (4, 5, 6)
              AND power_on_time IN (0, 1, 2, 3, 4, 5, 6)
            GROUP BY sn, year(op.power_on_day), month(op.power_on_day)
            ) t
          WHERE m_dura >= 2
            AND m_cnt >= 4
        """.stripMargin).rdd.map(line => {
        (line.toString, "单身")
      })
    //   println("singlerSNRDD:" + singlerSNRDD.count)
    //    //1525678
    //
//        singlerSNRDD.saveAsTextFile("/user/hdfs/rsync/userstat/singler" + System.currentTimeMillis)

    //TODO 二人世界: 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件=====================================
    /**
      * ･ 数据源：点播日志
      * ･ 傍晚：17-19点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日傍晚点播launcher占比=工作日傍晚点播launcher时长/工作日傍晚点播总时长
      * 周末傍晚点播launcher占比=周末傍晚点播launcher时长/周末傍晚点播总时长
      * 工作日比周末增长率=工作日傍晚点播launcher占比/周末傍晚点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件
      */

    //TODO 工作日（非节假日）傍晚点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    //TODO 工作日（非节假日）傍晚点播总时长  //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (17, 18, 19)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())


    //TODO  工作日傍晚点播launcher占比
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("workdayApkRatio:" + workdayApkRatio.count())

    //TODO 周末或节假日傍晚点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (17, 18, 19)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末或节假日傍晚点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末或节假日傍晚点播launcher占比
    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("restdayApkRatio:" + restdayApkRatio.count())



    val twoPersonRDD = workdayApkRatio.join(restdayApkRatio) //2282620


      .map(line => {
      //每个自然月内工作日比周末增长率>=20%
      val sn = line._1
      val restDayDura = line._2._1
      val workDayDura = line._2._2
      var result = (sn, 0.toDouble)
      if (workDayDura != 0) {
        result = (sn, (restDayDura / workDayDura))
      }

      result
    })
      .filter(line => {
        //每个自然月内工作日比周末增长率>=20%
        line._2 >= 1.2
      })

    //      .map(line => {
    //      val sn = line._1
    //      (sn, "二人世界")
    //    })


    //    println("twoPersonRDD:" + twoPersonRDD.count)
    //TODO 占比比较结果
//        twoPersonRDD.saveAsTextFile("/user/hdfs/rsync/userstat/twoPerson" + System.currentTimeMillis)


    //TODO 夫妻和小孩=========================
    /**
      * ･ 数据源：开关机日志
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 周末日均开机时长=周末累计使用时长/周末总天数
      * 工作日开机时长=工作日累计开机时长/工作日总天数
      * 周末比工作日开机时长增长率=周末日均开机时长/工作日开机时长-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日开机时长增长率>=18%，且4-6月每个月连续符合以上条件
      */

    //	tracker_oc_fact_partition.key	tracker_oc_fact_partition.sn	tracker_oc_fact_partition.power_on_day	tracker_oc_fact_partition.power_on_time	tracker_oc_fact_partition.power_on_length	tracker_oc_fact_partition.cnt	tracker_oc_fact_partition.date

    //TODO 周末日均开机时长=周末累计使用时长/周末总天数
    val restDayOcDurationRDD = hiveContext.sql(
      """
        select sn,sum(power_on_length)/sum(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE
          (
          dayOfWeek(date) IN  (6,7)
          OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                      '2017-05-28','2017-05-29','2017-05-30')
          )
          AND year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 工作日开机时长=工作日累计开机时长/工作日总天数
    val workDayOcDurtaionRDD = hiveContext.sql(
      """
      select sn,sum(power_on_length)/sum(distinct date)
            from hr.tracker_oc_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
              Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 周末比工作日开机时长比率=周末日均开机时长/工作日开机时长
    val ocRatio = restDayOcDurationRDD.join(workDayOcDurtaionRDD)
      .map(line => {
        val sn = line._1
        val restDayDura = line._2._1.toString.toDouble
        val workDayDura = line._2._2.toString.toDouble

        var result = (sn, 0.toDouble)

        if (workDayDura != 0) {
          result = (sn, (restDayDura / workDayDura))
        }

        result
      })

    //      .map(line => {
    //      val sn = line._1
    //      (sn, "夫妻和小孩")
    //    })

    //TODO 占比比较结果
    ocRatio
      .filter(line => {
        line._2 >= 1.18
      })

//      .saveAsTextFile("/user/hdfs/rsync/userstat/coupleAndChild" + System.currentTimeMillis)


    //TODO 夫妻和老人===============================================
    /** ･ 数据源：点播日志
      * ･ 黄金时段：19-22点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日黄金点播launcher占比=工作日黄金点播launcher时长/工作日黄金点播总时长
      * 周末黄金点播launcher占比=周末黄金点播launcher时长/周末黄金点播总时长
      * 周末比工作日增长率=周末黄金点播launcher占比/工作日黄金点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日增长率>=50%，且4-6月每个月连续符合以上条件
      */

    //TODO 工作日（非节假日）黄金时段点播launcher时长 非节假日
    val workdayLauncherApkPairRDD2 = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (19, 20, 21,22)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    //TODO 工作日（非节假日）黄金时段点播总时长  //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD2 = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (19, 20, 21,22)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())


    //TODO  工作日黄金时段点播launcher占比
    val workdayApkRatio2 = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("workdayApkRatio2:" + workdayApkRatio2.count())

    //TODO 周末或节假日黄金时段点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD2 = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (19, 20, 21,22)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末或节假日黄金时段点播总时长
    val restdayAllApkPairRDD2 = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (19, 20, 21,22)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末或节假日黄金时段点播launcher占比
    val restdayApkRatio2 = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("restdayApkRatio2:" + restdayApkRatio2.count())



    val coupleAndOlderRDD = workdayApkRatio2.join(restdayApkRatio2) //2282620
      .map(line => {
      val sn = line._1
      val workdayRatio = line._2._1
      val restdayRatio = line._2._2

      var ratio = 0.toDouble
      if (restdayRatio != 0) {
        ratio = workdayRatio / restdayRatio
      }

      (sn, ratio)
    })
      .filter(line => {
        //每个自然月内周末比工作日增长率>=50%
        line._2 >= 1.5
      })


    //      .map(line => {
    //      val sn = line._1
    //      (sn, "夫妻和老人")
    //    })



    //    println("coupleAndOlderRDD:" + coupleAndOlderRDD.count)

    //TODO 占比比较结果
    coupleAndOlderRDD.saveAsTextFile("/user/hdfs/rsync/userstat/coupleAndOlder" + System.currentTimeMillis)


    //TODO 夫妻、老人和小孩===============================================
    /**
      * 黄金时段看直播央视频道大于省级卫视频道30%以上
      * ･ 数据源：直播日志
      * ･ 黄金时段：19-22点
      * ･ 指标：
      * 黄金时段直播央视占比=黄金时段直播央视观看时长/黄金时段直播总观看时长
      * 黄金时段直播卫视占比=黄金时段直播卫视观看时长/黄金时段直播总观看时长
      * 黄金时段央视比省级卫视增长率=黄金时段直播央视占比/黄金时段直播卫视占比-1
      * ･ 标签判断标准
      * 每个自然月内黄金时段央视比省级卫视增长率>=30%，且4-6月每个月连续符合以上条件
      */

    //TODO 黄金时段直播央视观看时长

    val cctvLivePrimeTimeDurtionRDD = hiveContext.sql(
      """
        select 
        		dim_sn,sum(fact_time_length)
        	from hr.tracker_live_fact_partition 
        	where year(date)=2017 and month(date) in (4,5,6)
        	and dim_hour in (19,20,21,22) and dim_channel like 'CCTV%' 
        	group by dim_sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 黄金时段直播卫视观看时长
    val weishiLivePrimeTimeDurtionRDD = hiveContext.sql(
      """
        select
        		dim_sn,sum(fact_time_length)
        	from hr.tracker_live_fact_partition
        	where year(date)=2017 and month(date) in (4,5,6)
        	and dim_hour in (19,20,21,22) and dim_channel not like 'CCTV%' and dim_channel <> '其他'
        	group by dim_sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 黄金时段央视比省级卫视比率
    val coupleAndChildAndOldRDD = cctvLivePrimeTimeDurtionRDD.join(weishiLivePrimeTimeDurtionRDD)
      .map(line => {
        val sn = line._1
        val cctvDuration = line._2._1.toString.toDouble
        val weishiDuration = line._2._2.toString.toDouble

        var ratio = 0.toDouble
        if (weishiDuration != 0) {
          ratio = cctvDuration / weishiDuration
        }
        (sn, ratio)
      })

      //每个自然月内黄金时段央视比省级卫视增长率>=30%
      .filter(line => {
      line._2 >= 1.3
    })

    //      .map(line => {
    //      val sn = line._1
    //      (sn, "夫妻、老人和小孩")
    //    })


    //TODO 占比比较结果
    coupleAndChildAndOldRDD.saveAsTextFile("/user/hdfs/rsync/userstat/coupleAndChildAndOld" + System.currentTimeMillis())


  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext

/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * 将各结构的sn 汇总成一张表
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyComposeAll {

  case class TagFamilyCompose(
                               sn: String,
                               single: Int,
                               couple: Int,
                               coupleAndChild: Int,
                               coupleAndOld: Int,
                               coupleAndChildAndOld: Int
                             )

  def main(args: Array[String]) {

    //    val arr = Array[String](
    //      "二人世界",
    //      "夫妻和老人",
    //      "单身",
    //      "夫妻、小孩和老人",
    //    "夫妻和小孩"
    //    )
    //
    //    scala.util.Sorting.stableSort(arr)
    //
    //    for(i<- 0 until arr.length){
    //      println(arr(i))
    //    }


    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    val snSinglePair = TagFamilyComposeSingle.run(sc, analysisDate, recentDaysNum)
    val snCouplePair = TagFamilyComposeCouple.run(sc, analysisDate, recentDaysNum)
    val snCoupleAndChildPair = TagFamilyComposeCoupleAndChild.run(sc, analysisDate, recentDaysNum)
    val snCoupleAndOldPair = TagFamilyComposeCoupleAndOld.run(sc, analysisDate, recentDaysNum)
    val snCoupleAndChildAndOlderPair = TagFamilyComposeCoupleAndChildAndOld.run(sc, analysisDate, recentDaysNum)


   val resultDF =  snSinglePair.union(snCouplePair).union(snCoupleAndChildPair).union(snCoupleAndOldPair).union    (snCoupleAndChildAndOlderPair)
      .distinct()
      // 按SN、家庭结构二次排序
      // .sortBy(_._1).sortBy(_._2)
      .reduceByKey((pre, post) => {
      pre + "\t" + post
    }).map(line => {
      val sn = line._1
      val composeArr = line._2.split("\t")
      //家庭结构二次排序
      scala.util.Sorting.stableSort(composeArr)

      //顺序示例
      //      二人世界
      //      单身
      //      夫妻、小孩和老人
      //      夫妻和小孩
      //      夫妻和老人

      var single = 0
      var couple = 0
      var coupleAndChild = 0
      var coupleAndOld = 0
      var coupleAndChildAndOld = 0

      for (i <- 0 until composeArr.length) {
        composeArr(i) match {
          case "单身" => (single = 1);
          case "二人世界" => (couple = 1);
          case "夫妻和小孩" => (coupleAndChild = 1);
          case "夫妻和老人" => (coupleAndOld = 1);
          case "夫妻、老人和小孩" => (coupleAndChildAndOld = 1);
        }
      }


      TagFamilyCompose(
        sn: String,
        single: Int,
        couple: Int,
        coupleAndChild,
        coupleAndOld,
        coupleAndChildAndOld)

    }).toDF
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "TagFamilyCompose", false,
      SaveMode.Append)

  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyComposeCouple {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String): RDD[(String, String)] = {


    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



    //TODO 二人世界: 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件=====================================
    /**
      * ･ 数据源：点播日志
      * ･ 傍晚：17-19点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日傍晚点播launcher占比=工作日傍晚点播launcher时长/工作日傍晚点播总时长
      * 周末傍晚点播launcher占比=周末傍晚点播launcher时长/周末傍晚点播总时长
      * 工作日比周末增长率=工作日傍晚点播launcher占比/周末傍晚点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内工作日比周末增长率>=30%，且4-6月每个月连续符合以上条件
      */

    //TODO 工作日（非节假日）傍晚点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    //TODO 工作日（非节假日）傍晚点播总时长  //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (17, 18, 19)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())


    //TODO  工作日傍晚点播launcher占比
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("workdayApkRatio:" + workdayApkRatio.count())

    //TODO 周末或节假日傍晚点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (17, 18, 19)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末或节假日傍晚点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末或节假日傍晚点播launcher占比
    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("restdayApkRatio:" + restdayApkRatio.count())


    val coupleRDD = workdayApkRatio.join(restdayApkRatio) //2282620


      .map(line => {
      //每个自然月内工作日比周末增长率>=20%
      val sn = line._1
      val restDayDura = line._2._1
      val workDayDura = line._2._2
      var result = (sn, 0.toDouble)
      if (workDayDura != 0) {
        result = (sn, (restDayDura / workDayDura))
      }

      result
    })
      .filter(line => {
        //每个自然月内工作日比周末增长率>=20%
        line._2 >= 1.2
      })

      .map(line => {
        val sn = line._1.toString
        (sn, "二人世界")
      })


    //    println("coupleRDD:" + coupleRDD.count)
    //TODO 占比比较结果
    coupleRDD.saveAsTextFile("/user/hdfs/rsync/userstat/couple" + System.currentTimeMillis)

    coupleRDD
  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  *

3.2周末午间看launcher时长高于工作日30%以上
･ 数据源：点播日志
･ 午间：12-14点
･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
･ 指标：
周末午间看launcher占比=周末午间看launcher时长/周末午间点播总时长
工作日午间看launcher占比=工作日午间看launcher时长/工作日午间点播总时长
周末比工作日午间看launcher的增长率=周末午间看launcher占比/工作日午间看launcher占比-1
･ 标签判断标准
每个自然月内周末比工作日午间看launcher的增长率>=30%，且4-6月每个月连续符合以上条件
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyComposeCoupleAndChild {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String): RDD[(String, String)] = {

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")




    //TODO 夫妻和小孩=========================

    /**
      * ･ 数据源：开关机日志
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 周末日均开机时长=周末累计使用时长/周末总天数
      * 工作日开机时长=工作日累计开机时长/工作日总天数
      * 周末比工作日开机时长增长率=周末日均开机时长/工作日开机时长-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日开机时长增长率>=18%，且4-6月每个月连续符合以上条件
      */

    //	tracker_oc_fact_partition.key	tracker_oc_fact_partition.sn	tracker_oc_fact_partition.power_on_day	tracker_oc_fact_partition.power_on_time	tracker_oc_fact_partition.power_on_length	tracker_oc_fact_partition.cnt	tracker_oc_fact_partition.date

    //TODO 周末日均开机时长=周末累计使用时长/周末总天数
    val restDayOcDurationRDD = hiveContext.sql(
      """
        select sn,sum(power_on_length)/count(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE
          (
          dayOfWeek(date) IN  (6,7)
          OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                      '2017-05-28','2017-05-29','2017-05-30')
          )
          AND year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("周末日均开机时长:" + restDayOcDurationRDD.count)

    //TODO 工作日开机时长=工作日累计开机时长/工作日总天数
    val workDayOcDurtaionRDD = hiveContext.sql(
      """
      select sn,sum(power_on_length)/count(distinct date)
            from hr.tracker_oc_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
              Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("工作日开机时长:" + workDayOcDurtaionRDD.count)

    //TODO 周末比工作日开机时长比率=周末日均开机时长/工作日开机时长
    val ocRatio = restDayOcDurationRDD.join(workDayOcDurtaionRDD)
      .map(line => {
        val sn = line._1
        val restDayDura = line._2._1.toString.toDouble
        val workDayDura = line._2._2.toString.toDouble

        var result = (sn, 0.toDouble)

        if (workDayDura != 0) {
          result = (sn, (restDayDura / workDayDura))
        }

        result
      })


    //TODO 占比比较结果
    val coupleAndChildRDD = ocRatio
      .filter(line => {
        line._2 >= 1.18
      })

      .map(line => {
        val sn = line._1.toString
        (sn, "夫妻和小孩")
      })


//      .saveAsTextFile("/user/hdfs/rsync/userstat/coupleAndChild" + System.currentTimeMillis)

    coupleAndChildRDD
  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyComposeCoupleAndChildAndOld {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String): RDD[(String, String)] = {


    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")




    //TODO 夫妻、老人和小孩===============================================
    /**
      * 黄金时段看直播央视频道大于省级卫视频道30%以上
      * ･ 数据源：直播日志
      * ･ 黄金时段：19-22点
      * ･ 指标：
      * 黄金时段直播央视占比=黄金时段直播央视观看时长/黄金时段直播总观看时长
      * 黄金时段直播卫视占比=黄金时段直播卫视观看时长/黄金时段直播总观看时长
      * 黄金时段央视比省级卫视增长率=黄金时段直播央视占比/黄金时段直播卫视占比-1
      * ･ 标签判断标准
      * 每个自然月内黄金时段央视比省级卫视增长率>=30%，且4-6月每个月连续符合以上条件
      */

    //TODO 黄金时段直播央视观看时长

    val cctvLivePrimeTimeDurtionRDD = hiveContext.sql(
      """
        select 
        		dim_sn,sum(fact_time_length)
        	from hr.tracker_live_fact_partition 
        	where year(date)=2017 and month(date) in (4,5,6)
        	and dim_hour in (19,20,21,22) and dim_channel like 'CCTV%' 
        	group by dim_sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 黄金时段直播卫视观看时长
    val weishiLivePrimeTimeDurtionRDD = hiveContext.sql(
      """
        select
        		dim_sn,sum(fact_time_length)
        	from hr.tracker_live_fact_partition
        	where year(date)=2017 and month(date) in (4,5,6)
        	and dim_hour in (19,20,21,22) and dim_channel not like 'CCTV%' and dim_channel <> '其他'
        	group by dim_sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 黄金时段央视比省级卫视比率
    val coupleAndChildAndOldRDD = cctvLivePrimeTimeDurtionRDD.join(weishiLivePrimeTimeDurtionRDD)
      .map(line => {
        val sn = line._1
        val cctvDuration = line._2._1.toString.toDouble
        val weishiDuration = line._2._2.toString.toDouble

        var ratio = 0.toDouble
        if (weishiDuration != 0) {
          ratio = cctvDuration / weishiDuration
        }
        (sn, ratio)
      })

      //每个自然月内黄金时段央视比省级卫视增长率>=30%
      .filter(line => {
      line._2 >= 1.3
    })

      .map(line => {
        val sn = line._1.toString
        (sn, "夫妻、老人和小孩")
      })


    //TODO 占比比较结果
    coupleAndChildAndOldRDD.saveAsTextFile("/user/hdfs/rsync/userstat/coupleAndChildAndOld" + System.currentTimeMillis())

    coupleAndChildAndOldRDD
  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext


/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyComposeCoupleAndOld {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String): RDD[(String, String)] = {

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



    //TODO 夫妻和老人===============================================
    /** ･ 数据源：点播日志
      * ･ 黄金时段：19-22点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 工作日黄金点播launcher占比=工作日黄金点播launcher时长/工作日黄金点播总时长
      * 周末黄金点播launcher占比=周末黄金点播launcher时长/周末黄金点播总时长
      * 周末比工作日增长率=周末黄金点播launcher占比/工作日黄金点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日增长率>=50%，且4-6月每个月连续符合以上条件
      */

    //TODO 工作日（非节假日）黄金时段点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (19, 20, 21,22)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    //TODO 工作日（非节假日）黄金时段点播总时长  //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (19, 20, 21,22)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())


    //TODO  工作日黄金时段点播launcher占比
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("workdayApkRatio2:" + workdayApkRatio2.count())

    //TODO 周末或节假日黄金时段点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (19, 20, 21,22)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末或节假日黄金时段点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (19, 20, 21,22)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末或节假日黄金时段点播launcher占比
    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("restdayApkRatio2:" + restdayApkRatio2.count())


    val coupleAndOlderRDD = workdayApkRatio.join(restdayApkRatio) //2282620
      .map(line => {
      val sn = line._1
      val workdayRatio = line._2._1
      val restdayRatio = line._2._2

      var ratio = 0.toDouble
      if (restdayRatio != 0) {
        ratio = workdayRatio / restdayRatio
      }

      (sn, ratio)
    })
      .filter(line => {
        //每个自然月内周末比工作日增长率>=50%
        line._2 >= 1.5
      })


      .map(line => {
        val sn = line._1.toString
        (sn, "夫妻和老人")
      })


    //    println("coupleAndOlderRDD:" + coupleAndOlderRDD.count)

    //TODO 占比比较结果
//    coupleAndOlderRDD.saveAsTextFile("/user/hdfs/rsync/userstat/coupleAndOlder" + System.currentTimeMillis)

    coupleAndOlderRDD
  }
}
package com.avcdata.spark.job.tag.evaluate

import com.avcdata.spark.job.etl.util.UDFUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.rdd.RDD

/**
  * 需求具体描述：
  *
  * 目的：建立用户画像标签库，根据下列规则，给每个终端贴上家庭结构的标签，一个终端可贴多个标签。
  * 标签包括：单身、二人世界、夫妻+老人、夫妻+小孩、三代同堂5个
  * 时间周期：2017年4月-2017年6月，共3个月，以每个完整自然月为一个统计周期
  * ･       单身：一个月内，凌晨开机2小时以上的次数不低于4次
  * ･       二人世界：工作日傍晚点播看launcher比周末高30%以上
  * ･       夫妻+小孩：周末日均开机时长比工作日高18%以上
  * ･       夫妻+老人：周末黄金看点播launcher比工作日高50%及以上
  * ･       三代同堂：黄金时段看直播央视大于省级卫视30%以上
  *
  * @author zhangyongtian
  * @define 用户画像标签库 -家庭结构-算法统计
  */
object TagFamilyComposeSingle {


  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }


    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String):RDD[(String, String)]  = {

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //TODO 单身：每个自然月内凌晨开机2小时以上的次数>=4次，且4-6月每个月连续符合以上条件====================

    val singleSNRDD = hiveContext.sql(
      """
          SELECT DISTINCT sn
          FROM (SELECT sn, concat(year(op.power_on_day), '-', month(op.power_on_day)) AS y_m, SUM(power_on_length) / 60 AS m_dura, SUM(cnt) AS m_cnt
            FROM hr.tracker_oc_fact_partition op
            WHERE year(op.power_on_day) = '2017'
              AND month(op.power_on_day) IN (4, 5, 6)
              AND power_on_time IN (0, 1, 2, 3, 4, 5, 6)
            GROUP BY sn, year(op.power_on_day), month(op.power_on_day)
            ) t
          WHERE m_dura >= 2
            AND m_cnt >= 4
      """.stripMargin).rdd.map(line => {
      (line.toString, "单身")
    })
    //   println("singleSNRDD:" + singleSNRDD.count)
    //    //1525678
    //
    singleSNRDD.saveAsTextFile("/user/hdfs/rsync/userstat/single" + System.currentTimeMillis)

    singleSNRDD
  }
}
package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
10.1	湖南卫视	周五22:00-23:00	天天向上	P15-29
10.1	湖南卫视	周六20:10-22:00	快乐大本营	P15-29


10.1 周五22:00-23:00或周六20:10-22:00时间段内收看湖南卫视的时长大于30分钟
ž 数据源：直播表，取频道、播出时间段两个字段匹配
ž 判断标准：每个自然月内，周五22:00-23:00或周六20:10-22:00时间段内收看湖南卫视的时长大于30分钟，每个月连续符合以上条件
 */
object TagRuler10P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
10.2观看年龄标签规则表-P15-29中剧情类型节目，且观看时长大于30分钟的次数超过4次
ž 数据源：点播表、年龄标签规则表-P15-29，按剧情类型字段匹配
ž 判断标准：每个自然月内，观看该类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler10P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
11观看年龄标签规则表-P30-45中剧情类型节目，且观看时长大于30分钟的次数超过4次
ž 数据源：点播表、年龄标签规则表-P30-45，按剧情类型字段匹配
ž 判断标准：每个自然月内，观看该剧情类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler11 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*

12.1	CCTV-11	06:00-11:30	戏剧频道	P45+
12.1	CCTV-11	14:14-21:40	戏剧频道	P45+
12.1	CCTV-4	09:00-13:30	中文国际频道国情新闻类	P45+
12.1	CCTV-4	18:00-22:00	中文国际频道国情新闻类	P45+
12.1	河南卫视	每周日19:30-21:00	梨园春	P45+
12.1	北京卫视	17:20-18：20	养生堂	P45+



12.1满足年龄标签规则表-P45+中的频道及播出时间段内观看，且时长大于30分钟的次数超过4次
ž 数据源：直播表、年龄标签规则表-P45+，按频道、播出时间段两个字段匹配
ž 判断标准：每个自然月内，该频道该播出时间内，使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler12P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
12.2观看年龄标签规则表-P45+中剧情类型节目，且观看时长大于30分钟的次数超过4次
ž 数据源：点播表、年龄标签规则表-P45+，按剧情类型字段匹配
ž 判断标准：每个自然月内，收看该剧情类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler12P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

      .setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



    //      .map(line => {
    //        line.sn + "\t" + line.rulerId + "\t" + line.result
    //      })
    //      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
    //

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler1P1 {

  //result 符号规则为1 不符合规则为0
  case class RulerResult(sn: String, rulerId: String, result: Int)

  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName("TagRuler1P1")

    val sc = new SparkContext(sparkConf)


    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    1.1凌晨开机2小时以上，一个月6次及以上
    //    凌晨：0-6点 标签判断标准
    //      每个自然月内凌晨开机2小时以上的次数>=6次，且4-6月每个月连续符合以上条件
    hiveContext.sql(
      """
        SELECT sn,month(date) AS m_month,SUM(coalesce(power_on_length,0)) / 60 AS m_dura, SUM(coalesce(cnt,0)) AS m_cnt
        FROM hr.tracker_oc_fact_partition op
        WHERE year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          AND power_on_time IN (0, 1, 2, 3, 4, 5, 6)
        GROUP BY sn,month(date)
      """.stripMargin).map(row => {
      val sn = row.getAs[String]("sn")
      val month = row.getAs[Int]("m_month")
      val dura = row.getAs[Double]("m_dura")
      val cnt = row.getAs[Double]("m_cnt")

      (sn, month + "\t" + dura + "\t" + cnt)
    })
      .reduceByKey((pre, post) => {
        pre + "#" + post
      })
      .map(line => {
        val sn = line._1.toString
        val cols = line._2.split("#")

        val arr = scala.collection.mutable.ArrayBuffer[Int]()

        for (i <- 0 until cols.length) {
          val cols2 = cols(i).split("\t")
          val month = cols2(0)
          val dura = cols2(1).toDouble
          val cnt = cols2(2).toDouble
          if (dura >= 2 && cnt >= 6) {
            arr += 1
          }
        }

        if (arr.length == cols.length) {
          RulerResult(sn, "1.1", 1)
        } else {
          RulerResult(sn, "1.1", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P1" + System.currentTimeMillis)

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler1P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO 1.2周末每终端开机次数比工作日高20%以上
    //      * ･ 数据源：开关机日志
    //    * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
    //    * ･ 指标：
    //    * 周末日均开机次数=周末累计开机次数/周末总天数
    //    * 工作日日均开机次数=工作日累计开机次数/工作日总天数
    //    * 周末比工作日开机次数增长率=周末日均开机次数/工作日日均开机次数-1
    //    * ･ 标签判断标准：
    //    * 每个自然月内周末比工作日开机次数增长率>=20%，且4-6月每个月连续符合以上条件

    //TODO 周末日均开机次数=周末累计开机次数/周末总天数
    val restDayOcCntRDD = hiveContext.sql(
      """
        select sn,sum(cnt)/count(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE
          (
          dayOfWeek(date) IN  (6,7)
          OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                      '2017-05-28','2017-05-29','2017-05-30')
          )
          AND year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("周末日均开机时长:" + restDayOcCntRDD.count)

    //TODO 工作日日均开机次数=工作日累计开机次数/工作日总天数
    val workDayOcCntRDD = hiveContext.sql(
      """
      select sn,sum(cnt)/count(distinct date)
            from hr.tracker_oc_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
              Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

//    println("工作日开机时长:" + workDayOcCntRDD.count)

    //TODO  周末比工作日开机次数增长率=周末日均开机次数/工作日日均开机次数
    val ocRatio = restDayOcCntRDD.join(workDayOcCntRDD)
      .map(line => {
        val sn = line._1.toString
        val restDayDura = line._2._1.toString.toDouble
        val workDayDura = line._2._2.toString.toDouble

        var result = (sn, 0.toDouble)

        if (workDayDura != 0) {
          result = (sn, (restDayDura / workDayDura))
        }
        result
      })

    //TODO 占比比较结果
    val rdd12 = ocRatio
      .map(line => {
        val sn = line._1
        if (line._2 >= 1.2) {
          RulerResult(sn, "1.2", 1)
        } else {
          RulerResult(sn, "1.2", 0)
        }
      })
      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P2" + System.currentTimeMillis)

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler1P3 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //TODO    1.3黄金时段，周末平均每天开机时长低于工作日
    //    * ･ 数据源：开关机日志
    //    * ･ 黄金时段：19-22点
    //      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
    //    * ･ 指标：
    //    * 周末黄金时段日均开机时长=周末黄金时段累计开机时长/周末总天数
    //    * 工作日黄金时段日均开机时长=工作日黄金时段累计开机时长/工作日总天数
    //    * 周末与工作日黄金时段的开机时长之差=周末黄金时段日均开机时长-工作日黄金时段日均开机时长
    //    * ･ 标签判断标准：
    //    * 每个自然月内周末与工作日黄金时段的开机时长之差<0，且4-6月每个月连续符合此条件


    //TODO 周末黄金时段日均开机时长=周末黄金时段累计开机时长/周末总天数
    val restdayPrimeTimeOcDurationRDD = hiveContext.sql(
      """
        select sn,sum(power_on_length)/count(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE
          (
          dayOfWeek(date) IN  (6,7)
          OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                      '2017-05-28','2017-05-29','2017-05-30')
          )
          AND year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          AND power_on_time IN (19, 20, 21,22)
          Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

//    println("周末黄金时段日均开机时长:" + restdayPrimeTimeOcDurationRDD.count)



    //TODO  工作日黄金时段日均开机时长=工作日黄金时段累计开机时长/工作日总天数 周一至周五中的非节假日
    val workdayPrimeTimeOcDurationRDD = hiveContext.sql(
      """
       select sn,sum(power_on_length)/count(distinct date)
                  from hr.tracker_oc_fact_partition
                 WHERE
                    (
                    dayOfWeek(date) IN (1, 2, 3, 4, 5)
                        AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)
                         AND power_on_time IN (19, 20, 21,22)
                    Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

//    println("工作日黄金时段日均开机时长:" + workdayPrimeTimeOcDurationRDD.count)


    //TODO 比较结果
    restdayPrimeTimeOcDurationRDD.join(workdayPrimeTimeOcDurationRDD)
      //    每个自然月内周末与工作日黄金时段的开机时长之差<0
      .map(line => {
      val sn = line._1.toString
      if (line._2._1.toString.toDouble < line._2._2.toString.toDouble) {
        RulerResult(sn, " 1.3", 1)
      } else {
        RulerResult(sn, " 1.3", 0)
      }
    })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler1P4 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P4")

    val sc = new SparkContext(sparkConf)
    
    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO      * 1.4次黄金时段，周末平均每天开机时长低于工作日
    //    * ･ 数据源：开关机日志
    //    * ･ 次黄金时段：22-24点
    //      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
    //    * ･ 指标：
    //    * 周末次黄金时段日均开机时长=周末次黄金时段累计开机时长/周末总天数
    //    * 工作日次黄金时段日均开机时长=工作日次黄金时段累计开机时长/工作日总天数
    //    * 周末与工作日次黄金时段的开机时长之差=周末次黄金时段日均开机时长-工作日次黄金时段日均开机时长
    //    * ･ 标签判断标准：
    //    * 每个自然月内周末与工作日次黄金时段的开机时长之差<0，且4-6月每个月连续符合此条件

    //TODO 周末次黄金时段日均开机时长=周末次黄金时段累计开机时长/周末总天数
    val restdaySecTimeOcDurationRDD = hiveContext.sql(
      """
        select sn,sum(power_on_length)/count(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE
          (
          dayOfWeek(date) IN  (6,7)
          OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                      '2017-05-28','2017-05-29','2017-05-30')
          )
          AND year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          AND power_on_time IN (22,23,24)
          Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("周末次黄金时段日均开机时长:" + restdaySecTimeOcDurationRDD.count)



    //TODO  工作日次黄金时段日均开机时长=工作日次黄金时段累计开机时长/工作日总天数 周一至周五中的非节假日
    val workdaySecTimeOcDurationRDD = hiveContext.sql(
      """
       select sn,sum(power_on_length)/count(distinct date)
                  from hr.tracker_oc_fact_partition
                 WHERE
                    (
                    dayOfWeek(date) IN (1, 2, 3, 4, 5)
                        AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)
                         AND power_on_time IN (22,23,24)
                    Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("工作日次黄金时段日均开机时长:" + workdaySecTimeOcDurationRDD.count)


    //TODO 比较结果
    restdaySecTimeOcDurationRDD.join(workdaySecTimeOcDurationRDD)
      //     每个自然月内周末与工作日次黄金时段的开机时长之差<0
      .map(line => {
      val sn = line._1.toString
      if (line._2._1.toString.toDouble < line._2._2.toString.toDouble) {
        RulerResult(sn, "1.4", 1)
      } else {
        RulerResult(sn, "1.4", 0)
      }
    })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P4" + System.currentTimeMillis)

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler2P1 {
  def main(args: Array[String]) {

    println(this.getClass.getSimpleName)

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName("TagRuler1P1")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    2.1工作日傍晚点播看launcher比周末高30%以上
    //      ･ 数据源：点播日志
    //    ･ 傍晚：17-19点
    //      ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
    //    ･ 指标：
    //    工作日傍晚点播launcher占比=工作日傍晚点播launcher时长/工作日傍晚点播总时长
    //    周末傍晚点播launcher占比=周末傍晚点播launcher时长/周末傍晚点播总时长
    //    工作日比周末傍晚点播launcher增长率=工作日傍晚点播launcher占比/周末傍晚点播launcher占比-1
    //    ･ 标签判断标准：
    //    每个自然月内工作日傍晚点播launcher比周末傍晚点播launcher增长率>=20%，且4-6月每个月连续符合以上条件

    //TODO 工作日（非节假日）傍晚点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    //TODO 工作日（非节假日）傍晚点播总时长  //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (17, 18, 19)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())


    //TODO  工作日傍晚点播launcher占比
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("workdayApkRatio:" + workdayApkRatio.count())

    //TODO 周末或节假日傍晚点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (17, 18, 19)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末或节假日傍晚点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (17, 18, 19)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末或节假日傍晚点播launcher占比
    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("restdayApkRatio:" + restdayApkRatio.count())


    val coupleRDD = workdayApkRatio.join(restdayApkRatio) //2282620


      .map(line => {
      //每个自然月内工作日比周末增长率>=20%
      val sn = line._1.toString
      val restDayDura = line._2._1
      val workDayDura = line._2._2
      var result = 0.toDouble
      if (workDayDura != 0) {
        result = restDayDura / workDayDura
      }
      //每个自然月内工作日比周末增长率>=20%
      if (result >= 1.2) {
        RulerResult(sn, "2.1", 1)
      } else {
        RulerResult(sn, "2.1", 1)
      }
    })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler2P1" + System.currentTimeMillis)
    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler2P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    2.2早间直播看其他频道，一个月4次以上，每次超过20分钟
    //    * ･ 数据源：直播日志
    //    * ･ 早间：6-8点
    //        标签：早间直播看其他频道时长超过20分钟的次数
    //    * ･ 标签判断标准：
    //    * 每个自然月内早间直播看其他频道时长超过20分钟的次数>4次，且4-6月每个月连续符合以上条件
    val moringLiveOtherChannelRDD = hiveContext.sql(
      """
        select
        		dim_sn,month(date),sum(fact_cnt)
        	from hr.tracker_live_fact_partition
        	where year(date)='2017' and month(date) in (4,5,6)
        	and dim_hour in (6,7,8) and dim_channel = '其他'
        	and fact_time_length > 1200
          group by dim_sn,month(date)
      """.stripMargin).rdd.map(line => {
        val sn = line(0)
        val month = line(1).toString.toDouble
        val cnt = line(2).toString.toDouble
        (sn, month + "\t" + cnt)
      })
      .reduceByKey((pre, post) => {
        pre + "#" + post
      })
      .map(line => {
        val sn = line._1.toString
        val cols = line._2.split("#")

        val arr = scala.collection.mutable.ArrayBuffer[Int]()

        for (i <- 0 until cols.length) {
          val cols2 = cols(i).split("\t")
          val month = cols2(0)
          val cnt = cols2(1)
          if (cols2(1).toDouble > 4) {
            arr += 1
          }
        }

        if (arr.length == cols.length) {
          RulerResult(sn, "2.2", 1)
        } else {
          RulerResult(sn, "2.2", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/"+this.getClass.getSimpleName + System.currentTimeMillis)
    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler2P3 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName(this.getClass.getSimpleName)

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    2.3周末上午直播只看CCTV
    //      * ･ 数据源：直播日志
    //    * ･ 上午：8-12点
    //      * ･ 周末标准：周六、周日、节假日
    //    * ･ 指标：
    //    * 周末上午直播看CCTV的占比=周末上午直播看CCTV的时长/周末上午直播的总时长
    //    * ･ 标签判断标准：
    //    * 每个自然月内周末上午直播看CCTV的占比=100%，且4-6月每个月连续符合以上条件
    val restdayAMCCTVLiveRDD = hiveContext.sql(
      """
        select a.dim_sn as sn,a.dura/b.dura as ratio from
        (select
        		dim_sn,sum(fact_time_length) as dura
        	from hr.tracker_live_fact_partition
        	where year(date)='2017' and month(date) in (4,5,6)
          and
         (
        dayOfWeek(date) IN  (6,7)
        OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                    '2017-05-28','2017-05-29','2017-05-30')
         )
         and dim_hour in (8,9,10,11,12) and dim_channel like 'CCTV%'
         group by dim_sn
        ) a
         join
       (
         select dim_sn,sum(fact_time_length)  as dura
                from hr.tracker_live_fact_partition
                where year(date)='2017' and month(date) in (4,5,6)
                and
               (
              dayOfWeek(date) IN  (6,7)
              OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                          '2017-05-28','2017-05-29','2017-05-30')
               )
               and dim_hour in (8,9,10,11,12)
        group by dim_sn
       ) b

       on a.dim_sn = b.dim_sn

      """.stripMargin).map(row => {
      val sn = row.getAs[String]("sn")
      val ratio = row.getAs[Double]("ratio")

      if (ratio == 1) {
        RulerResult(sn, "2.3", 1)
      } else {
        RulerResult(sn, "2.3", 0)
      }
    })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/"+this.getClass.getSimpleName + System.currentTimeMillis)

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler2P4 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    * 2.4周末下午点播只看launcher
    //      * ･ 数据源：点播日志
    //    * ･ 下午：14-17点
    //      * ･ 周末标准：周六、周日、节假日
    //    * ･ 指标：
    //    * 周末下午点播launcher的占比=周末下午点播launcher的时长/周末下午点播的总时长
    //    * ･ 标签判断标准：
    //    * 每个自然月内周末下午点播launcher的占比=100%，且4-6月每个月连续符合以上条件


    //TODO 周末下午点播launcher的时长

    val restdayLauncherApkPairRDD2 = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (14,15,16,17)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末下午点播的总时长
    val restdayAllApkPairRDD2 = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (14,15,16,17)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 每个自然月内周末下午点播launcher的占比
    val restdayApkRatio2 = restdayLauncherApkPairRDD2.join(restdayAllApkPairRDD2).map(line => {

      val sn = line._1.toString
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var ratio = 0.toDouble
      if (allApkDura != 0) {
        ratio = launcherApkDura / allApkDura
      }

      if (ratio == 1) {
        RulerResult(sn, "2.4", 1)
      } else {
        RulerResult(sn, "2.4", 0)
      }
    })



      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/"+this.getClass.getSimpleName + System.currentTimeMillis)

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler3P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    3.1周末日均每终端开机时长比工作日高18%以上
    //      ･ 数据源：开关机日志
    //    ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
    //    ･ 指标：
    //    周末日均开机时长=周末累计开机时长/周末总天数
    //    工作日日均开机时长=工作日累计开机时长/工作日总天数
    //    周末比工作日开机时长增长率=周末日均开机时长/工作日日均开机时长-1
    //    ･ 标签判断标准：
    //    每个自然月内周末比工作日开机时长增长率>=18%，且4-6月每个月连续符合以上条件


    //	tracker_oc_fact_partition.key	tracker_oc_fact_partition.sn	tracker_oc_fact_partition.power_on_day	tracker_oc_fact_partition.power_on_time	tracker_oc_fact_partition.power_on_length	tracker_oc_fact_partition.cnt	tracker_oc_fact_partition.date

    //TODO 周末日均开机时长=周末累计使用时长/周末总天数
    val restDayOcDurationRDD = hiveContext.sql(
      """
        select sn,sum(power_on_length)/count(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE
          (
          dayOfWeek(date) IN  (6,7)
          OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                      '2017-05-28','2017-05-29','2017-05-30')
          )
          AND year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("周末日均开机时长:" + restDayOcDurationRDD.count)

    //TODO 工作日开机时长=工作日累计开机时长/工作日总天数
    val workDayOcDurtaionRDD = hiveContext.sql(
      """
      select sn,sum(power_on_length)/count(distinct date)
            from hr.tracker_oc_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
              Group by sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    println("工作日开机时长:" + workDayOcDurtaionRDD.count)

    //TODO 周末比工作日开机时长比率=周末日均开机时长/工作日开机时长

    restDayOcDurationRDD.join(workDayOcDurtaionRDD)
      .map(line => {
        val sn = line._1.toString
        val restDayDura = line._2._1.toString.toDouble
        val workDayDura = line._2._2.toString.toDouble


        //TODO 占比比较结果
        var ratio =  0.toDouble
        if (workDayDura != 0) {
          ratio =   restDayDura / workDayDura
        }
        if (ratio >= 1.18) {
          RulerResult(sn, "3.1", 1)
        } else {
          RulerResult(sn, "3.1", 0)
        }
      })

      .map(line => {
      line.sn + "\t" + line.rulerId + "\t" + line.result
    })
      .saveAsTextFile("/user/hdfs/rsync/usertag/"+this.getClass.getSimpleName + System.currentTimeMillis)
    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler3P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")
    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    3.2周末午间看launcher时长高于工作日30%以上
    //      ･ 数据源：点播日志
    //    ･ 午间：12-14点
    //      ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
    //    ･ 指标：
    //    周末午间看launcher占比=周末午间看launcher时长/周末午间点播总时长
    //    工作日午间看launcher占比=工作日午间看launcher时长/工作日午间点播总时长
    //    周末比工作日午间看launcher的增长率=周末午间看launcher占比/工作日午间看launcher占比-1
    //    ･ 标签判断标准
    //      每个自然月内周末比工作日午间看launcher的增长率>=30%，且4-6月每个月连续符合以上条件

    //TODO 周末或节假日午间看launcher时长

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (12, 13, 14)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //周末午间点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (12, 13, 14)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末午间看launcher占比
    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1.toString
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })




    // 工作日午间看launcher时长
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (12, 13, 14)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580


    // 工作日午间点播总时长
    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (12, 13, 14)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //TODO    工作日午间看launcher占比=工作日午间看launcher时长/工作日午间点播总时长
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1.toString
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    // 周末比工作日午间看launcher的增长率=周末午间看launcher占比/工作日午间看launcher占比-1
    //TODO    每个自然月内周末比工作日午间看launcher的增长率>=30%，且4-6月每个月连续符合以上条件
    restdayApkRatio.join(workdayApkRatio) //2282620
      .map(line => {
      val sn = line._1
      val restdayRatio = line._2._1
      val workdayRatio = line._2._2

      var ratio = 0.toDouble
      if (workdayRatio != 0) {
        ratio = restdayRatio / workdayRatio
      }

      if (ratio >= 1.3) {
        RulerResult(sn, "3.2", 1)
      } else {
        RulerResult(sn, "3.2", 0)
      }

    })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)


    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler4P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
    //      .set("spark.default.parallelism", "1000")


    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)




    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    /**
      * TODO 4.1周末黄金时段看点播launcher比工作日高50%及以上
      * ･ 数据源：点播日志
      * ･ 黄金时段：19-22点
      * ･ 工作日周末标准：工作日：周一至周五中的非节假日；周末：周六、周日、节假日
      * ･ 指标：
      * 周末黄金时段点播launcher占比=周末黄金时段点播launcher时长/周末黄金时段点播总时长
      * 工作日黄金时段点播launcher占比=工作日黄金时段点播launcher时长/工作日黄金时段点播总时长
      * 周末比工作日黄金时段点播launcher增长率=周末黄金时段点播launcher占比/工作日黄金时段点播launcher占比-1
      * ･ 标签判断标准：
      * 每个自然月内周末比工作日黄金时段点播launcher增长率>=50%，且4-6月每个月连续符合以上条件
      */

    //TODO 工作日（非节假日）黄金时段点播launcher时长 非节假日
    val workdayLauncherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                 (
                  dayOfWeek(date) IN (1, 2, 3, 4, 5)
                  AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01', '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (19, 20, 21,22)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayLauncherApkPairRDD:" + workdayLauncherApkPairRDD.count())
    //    3114580

    //TODO 工作日（非节假日）黄金时段点播总时长  //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val workdayAllApkPairRDD = hiveContext.sql(
      """
       SELECT sn, SUM(ap.fact_duration)
        FROM hr.sample_terminal_three t
          JOIN
           (
           select * from hr.tracker_apk_fact_partition
           WHERE
              (
              dayOfWeek(date) IN (1, 2, 3, 4, 5)
              AND date not in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01','2017-05-28','2017-05-29','2017-05-30')
              )
              AND year(date) = '2017'
              AND month(date) IN (4, 5, 6)
              AND dim_hour IN (19, 20, 21,22)
           ) ap
          ON t.sn = ap.dim_sn
          JOIN
          (SELECT DISTINCT packagename, appname FROM hr.apkinfo
            ) ai
          ON ap.dim_apk = ai.packagename
        GROUP BY sn
      """.stripMargin
    ).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    println("workdayAllApkPairRDD:" + workdayAllApkPairRDD.count())


    //TODO  工作日黄金时段点播launcher占比
    val workdayApkRatio = workdayLauncherApkPairRDD.join(workdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("workdayApkRatio2:" + workdayApkRatio2.count())

    //TODO 周末或节假日黄金时段点播launcher时长 //	4月2日~4月4日
    // 4月29日~5月1日
    // 	5月28日~5月30日

    val restdayLauncherApkPairRDD = hiveContext.sql(
      """
        SELECT sn, SUM(ap.fact_duration)
              FROM hr.sample_terminal_three t
                 JOIN
                 (
                 select * from hr.tracker_apk_fact_partition
                 WHERE  (
                      dayOfWeek(date) IN (6,7)
                       OR
                       date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                                '2017-05-28','2017-05-29','2017-05-30')
                        )
                        AND year(date) = '2017'
                        AND month(date) IN (4, 5, 6)

                        AND dim_hour IN (19, 20, 21,22)
                 ) ap
                ON t.sn = ap.dim_sn
                JOIN
                (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                  ) ai
                ON ap.dim_apk = ai.packagename
              WHERE
                (ai.appname = '银河·奇异果'
                  AND t.license = 'yinhe')
                OR
                (ai.appname = 'CIBN环球影视'
                  AND t.license = 'youku')
                OR
                 (ai.appname = '腾讯视频TV端'
                 AND t.license = 'tencent')
              GROUP BY sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })

    //    print("restdayLauncherApkPairRDD:" + restdayLauncherApkPairRDD.count())

    //TODO 周末或节假日黄金时段点播总时长
    val restdayAllApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration)
            FROM hr.sample_terminal_three t
              JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                  dayOfWeek(date) IN  (6,7)
                  OR date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (19, 20, 21,22)
               ) ap
              ON t.sn = ap.dim_sn
              JOIN
              (SELECT DISTINCT packagename, appname FROM hr.apkinfo
                ) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn

      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duRatio2n = line(1)
      (sn, duRatio2n)
    })
    //    print("restdayAllApkPairRDD:" + restdayAllApkPairRDD.count())


    //TODO 周末或节假日黄金时段点播launcher占比
    val restdayApkRatio = restdayLauncherApkPairRDD.join(restdayAllApkPairRDD).map(line => {

      val sn = line._1
      val launcherApkDura = line._2._1.toString.toDouble
      val allApkDura = line._2._2.toString.toDouble

      var result = (sn, 0.toDouble)
      if (allApkDura != 0) {
        result = (sn, launcherApkDura / allApkDura)
      }
      result
    })

    //    print("restdayApkRatio2:" + restdayApkRatio2.count())


    val coupleAndOlderRDD = workdayApkRatio.join(restdayApkRatio) //2282620
      .map(line => {
      val sn = line._1.toString
      val workdayRatio = line._2._1
      val restdayRatio = line._2._2

      var ratio = 0.toDouble
      if (restdayRatio != 0) {
        ratio = workdayRatio / restdayRatio
      }
      //每个自然月内周末比工作日增长率>=50%
      if (ratio >= 1.5) {
        RulerResult(sn, "4.1", 1)
      } else {
        RulerResult(sn, "4.1", 0)
      }

    })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)


    sc.stop


  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler4P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
    //      .set("spark.default.parallelism", "1000")

    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30


    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)


    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    4.2次黄金时段点播只看launcher
    //      ･ 数据源：点播日志
    //    ･ 次黄金时段：22-24点
    //      ･ 指标：
    //    次黄金时段点播看launcher的占比=次黄金时段点播看launcher时长/次黄金时段点播的总时长
    //    ･ 标签判断标准：
    //    每个自然月内次黄金时段点播launcher的占比=100%，且4-6月每个月连续符合此条件


    //TODO 次黄金时段点播看launcher时长
    val launcherApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration) as dura
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (22,23,24)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
            	(ai.appname = '银河·奇异果'
                AND t.license = 'yinhe')
              OR
              (ai.appname = 'CIBN环球影视'
                AND t.license = 'youku')
              OR
               (ai.appname = '腾讯视频TV端'
               AND t.license = 'tencent')
            GROUP BY sn
      """.stripMargin).map(row => {
      val sn = row.getAs[String]("sn")
      val dura = row.getAs[Double]("dura")
      (sn, dura)
    })



    //TODO 次黄金时段点播的总时长
    val apkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration) as dura
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (22,23,24)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            GROUP BY sn
      """.stripMargin).map(row => {
      val sn = row.getAs[String]("sn")
      val dura = row.getAs[Double]("dura")
      (sn, dura)
    })

    launcherApkPairRDD.join(apkPairRDD)
      .map(line => {
        val sn = line._1
        val launcherApkDura = line._2._1.toString.toDouble
        val allApkDura = line._2._2.toString.toDouble

        if (launcherApkDura.equals(allApkDura)) {
          RulerResult(sn, "4.2", 1)
        } else {
          RulerResult(sn, "4.2", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)

    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{ConfigFactory, Config}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext

object TagRuler4P3 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
    //      .set("spark.default.parallelism", "1000")

    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30


    sparkConf.setAppName(this.getClass.getSimpleName)
    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO    4.3早间看点播不看合规app
    //      ･ 数据源：点播日志
    //    ･ 早间：6-8点
    //      ･ 指标：
    //    早间点播看合规app的占比=早间点播看合规app的时长/早间点播的总时长
    //    ･ 标签判断标准：
    //    每个自然月内早间看点播合规app的占比=0%，且4-6月每个月连续符合以上条件

    //    ai.appname in
    //      (
    //        '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', 'CIBN聚精彩', '云视听·泰捷', '芒果TV', 'PPTV聚体育', 'CIBN悦厅TV', 'CIBN微视听'
    //    )
    //          And
    //    (ai.appname<>'银河·奇异果' and license='yinhe')
    //    or
    //    (ai.appname<>'CIBN环球影视' and license='youku')
    //    or
    //    (ai.appname<>'腾讯视频TV端' and license='tencent')
    //    or
    //    (ai.appname<>'CIBN环球影视' and license='YUNOS')


    //TODO 早间点播看合规app的时长
    val normApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration) as dura
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
                  AND dim_hour IN (6,7,8)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
              ai.appname in
              (
                '银河·奇异果', '腾讯视频TV端', 'CIBN环球影视', 'CIBN聚精彩', '云视听·泰捷', '芒果TV', 'PPTV聚体育', 'CIBN悦厅TV', 'CIBN微视听'
            )
              And
              (ai.appname<>'银河·奇异果' and t.license='yinhe')
              OR
              (ai.appname<>'CIBN环球影视' and t.license='youku')
              OR
              (ai.appname<>'腾讯视频TV端' and t.license='tencent')
              OR
              (ai.appname<>'CIBN环球影视' and t.license='YUNOS')
            GROUP BY sn
      """.stripMargin).map(row => {
      val sn = row.getAs[String]("sn")
      val dura = row.getAs[Double]("dura")
      (sn, dura)
    })

      .map(line=>{
        val sn = line._1
        val dura=line._2
        if(dura.equals(0)){
          RulerResult(sn, "4.3", 1)
        }else{
          RulerResult(sn, "4.3", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)

//    //TODO 早间点播的总时长
//    val normApkAllRDD = hiveContext.sql(
//      """
//           SELECT sn, SUM(ap.fact_duration) as dura
//            FROM hr.sample_terminal_three t
//            	 JOIN
//               (
//               select * from hr.tracker_apk_fact_partition
//               WHERE
//                  year(date) = '2017'
//                  AND month(date) IN (4, 5, 6)
//                  AND dim_hour IN (6,7,8)
//               ) ap
//              ON t.sn = ap.dim_sn
//            	JOIN
//            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
//            		) ai
//              ON ap.dim_apk = ai.packagename
//            GROUP BY sn
//      """.stripMargin).map(row => {
//      val sn = row.getAs[String]("sn")
//      val dura = row.getAs[Double]("dura")
//      (sn, dura)
//    })
//
//    normApkPairRDD.join(normApkAllRDD)
//      .map(line => {
//        val sn = line._1
//        val launcherApkDura = line._2._1.toString.toDouble
//        val allApkDura = line._2._2.toString.toDouble
//
//        if (launcherApkDura.equals(allApkDura)) {
//          RulerResult(sn, "4.3", 1)
//        } else {
//          RulerResult(sn, "4.3", 0)
//        }
//      })





















    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler4P4 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
    //      .set("spark.default.parallelism", "1000")
      .setAppName(this.getClass.getSimpleName)
    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30


    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO      4.4周末点播只看launcher
    //      ･ 数据源：点播日志
    //    ･ 周末标准：周六、周日、节假日
    //    ･ 指标：
    //    周末点播看launcher的占比=周末点播看launcher的时长/周末点播的总时长
    //    ･ 标签判断标准：
    //    每个自然月周末点播看launcher的占比=100%，且4-6月每个月连续符合以上条件

    //TODO 周末点播看非launcher 时长为0
    val normApkPairRDD = hiveContext.sql(
      """
           SELECT sn, SUM(ap.fact_duration) as dura
            FROM hr.sample_terminal_three t
            	 JOIN
               (
               select * from hr.tracker_apk_fact_partition
               WHERE
                  (
                    dayOfWeek(date) IN (6,7)
                     OR
                     date in ('2017-04-02','2017-04-03','2017-04-04','2017-04-29','2017-04-30','2017-05-01',
                              '2017-05-28','2017-05-29','2017-05-30')
                  )
                  AND year(date) = '2017'
                  AND month(date) IN (4, 5, 6)
               ) ap
              ON t.sn = ap.dim_sn
            	JOIN
            	(SELECT DISTINCT packagename, appname FROM hr.apkinfo
            		) ai
              ON ap.dim_apk = ai.packagename
            WHERE
              (ai.appname<>'银河·奇异果' and t.license='yinhe')
              OR
              (ai.appname<>'CIBN环球影视' and t.license='youku')
              OR
              (ai.appname<>'腾讯视频TV端' and t.license='tencent')
              OR
              (ai.appname<>'CIBN环球影视' and t.license='YUNOS')
            GROUP BY sn
      """.stripMargin).map(row => {
      val sn = row.getAs[String]("sn")
      val dura = row.getAs[Double]("dura")
      (sn, dura)
    })

      .map(line => {
        val sn = line._1
        val dura = line._2
        if (dura.equals(0)) {
          RulerResult(sn, "4.4", 1)
        } else {
          RulerResult(sn, "4.4", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)
    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler5P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
      //      .set("spark.default.parallelism", "1000")
      .setAppName(this.getClass.getSimpleName)
    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30

    val sc = new SparkContext(sparkConf)


    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //    5.1黄金时段看直播央视大于省级卫视30%以上
    //      ･ 数据源：直播日志
    //    ･ 黄金时段：19-22点
    //      ･ 指标：
    //    黄金时段直播央视占比=黄金时段直播央视观看时长/黄金时段直播总观看时长
    //    黄金时段直播省级卫视占比=黄金时段直播省级卫视观看时长/黄金时段直播总观看时长
    //    黄金时段央视比省级卫视增长率=黄金时段直播央视占比/黄金时段直播省级卫视占比-1
    //    ･ 标签判断标准
    //      每个自然月内黄金时段央视比省级卫视增长率>=30%，且4-6月每个月连续符合以上条件

    //TODO 黄金时段直播央视观看时长

    val cctvLivePrimeTimeDurtionRDD = hiveContext.sql(
      """
        select
        		dim_sn,sum(fact_time_length)
        	from hr.tracker_live_fact_partition
        	where year(date)=2017 and month(date) in (4,5,6)
        	and dim_hour in (19,20,21,22) and dim_channel like 'CCTV%'
        	group by dim_sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 黄金时段直播卫视观看时长
    val weishiLivePrimeTimeDurtionRDD = hiveContext.sql(
      """
        select
        		dim_sn,sum(fact_time_length)
        	from hr.tracker_live_fact_partition
        	where year(date)=2017 and month(date) in (4,5,6)
        	and dim_hour in (19,20,21,22) and dim_channel not like 'CCTV%' and dim_channel <> '其他'
        	group by dim_sn
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val duration = line(1)
      (sn, duration)
    })

    //TODO 黄金时段央视比省级卫视比率
    val coupleAndChildAndOldRDD = cctvLivePrimeTimeDurtionRDD.join(weishiLivePrimeTimeDurtionRDD)
      .map(line => {
        val sn = line._1.toString
        val cctvDuration = line._2._1.toString.toDouble
        val weishiDuration = line._2._2.toString.toDouble

        var ratio = 0.toDouble
        if (weishiDuration != 0) {
          ratio = cctvDuration / weishiDuration
        }
        //每个自然月内黄金时段央视比省级卫视增长率>=30%
        if (ratio >= 1.3) {
          RulerResult(sn, "5.1", 1)
        } else {
          RulerResult(sn, "5.1", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)

    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRuler5P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
      //      .set("spark.default.parallelism", "1000")
      .setAppName(this.getClass.getSimpleName)
    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //    5.2每终端日均开机次数大于3次
    //      ･ 数据源：开关机日志
    //    ･ 指标：
    //    每终端日均开机次数=每个月累计开机次数/这个月总天数
    //    ･ 标签判断标准
    //      每个自然月内每终端日均开机次数>3次，且4-6月每个月连续符合以上条件

    //TODO  每终端日均开机次数
    val restDayOcCntRDD = hiveContext.sql(
      """
        select sn,month(date),sum(cnt)/count(distinct date)
        from hr.tracker_oc_fact_partition
       WHERE year(date) = '2017'
          AND month(date) IN (4, 5, 6)
          Group by sn,month(date)
      """.stripMargin).rdd.map(line => {
      val sn = line(0)
      val month = line(1).toString.toDouble
      val cnt = line(2).toString.toDouble
      (sn, month + "\t" + cnt)
    })  .reduceByKey((pre, post) => {
      pre + "#" + post
    })
      .map(line => {
        val sn = line._1.toString
        val cols = line._2.split("#")

        val arr = scala.collection.mutable.ArrayBuffer[Int]()

        for (i <- 0 until cols.length) {
          val cols2 = cols(i).split("\t")
          val month = cols2(0)
          val cnt = cols2(1)
          if (cols2(1).toDouble > 4) {
            arr += 1
          }
        }

        if (arr.length == cols.length) {
          RulerResult(sn, "5.2", 1)
        } else {
          RulerResult(sn, "5.2", 0)
        }
      })

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/"+this.getClass.getSimpleName + System.currentTimeMillis)
    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.common.util.StringUtils
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.avcdata.spark.job.until.TimeUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 6.1符合性别标签规则表-男
  * * 中的频道及对应的播出时间段内观看，且时长大于30分钟的次数超过4次
  *
  * ž 数据源：直播表、性别标签规则表-男，按频道、播出时间段两个字段匹配

  * ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
  */
object TagRuler6P1 {

  case class LiveTimeRange(sn: String, channel: String, month: String, startTime: String, endTime: String)

  def main(args: Array[String]) {

    val starTime = TimeUtils.convertDateStr2TimeStamp("14:00:00", "HH:mm:ss")
    val endTime = TimeUtils.convertDateStr2TimeStamp("14:30", "HH:mm")
    val dura = (endTime - starTime) / 1000 / 60
    //
    println(dura)


    //    for(ele<-"1000101100".toCharArray){
    //      println(ele)
    //    }


    //    println("1000101100".toCharArray.contains('2'))

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //
    //    sc.textFile("E:\\aowei\\tracker-user\\data\\live-partition-sample.txt")
    //      .map(line => {
    //        val cols = line.split("\t")
    //        val sn = cols(0)
    //        val channel = cols(1)
    //        val date = cols(2)
    //        val hour = cols(3)
    //        val min = cols(4)
    //        val dura = cols(5)
    //        val cnt = cols(6)
    //
    //        (sn + "\t" + channel + "\t" + date + "\t" + hour, (cnt, min, dura))
    //
    //      })
    //      .sortByKey(true)
    //      .reduceByKey((pre, post) => {
    //        val pre_cnt = pre._1
    //        val pre_min = pre._2
    //        val pre_dura = pre._3
    //
    //        val post_cnt = post._1
    //        val post_min = post._2
    //        val post_dura = post._3
    //        (pre_cnt + post_cnt, pre_min + "#" + post_min, pre_dura + "#" + post_dura)
    //      })
    //      //(00:00:00:00:00:00	CCTV-13	2017-07-25	20,(1000101100,13#14#15#16#19#20#21#23#24#25,38#60#60#46#38#2#33#38#60#60))
    //      .flatMap(line => {
    //
    //      val result = new scala.collection.mutable.ArrayBuffer[LiveTimeRange]()
    //      val info = line._1
    //
    //      val info_cols = info.split("\t")
    //      val sn = info_cols(0)
    //      val channel = info_cols(1)
    //      val date = info_cols(2)
    //      val hour = info_cols(3)
    //
    //      val pre_cnts = line._2._1
    //      val pre_cnt_arr = line._2._1.toCharArray
    //      val pre_min_arr = line._2._2.split("#")
    //      val pre_dura_arr = line._2._3.split("#")
    //
    //      if (!pre_cnt_arr.contains('1')) {
    //        //没有1
    //        val startTime = TimeUtils.addZero(hour) + ":00:00"
    //        val endTime = TimeUtils.addZero(hour) + ":59:59"
    //        result += new LiveTimeRange(sn, channel, date, startTime, endTime)
    //      } else {
    //        val one_indexs = StringUtils.getIndexsOfCharOnStr(pre_cnts, '1')
    //        for (i <- 0 until one_indexs.length) {
    //          if (i == (one_indexs.length - 1)) {
    //            //最后一个1
    //            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
    //
    //            //到结尾
    //            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(pre_cnt_arr.length - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(pre_cnt_arr.length - 1))
    //
    //            result += new LiveTimeRange(sn, channel, date, startTime, endTime)
    //          } else {
    //            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
    //            //到下一个1
    //            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i + 1) - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i + 1) - 1))
    //
    //            result += new LiveTimeRange(sn, channel, date, startTime, endTime)
    //          }
    //
    //        }
    //      }
    //      result
    //    })
    //
    //
    //      .foreach(println(_))
    //
    //
    //    //        run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      //      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shuffle.memoryFracton", "0.3")
      .set("spark.shffle.manager", "sort")
      //      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
      .set("spark.shuffle.sort.bypassMergeThreshold", "200")
      .set("Spark.sql.autoBroadcastJoinThreshold", "10m")
      //      .set("spark.sql.shuffle.partitions", "true")
      //      .set("spark.default.parallelism", "1000")
      .setAppName(this.getClass.getSimpleName)
      //    --executor-cores 4
      //    --executor-memory 4g
      //    --num-executors 30
      .setExecutorEnv("SPARK_JAVA_OPTS", "-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps")

    //      .setExecutorEnv("SPARK_JAVA_OPTS", "-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+DisableExplicitGC -Xms1024m -Xmx2048m -XX:MaxPermSize=256m")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //    * 6.1符合性别标签规则表-男中的频道及对应的播出的开始时间段内观看，且时长大于30分钟的次数超过4次
    //    * ž 数据源：直播表、性别标签规则表-男，按频道、播出时间段两个字段匹配
    //    * ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
    //    (4,5,6)
    //TODO 月的观看行为
    val baseRDD = hiveContext.sql(
      """
          select
              dim_sn,concat(year(date),'-',month(date)) as month,dim_channel,dim_hour,dim_min,fact_time_length as dura, fact_cnt as cnt
            from hr.tracker_live_fact_partition
            where date between '2017-04-01' and  '2017-04-30' and dim_hour is not null and  dim_min is not null
      """.stripMargin)
      .map(row => {
        val sn = row.getString(0)
        val month = row.getString(1)
        val dim_channel = row.getString(2)
        val dim_hour = row.getString(3)
        val dim_min = row.getString(4)
        val fact_time_length = row.getString(5)
        val fact_cnt = row.getString(6)

        (sn, month, dim_channel, TimeUtils.addZero(dim_hour), TimeUtils.addZero(dim_min), fact_time_length, fact_cnt)
      })
      .sortBy(x => x)

      //TODO 转化成时段数据
      .map(line => {
      //      (sn,month,dim_channel,dim_hour,dim_min,fact_time_length,fact_cnt)
      val sn = line._1
      val month = line._2
      val channel = line._3
      val hour = line._4
      val min = line._5
      val dura = line._6
      val cnt = line._7

      (sn + "\t" + channel + "\t" + month, (cnt, min, dura, hour))
    })

      .reduceByKey((pre, post) => {
        val pre_cnt = pre._1
        val pre_min = pre._2
        val pre_dura = pre._3
        val pre_hour = pre._4

        val post_cnt = post._1
        val post_min = post._2
        val post_dura = post._3
        val post_hour = post._4
        (pre_cnt + "" + post_cnt, pre_min + "#" + post_min, pre_dura + "#" + post_dura, pre_hour + "#" + post_hour)
      })
      //(00:00:00:00:00:00	CCTV-13	2017-07-25	20,(1000101100,13#14#15#16#19#20#21#23#24#25,38#60#60#46#38#2#33#38#60#60))
      .flatMap(line => {

      //      val result = new scala.collection.mutable.ArrayBuffer[LiveTimeRange]()
      val result = new scala.collection.mutable.ArrayBuffer[String]()
      val info = line._1

      val info_cols = info.split("\t")
      val sn = info_cols(0)
      val channel = info_cols(1)
      val month = info_cols(2)

      val cnts = line._2._1
      val cnt_arr = line._2._1.toCharArray
      val min_arr = line._2._2.split("#")
      val dura_arr = line._2._3.split("#")
      val hour_arr = line._2._4.split("#")


      for (j <- 0 until hour_arr.length)
        if (!cnt_arr.contains('1')) {
          //没有1
          val startTime = hour_arr(j) + ":00:00"
          val endTime = hour_arr(j) + ":59:59"
          result += sn + "\t" + channel + "\t" + month + "\t" + startTime + "\t" + endTime
        } else {
          val one_indexs = StringUtils.getIndexsOfCharOnStr(cnts, '1')
          for (i <- 0 until one_indexs.length) {
            if (i == (one_indexs.length - 1)) {
              //最后一个1
              val startTime = hour_arr(one_indexs(i)) + ":" + min_arr(one_indexs(i)) + ":" + dura_arr(one_indexs(i))

              //到结尾
              val endTime = hour_arr(cnt_arr.length - 1) + ":" + min_arr(cnt_arr.length - 1) + ":" + dura_arr(cnt_arr.length - 1)

              result += sn + "\t" + channel + "\t" + month + "\t" + startTime + "\t" + endTime
            } else {
              val startTime = hour_arr(one_indexs(i)) + ":" + min_arr(one_indexs(i)) + ":" + dura_arr(one_indexs(i))
              //到下一个1
              val endTime = hour_arr(one_indexs(i + 1) - 1) + ":" + min_arr(one_indexs(i + 1) - 1) + ":" + dura_arr(one_indexs(i + 1) - 1)

              result += sn + "\t" + channel + "\t" + month + "\t" + startTime + "\t" + endTime
            }

          }
        }
      result
    })
      //TOD0 秒满60进1
      .map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val channel = cols(1)
      val month = cols(2)
      val startTime = TimeUtils.fullSecondAddMin(cols(3))
      val endTime = TimeUtils.fullSecondAddMin(cols(4))

      sn + "\t" + channel + "\t" + month + "\t" + startTime + "\t" + endTime
    })

    //    baseRDD.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)

    //    baseRDD.saveAsTextFile("/user/hdfs/rsync/usertag/baseRDD" + System.currentTimeMillis())


    // TODO 全时段的规则处理
    //6.1	CCTV-12	全时段	社会与法频道	男
    //2	6.1	CCTV-13	全时段	新闻频道	男
    //3	6.1	CCTV-5	全时段	体育频道	男
    val allTimeRangeRDD01 = baseRDD.filter(line => {
      val cols = line.split("\t")
      val channel = cols(1)

      val ruler_channel_arr = Array[String](
        "CCTV-12",
        "CCTV-13",
        "CCTV-5"
      )
      ruler_channel_arr.contains(channel)
    })
      //TODO 时长大于30分钟的行为
      .filter(line => {

      val cols = line.split("\t")
      val sn = cols(0)
      val channel = cols(1)
      val month = cols(2)

      val starTime = TimeUtils.convertDateStr2TimeStamp(cols(3), "HH:mm:ss")
      val endTime = TimeUtils.convertDateStr2TimeStamp(cols(4), "HH:mm:ss")
      val dura = (endTime - starTime) / 1000 / 60
      //      时长大于30分钟
      dura > 30
    })
      //      .repartition(200)
      //TODO  次数超过4次的行为
      .map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val channel = cols(1)
      val month = cols(2)

      ((sn + "\t" + month), 1)
    })

//    val sampledRDD = allTimeRangeRDD01.sample(false, 0.1, 9);
//
//    val skewedUserid = sampledRDD
//      .reduceByKey(_ + _)
//      .map(line => {
//        (line._2, line._1)
//      })
//      .sortByKey(false).take(3)

    //    (0)._2

//    sc.makeRDD(skewedUserid).saveAsTextFile("/tmp/skewedUserid"+ System.currentTimeMillis)

//    allTimeRangeRDD01.reduceByKey(_ + _)
//      .filter(line => {
//        line._2 > 4
//      })
//
//      //TODO 连续三个月
//      .map(line => {
//      val cols = line._1.split("\t")
//      val sn = cols(0)
//      val month = cols(1)
//      (sn, month)
//    }).distinct.map(line => {
//      val sn = line._1
//      (sn, 1)
//    }).reduceByKey(_ + _).filter(line => {
//      line._2 == 3
//    }).map(line => {
//      RulerResult(line._1, "6.1", 1)
//    })
//
//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      }).distinct()
//      .saveAsTextFile("/tmp/allTimeRangeRDD" + System.currentTimeMillis)

    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //    //TODO 直播性别规则表
    //    val ruler_map_live_arr = hiveContext.sql("select rulerid, channel, timerange from hr.user_tag_ruler_map_live " +
    //      "where rulerid = '6.1'")
    //      .collect
    //
    //    val ruler_map_live_arr_bv = sc.broadcast(ruler_map_live_arr)
    //
    //
    //    //TODO 不符合全时段规则的 按指定时段的规则处理
    //    val otherRDD = baseRDD.filter(line => {
    //
    //      //不在全时段的频道
    //      val ruler_channel_arr = Array[String](
    //        "CCTV-12",
    //        "CCTV-13",
    //        "CCTV-5"
    //      )
    //      val cond01 = !ruler_channel_arr.contains(line.channel)
    //
    //
    //      //时长大于30分钟
    //      val starTime = TimeUtils.convertDateStr2TimeStamp(line.startTime, "HH:mm:ss")
    //      val endTime = TimeUtils.convertDateStr2TimeStamp(line.endTime, "HH:mm:ss")
    //      val dura = (endTime - starTime) / 1000 / 60
    //      val cond02 = dura > 30
    //
    //
    //      //符合指定规则
    //      var cond03 = false
    //
    //      val rulerArr = ruler_map_live_arr_bv.value
    //      breakable(
    //        for (ele <- rulerArr) {
    //          val timeRange = ele.getString(2).split("-")
    //          val ruler_starTime = TimeUtils.convertDateStr2TimeStamp(timeRange(0), "HH:mm")
    //          val ruler_endTime = TimeUtils.convertDateStr2TimeStamp(timeRange(1), "HH:mm")
    //
    //          if (starTime >= ruler_starTime && endTime <= ruler_endTime) {
    //            cond03 = true
    //            break
    //          }
    //
    //        }
    //      )
    //      cond01 && cond02 && cond03
    //    }) //TODO  次数超过4次的行为
    //      .map(line => {
    //      val sn = line.sn
    //      val month = line.month
    //      ((sn, month), 1)
    //    }).reduceByKey(_ + _)
    //      .filter(line => {
    //        line._2 > 4
    //      })
    //
    //      //TODO 连续三个月
    //      .map(line => {
    //      val sn = line._1._1
    //      val month = line._1._2
    //      (sn, month)
    //    }).distinct.map(line => {
    //      val sn = line._1
    //      (sn, 1)
    //    }).reduceByKey(_ + _).filter(line => {
    //      line._2 == 3
    //    }).map(line => {
    //      RulerResult(line._1, "6.1", 1)
    //    })
    //
    //    //////////////////////////////////////////////////////////////////////////////////////////////////////////
    //    //不符合规则的sn
    //    val notRDD = baseRDD.map(line => {
    //      line.sn
    //    }).distinct.subtract(allTimeRangeRDD.map(_.sn).union(otherRDD.map(_.sn))).map(sn => {
    //      RulerResult(sn, "6.1", 0)
    //    })
    //
    //    notRDD.union(allTimeRangeRDD.union(otherRDD))
    //
    //      .map(line => {
    //        line.sn + "\t" + line.rulerId + "\t" + line.result
    //      })
    //      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)
    //
    //    baseRDD.unpersist(true)

    sc.stop


  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.common.util.StringUtils
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.until.TimeUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 6.1符合性别标签规则表-男
  * * 中的频道及对应的播出时间段内观看，且时长大于30分钟的次数超过4次
  *
  * ž 数据源：直播表、性别标签规则表-男，按频道、播出时间段两个字段匹配

  * ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
  */
object TagRuler6P120170822 {

  case class LiveTimeRange(sn: String, channel: String, month: String, startTime: String, endTime: String)

  def main(args: Array[String]) {

    val starTime = TimeUtils.convertDateStr2TimeStamp("14:00:00", "HH:mm:ss")
    val endTime = TimeUtils.convertDateStr2TimeStamp("14:30", "HH:mm")
    val dura = (endTime - starTime) / 1000 / 60
    //
    println(dura)


    //    for(ele<-"1000101100".toCharArray){
    //      println(ele)
    //    }


    //    println("1000101100".toCharArray.contains('2'))

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //
    //    sc.textFile("E:\\aowei\\tracker-user\\data\\live-partition-sample.txt")
    //      .map(line => {
    //        val cols = line.split("\t")
    //        val sn = cols(0)
    //        val channel = cols(1)
    //        val date = cols(2)
    //        val hour = cols(3)
    //        val min = cols(4)
    //        val dura = cols(5)
    //        val cnt = cols(6)
    //
    //        (sn + "\t" + channel + "\t" + date + "\t" + hour, (cnt, min, dura))
    //
    //      })
    //      .sortByKey(true)
    //      .reduceByKey((pre, post) => {
    //        val pre_cnt = pre._1
    //        val pre_min = pre._2
    //        val pre_dura = pre._3
    //
    //        val post_cnt = post._1
    //        val post_min = post._2
    //        val post_dura = post._3
    //        (pre_cnt + post_cnt, pre_min + "#" + post_min, pre_dura + "#" + post_dura)
    //      })
    //      //(00:00:00:00:00:00	CCTV-13	2017-07-25	20,(1000101100,13#14#15#16#19#20#21#23#24#25,38#60#60#46#38#2#33#38#60#60))
    //      .flatMap(line => {
    //
    //      val result = new scala.collection.mutable.ArrayBuffer[LiveTimeRange]()
    //      val info = line._1
    //
    //      val info_cols = info.split("\t")
    //      val sn = info_cols(0)
    //      val channel = info_cols(1)
    //      val date = info_cols(2)
    //      val hour = info_cols(3)
    //
    //      val pre_cnts = line._2._1
    //      val pre_cnt_arr = line._2._1.toCharArray
    //      val pre_min_arr = line._2._2.split("#")
    //      val pre_dura_arr = line._2._3.split("#")
    //
    //      if (!pre_cnt_arr.contains('1')) {
    //        //没有1
    //        val startTime = TimeUtils.addZero(hour) + ":00:00"
    //        val endTime = TimeUtils.addZero(hour) + ":59:59"
    //        result += new LiveTimeRange(sn, channel, date, startTime, endTime)
    //      } else {
    //        val one_indexs = StringUtils.getIndexsOfCharOnStr(pre_cnts, '1')
    //        for (i <- 0 until one_indexs.length) {
    //          if (i == (one_indexs.length - 1)) {
    //            //最后一个1
    //            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
    //
    //            //到结尾
    //            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(pre_cnt_arr.length - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(pre_cnt_arr.length - 1))
    //
    //            result += new LiveTimeRange(sn, channel, date, startTime, endTime)
    //          } else {
    //            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
    //            //到下一个1
    //            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i + 1) - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i + 1) - 1))
    //
    //            result += new LiveTimeRange(sn, channel, date, startTime, endTime)
    //          }
    //
    //        }
    //      }
    //      result
    //    })
    //
    //
    //      .foreach(println(_))
    //
    //
    //    //        run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
//      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shuffle.memoryFracton", "0.3")
      .set("spark.shffle.manager", "sort")
      //      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
      .set("spark.shuffle.sort.bypassMergeThreshold", "200")
      //      .set("spark.sql.shuffle.partitions", "true")
      //      .set("spark.default.parallelism", "1000")
      .setAppName(this.getClass.getSimpleName)
    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //    * 6.1符合性别标签规则表-男中的频道及对应的播出的开始时间段内观看，且时长大于30分钟的次数超过4次
    //    * ž 数据源：直播表、性别标签规则表-男，按频道、播出时间段两个字段匹配
    //    * ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
    //    (4,5,6)
    //TODO 月的观看行为
    val baseRDD = hiveContext.sql(
      """
          select
              dim_sn,concat(year(date),'-',month(date)) as month,dim_channel,dim_hour,dim_min,fact_time_length as dura,
              fact_cnt as cnt
            from hr.tracker_live_fact_partition
            where date between '2017-04-01' and  '2017-04-30' and dim_hour is not null and
            dim_min
             is not null
      """.stripMargin)
      .map(row => {
        val sn = row.getString(0)
        val month = row.getString(1)
        val dim_channel = row.getString(2)
        val dim_hour = row.getString(3)
        val dim_min = row.getString(4)
        val fact_time_length = row.getString(5)
        val fact_cnt = row.getString(6)

        (sn, month, dim_channel, TimeUtils.addZero(dim_hour), TimeUtils.addZero(dim_min), fact_time_length, fact_cnt)
      })
      .sortBy(x => x)

      //TODO 转化成时段数据
      .map(line => {
      //      (sn,month,dim_channel,dim_hour,dim_min,fact_time_length,fact_cnt)
      val sn = line._1
      val month = line._2
      val channel = line._3
      val hour = line._4
      val min = line._5
      val dura = line._6
      val cnt = line._7

      (sn + "\t" + channel + "\t" + month, (cnt, min, dura, hour))
    })

      .reduceByKey((pre, post) => {
        val pre_cnt = pre._1
        val pre_min = pre._2
        val pre_dura = pre._3
        val pre_hour = pre._4

        val post_cnt = post._1
        val post_min = post._2
        val post_dura = post._3
        val post_hour = post._4
        (pre_cnt + "" + post_cnt, pre_min + "#" + post_min, pre_dura + "#" + post_dura, pre_hour + "#" + post_hour)
      })
      //(00:00:00:00:00:00	CCTV-13	2017-07-25	20,(1000101100,13#14#15#16#19#20#21#23#24#25,38#60#60#46#38#2#33#38#60#60))
      .flatMap(line => {

      val result = new scala.collection.mutable.ArrayBuffer[LiveTimeRange]()
      val info = line._1

      val info_cols = info.split("\t")
      val sn = info_cols(0)
      val channel = info_cols(1)
      val month = info_cols(2)

      val cnts = line._2._1
      val cnt_arr = line._2._1.toCharArray
      val min_arr = line._2._2.split("#")
      val dura_arr = line._2._3.split("#")
      val hour_arr = line._2._4.split("#")


      for (j <- 0 until hour_arr.length)
        if (!cnt_arr.contains('1')) {
          //没有1
          val startTime = hour_arr(j) + ":00:00"
          val endTime = hour_arr(j) + ":59:59"
          result += new LiveTimeRange(sn, channel, month, startTime, endTime)
        } else {
          val one_indexs = StringUtils.getIndexsOfCharOnStr(cnts, '1')
          for (i <- 0 until one_indexs.length) {
            if (i == (one_indexs.length - 1)) {
              //最后一个1
              val startTime = hour_arr(one_indexs(i)) + ":" + min_arr(one_indexs(i)) + ":" + dura_arr(one_indexs(i))

              //到结尾
              val endTime = hour_arr(cnt_arr.length - 1) + ":" + min_arr(cnt_arr.length - 1) + ":" + dura_arr(cnt_arr.length - 1)

              result += new LiveTimeRange(sn, channel, month, startTime, endTime)
            } else {
              val startTime = hour_arr(one_indexs(i)) + ":" + min_arr(one_indexs(i)) + ":" + dura_arr(one_indexs(i))
              //到下一个1
              val endTime = hour_arr(one_indexs(i + 1) - 1) + ":" + min_arr(one_indexs(i + 1) - 1) + ":" + dura_arr(one_indexs(i + 1) - 1)

              result += new LiveTimeRange(sn, channel, month, startTime, endTime)
            }

          }
        }
      result
    }).distinct()
      //TOD0 秒满60进1
      .map(line => {
      LiveTimeRange(line.sn, line.channel, line.month, TimeUtils.fullSecondAddMin(line.startTime), TimeUtils
        .fullSecondAddMin(line.endTime))
    })

//    baseRDD.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)

        baseRDD.saveAsTextFile("/user/hdfs/rsync/usertag/baseRDD"+System.currentTimeMillis())

//    //TODO 全时段的规则处理
//    //6.1	CCTV-12	全时段	社会与法频道	男
//    //2	6.1	CCTV-13	全时段	新闻频道	男
//    //3	6.1	CCTV-5	全时段	体育频道	男
//    val allTimeRangeRDD = baseRDD.filter(line => {
//      val ruler_channel_arr = Array[String](
//        "CCTV-12",
//        "CCTV-13",
//        "CCTV-5"
//      )
//      ruler_channel_arr.contains(line.channel)
//    })
//      //TODO 时长大于30分钟的行为
//      .filter(line => {
//      val starTime = TimeUtils.convertDateStr2TimeStamp(line.startTime, "HH:mm:ss")
//      val endTime = TimeUtils.convertDateStr2TimeStamp(line.endTime, "HH:mm:ss")
//      val dura = (endTime - starTime) / 1000 / 60
//      //      时长大于30分钟
//      dura > 30
//    })
//      //TODO  次数超过4次的行为
//      .map(line => {
//      val sn = line.sn
//      val month = line.month
//      ((sn, month), 1)
//    }).reduceByKey(_ + _)
//      .filter(line => {
//        line._2 > 4
//      })
//
//      //TODO 连续三个月
//      .map(line => {
//      val sn = line._1._1
//      val month = line._1._2
//      (sn, month)
//    }).distinct.map(line => {
//      val sn = line._1
//      (sn, 1)
//    }).reduceByKey(_ + _).filter(line => {
//      line._2 == 3
//    }).map(line => {
//      RulerResult(line._1, "6.1", 1)
//    })
//
//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/tmp/allTimeRangeRDD" + System.currentTimeMillis)

    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //    //TODO 直播性别规则表
    //    val ruler_map_live_arr = hiveContext.sql("select rulerid, channel, timerange from hr.user_tag_ruler_map_live " +
    //      "where rulerid = '6.1'")
    //      .collect
    //
    //    val ruler_map_live_arr_bv = sc.broadcast(ruler_map_live_arr)
    //
    //
    //    //TODO 不符合全时段规则的 按指定时段的规则处理
    //    val otherRDD = baseRDD.filter(line => {
    //
    //      //不在全时段的频道
    //      val ruler_channel_arr = Array[String](
    //        "CCTV-12",
    //        "CCTV-13",
    //        "CCTV-5"
    //      )
    //      val cond01 = !ruler_channel_arr.contains(line.channel)
    //
    //
    //      //时长大于30分钟
    //      val starTime = TimeUtils.convertDateStr2TimeStamp(line.startTime, "HH:mm:ss")
    //      val endTime = TimeUtils.convertDateStr2TimeStamp(line.endTime, "HH:mm:ss")
    //      val dura = (endTime - starTime) / 1000 / 60
    //      val cond02 = dura > 30
    //
    //
    //      //符合指定规则
    //      var cond03 = false
    //
    //      val rulerArr = ruler_map_live_arr_bv.value
    //      breakable(
    //        for (ele <- rulerArr) {
    //          val timeRange = ele.getString(2).split("-")
    //          val ruler_starTime = TimeUtils.convertDateStr2TimeStamp(timeRange(0), "HH:mm")
    //          val ruler_endTime = TimeUtils.convertDateStr2TimeStamp(timeRange(1), "HH:mm")
    //
    //          if (starTime >= ruler_starTime && endTime <= ruler_endTime) {
    //            cond03 = true
    //            break
    //          }
    //
    //        }
    //      )
    //      cond01 && cond02 && cond03
    //    }) //TODO  次数超过4次的行为
    //      .map(line => {
    //      val sn = line.sn
    //      val month = line.month
    //      ((sn, month), 1)
    //    }).reduceByKey(_ + _)
    //      .filter(line => {
    //        line._2 > 4
    //      })
    //
    //      //TODO 连续三个月
    //      .map(line => {
    //      val sn = line._1._1
    //      val month = line._1._2
    //      (sn, month)
    //    }).distinct.map(line => {
    //      val sn = line._1
    //      (sn, 1)
    //    }).reduceByKey(_ + _).filter(line => {
    //      line._2 == 3
    //    }).map(line => {
    //      RulerResult(line._1, "6.1", 1)
    //    })
    //
    //    //////////////////////////////////////////////////////////////////////////////////////////////////////////
    //    //不符合规则的sn
    //    val notRDD = baseRDD.map(line => {
    //      line.sn
    //    }).distinct.subtract(allTimeRangeRDD.map(_.sn).union(otherRDD.map(_.sn))).map(sn => {
    //      RulerResult(sn, "6.1", 0)
    //    })
    //
    //    notRDD.union(allTimeRangeRDD.union(otherRDD))
    //
    //      .map(line => {
    //        line.sn + "\t" + line.rulerId + "\t" + line.result
    //      })
    //      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)
    //
    //    baseRDD.unpersist(true)

    sc.stop


  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.common.util.StringUtils
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.avcdata.spark.job.until.TimeUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.control.Breaks._

/**
  * 6.1符合性别标签规则表-男
  * * 中的频道及对应的播出时间段内观看，且时长大于30分钟的次数超过4次
  *
  * ž 数据源：直播表、性别标签规则表-男，按频道、播出时间段两个字段匹配

  * ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
  */
object TagRuler6P1_hour {

  case class LiveTimeRange(sn: String, channel: String, month: String, startTime: String, endTime: String)

  def main(args: Array[String]) {

    val starTime = TimeUtils.convertDateStr2TimeStamp("14:00:00", "HH:mm:ss")
    val endTime = TimeUtils.convertDateStr2TimeStamp("14:30", "HH:mm")
    val dura = (endTime - starTime) / 1000 / 60
//
    println(dura)


    //    for(ele<-"1000101100".toCharArray){
    //      println(ele)
    //    }


    //    println("1000101100".toCharArray.contains('2'))

//    val conf = new SparkConf()
//      .setMaster("local[1]")
//      .setAppName("FamilySampleLiveApkTimeCnt")
//    val sc = new SparkContext(conf)
//
//    sc.textFile("E:\\aowei\\tracker-user\\data\\live-partition-sample.txt")
//      .map(line => {
//        val cols = line.split("\t")
//        val sn = cols(0)
//        val channel = cols(1)
//        val date = cols(2)
//        val hour = cols(3)
//        val min = cols(4)
//        val dura = cols(5)
//        val cnt = cols(6)
//
//        (sn + "\t" + channel + "\t" + date + "\t" + hour, (cnt, min, dura))
//
//      })
//      .sortByKey(true)
//      .reduceByKey((pre, post) => {
//        val pre_cnt = pre._1
//        val pre_min = pre._2
//        val pre_dura = pre._3
//
//        val post_cnt = post._1
//        val post_min = post._2
//        val post_dura = post._3
//        (pre_cnt + post_cnt, pre_min + "#" + post_min, pre_dura + "#" + post_dura)
//      })
//      //(00:00:00:00:00:00	CCTV-13	2017-07-25	20,(1000101100,13#14#15#16#19#20#21#23#24#25,38#60#60#46#38#2#33#38#60#60))
//      .flatMap(line => {
//
//      val result = new scala.collection.mutable.ArrayBuffer[LiveTimeRange]()
//      val info = line._1
//
//      val info_cols = info.split("\t")
//      val sn = info_cols(0)
//      val channel = info_cols(1)
//      val date = info_cols(2)
//      val hour = info_cols(3)
//
//      val pre_cnts = line._2._1
//      val pre_cnt_arr = line._2._1.toCharArray
//      val pre_min_arr = line._2._2.split("#")
//      val pre_dura_arr = line._2._3.split("#")
//
//      if (!pre_cnt_arr.contains('1')) {
//        //没有1
//        val startTime = TimeUtils.addZero(hour) + ":00:00"
//        val endTime = TimeUtils.addZero(hour) + ":59:59"
//        result += new LiveTimeRange(sn, channel, date, startTime, endTime)
//      } else {
//        val one_indexs = StringUtils.getIndexsOfCharOnStr(pre_cnts, '1')
//        for (i <- 0 until one_indexs.length) {
//          if (i == (one_indexs.length - 1)) {
//            //最后一个1
//            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
//
//            //到结尾
//            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(pre_cnt_arr.length - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(pre_cnt_arr.length - 1))
//
//            result += new LiveTimeRange(sn, channel, date, startTime, endTime)
//          } else {
//            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
//            //到下一个1
//            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i + 1) - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i + 1) - 1))
//
//            result += new LiveTimeRange(sn, channel, date, startTime, endTime)
//          }
//
//        }
//      }
//      result
//    })
//
//
//      .foreach(println(_))
//
//
//    //        run(sc, "2017-03-31", "31")
//    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()
    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .set("spark.scheduler.mode", "FAIR")
      //      .setIfMissing("spark.cleaner.ttl", "3600")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.codegen", "true")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.reducer.maxSizeInFlight", "96m")
      .set("spark.shuffle.io.maxRetries", "60")
      .set("spark.shuffle.io.retryWait", "60s")
      .set("spark.shffle.manager", "hash")
      .set("spark.shuffle.consolidateFiles", "true")
      //      .set("spark.default.parallelism", "1000")
      .setAppName(this.getClass.getSimpleName)
    //    --executor-cores 4
    //    --executor-memory 4g
    //    --num-executors 30

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")


    //    * 6.1符合性别标签规则表-男中的频道及对应的播出的开始时间段内观看，且时长大于30分钟的次数超过4次
    //    * ž 数据源：直播表、性别标签规则表-男，按频道、播出时间段两个字段匹配
    //    * ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件

    //TODO 月的观看行为
    val baseRDD = hiveContext.sql(
      """
          select
              dim_sn,concat(year(date),'-',month(date)) as month,dim_channel,dim_hour,dim_min,fact_time_length as dura,
              fact_cnt as cnt
            from hr.tracker_live_fact_partition
            where year(date)='2017' and month(date) in (4,5,6)
      """.stripMargin)
      .map(row => {
        val sn = row.getString(0)
        val month = row.getString(1)
        val dim_channel = row.getString(2)
        val dim_hour = row.getString(3)
        val dim_min = row.getString(4)
        val fact_time_length = row.getString(5)
        val fact_cnt = row.getString(6)

        (sn, month, dim_channel, dim_hour, dim_min, fact_time_length, fact_cnt)
      })

      //TODO 转化成时段数据
      .map(line => {
      //      (sn,month,dim_channel,dim_hour,dim_min,fact_time_length,fact_cnt)
      val sn = line._1
      val month = line._2
      val channel = line._3
      val hour = line._4
      val min = line._5
      val dura = line._6
      val cnt = line._7

      (sn + "\t" + channel + "\t" + month + "\t" + hour, (cnt, min, dura))
    }).sortByKey(true)
      .reduceByKey((pre, post) => {
        val pre_cnt = pre._1
        val pre_min = pre._2
        val pre_dura = pre._3

        val post_cnt = post._1
        val post_min = post._2
        val post_dura = post._3
        (pre_cnt +""+ post_cnt, pre_min + "#" + post_min, pre_dura + "#" + post_dura)
      })
      //(00:00:00:00:00:00	CCTV-13	2017-07-25	20,(1000101100,13#14#15#16#19#20#21#23#24#25,38#60#60#46#38#2#33#38#60#60))
      .flatMap(line => {

      val result = new scala.collection.mutable.ArrayBuffer[LiveTimeRange]()
      val info = line._1

      val info_cols = info.split("\t")
      val sn = info_cols(0)
      val channel = info_cols(1)
      val month = info_cols(2)
      val hour = info_cols(3)

      val pre_cnts = line._2._1
      val pre_cnt_arr = line._2._1.toCharArray
      val pre_min_arr = line._2._2.split("#")
      val pre_dura_arr = line._2._3.split("#")

      if (!pre_cnt_arr.contains('1')) {
        //没有1
        val startTime = TimeUtils.addZero(hour) + ":00:00"
        val endTime = TimeUtils.addZero(hour) + ":59:59"
        result += new LiveTimeRange(sn, channel, month, startTime, endTime)
      } else {
        val one_indexs = StringUtils.getIndexsOfCharOnStr(pre_cnts, '1')
        for (i <- 0 until one_indexs.length) {
          if (i == (one_indexs.length - 1)) {
            //最后一个1
            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))

            //到结尾
            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(pre_cnt_arr.length - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(pre_cnt_arr.length - 1))

            result += new LiveTimeRange(sn, channel, month, startTime, endTime)
          } else {
            val startTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i))) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i)))
            //到下一个1
            val endTime = TimeUtils.addZero(hour) + ":" + TimeUtils.addZero(pre_min_arr(one_indexs(i + 1) - 1)) + ":" + TimeUtils.addZero(pre_dura_arr(one_indexs(i + 1) - 1))

            result += new LiveTimeRange(sn, channel, month, startTime, endTime)
          }

        }
      }
      result
    })

    baseRDD.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)

    //TODO 全时段的规则处理
    //6.1	CCTV-12	全时段	社会与法频道	男
    //2	6.1	CCTV-13	全时段	新闻频道	男
    //3	6.1	CCTV-5	全时段	体育频道	男
    val allTimeRangeRDD = baseRDD.filter(line => {
      val ruler_channel_arr = Array[String](
        "CCTV-12",
        "CCTV-13",
        "CCTV-5"
      )
      ruler_channel_arr.contains(line.channel)
    })
      //TODO 时长大于30分钟的行为
      .filter(line => {
      val starTime = TimeUtils.convertDateStr2TimeStamp(line.startTime, "HH:mm:ss")
      val endTime = TimeUtils.convertDateStr2TimeStamp(line.endTime, "HH:mm:ss")
      val dura = (endTime - starTime) / 1000 / 60
      //      时长大于30分钟
      dura > 30
    })
      //TODO  次数超过4次的行为
      .map(line => {
      val sn = line.sn
      val month = line.month
      ((sn, month), 1)
    }).reduceByKey(_ + _)
      .filter(line => {
        line._2 > 4
      })

      //TODO 连续三个月
      .map(line => {
      val sn = line._1._1
      val month = line._1._2
      (sn, month)
    }).distinct.map(line => {
      val sn = line._1
      (sn, 1)
    }).reduceByKey(_ + _).filter(line => {
      line._2 == 3
    }).map(line => {
      RulerResult(line._1, "6.1", 1)
    })

    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //TODO 直播性别规则表
    val ruler_map_live_arr = hiveContext.sql("select rulerid, channel, timerange from hr.user_tag_ruler_map_live " +
      "where rulerid = '6.1'")
      .collect

    val ruler_map_live_arr_bv = sc.broadcast(ruler_map_live_arr)


    //TODO 不符合全时段规则的 按指定时段的规则处理
    val otherRDD = baseRDD.filter(line => {

      //不在全时段的频道
      val ruler_channel_arr = Array[String](
        "CCTV-12",
        "CCTV-13",
        "CCTV-5"
      )
      val cond01 = !ruler_channel_arr.contains(line.channel)


      //时长大于30分钟
      val starTime = TimeUtils.convertDateStr2TimeStamp(line.startTime, "HH:mm:ss")
      val endTime = TimeUtils.convertDateStr2TimeStamp(line.endTime, "HH:mm:ss")
      val dura = (endTime - starTime) / 1000 / 60
      val cond02 = dura > 30


      //符合指定规则
      var cond03 = false

      val rulerArr = ruler_map_live_arr_bv.value
      breakable(
        for (ele <- rulerArr) {
          val timeRange = ele.getString(2).split("-")
          val ruler_starTime = TimeUtils.convertDateStr2TimeStamp(timeRange(0), "HH:mm")
          val ruler_endTime = TimeUtils.convertDateStr2TimeStamp(timeRange(1), "HH:mm")

          if (starTime >= ruler_starTime && endTime <= ruler_endTime) {
            cond03 = true
            break
          }

        }
      )


      cond01 && cond02 && cond03
    }) //TODO  次数超过4次的行为
      .map(line => {
      val sn = line.sn
      val month = line.month
      ((sn, month), 1)
    }).reduceByKey(_ + _)
      .filter(line => {
        line._2 > 4
      })

      //TODO 连续三个月
      .map(line => {
      val sn = line._1._1
      val month = line._1._2
      (sn, month)
    }).distinct.map(line => {
      val sn = line._1
      (sn, 1)
    }).reduceByKey(_ + _).filter(line => {
      line._2 == 3
    }).map(line => {
      RulerResult(line._1, "6.1", 1)
    })

    //////////////////////////////////////////////////////////////////////////////////////////////////////////
    //不符合规则的sn
    val notRDD = baseRDD.map(line => {
      line.sn
    }).distinct.subtract(allTimeRangeRDD.map(_.sn).union(otherRDD.map(_.sn))).map(sn => {
      RulerResult(sn, "6.1", 0)
    })

    notRDD.union(allTimeRangeRDD.union(otherRDD))

      .map(line => {
        line.sn + "\t" + line.rulerId + "\t" + line.result
      })
      .saveAsTextFile("/user/hdfs/rsync/usertag/" + this.getClass.getSimpleName + System.currentTimeMillis)

    baseRDD.unpersist(true)

    sc.stop


  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}


/**
  * 6.2观看性别标签规则表-男中的剧情类型节目，且时长大于30分钟的次数超过4次
ž 数据源：点播表、性别标签规则表-男，按剧情类型字段匹配
ž 判断标准：每个自然月内，观看该类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件

  用到的表
  tracker_player_fact_ch


key	dim_sn
dim_name
dim_title
dim_awcid
dim_part
dim_hour
fact_vv
fact_duration
dim_year
dim_model
dim_crowd
dim_region
date

user_tag_ruler_map_plot
rulerid
plot
tag


  */
object TagRuler6P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")

    //TODO 性别标签规则表
    val ruler_map_plot_arr = hiveContext.sql("select plot from hr.user_tag_ruler_map_plot " +
      "where rulerid = '6.2'").collect

    val ruler_map_plot_arr_bv = sc.broadcast(ruler_map_plot_arr)

    hiveContext.sql(
      """
          select
              dim_sn,concat(year(date),'-',month(date)) as month,dim_channel,dim_hour,dim_min,fact_time_length as dura,
              fact_cnt as cnt
            from hr.tracker_player_fact_ch
            where year(date)='2017' and month(date) in (4,5,6)
      """.stripMargin)
      .map(row => {
        val sn = row.getString(0)
        val month = row.getString(1)
        val dim_channel = row.getString(2)
        val dim_hour = row.getString(3)
        val dim_min = row.getString(4)
        val fact_time_length = row.getString(5)
        val fact_cnt = row.getString(6)

        (sn, month, dim_channel, dim_hour, dim_min, fact_time_length, fact_cnt)
      })



    //      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

    sc.stop


  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}


/**
  * 7.1符合性别标签规则表-女中的频道及对应的播出时间段内观看，且时长大于30分钟的次数超过4次
ž 数据源：直播表、性别标签规则表-女：按频道、播出时间段两个字段匹配
ž 判断标准：每个自然月内，该频道该播出时间内的使用时长大于30分钟，且次数>4次，且4-6月每个月连续符合以上条件
  */
object TagRuler7P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//
    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}


/**
  * 7.2观看性别标签规则表-女中的剧情类型节目，且时长大于30分钟的次数超过4次
ž 数据源：点播表、性别标签规则表-女，按剧情类型字段匹配
ž 判断标准：每个自然月内，观看该类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
  */
object TagRuler7P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//
    sc.stop

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
8观看年龄标签规则表-P0-3中剧情类型节目，且时长大于30分钟的次数超过4次
ž 数据源：点播表、年龄标签规则表-P0-3，按剧情类型字段匹配
ž 判断标准：每个自然月内，观看该类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler8 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

    sc.stop
  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
9.1在06:00-22:30时间段内观看CCTV-14频道的时长大于30分钟且次数大于4次
ž 数据源：直播表，取频道、播出时间段两个字段匹配
ž 时间段：06:00-22:30、CCTV-14频道
ž 判断标准：每个自然月内，在06:00-22:30时间段内收看CCTV-14频道的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler9P1 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")




//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/*
9.2观看年龄标签规则表-P4-14中剧情类型节目，且观看时长大于30分钟的次数超过4次
ž 数据源：点播表、年龄标签规则表-P4-14，按剧情类型字段匹配
ž 判断标准：每个自然月内，收看此剧情类型节目的时长大于30分钟且次数>4次，且4-6月每个月连续符合以上条件
 */
object TagRuler9P2 {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRulerExportAll {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

      .setAppName(this.getClass.getSimpleName)

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)
    import  hiveContext.implicits._;

    hiveContext.sql("use hr")


    sc.textFile("/user/hdfs/rsync/usertag/*").distinct.map(line=>{
      val cols = line.split("\t")
      val sn = cols(0)
      val rulerId = cols(1)
      val result = cols(2).toInt

      RulerResult(sn,rulerId,result)
    })

      .toDF.write.mode(SaveMode.Overwrite).saveAsTable("hr.user_tag_ruler_result")



    //      .map(line => {
    //        line.sn + "\t" + line.rulerId + "\t" + line.result
    //      })
    //      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
    //

  }
}package com.avcdata.spark.job.tag.evaluate2

import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.tag.evaluate2.TagRuler1P1.RulerResult
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TagRulerTemplate {
  def main(args: Array[String]) {

    //    val test = "haha/heihei/".split("/")
    //    println(test.length)
    //    for (i <- 0 until test.length - 1) {
    //      println(test(i))
    //    }

    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("FamilySampleLiveApkTimeCnt")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31", "31")
    //    sc.stop()

  }


  //RDD[(String, String)]
  def run(analysisDate: String, recentDaysNum: String) = {

    //默认配置文件读取
    val config: Config = ConfigFactory.load()

    val sparkConf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))
      .setIfMissing("redis.host", config.getString("redis.host"))
      .setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("es.nodes", config.getString("es.nodes"))
      .setIfMissing("es.port", config.getString("es.port")).setIfMissing("redis.port", config.getString("redis.port"))
      .setIfMissing("mysql.host", config.getString("mysql.host"))
      .setIfMissing("mysql.user", config.getString("mysql.user"))
      .setIfMissing("mysql.password", config.getString("mysql.password"))
      .setIfMissing("mysql.db", config.getString("mysql.db"))
      .setIfMissing("hbase.zookeeper.quorum", config.getString("hbase.zookeeper.quorum"))
      .setIfMissing("spark.cleaner.ttl", "3600")

    sparkConf.setAppName("TagRuler1P3")

    val sc = new SparkContext(sparkConf)

    val hiveContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(hiveContext, "dayOfWeek")
    hiveContext.sql("use hr")



//      .map(line => {
//        line.sn + "\t" + line.rulerId + "\t" + line.result
//      })
//      .saveAsTextFile("/user/hdfs/rsync/usertag/TagRuler1P3" + System.currentTimeMillis)
//

  }
}package com.avcdata.spark.job.pm

import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 提取各个终端apk的分布情况
  */
object TerminalApkDistJob {

  val log = Logger.getLogger(getClass.getName)


  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("TerminalApkDistJob")
    val sc = new SparkContext(conf)
    run(sc, "2017-03")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentMonth: String) = {

    val initRDD = sc.textFile("S:\\aowei\\tracker-job\\doc\\data\\tracker_apk_fact_partition").distinct


    initRDD.
      //TODO 过滤不需要统计的apkName
      filter(line => {
      val cols = line.split('\t')
      val sn = cols(1)
      val apk = cols(2)

      val apkArr = Array[String](
        "电视猫视频", "银河·奇异果", "腾讯视频TV端", "CIBN环球影视", "芒果TV", "CIBN微视听",
        "云视听·泰捷", "CIBN聚精彩", "PPTV聚体育", "CIBN悦厅TV")
      apkArr.contains(apk)
    })
      //TODO 获取指定字段 并给应用打上是否使用的标记
      .map(line => {
      val cols = line.split('\t')

      val month = currentMonth
      val sn = cols(1)
      val apk = cols(2)

      (sn, apk)
    }).distinct().reduceByKey((pre, post) => {
      if(pre.compareTo(post)>0){
        pre + "\t" + post
      }else{
        post + "\t" + pre
      }

    }).map(line => {
      (line._2, 1)
    }).reduceByKey(_ + _)
      .map(line => {
        currentMonth + "\t" + "北京市" + "\t" + line._1 + "\t" + line._2
      })

      //      .map(line => {
      //      val cols = line._2.split("\t")
      //      currentMonth + "\t" + "北京市" + "\t" + line._1 + "\t" + line._2 + "\t" + cols.length
      //    })


        .repartition(1).saveAsTextFile("D:\\apk_overlap")



    val hiveContext: HiveContext = new HiveContext(sc)


    //    //加载数据到hive分区表
    //    sqlContext.sql("set hive.exec.dynamic.partition=true")
    //    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //    //重复的不会覆盖
    //    sqlContext.sql("INSERT OVERWRITE table hr.terminal_partition partition(br) SELECT key,sn,brand,last_poweron,area,province,city,citylevel,size,model,license,brand as br FROM hr.terminal ")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TerminalBrandCnt {

  case class TerminalBrandDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   brand: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("TerminalBrandCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snBrandDistRDD = sqlContext.sql("select sn,stat_date,period,brand from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val brand_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .TERMINAL_BRAND_ARR)

      (sn + "\t" + stat_date + "\t" + period, brand_map)

    })

    val TerminalBrandDistCntDF = snCidRDD.join(snBrandDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val brand_map = line._2._2

      val resultArr = new Array[TerminalBrandDistCnt](brand_map.size)

      val brand_arr = brand_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalBrandDistCnt(
          sn,
          stat_date,
          period,
          brand_arr(i),
          brand_map.get(brand_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalBrandDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalBrandDistCntDF.registerTempTable("TerminalBrandDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.brand,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,brand,sum(cnt) as cnt  from TerminalBrandDistCnt group by stat_date,period,cluster_id,brand
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalBrandDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_brand_dist_cnt_k" + clusterNum, false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.common.Codearea
import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 酷开终端数据清洗
  */
object TerminalDataLoad2SampleTerminalTwo {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate)

    //    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate + "/aowei_deviceinfo" + analysisDate + ".txt")


    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      ValidateUtils.isNumber(cols(5)) && cols(5).toInt > 0
    })


      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("sample_terminal_two")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = cols(6)

          var province = cols(1)

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""


          //型号
          val model = cols(3) + "_" + cols(4)
          val size = cols(5)
          val city = cols(2)

          val sn = cols(0)

          //大区
          val area = Codearea.getArea(province)

          //城市级别
          val citylevel = Codearea.getCl(city)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )

    val sqlContext: HiveContext = new HiveContext(sc)

    //加载数据到hive分区表
//    sqlContext.sql("set hive.exec.dynamic.partition=true")
//    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
//
//    sqlContext.sql("INSERT OVERWRITE table hr.terminal_partitionz partition(br) SELECT key,sn,brand,last_poweron,area,province,city,citylevel,size,model,license,brand as br FROM hr.terminal ")

//////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.konka

import com.avcdata.spark.job.common.Codearea
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by wxy on 8/29/16.
  * 终端处理
  */
object TerminalDataLoadJob {
    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("terminalProperty")
        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimBrandCol = Bytes.toBytes("brand")
        val dimLicenseCol = Bytes.toBytes("license")
        val dimLastPowerOnCol = Bytes.toBytes("last_poweron")
        val dimAreaCol = Bytes.toBytes("area")
        val dimProvinceCol = Bytes.toBytes("province")
        val dimCityCol = Bytes.toBytes("city")
        val dimCitylevelCol = Bytes.toBytes("citylevel")
        val dimSizeCol = Bytes.toBytes("size")
        val dimModelCol = Bytes.toBytes("model")

        val sqlc = new HiveContext(sc)
        val snBrand = sqlc.sql("select brand,model,license from hr.ko_mp_brand").mapPartitions(items => {
            items.map(line => {
                (line(0).toString+"0x01"+line(1).toString, line(2).toString)
            })
        })

        //sc.textFile("F:/avc/docs/konka/duration.log.2017-03-29")
        val koRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/duration.log." + analysisDate)
            .filter(x => x.split('|').length > 7).mapPartitions(items =>{
            items.map(item => {
                val cols = item.split('|')
                val sn = cols(0)
                val brand = "KO"
                val model = cols(2)
                val lastPowerOn = cols(6)
                val size = cols(4)
                //val area = cols(5).substring(0, 3)
                val area = Codearea.getPc(cols(5)).split(",")(0)
                val province = Codearea.getPc(cols(5)).split(",")(1)
                val city = Codearea.getPc(cols(5)).split(",")(2)
                val clevel = Codearea.getCl(city)
                (brand+"0x01"+model, item)
            })
        })

        val reRdd = koRdd.leftOuterJoin(snBrand).filter(x => !x._2.toString.contains("None")).mapPartitions(items => {
            items.map(item => {
                (item._2._2.get, item._2._1)
            })
        })
        //sc.textFile("/user/hdfs/rsync/KONKA/history/duration.log.since_10-01")
        reRdd.foreachPartition(items => {
                val myConf = HBaseConfiguration.create()
                myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
                myConf.set("hbase.zookeeper.property.clientPort", "2181")
                val hbaseConn = ConnectionFactory.createConnection(myConf)
                val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal"))
                //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_terminal"))

                try {
                    items.foreach(line => {
                        val cols = line._2.split('|')
                        val sn = cols(0)
                        val brand = "KO"
                        val model = cols(2)
                        val lastPowerOn = cols(6)
                        val size = cols(4)
                        //val area = cols(5).substring(0, 3)
                        val area = Codearea.getPc(cols(5)).split(",")(0)
                        val province = Codearea.getPc(cols(5)).split(",")(1)
                        val city = Codearea.getPc(cols(5)).split(",")(2)
                        val clevel = Codearea.getCl(city)
                        val license = line._1

                        val put = new Put(Bytes.toBytes(sn + "KO"))
                        put.addColumn(dimFamilyCol, dimBrandCol, Bytes.toBytes(brand))
                        put.addColumn(dimFamilyCol, dimLicenseCol, Bytes.toBytes(license))
                        put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                        put.addColumn(dimFamilyCol, dimModelCol, Bytes.toBytes(model))
                        put.addColumn(dimFamilyCol, dimLastPowerOnCol, Bytes.toBytes(lastPowerOn))
                        put.addColumn(dimFamilyCol, dimSizeCol, Bytes.toBytes(size))
                        put.addColumn(dimFamilyCol, dimProvinceCol, Bytes.toBytes(province))
                        put.addColumn(dimFamilyCol, dimCityCol, Bytes.toBytes(city))
                        put.addColumn(dimFamilyCol, dimAreaCol, Bytes.toBytes(area))
                        put.addColumn(dimFamilyCol, dimCitylevelCol, Bytes.toBytes(clevel))
                        //println(cols(5) + "\t" + area + "\t" + province + "\t" + city)
                        mutator.mutate(put)
                    })
                    mutator.flush()
                } finally {
                    mutator.close()
                    hbaseConn.close()
                }
            })
    }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.common.Codearea
import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.control.Breaks._

/**
  * @author zhangyongtian
  * @define 酷开终端数据清洗
  * @deprecated 2017-07-05
  */
object TerminalDataLoadJob {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val provinArr = Array[String](
      "河北省", "山西省", "辽宁省", "吉林省", "黑龙江省", "江苏省",
      "浙江省", "安徽省", "福建省", "江西省", "山东省", "河南省",
      "湖北省", "湖南省", "广东省", "海南省", "四川省", "贵州省",
      "云南省", "陕西省", "甘肃省", "青海省", "台湾省",
      "内蒙古自治区", "广西壮族自治区", "西藏自治区", "宁夏回族自治区",
      "新疆维吾尔自治区", "北京市", "天津市", "上海市",
      "重庆市", "香港特别行政区", "澳门特别行政区"
    )

    val htableName = "tracker_terminal"
    //    val htableName = "sample_terminal_three"
    println(htableName)

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate)

    //    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate + "/aowei_deviceinfo" + analysisDate + ".txt")

    //TODO 城市级别映射表
    val hiveContext = new HiveContext(sc)

    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    val pccArrBroadcast = sc.broadcast(pccArr)


    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      var province = cols(1)

      ValidateUtils.isNumber(cols(5)) && cols(5).toInt > 0
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator(htableName)

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = cols(6)

          var province = cols(1)


          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""

          //型号
          val model = cols(3) + "_" + cols(4)
          val size = cols(5)
          val city = cols(2)
          val sn = cols(0)

          //大区
          val area = Codearea.getArea(province)

          //城市级别
          //          val citylevel = Codearea.getCl(city)

          val arr = Array[Int](1, 2, 3, 4, 5, 6, 7)

          for (i <- 0 until arr.length) {
            breakable(
              if (arr(i) == 3) {
                break
              }
            )
          }


          //TODO 根据映射表获取城市级别
          var citylevel = "港澳台及国外"
          val pccArrValue = pccArrBroadcast.value
          breakable(
            for (i <- 0 until pccArrValue.length) {
              if (city.equals(pccArrValue(i).getString(1))) {
                citylevel = pccArrValue(i).getString(2)
                break
              }
            }
          )
          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )


    //    val sqlContext: HiveContext = new HiveContext(sc)

    //    //加载数据到hive分区表
    //    sqlContext.sql("set hive.exec.dynamic.partition=true")
    //    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //    //重复的不会覆盖
    //    sqlContext.sql("INSERT OVERWRITE table hr.terminal_partition partition(br) SELECT key,sn,brand,last_poweron,area,province,city,citylevel,size,model,license,brand as br FROM hr.terminal ")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.common.Codearea
import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.control.Breaks._

/**
  * @author zhangyongtian
  * @define 酷开终端数据清洗
  * @deprecated 2017-07-05
  */
object TerminalDataLoadJob01 {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val provinArr = Array[String](
      "河北省", "山西省", "辽宁省", "吉林省", "黑龙江省", "江苏省",
      "浙江省", "安徽省", "福建省", "江西省", "山东省", "河南省",
      "湖北省", "湖南省", "广东省", "海南省", "四川省", "贵州省",
      "云南省", "陕西省", "甘肃省", "青海省", "台湾省",
      "内蒙古自治区", "广西壮族自治区", "西藏自治区", "宁夏回族自治区",
      "新疆维吾尔自治区", "北京市", "天津市", "上海市",
      "重庆市", "香港特别行政区", "澳门特别行政区"
    )

    val htableName = "tracker_terminal"
    //    val htableName = "sample_terminal_three"
    println(htableName)

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate)

    //    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate + "/aowei_deviceinfo" + analysisDate + ".txt")

    //TODO 城市级别映射表
    val hiveContext = new HiveContext(sc)

    val pccArr = hiveContext.sql("select province,cityname,citylevelv from hr.citystlevel").rdd.collect()
    val pccArrBroadcast = sc.broadcast(pccArr)

    //过滤
    initRDD.filter(line => {

      val cols = line.split('\t')

      var province = cols(1)

      //TODO 过滤非31省份
      var isProvinceRight = false
      val pccArrValue = pccArrBroadcast.value

      breakable(
        for (i <- 0 until pccArrValue.length) {
          if (province.equals(pccArrValue(i).getString(0))) {
            isProvinceRight = true
            break;
          }
        }
      )


      ValidateUtils.isNumber(cols(5)) && cols(5).toInt > 0 && isProvinceRight
    })

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator(htableName)

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          //牌照
          val license = cols(6)

          var province = cols(1)


          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""

          //型号
          val model = cols(3) + "_" + cols(4)
          val size = cols(5)
          val city = cols(2)
          val sn = cols(0)

          //大区
          val area = Codearea.getArea(province)

          //城市级别
//          val citylevel = Codearea.getCl(city)

          var citylevel = "港澳台及国外"
          val pccArrValue = pccArrBroadcast.value
          breakable(
            for (i <- 0 until pccArrValue.length) {
              if (city.equals(pccArrValue(i).getString(1))) {
                citylevel = pccArrValue(i).getString(2)
                break;
              }
            }
          )



          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal("CC", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )
    

    //    val sqlContext: HiveContext = new HiveContext(sc)

    //    //加载数据到hive分区表
    //    sqlContext.sql("set hive.exec.dynamic.partition=true")
    //    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //    //重复的不会覆盖
    //    sqlContext.sql("INSERT OVERWRITE table hr.terminal_partition partition(br) SELECT key,sn,brand,last_poweron,area,province,city,citylevel,size,model,license,brand as br FROM hr.terminal ")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.common.Codearea
import com.avcdata.spark.job.until.{TimeUtils, ValidateUtils}
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.control.Breaks._

/**
  * @author zhangyongtian
  * @define 酷开终端数据清洗
  */
object TerminalDataLoadJob02 {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val provinArr = Array[String](
      "河北省", "山西省", "辽宁省", "吉林省", "黑龙江省", "江苏省",
      "浙江省", "安徽省", "福建省", "江西省", "山东省", "河南省",
      "湖北省", "湖南省", "广东省", "海南省", "四川省", "贵州省",
      "云南省", "陕西省", "甘肃省", "青海省", "台湾省",
      "内蒙古自治区", "广西壮族自治区", "西藏自治区", "宁夏回族自治区",
      "新疆维吾尔自治区", "北京市", "天津市", "上海市",
      "重庆市", "香港特别行政区", "澳门特别行政区"
    )

    val htableName = "tracker_terminal"
    //    val htableName = "sample_terminal_three"
    println(htableName)

    val analysisDate = TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(currentDate, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)


    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate)

    //    val initRDD = sc.textFile("/user/hdfs/rsync/COOCAA/" + analysisDate + "/aowei_deviceinfo" + analysisDate + "/aowei_deviceinfo" + analysisDate + ".txt")

    //TODO 城市级别映射表
    val hiveContext = new HiveContext(sc)

    val pccArr = hiveContext.sql("select province,cityname,citylevel from hr.citystlevel").rdd.collect()
    val pccArrBroadcast = sc.broadcast(pccArr)

    //过滤
    val filteredRDD = initRDD.filter(line => {

      val cols = line.split('\t')

      var province = cols(1)

      //TODO 过滤非31省份
//      var isProvinceRight = false
//      val pccArrValue = pccArrBroadcast.value
//
//      breakable(
//        for (i <- 0 until pccArrValue.length) {
//          if (province.equals(pccArrValue(i).getString(0))) {
//            isProvinceRight = true
//            break;
//          }
//        }
//      )

      ValidateUtils.isNumber(cols(5)) && cols(5).toInt > 0
    })

    val resultRDD = filteredRDD.map(line => {
      val cols = line.split('\t')

      //牌照
      val license = cols(6)

      var province = cols(1)


      if (province.equals("未匹配")) {
        province = "其他"
      }

      //激活时间
      val last_poweron = ""

      //型号
      val model = cols(3) + "_" + cols(4)
      val size = cols(5)
      val city = cols(2)
      val sn = cols(0)

      //大区
      val area = Codearea.getArea(province)

      //TODO 根据映射表获取城市级别
      //          val citylevel = Codearea.getCl(city)
      var citylevel = "港澳台及国外"
      val pccArrValue = pccArrBroadcast.value
      breakable(
        for (i <- 0 until pccArrValue.length) {
          if (city.equals(pccArrValue(i).getString(1))) {
            citylevel = pccArrValue(i).getString(2)
            break
          }
        }
      )
      val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

      orderedLine
    })

    resultRDD.saveAsTextFile("/tmp/terminal-out"+System.currentTimeMillis)

    pccArrBroadcast.destroy

    //    val sqlContext: HiveContext = new HiveContext(sc)

    //    //加载数据到hive分区表
    //    sqlContext.sql("set hive.exec.dynamic.partition=true")
    //    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //    //重复的不会覆盖
    //    sqlContext.sql("INSERT OVERWRITE table hr.terminal_partition partition(br) SELECT key,sn,brand,last_poweron,area,province,city,citylevel,size,model,license,brand as br FROM hr.terminal ")

    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 给Terminal补充尺寸信息
  */
object TerminalDataLoadJobSize {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val sqlContext = new HiveContext(sc)

    //TODO 获取terminal数据
    val terminalPair = sqlContext.sql("select sn, brand,last_poweron,area,province,city,citylevel,size,model,license from hr.terminal")
      .rdd.map(line => {
      val sn = line(0)
      val brand = line(1).toString.trim
      val last_poweron = line(2)
      val area = line(3)
      val province = line(4)
      val city = line(5)
      val citylevel = line(6)
      val size = line(7)
      val model = line(8).toString.trim
      val license = line(9)

      (brand + "\t" + model, (sn, brand, last_poweron, area, province, city, citylevel, size, model, license))
    })

    //TODO 获取价格匹配数据
    val pricePair = sc.textFile("/user/hdfs/rsync/userdata/terminal-price.csv").map(line => {
      val cols = line.split(",")
      val brand = cols(0).trim
      val model = cols(1).trim
      val size = cols(2)
      val price = cols(3)
      (brand + "\t" + model, size)
    })

    terminalPair.join(pricePair).map(line => {
      val terminalTuple = line._2._1

      val sn = terminalTuple._1
      val brand = terminalTuple._2
      val last_poweron = terminalTuple._3
      val area = terminalTuple._4
      val province = terminalTuple._5
      val city = terminalTuple._6
      val citylevel = terminalTuple._7
      val size = line._2._2
      val model = terminalTuple._9
      val license = terminalTuple._10

      license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel + "\t" + brand

    })

      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          val license = cols(0)
          val province = cols(1)
          val last_poweron = cols(2)
          val model = cols(3)
          val size = cols(4)
          val city = cols(5)
          val sn = cols(6)
          val area = cols(7)
          val citylevel = cols(8)
          val brand = cols(9)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal(brand, orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

    /////////////////////////sample_terminal_three/////////////////////////////////


    //TODO 获取terminal数据
    val terminalPair2 = sqlContext.sql("select sn, brand,last_poweron,area,province,city,citylevel,size,model,license from hr.sample_terminal_three")
      .rdd.map(line => {
      val sn = line(0)
      val brand = line(1).toString.trim
      val last_poweron = line(2)
      val area = line(3)
      val province = line(4)
      val city = line(5)
      val citylevel = line(6)
      val size = line(7)
      val model = line(8).toString.trim
      val license = line(9)

      (brand + "\t" + model, (sn, brand, last_poweron, area, province, city, citylevel, size, model, license))
    })

    //TODO 获取价格匹配数据
    val pricePair2 = sc.textFile("/user/hdfs/rsync/userdata/terminal-price.csv").map(line => {
      val cols = line.split(",")
      val brand = cols(0).trim
      val model = cols(1).trim
      val size = cols(2)
      val price = cols(3)
      (brand + "\t" + model, size)
    })

    terminalPair2.join(pricePair2).map(line => {
      val terminalTuple = line._2._1

      val sn = terminalTuple._1
      val brand = terminalTuple._2
      val last_poweron = terminalTuple._3
      val area = terminalTuple._4
      val province = terminalTuple._5
      val city = terminalTuple._6
      val citylevel = terminalTuple._7
      val size = line._2._2
      val model = terminalTuple._9
      val license = terminalTuple._10

      license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel + "\t" + brand

    })

      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("sample_terminal_three")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          val license = cols(0)
          val province = cols(1)
          val last_poweron = cols(2)
          val model = cols(3)
          val size = cols(4)
          val city = cols(5)
          val sn = cols(6)
          val area = cols(7)
          val citylevel = cols(8)
          val brand = cols(9)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal(brand, orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )






    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.vbox.other

import com.avcdata.vbox.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define 给Terminal补充尺寸信息
  */
object TerminalDataLoadJobSize {

  val log = Logger.getLogger(getClass.getName)

  ///////////////////////////////test////////////////////////////////
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-TerminalDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-15")

    sc.stop()

  }

  ///////////////////////////////test////////////////////////////////

  def run(sc: SparkContext, currentDate: String) = {

    val sqlContext = new HiveContext(sc)

    //TODO 获取terminal数据
    val terminalPair = sqlContext.sql("select sn, brand,last_poweron,area,province,city,citylevel,size,model,license from hr.terminal")
      .rdd.map(line => {
      val sn = line(0)
      val brand = line(1).toString.trim
      val last_poweron = line(2)
      val area = line(3)
      val province = line(4)
      val city = line(5)
      val citylevel = line(6)
      val size = line(7)
      val model = line(8).toString.trim
      val license = line(9)

      (brand + "\t" + model, (sn, brand, last_poweron, area, province, city, citylevel, size, model, license))
    })

    //TODO 获取价格匹配数据
    val pricePair = sc.textFile("/user/hdfs/rsync/userdata/terminal-price.csv").map(line => {
      val cols = line.split(",")
      val brand = cols(0).trim
      val model = cols(1).trim
      val size = cols(2)
      val price = cols(3)
      (brand + "\t" + model, size)
    })

    terminalPair.join(pricePair).map(line => {
      val terminalTuple = line._2._1

      val sn = terminalTuple._1
      val brand = terminalTuple._2
      val last_poweron = terminalTuple._3
      val area = terminalTuple._4
      val province = terminalTuple._5
      val city = terminalTuple._6
      val citylevel = terminalTuple._7
      val size = line._2._2
      val model = terminalTuple._9
      val license = terminalTuple._10

      license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel + "\t" + brand

    })

      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("tracker_terminal")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          val license = cols(0)
          val province = cols(1)
          val last_poweron = cols(2)
          val model = cols(3)
          val size = cols(4)
          val city = cols(5)
          val sn = cols(6)
          val area = cols(7)
          val citylevel = cols(8)
          val brand = cols(9)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal(brand, orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )

    /////////////////////////sample_terminal_three/////////////////////////////////


    //TODO 获取terminal数据
    val terminalPair2 = sqlContext.sql("select sn, brand,last_poweron,area,province,city,citylevel,size,model,license from hr.sample_terminal_three")
      .rdd.map(line => {
      val sn = line(0)
      val brand = line(1).toString.trim
      val last_poweron = line(2)
      val area = line(3)
      val province = line(4)
      val city = line(5)
      val citylevel = line(6)
      val size = line(7)
      val model = line(8).toString.trim
      val license = line(9)

      (brand + "\t" + model, (sn, brand, last_poweron, area, province, city, citylevel, size, model, license))
    })

    //TODO 获取价格匹配数据
    val pricePair2 = sc.textFile("/user/hdfs/rsync/userdata/terminal-price.csv").map(line => {
      val cols = line.split(",")
      val brand = cols(0).trim
      val model = cols(1).trim
      val size = cols(2)
      val price = cols(3)
      (brand + "\t" + model, size)
    })

    terminalPair2.join(pricePair2).map(line => {
      val terminalTuple = line._2._1

      val sn = terminalTuple._1
      val brand = terminalTuple._2
      val last_poweron = terminalTuple._3
      val area = terminalTuple._4
      val province = terminalTuple._5
      val city = terminalTuple._6
      val citylevel = terminalTuple._7
      val size = line._2._2
      val model = terminalTuple._9
      val license = terminalTuple._10

      license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel + "\t" + brand

    })

      //TODO 写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("sample_terminal_three")

      try {

        items.foreach(line => {
          val cols = line.split('\t')

          val license = cols(0)
          val province = cols(1)
          val last_poweron = cols(2)
          val model = cols(3)
          val size = cols(4)
          val city = cols(5)
          val sn = cols(6)
          val area = cols(7)
          val citylevel = cols(8)
          val brand = cols(9)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel

          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_terminal(brand, orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }
    )






    //////////////////////////////////////////////方法结束///////////////////////////////////////////////////
  }
}
package com.avcdata.spark.job.coocaa

import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * @author zhangyongtian
  * @define terminal COOCAA样本库导入 一个月
  */
object TerminalPartition2SampleTerminalTwoLoadJob {
  val log = Logger.getLogger(getClass.getName)

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("coocaa-ApkDataLoadJob")
    val sc = new SparkContext(conf)
    run(sc, "2016-11-18")

    sc.stop()

  }


  def run(sc: SparkContext, currentDate: String) = {

    val sqlContext = new HiveContext(sc)
    sqlContext.sql("use hr")
    sqlContext.sql("select * from hr.terminal")
      //    val terminalDF = sqlContext.sql("select * from hr.terminal_partition where br = 'CC'")

      //    terminalDF.write.mode(SaveMode.Append).insertInto("hr.sample_terminal")

      //写入hbase
      .foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("sample_terminal_two")

      //res0: org.apache.spark.sql.DataFrame = [key: string, sn: string, brand: string, last_poweron: string, area: string, province: string, city: string, citylevel: string, size: string, model: string, license: string]

      try {

        items.foreach(line => {

          //牌照
          val license = line(10)

          var province = line(5).toString

          if (province.equals("未匹配")) {
            province = "其他"
          }

          //激活时间
          val last_poweron = ""

          //型号
          val model = line(9)
          val size = line(8)
          val city = line(6).toString

          val sn = line(1)

          //大区
          val area = line(4)

          //城市级别
          val citylevel = line(7)

          val orderedLine = license + "\t" + province + "\t" + last_poweron + "\t" + model + "\t" + size + "\t" + city + "\t" + sn + "\t" + area + "\t" + citylevel


          /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
          mutator.mutate(HBaseUtils.getPut_SampleTerminal("CC", orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    }

    )
  }

}
package com.avcdata.spark.job.konka

import java.sql.Timestamp
import java.text.SimpleDateFormat

import com.github.nscala_time.time.Imports.DateTime
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.spark.SparkContext

import scala.collection.SortedMap

/**
  * Created by wxy on 8/29/16.
  * 开关机的处理,根据startTime过滤数据
  */
object TerminalPowerOnDataLoadJob {
    /**
      * 时间戳转为时间
      *
      * @param timestamp 时间戳
      * @return
      */
    def timestampToDate(timestamp: String): String = {
        val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
        val d = format.format(timestamp.toLong)
        d
    }

    /**
      * 时间转为时间戳,单位是毫秒
      *
      * @param date 时间
      * @return
      */
    def dateTotimestamp(date: String): Long = {
        val format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
        val d = format.parse(date)
        val time = new Timestamp(d.getTime())
        (time.getTime)
    }

    /**
      * 获取关机的时间，供后面的时长和次数使用
      *
      * @param duration  时长，分钟
      * @param startTime 开始时间，格式是2016-11-02 14:59:10
      * @return
      */
    def getEndTime(duration: String, startTime: String): String = {
        val start = dateTotimestamp(startTime)
        val endtTime = timestampToDate((start + (duration).toLong * 60 * 1000).toString)

        endtTime
    }

    /**
      * 直播拆分为小时单位
      *
      * @param duration 时长(分钟)
      * @param eTime  结束时间
      * @param startTime    开始时间
      * @return
      */
    def durationSplitByHour(duration: String, eTime: String, startTime: String): SortedMap[String, String] = {
        var endTime = eTime
        var startHour = startTime.substring(11, 13) //获取小时
        val endHour = endTime.substring(11, 13)
        var startDay = startTime.substring(8, 10) //获取天
        val endDay = endTime.substring(8, 10)
        var hours = 0

        /*if (endDay == startDay) {
            //同一天
            hours = endHour.toInt - startHour.toInt
        } else if (endDay > startDay) {
            //跨天
            startTime = endTime.substring(0, 10) + " 00:00:00"
            hours = endHour.toInt
        }*/
        if (endTime.substring(5, 7) == startTime.substring(5, 7)) {
            if (endDay == startDay) {//同一天
                hours = endHour.toInt - startHour.toInt
            } else if (endDay > startDay) {//跨天
                endTime = startTime.substring(0, 10) + " 23:60:00"
                hours = 23 - startHour.toInt
            }
        } else if (endTime.substring(5, 7) > startTime.substring(5, 7)){ //跨月
            endTime = startTime.substring(0, 10) + " 23:60:00"
            hours = 23 - startHour.toInt
        }

        val map = handleTime(duration, startTime, endTime, hours)
        map
    }

    /**
      * 处理开关机时间
      *
      * @param duration  时长(分钟)
      * @param startTime 开始时间
      * @param endTime   结束时间
      * @param hours     开始到结束的小时数
      * @return
      */
    def handleTime(duration: String, startTime: String, endTime: String, hours: Int): SortedMap[String, String] = {
        var currTime = startTime
        var map: SortedMap[String, String] = SortedMap()
        var cntFlag = 0

        if (hours == 0) {
            //map += (startTime.substring(11, 13) -> (duration + ";1"))
            //println("================ hours : " + hours)
            var dura = duration.toLong*60000
            //当出现endtime为2016-11-14 00:51:37,但时长又是夸了天的，但是这个时候把startTime置为2016-11-14 00:00:00了
            //需要对时长duration做一个处理
            if (startTime.substring(11, 13).equals("00") && endTime.substring(11, 13).equals("00"))
                dura = dateTotimestamp(endTime) - dateTotimestamp(startTime)
            map += (startTime.substring(11, 13) -> ((dura/60000).toString + ";1"))
            map
        }
        else {
            for (i <- 0 to hours) {
                var hour = currTime.substring(11, 13).toInt //获取当前的小时时间
                var nextTime = ""
                if (hour + 1 < 10) {
                    nextTime = currTime.substring(0, 10) + " 0" + (hour + 1) + ":00:00" //个位数补0
                } else {
                    nextTime = currTime.substring(0, 10) + " " + (hour + 1) + ":00:00" //得到下一小时的时间
                }
                var dura = 0l

                //println("nextTime : " + nextTime)
                //println("hour : " + hour + ", currtime : " + currTime)
                //最后一个小时的
                if (hour == endTime.substring(11, 13).toInt) {
                    //  毫秒/60000-->分钟
                    dura = dateTotimestamp(endTime) - dateTotimestamp(currTime)
                    map += (endTime.substring(11, 13) -> ((dura/60000).toString + ";0"))
                    cntFlag = 0
                    //println("=======2========= hours : " + hours)
                } else {
                    //  毫秒/60000-->分钟
                    dura = dateTotimestamp(nextTime) - dateTotimestamp(currTime)
                    if (cntFlag == 0) {
                        map += (currTime.substring(11, 13) -> ((dura/60000).toString + ";1"))
                        cntFlag = 1
                    } else {
                        map += (currTime.substring(11, 13) -> ((dura/60000).toString + ";0"))
                    }

                }

                currTime = nextTime
            }
            map
        }

    }

    def run(sc: SparkContext, analysisDate: String) = {
        val dimFamilyCol = Bytes.toBytes("dim")
        val factFamilyCol = Bytes.toBytes("fact")

        val dimSeriesNoCol = Bytes.toBytes("sn")
        val dimLogtimeCol = Bytes.toBytes("logtime")
        val dimPowerONDateCol = Bytes.toBytes("power_on_day")
        val dimPowerONTimeCol = Bytes.toBytes("power_on_time")
        val factPowerLenghtCol = Bytes.toBytes("power_on_length")
        val factCntCol = Bytes.toBytes("cnt")


        val preDate = DateTime.parse(analysisDate).plusDays(-1).toString("yyyy-MM-dd")
        val yesBeforeDate = DateTime.parse(analysisDate).plusDays(-2).toString("yyyy-MM-dd")

        //    LWR1415YD0019292UU51 串号
        //      116.207.89.124  IP
        //      LED42K70U_1BOM  机型
        //      rtd2995d 平台
        //      42 尺寸
        //      421000 地域代码
        //      2016-07-12 23:05:42 开机时间
        //      65 开机时长（单位：分钟）
        //    2016-07-15 04:22:31  记录创建时间

        //val terRdd = sc.textFile("F:/avc/docs/konka/duration.log.2016-08-22")
        val terRdd = sc.textFile("/user/hdfs/rsync/KONKA/" + analysisDate + "/duration.log." + analysisDate)
        //val terRdd = sc.textFile("/user/hdfs/rsync/KONKA/history/duration.log.since_10-01")
            .filter(x => x.split('|').length > 7)
            .filter(x => {
                val date = x.split('|')(6).substring(0, 10)
                date == analysisDate || date == preDate || date == yesBeforeDate
            })
            .mapPartitions(items => {
                items.map(line => {
                    val cols = line.split('|')
                    val sn = cols(0)
                    val lastPowerOnDate = cols(6).substring(0, 10)
                    val lastPowerOnTime = cols(6).substring(11, 13)
                    val timeLenth = cols(7)
                    val endTime = getEndTime(timeLenth, cols(6))
                    (sn, cols(6), timeLenth, endTime)
                })
            })

        terRdd.foreachPartition(items => {
            val myConf = HBaseConfiguration.create()
            myConf.set("hbase.zookeeper.quorum", "192.168.1.11,192.168.1.12,192.168.1.13")
            myConf.set("hbase.zookeeper.property.clientPort", "2181")
            //myConf.set("hbase.zookeeper.quorum", "192.168.2.66")
            val hbaseConn = ConnectionFactory.createConnection(myConf)
            val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("tracker_terminal_active_fact")) //tracker_terminal_active_fact
            //val mutator = hbaseConn.getBufferedMutator(TableName.valueOf("test_terminal_active_fact"))

            try {
                items.foreach(line => {
                    val sn = line._1
                    val startTime = line._2
                    val date = line._2.substring(0, 10)
                    val timeLenth = line._3
                    val endTime = line._4
                    val map = durationSplitByHour(timeLenth, endTime, startTime)
                    for (m <- map) {
                        val hour = m._1.toInt
                        val duraPerHour = m._2.split(";")(0)
                        val cnt = m._2.split(";")(1)
                        val put = new Put(Bytes.toBytes(sn + startTime + hour + "KO"))
                        put.addColumn(dimFamilyCol, dimSeriesNoCol, Bytes.toBytes(sn))
                        //put.addColumn(dimFamilyCol, dimLogtimeCol, Bytes.toBytes(analysisDate))
                        put.addColumn(dimFamilyCol, dimPowerONDateCol, Bytes.toBytes(date))
                        put.addColumn(dimFamilyCol, dimPowerONTimeCol, Bytes.toBytes(hour.toString))
                        //put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(timeLenth))
                        put.addColumn(factFamilyCol, factPowerLenghtCol, Bytes.toBytes(duraPerHour.toString))
                        put.addColumn(factFamilyCol, factCntCol, Bytes.toBytes(cnt))
                        //println(sn + "\t" + date + "\t" + startTime + "\t" + endTime
                        //    + "\t" + timeLenth + "\t" + hour + "\t" + duraPerHour + "\t" + cnt)
                        mutator.mutate(put)
                    }
                })
                mutator.flush()
            } finally {
                mutator.close()
                hbaseConn.close()
            }
        })
    }
}
package com.avcdata.spark.job.total.time

import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zyt
  * @define 将开关机推总数据导入到分区表
  */
object TerminalPowerTimeTotal2Partition {

  def main(args: Array[String]) {
    val currentDate = "2017-02-13"
    val sql = "INSERT OVERWRITE TABLE hr.tracker_total_oc_fact_partition partition (date='" + currentDate + "')  " +
      "select " +
      "key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_total_oc_fact where power_on_day='" + currentDate + "'"
    println(sql)
  }

  def run(sc: SparkContext, currentDate: String): Unit = {

    val sqlContext = new HiveContext(sc)

    //加载数据到hive分区表
    sqlContext.sql("set hive.exec.dynamic.partition=true")
    sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    val sql = "INSERT OVERWRITE TABLE hr.tracker_total_oc_fact_partition partition (date='" + currentDate + "')  " +
      "select " +
      "key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_total_oc_fact where power_on_day='" + currentDate + "'"

    println(sql)

    sqlContext.sql(sql)


    //加载三天前的数据
//    val twoDaysAgoTime = TimeUtils.dateStrAddDays2TimeStamp(currentDate, TimeUtils.DAY_DATE_FORMAT_ONE, -2)
//
//    val twoDaysAgoDate = TimeUtils.convertTimeStamp2DateStr(twoDaysAgoTime, TimeUtils.DAY_DATE_FORMAT_ONE)
//
//    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_total_oc_fact_partition partition (date='" + twoDaysAgoDate + "')  " +
//      "select " +
//      "key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_total_oc_fact where power_on_day='" + twoDaysAgoDate + "'")

//    sqlContext.sql("INSERT OVERWRITE TABLE hr.tracker_total_oc_fact_partition PARTITION (date) select key,sn,power_on_day,power_on_time,power_on_length,cnt,power_on_day as date from hr.tracker_total_oc_fact")

  }

}
package com.avcdata.spark.job.total.time

import com.avcdata.spark.job.total.Sql
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext

/**
  * @author zhangyongtian
  * @define 开关机次数和时长推总
  *
  */
object TerminalPowerTimeTotalJob {

  val log = Logger.getLogger(getClass.getName)

  /////////////////////////////////test///////////////////////////////////////
  //    def main(args: Array[String]) {
  //      val conf = new SparkConf()
  //        .setMaster("local[1]")
  //        .setAppName("TerminalPowerTotalJob")
  //      val sc = new SparkContext(conf)
  //      run(sc, "2016-11-18")
  //      sc.stop()
  //
  //    }
  /////////////////////////////////test///////////////////////////////////////

  def run(sc: SparkContext, analysisDate: String) = {

    val hiveContext: HiveContext = new HiveContext(sc)


    val totalDF: DataFrame = hiveContext.sql(
      Sql.getTracker_total_terminal_active_fact_HQL(analysisDate))

    //    totalDF.write.mode(SaveMode.Append).saveAsTable("hr.tracker_total_apk_fact")

    val totalRDD = totalDF.rdd

      //TODO 过滤异常值
      .filter(line => {
      var i = 0

      val key = line(i).toString
      i = i + 1

      val sn = line(i).toString
      i = i + 1

      val date = line(i).toString
      i = i + 1

      val hour = line(i).toString
      i = i + 1

      val cnt = line(i).toString
      i = i + 1

      val duration = line(i).toString

      duration.toDouble >= 0

    })

    //存入Hbase
    totalRDD
      .foreachPartition(lines => {

        val mutator = HBaseUtils.getMutator(Sql.tracker_total_terminal_active_fact_HtableName)

        try {

          lines.foreach(line => {
            var i = 0

            val key = line(i).toString
            i = i + 1

            val sn = line(i).toString
            i = i + 1

            val date = line(i).toString
            i = i + 1

            val hour = line(i).toString
            i = i + 1

            val cnt = line(i).toString
            i = i + 1

            val duration = line(i).toString


            val sortedLine = key + "\t" + sn + "\t" + date + "\t" + hour + "\t" + cnt + "\t" + duration


            /////////////////////////////////////写入HBase///////////////////////////////////////////////////////////
            mutator.mutate(HBaseUtils.getPut_total_oc(sortedLine))
          })
          mutator.flush()

        } finally {
          mutator.close()
        }
      }

      )

    //加载数据到hive分区表
    //    hiveContext.sql("set hive.exec.dynamic.partition=true")
    //    hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
    //
    //
    //    hiveContext.sql("INSERT INTO TABLE hr.tracker_total_oc_fact_partition partition (date='" + analysisDate + "')  " +
    //      "select " +
    //      "key,sn,power_on_day,power_on_time,power_on_length,cnt from hr.tracker_total_oc_fact where power_on_day='" + analysisDate + "'")
  }


}package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TerminalPriceCnt {

  case class TerminalPriceDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   price: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("TerminalPriceCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snPriceDistRDD = sqlContext.sql("select sn,stat_date,period,price from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val price_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .TV_PRICE_ARR)

      (sn + "\t" + stat_date + "\t" + period, price_map)

    })

    val TerminalPriceDistCntDF = snCidRDD.join(snPriceDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val price_map = line._2._2

      val resultArr = new Array[TerminalPriceDistCnt](price_map.size)

      val price_arr = price_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalPriceDistCnt(
          sn,
          stat_date,
          period,
          price_arr(i),
          price_map.get(price_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalPriceDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalPriceDistCntDF.registerTempTable("TerminalPriceDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.price,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,price,sum(cnt) as cnt  from TerminalPriceDistCnt group by stat_date,period,cluster_id,price
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalPriceDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_price_dist_cnt_k" + clusterNum, false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TerminalProvinceCnt {

  case class TerminalProvinceDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   province: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("TerminalprovinceCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snprovinceDistRDD = sqlContext.sql("select sn,stat_date,period,province from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val province_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .TERMINAL_PROVINCE_ARR)

      (sn + "\t" + stat_date + "\t" + period, province_map)

    })

    val TerminalprovinceDistCntDF = snCidRDD.join(snprovinceDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val province_map = line._2._2

      val resultArr = new Array[TerminalProvinceDistCnt](province_map.size)

      val province_arr = province_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalProvinceDistCnt(
          sn,
          stat_date,
          period,
          province_arr(i),
          province_map.get(province_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalprovinceDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalprovinceDistCntDF.registerTempTable("TerminalprovinceDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.province,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,province,sum(cnt) as cnt  from TerminalprovinceDistCnt group by stat_date,period,cluster_id,province
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalprovinceDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_province_dist_cnt_k" + clusterNum, false,
      SaveMode.Append)


  }
}
package com.avcdata.spark.job.etl.stat.user

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object TerminalSizeCnt {

  case class TerminalSizeDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   size: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("TerminalSizeCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snSizeDistRDD = sqlContext.sql("select sn,stat_date,period,size from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val size_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .TV_SIZE_ARR)

      (sn + "\t" + stat_date + "\t" + period, size_map)

    })

    val TerminalSizeDistCntDF = snCidRDD.join(snSizeDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val size_map = line._2._2

      val resultArr = new Array[TerminalSizeDistCnt](size_map.size)

      val size_arr = size_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = TerminalSizeDistCnt(
          sn,
          stat_date,
          period,
          size_arr(i),
          size_map.get(size_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    TerminalSizeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    TerminalSizeDistCntDF.registerTempTable("TerminalSizeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.size,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,size,sum(cnt) as cnt  from TerminalSizeDistCnt group by stat_date,period,cluster_id,size
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from TerminalSizeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_size_dist_cnt_k" + clusterNum, false,
      SaveMode.Append)

  }
}
package com.avcdata.spark.job.etl.clean

import com.avcdata.spark.job.etl.clean.TestAllTerminalETL.AllTerminal
import com.avcdata.spark.job.mllib.HDFSUtils
import com.avcdata.spark.job.util.HdfsUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 取得开关机,直播,应用三个日志共有的终端
  * 根据以上终端,获得以下数据:
  * 1.终端数量
  * 2.终端省份分布
  * 3.终端城市级别分布
  * 4.终端尺寸分布
  * 5.终端价格分布
  * 6.与2700份标注样本的匹配程度,及匹配上的终端数量
  */
object TestAllTerminalETL {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("GenerateUserTerminalData")
    val sc = new SparkContext(conf)
    run(sc)

    sc.stop()
  }

  case class AllTerminal(
                          sn: String,
                          brand: String,
                          province: String,
                          city: String,
                          citylevel: String,
                          area: String,
                          price: String,
                          size: String,
                          model: String,
                          license: String

                        )


  def run(sc: SparkContext) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO terminal_info RDD
    val terminal_info = sqlContext.sql(
      """
        SELECT t.sn, t.brand, t.area, t.province, t.city
                		, t.citylevel, t.size, t.model, t.license, tsp.price
                	FROM hr.terminal t
                	LEFT	JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
                			AND t.model = tsp.model
      """.stripMargin).registerTempTable("tr")

    //TODO 开关机
    val oc_rdd =
      sqlContext.sql(
        """
     SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM tr
        JOIN (SELECT DISTINCT sn
        		FROM hr.tracker_oc_fact_partition
        		) oc ON tr.sn = oc.sn
        """.stripMargin).rdd.distinct

    val oc_path = "/user/hdfs/rsync/all_terminal_oc"
    HdfsUtils.rm(oc_path,true)
    oc_rdd.saveAsTextFile(oc_path)

    //TODO 直播
    val live_rdd =
      sqlContext.sql(
        """
        SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM  tr
        JOIN (SELECT DISTINCT dim_sn
        		FROM hr.tracker_live_fact_partition
        		) live ON tr.sn = live.dim_sn
        """.stripMargin).rdd.distinct

    val live_path = "/user/hdfs/rsync/all_terminal_live"
    HdfsUtils.rm(live_path,true)
    live_rdd.saveAsTextFile(live_path)


    //TODO APK
    val apk_rdd =
      sqlContext.sql(
        """
            SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
              , tr.citylevel, tr.size, tr.model, tr.license, tr.price
              FROM  tr
              JOIN (SELECT DISTINCT dim_sn
                FROM hr.tracker_apk_fact_partition
          ) apk ON tr.sn = apk.dim_sn

        """.stripMargin).rdd.distinct

    val apk_path = "/user/hdfs/rsync/all_terminal_apk"
    HdfsUtils.rm(apk_path,true)
    apk_rdd.saveAsTextFile(apk_path)


    //TODO 合并三类数据
    val oc_RDD = sc.textFile("/user/hdfs/rsync/all_terminal_oc")
    val live_RDD = sc.textFile("/user/hdfs/rsync/all_terminal_live")
    val apk_RDD = sc.textFile("/user/hdfs/rsync/all_terminal_apk")

    val all_RDD = oc_RDD.intersection(live_RDD).intersection(apk_RDD).distinct

    println(all_RDD.count)

    all_RDD .map(line => {
      val cols = line.substring(1, line.length - 1).split(",")

      val sn = cols(0)
      val brand = cols(1)
      val area = cols(2)
      val province = cols(3)
      val city = cols(4)
      val citylevel = cols(5)
      val size = cols(6)
      val model = cols(7)
      val license = cols(8)
      val price = cols(9)

      AllTerminal(
        sn: String,
        brand: String,
        province: String,
        city: String,
        citylevel: String,
        area: String,
        price: String,
        size: String,
        model: String,
        license: String

      )

    })
      .toDF.write.mode(SaveMode.Overwrite).saveAsTable("hr.all_terminal")

    //    val resultRDD2 = apk_rdd
    //      oc_rdd.union(live_rdd).union(apk_rdd).distinct


    //        .map(line=>{
    //          val sn = line(0).toString
    //          val brand = line(1).toString
    //          val area = line(2).toString
    //          val province = line(3).toString
    //          val city = line(4).toString
    //          val citylevel = line(5).toString
    //          val size = line(6).toString
    //          val model = line(7).toString
    //          val license = line(8).toString
    //          val price = line(9).toString
    //
    //          AllTerminal(
    //            sn,
    //            brand,
    //            province,
    //            city,
    //            citylevel,
    //            area,
    //            price,
    //            size,
    //            model,
    //            license
    //
    //          )
    //
    //          sn+"\t"+brand+"\t"+area+"\t"+province+"\t"+city+"\t"+citylevel+"\t"+size+"\t"+model+"\t"+license+"\t"+price
    //        })


    //    resultRDD.saveAsTextFile("/user/hdfs/rsync/all_terminal"+System.currentTimeMillis())

    //    resultRDD.toDF.write.mode(SaveMode.Append).saveAsTable("hr.all_terminal")
  }

}
package com.avcdata.spark.job.etl.clean

import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 取得开关机,直播,应用三个日志共有的终端
  * 根据以上终端,获得以下数据:
  * 1.终端数量
  * 2.终端省份分布
  * 3.终端城市级别分布
  * 4.终端尺寸分布
  * 5.终端价格分布
  * 6.与2700份标注样本的匹配程度,及匹配上的终端数量
  */
object TestAllTerminalETL01 {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("GenerateUserTerminalData")
    val sc = new SparkContext(conf)
    run(sc)

    sc.stop()
  }

  case class AllTerminal(
                           sn: String,
                           brand: String,
                           province: String,
                           city: String,
                           citylevel: String,
                           area: String,
                           price: String,
                           size: String,
                           model: String,
                           license: String

                         )


  def run(sc: SparkContext) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._


    //TODO terminal_info RDD
    val terminal_info = sqlContext.sql("")


//    val oc_rdd =
//      sqlContext.sql(
//        """
//     SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
//        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
//        FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
//        		, t.citylevel, t.size, t.model, t.license, tsp.price
//        	FROM hr.terminal t
//        		JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
//        			AND t.model = tsp.model
//        	) tr
//
//        	JOIN (SELECT DISTINCT sn
//        		FROM hr.tracker_oc_fact_partition
//        		) oc ON tr.sn = oc.sn
//
//
//        """.stripMargin).rdd



//    val live_rdd =
//      sqlContext.sql(
//        """
//
//        SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
//        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
//        FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
//        		, t.citylevel, t.size, t.model, t.license, tsp.price
//        	FROM hr.terminal t
//        		JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
//        			AND t.model = tsp.model
//        	) tr
//
//        	JOIN (SELECT DISTINCT dim_sn
//        		FROM hr.tracker_live_fact_partition
//        		) live ON tr.sn = live.dim_sn
//
//
//        """.stripMargin).rdd


//    val apk_rdd =
//      sqlContext.sql(
//        """
//            SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
//              , tr.citylevel, tr.size, tr.model, tr.license, tr.price
//              FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
//              , t.citylevel, t.size, t.model, t.license, tsp.price
//              FROM hr.terminal t
//                JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
//              AND t.model = tsp.model
//              ) tr
//              JOIN (SELECT DISTINCT dim_sn
//                FROM hr.tracker_apk_fact_partition
//          ) apk ON tr.sn = apk.dim_sn
//
//        """.stripMargin).rdd
//    SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
//    , tr.citylevel, tr.size, tr.model, tr.license, tr.price
//    FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
//    , t.citylevel, t.size, t.model, t.license, tsp.price
//    FROM hr.terminal t
//      JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
//    AND t.model = tsp.model
//    ) tr
//    JOIN (SELECT DISTINCT dim_sn
//      FROM hr.tracker_apk_fact_partition
//    ) apk ON tr.sn = apk.dim_sn

//    val resultRDD2 = apk_rdd
//      oc_rdd.union(live_rdd).union(apk_rdd).distinct



    val resultRDD = sqlContext.sql(
      """
     SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
        		, t.citylevel, t.size, t.model, t.license, tsp.price
        	FROM hr.terminal t
        		JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
        			AND t.model = tsp.model
        	) tr

        	JOIN (SELECT DISTINCT sn
        		FROM hr.tracker_oc_fact_partition
        		) oc ON tr.sn = oc.sn

        UNION ALL

        SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
        		, t.citylevel, t.size, t.model, t.license, tsp.price
        	FROM hr.terminal t
        		JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
        			AND t.model = tsp.model
        	) tr

        	JOIN (SELECT DISTINCT dim_sn
        		FROM hr.tracker_live_fact_partition
        		) live ON tr.sn = live.dim_sn

        UNION ALL

        SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM (SELECT t.sn, t.brand, t.area, t.province, t.city
        		, t.citylevel, t.size, t.model, t.license, tsp.price
        	FROM hr.terminal t
        		JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
        			AND t.model = tsp.model
        	) tr

        	JOIN (SELECT DISTINCT dim_sn
        		FROM hr.tracker_apk_fact_partition
        		) apk ON tr.sn = apk.dim_sn

      """.stripMargin).rdd

        .map(line=>{
          val sn = line(0).toString
          val brand = line(1).toString
          val area = line(2).toString
          val province = line(3).toString
          val city = line(4).toString
          val citylevel = line(5).toString
          val size = line(6).toString
          val model = line(7).toString
          val license = line(8).toString
          val price = line(9).toString

          AllTerminal(
            sn,
            brand,
            province,
            city,
            citylevel,
            area,
            price,
            size,
            model,
            license

          )

          sn+"\t"+brand+"\t"+area+"\t"+province+"\t"+city+"\t"+citylevel+"\t"+size+"\t"+model+"\t"+license+"\t"+price
        })

//    val resultRDD = sc.parallelize(Array(1,2,3,4,5,6))

    resultRDD.saveAsTextFile("/user/hdfs/rsync/all_terminal"+System.currentTimeMillis())


//    resultRDD.toDF.write.mode(SaveMode.Append).saveAsTable("hr.all_terminal")



    // TODO 写入hbase
//    resultRDD.foreachPartition(items => {
//
//      val mutator = HBaseUtils.getMutator("all_terminal")
//
//      try {
//
//        items.foreach(line => {
//
//          val cols = line.split("\t")
//
//          val sn = cols(0)
//          val brand = cols(1)
//          val area = cols(2)
//          val province = cols(3)
//          val city = cols(4)
//          val citylevel = cols(5)
//          val size = cols(6)
//          val model = cols(7)
//          val license = cols(8)
//          val price = cols(9)
//
//          val orderedLine = sn+"\t"+brand+"\t"+area+"\t"+province+"\t"+city+"\t"+citylevel+"\t"+size+"\t"+model+"\t"+license+"\t"+price
//
//          mutator.mutate(HBaseUtils.getPut_AllTerminal(orderedLine))
//        })
//        mutator.flush()
//
//      } finally {
//        mutator.close()
//        HBaseUtils.getHbaseConn().close()
//      }
//    })


//      .saveAsTextFile("/user/hdfs/rsync/uservector/TestAllTerminalETL")

  }

}
package com.avcdata.spark.job.etl.clean

import com.avcdata.spark.job.util.HdfsUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 取得开关机,直播,应用三个日志共有的终端
  * 根据以上终端,获得以下数据:
  * 1.终端数量
  * 2.终端省份分布
  * 3.终端城市级别分布
  * 4.终端尺寸分布
  * 5.终端价格分布
  * 6.与2700份标注样本的匹配程度,及匹配上的终端数量
  */
object TestAllTerminalETLFullJoin {

  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("GenerateUserTerminalData")
    val sc = new SparkContext(conf)
    run(sc)

    sc.stop()
  }

  case class AllTerminal(
                          sn: String,
                          brand: String,
                          province: String,
                          city: String,
                          citylevel: String,
                          area: String,
                          price: String,
                          size: String,
                          model: String,
                          license: String

                        )


  def run(sc: SparkContext) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    //TODO terminal_info RDD
    val terminal_info = sqlContext.sql(
      """
        SELECT t.sn, t.brand, t.area, t.province, t.city
                		, t.citylevel, t.size, t.model, t.license, tsp.price
                	FROM hr.terminal t
                	LEFT	JOIN hr.terminal_size_price tsp ON t.brand = tsp.brand
                			AND t.model = tsp.model
      """.stripMargin).registerTempTable("tr")

    //TODO 开关机
    val oc_rdd =
      sqlContext.sql(
        """
     SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM tr
        LEFT	JOIN (SELECT DISTINCT sn
        		FROM hr.tracker_oc_fact_partition
        		) oc ON tr.sn = oc.sn
        """.stripMargin).rdd.distinct

    val oc_path = "/user/hdfs/rsync/all_terminal_oc"
    HdfsUtils.rm(oc_path,true)
    oc_rdd.saveAsTextFile(oc_path)

    //TODO 直播
    val live_rdd =
      sqlContext.sql(
        """
        SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
        	, tr.citylevel, tr.size, tr.model, tr.license, tr.price
        FROM  tr
        LEFT	JOIN (SELECT DISTINCT dim_sn
        		FROM hr.tracker_live_fact_partition
        		) live ON tr.sn = live.dim_sn
        """.stripMargin).rdd.distinct

    val live_path = "/user/hdfs/rsync/all_terminal_live"
    HdfsUtils.rm(live_path,true)
    live_rdd.saveAsTextFile(live_path)


    //TODO APK
    val apk_rdd =
      sqlContext.sql(
        """
            SELECT tr.sn, tr.brand, tr.area, tr.province, tr.city
              , tr.citylevel, tr.size, tr.model, tr.license, tr.price
              FROM  tr
            LEFT  JOIN (SELECT DISTINCT dim_sn
                FROM hr.tracker_apk_fact_partition
          ) apk ON tr.sn = apk.dim_sn

        """.stripMargin).rdd.distinct

    val apk_path = "/user/hdfs/rsync/all_terminal_apk"
    HdfsUtils.rm(apk_path,true)
    apk_rdd.saveAsTextFile(apk_path)


    //TODO 合并三类数据
    val oc_RDD = sc.textFile("/user/hdfs/rsync/all_terminal_oc")
    val live_RDD = sc.textFile("/user/hdfs/rsync/all_terminal_live")
    val apk_RDD = sc.textFile("/user/hdfs/rsync/all_terminal_apk")

    val all_RDD = oc_RDD.union(live_RDD).union(apk_RDD).distinct

    println(all_RDD.count)

    all_RDD .map(line => {
      val cols = line.substring(1, line.length - 1).split(",")

      val sn = cols(0)
      val brand = cols(1)
      val area = cols(2)
      val province = cols(3)
      val city = cols(4)
      val citylevel = cols(5)
      val size = cols(6)
      val model = cols(7)
      val license = cols(8)
      val price = cols(9)

      AllTerminal(
        sn: String,
        brand: String,
        province: String,
        city: String,
        citylevel: String,
        area: String,
        price: String,
        size: String,
        model: String,
        license: String

      )

    })
      .toDF.write.mode(SaveMode.Overwrite).saveAsTable("hr.all_terminal")

    //    val resultRDD2 = apk_rdd
    //      oc_rdd.union(live_rdd).union(apk_rdd).distinct


    //        .map(line=>{
    //          val sn = line(0).toString
    //          val brand = line(1).toString
    //          val area = line(2).toString
    //          val province = line(3).toString
    //          val city = line(4).toString
    //          val citylevel = line(5).toString
    //          val size = line(6).toString
    //          val model = line(7).toString
    //          val license = line(8).toString
    //          val price = line(9).toString
    //
    //          AllTerminal(
    //            sn,
    //            brand,
    //            province,
    //            city,
    //            citylevel,
    //            area,
    //            price,
    //            size,
    //            model,
    //            license
    //
    //          )
    //
    //          sn+"\t"+brand+"\t"+area+"\t"+province+"\t"+city+"\t"+citylevel+"\t"+size+"\t"+model+"\t"+license+"\t"+price
    //        })


    //    resultRDD.saveAsTextFile("/user/hdfs/rsync/all_terminal"+System.currentTimeMillis())

    //    resultRDD.toDF.write.mode(SaveMode.Append).saveAsTable("hr.all_terminal")
  }

}
package com.avcdata.vbox.launcher

import com.avcdata.vbox.clean.live.DataCleanCHLiveOld
import com.avcdata.vbox.clean.play.DataCleanCCPlayTest
import com.avcdata.vbox.clean.terminal.DataCleanTCLTerminal
import com.avcdata.vbox.common.Helper
import org.apache.log4j.Logger

object TestExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "000000000000000000000000000000000")

    val sc = Helper.sparkContext


    // TODO 长虹旧直播数据清洗-8月28日
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@DataCleanCHLiveOld start....")
      DataCleanCHLiveOld.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCHLiveOld end....")
    }


    //TODO TCL终端信息清洗
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@DataCleanTCLTerminal start....")
      DataCleanTCLTerminal.run(sc, analysisDate);
      println(analysisDate + "@DataCleanTCLTerminal end....")
    }

    // TODO 长虹直播终端信息的清洗
//    if (executePart.charAt(2) == '1') {
//      println(analysisDate + "@DataCleanCHLiveTerminal start....")
//      DataCleanCHLiveTerminal.run(sc, analysisDate);
//      println(analysisDate + "@DataCleanCHLiveTerminal end....")
//    }

    //TODO 酷开到剧清洗测试
    //    if (executePart.charAt(3) == '1') {
    //      println(analysisDate + "@GetCCPgOfPlay start....")
    //      GetCCPgOfPlay.run(sc, analysisDate);
    //      println(analysisDate + "@GetCCPgOfPlay end....")
    //    }

    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@DataCleanCCPlayTest start....")
      DataCleanCCPlayTest.run(sc, analysisDate);
      println(analysisDate + "@DataCleanCCPlayTest end....")
    }

    //    if (executePart.charAt(21) == '1') {
    //      println(analysisDate + "@DataCleanCCPlay start....")
    //      DataCleanCCPlay.run(sc, analysisDate);
    //      println(analysisDate + "@DataCleanCCPlay end....")
    //    }


    sc.stop()
  }

}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.executor.CCDataCleanExecutor._
import com.avcdata.spark.job.total.time.{TerminalPowerTimeTotal2Partition, LiveTimeTotal2Partition, ApkTimeTotal2Partition}
import org.apache.log4j.Logger

/**
  * Created by xiaoyuzhou on 2017/2/12.
  */
object Timetotal2partitionExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext

    //5core 5G 6
    //TODO 开关机次数和时长推总数据=>分区表
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition start ... ")
      TerminalPowerTimeTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-TerminalPowerTimeTotal2Partition end ... ")
    }

    //5core 5G 6
    //TODO Live次数和时长推总数据=>分区表
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition start ... ")
      LiveTimeTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-LiveTimeTotal2Partition end ... ")
    }

    //5core 5G 6
    //TODO APK次数和时长推总数据=>分区表
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition start ... ")
      ApkTimeTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@COOCAA-ApkTimeTotal2Partition end ... ")
    }


    ////TODO 到剧次数和时长推总数据=>分区表

  }

}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.total.time.{ApkTimeTotalJob, LiveTimeTotalJob, TerminalPowerTimeTotalJob}
import org.apache.log4j.Logger

object TimetotalExecutor {

  def main(args: Array[String]) {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //TODO 开关机推总
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@TerminalPowerTimeTotalJob start ... ")
      TerminalPowerTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@TerminalPowerTimeTotalJob end ... ")
    }


    //TODO 直播推总
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@LiveTimeTotalJob start ... ")
      LiveTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveTimeTotalJob end ... ")
    }

    //TODO APK推总
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@ApkTimeTotalJob start ... ")
      ApkTimeTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkTimeTotalJob end ... ")
    }


  }
}
package com.avcdata.spark.job.until

import java.text.SimpleDateFormat
import java.util.Calendar

import com.github.nscala_time.time.Imports._
import org.joda.time.DateTime

import scala.collection.mutable.ArrayBuffer

object TimeUtils {


  final val ONE_HOUR_MILLISECONDS = 60 * 60 * 1000

  final val SECOND_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss"

  final val DAY_DATE_FORMAT_ONE = "yyyy-MM-dd"

  final val DAY_DATE_FORMAT_TWO = "yyyyMMdd"

  //时间字符串=>时间戳
  def convertDateStr2TimeStamp(dateStr: String, pattern: String): Long = {
    new SimpleDateFormat(pattern).parse(dateStr).getTime
  }


  //时间字符串+天数=>时间戳
  def dateStrAddDays2TimeStamp(dateStr: String, pattern: String, days: Int): Long = {
    convertDateStr2Date(dateStr, pattern).plusDays(days).date.getTime
  }


  //时间字符串=>日期
  def convertDateStr2Date(dateStr: String, pattern: String): DateTime = {
    new DateTime(new SimpleDateFormat(pattern).parse(dateStr))
  }


  //时间戳=>日期
  def convertTimeStamp2Date(timestamp: Long): DateTime = {
    new DateTime(timestamp)
  }

  //时间戳=>字符串
  def convertTimeStamp2DateStr(timestamp: Long, pattern: String): String = {
    new DateTime(timestamp).toString(pattern)
  }

  //时间戳=>小时数
  def convertTimeStamp2Hour(timestamp: Long): Long = {
    new DateTime(timestamp).hourOfDay().getAsString().toLong
  }


  //时间戳=>分钟数
  def convertTimeStamp2Minute(timestamp: Long): Long = {
    new DateTime(timestamp).minuteOfHour().getAsString().toLong
  }

  //时间戳=>秒数
  def convertTimeStamp2Sec(timestamp: Long): Long = {
    new DateTime(timestamp).secondOfMinute().getAsString.toLong
  }

  //TODO 对开始和结束时间分时(一天之内)
  //other_info最后加上"\t"
  def splitTimeByHour(other_info: String, launchTimeStamp: Long, exitTimeStamp: Long): ArrayBuffer[String] = {

    val arr = ArrayBuffer[String]()

    val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)

    val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)

    val diffTimeStamp = (exitTimeStamp - launchTimeStamp)

    if (exitHour - launchHour == 0) {
      //其他信息 整点 次数 时长
      arr.append(other_info + launchHour + "\t" + 1 + "\t" + diffTimeStamp / 1000)
    }

    if (exitHour - launchHour > 0) {
      val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)
      val launchMinute = TimeUtils.convertTimeStamp2Minute(launchTimeStamp)
      val launchSec = TimeUtils.convertTimeStamp2Sec(launchTimeStamp)

      val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)
      val exitMinute = TimeUtils.convertTimeStamp2Minute(exitTimeStamp)
      val exitSec = TimeUtils.convertTimeStamp2Sec(exitTimeStamp)

      arr.append(other_info + launchHour + "\t" + 1 + "\t" + (60 * 60 - (launchMinute * 60 + launchSec)))

      var h = launchHour
      //需要分时
      while (h < exitHour) {
        h = h + 1
        if (h != exitHour) {
          arr.append(other_info + h +
            "\t" + 0 + "\t" + 1 * 60 * 60)
        } else {
          arr.append(other_info + h + "\t" + 0 + "\t" + (exitMinute * 60 + exitSec))
        }
      }
    }

    arr
  }

  //对开始和结束时间分时
  def splitTimeByMinute(other_info: String, startTime: String, endTime: String): ArrayBuffer[String] = {

    val arr = ArrayBuffer[String]()
    //00:01  00:29
    val startHour = startTime.charAt(0).toString.toInt == 0 match {
      case true => startTime.charAt(1).toString.toInt
      case false => startTime.substring(0, 2).toInt
    }

    //    println("startHour:" + startHour)


    val startMin = startTime.charAt(3).toString.toInt == 0 match {
      case true => startTime.charAt(4).toString.toInt
      case false => startTime.substring(3).toInt
    }

    //    println("startMin:" + startMin)


    val endHour = endTime.charAt(0).toString.toInt == 0 match {
      case true => endTime.charAt(1).toString.toInt
      case false => endTime.substring(0, 2).toInt
    }

    //    println("endHour:" + endHour)

    val endMin = endTime.charAt(3).toString.toInt == 0 match {
      case true => endTime.charAt(4).toString.toInt
      case false => endTime.substring(3).toInt
    }

    //    println("endMin:" + endMin)

    val diffHour = endHour - startHour

    if (diffHour == 0) {

      var i = startMin

      while (i <= endMin) {
        arr.append(other_info + startHour.toString + "\t" + i.toString)
        i = i + 1
      }

    }

    if (diffHour > 0) {
      var i = startMin

      while (i <= 59) {
        arr.append(other_info + startHour.toString + "\t" + i.toString)
        i = i + 1
      }


      var h = startHour
      //需要分时
      while (h < endHour) {
        h = h + 1
        if (h != endHour) {
          for (n <- 0 to 59) {
            arr.append(other_info + h.toString + "\t" + n.toString)
          }
        } else {
          for (n <- 0 to endMin) {
            arr.append(other_info + h.toString + "\t" + n.toString)
          }
        }
      }

      //      for (i <- 1 to diffHour) {
      //
      //
      //      }


    }

    arr
  }

  def addZero(hourOrMin: String): String = {
    if (hourOrMin.toInt <= 9)
      "0" + hourOrMin
    else
      hourOrMin

  }

  def delZero(hourOrMin: String): String = {
    var res = hourOrMin
    if (!hourOrMin.equals("0") && hourOrMin.startsWith("0"))
      res = res.replaceAll("^0", "")
    res
  }


  def getTimeNum(time: String): Int = {
    1
  }

  def dateStrPatternOne2Two(time: String): String = {
    TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(time, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)
  }

  /**
    * 获取下一个月的第一天
    * @param dateStr
    * @param pattern
    * @return
    */
  def getPerFirstDayOfMonth(dateStr: String, pattern: String): String = {
    val dft = new SimpleDateFormat(pattern);
    val calendar = java.util.Calendar.getInstance()
    calendar.setTime(dft.parse(dateStr));
    calendar.add(Calendar.MONTH, 1);
    calendar.set(Calendar.DAY_OF_MONTH, calendar.getActualMinimum(Calendar.DAY_OF_MONTH));
    return dft.format(calendar.getTime());
  }

  def main(args: Array[String]) {

    //1477483944000
    //2016/10/26 15:15   2016/10/26 18:10
    //    val arr = splitTime("", 1477483944000L, 1477484004000L)
    //            val arr = splitTime("",1477484004000L, 1477490400000L)
    //2016/10/25 23:3:24 2016/10/26 1:13:0
    //    val arr = splitTime("", 1477411200000L, 1477415580000L)
    //    for (i <- 0 until arr.length) yield println(arr(i)) //将得到ArrayBuffer(2,6,4,-2,-4)
    //    splitTimeByMinute("", "00:01", "02:29")
    //    println(delZero("2109"))
//    val res = None.toString
//
//    println(addZero("1"))

    println( splitTimeByMinute("", "00:01", "00:29"))
//    println(getPerFirstDayOfMonth("2017-04-27",TimeUtils.DAY_DATE_FORMAT_ONE))

  }


}



package com.avcdata.spark.job.until

import java.text.SimpleDateFormat
import java.util.{Calendar, Date}

import com.github.nscala_time.time.Imports._
import org.joda.time.DateTime

import scala.collection.mutable.ArrayBuffer

object TimeUtils {


  final val ONE_HOUR_MILLISECONDS = 60 * 60 * 1000

  final val SECOND_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss"

  final val DAY_DATE_FORMAT_ONE = "yyyy-MM-dd"

  final val DAY_DATE_FORMAT_TWO = "yyyyMMdd"

  //时间字符串=>时间戳
  def convertDateStr2TimeStamp(dateStr: String, pattern: String): Long = {
    new SimpleDateFormat(pattern).parse(dateStr).getTime
  }


  //时间字符串+天数=>时间戳
  def dateStrAddDays2TimeStamp(dateStr: String, pattern: String, days: Int): Long = {
    convertDateStr2Date(dateStr, pattern).plusDays(days).date.getTime
  }


  //时间字符串=>日期
  def convertDateStr2Date(dateStr: String, pattern: String): DateTime = {
    new DateTime(new SimpleDateFormat(pattern).parse(dateStr))
  }


  //时间戳=>日期
  def convertTimeStamp2Date(timestamp: Long): DateTime = {
    new DateTime(timestamp)
  }

  //时间戳=>字符串
  def convertTimeStamp2DateStr(timestamp: Long, pattern: String): String = {
    new DateTime(timestamp).toString(pattern)
  }

  //时间戳=>小时数
  def convertTimeStamp2Hour(timestamp: Long): Long = {
    new DateTime(timestamp).hourOfDay().getAsString().toLong
  }


  //时间戳=>分钟数
  def convertTimeStamp2Minute(timestamp: Long): Long = {
    new DateTime(timestamp).minuteOfHour().getAsString().toLong
  }

  //时间戳=>秒数
  def convertTimeStamp2Sec(timestamp: Long): Long = {
    new DateTime(timestamp).secondOfMinute().getAsString.toLong
  }

  //获取星期几
  def dayOfWeek(dateStr: String): Int = {
    val sdf = new SimpleDateFormat("yyyy-MM-dd")
    val date = sdf.parse(dateStr)

    //    val sdf2 = new SimpleDateFormat("EEEE")
    //    sdf2.format(date)

    val cal = Calendar.getInstance();
    cal.setTime(date);
    var w = cal.get(Calendar.DAY_OF_WEEK) - 1;

    //星期天 默认为0
    if (w <= 0)
      w = 7
    w
  }


  //判断是否是周末
  def isRestday(date: String): Boolean = {
    val dayNumOfWeek = dayOfWeek(date)
    dayNumOfWeek == 6 || dayNumOfWeek == 7
  }

  //TODO 对开始和结束时间分时(一天之内)
  //other_info最后加上"\t"
  def splitTimeByHour(other_info: String, launchTimeStamp: Long, exitTimeStamp: Long): ArrayBuffer[String] = {

    val arr = ArrayBuffer[String]()

    val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)

    val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)

    val diffTimeStamp = (exitTimeStamp - launchTimeStamp)

    if (exitHour - launchHour == 0) {
      //其他信息 整点 次数 时长
      arr.append(other_info + launchHour + "\t" + 1 + "\t" + diffTimeStamp / 1000)
    }

    if (exitHour - launchHour > 0) {
      val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)
      val launchMinute = TimeUtils.convertTimeStamp2Minute(launchTimeStamp)
      val launchSec = TimeUtils.convertTimeStamp2Sec(launchTimeStamp)

      val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)
      val exitMinute = TimeUtils.convertTimeStamp2Minute(exitTimeStamp)
      val exitSec = TimeUtils.convertTimeStamp2Sec(exitTimeStamp)

      arr.append(other_info + launchHour + "\t" + 1 + "\t" + (60 * 60 - (launchMinute * 60 + launchSec)))

      var h = launchHour
      //需要分时
      while (h < exitHour) {
        h = h + 1
        if (h != exitHour) {
          arr.append(other_info + h +
            "\t" + 0 + "\t" + 1 * 60 * 60)
        } else {
          arr.append(other_info + h + "\t" + 0 + "\t" + (exitMinute * 60 + exitSec))
        }
      }
    }

    arr
  }

  //对开始和结束时间分时
  def splitTimeByMinute(other_info: String, startTime: String, endTime: String): ArrayBuffer[String] = {

    val arr = ArrayBuffer[String]()
    //00:01  00:29
    val startHour = startTime.charAt(0).toString.toInt == 0 match {
      case true => startTime.charAt(1).toString.toInt
      case false => startTime.substring(0, 2).toInt
    }

    //    println("startHour:" + startHour)


    val startMin = startTime.charAt(3).toString.toInt == 0 match {
      case true => startTime.charAt(4).toString.toInt
      case false => startTime.substring(3).toInt
    }

    //    println("startMin:" + startMin)


    val endHour = endTime.charAt(0).toString.toInt == 0 match {
      case true => endTime.charAt(1).toString.toInt
      case false => endTime.substring(0, 2).toInt
    }

    //    println("endHour:" + endHour)

    val endMin = endTime.charAt(3).toString.toInt == 0 match {
      case true => endTime.charAt(4).toString.toInt
      case false => endTime.substring(3).toInt
    }

    //    println("endMin:" + endMin)

    val diffHour = endHour - startHour

    if (diffHour == 0) {

      var i = startMin

      while (i <= endMin) {
        arr.append(other_info + startHour.toString + "\t" + i.toString)
        i = i + 1
      }

    }

    if (diffHour > 0) {
      var i = startMin

      while (i <= 59) {
        arr.append(other_info + startHour.toString + "\t" + i.toString)
        i = i + 1
      }


      var h = startHour
      //需要分时
      while (h < endHour) {
        h = h + 1
        if (h != endHour) {
          for (n <- 0 to 59) {
            arr.append(other_info + h.toString + "\t" + n.toString)
          }
        } else {
          for (n <- 0 to endMin) {
            arr.append(other_info + h.toString + "\t" + n.toString)
          }
        }
      }

      //      for (i <- 1 to diffHour) {
      //
      //
      //      }


    }

    arr
  }

  def addZero(hourOrMin: String): String = {
    if (hourOrMin.toInt <= 9)
      "0" + hourOrMin
    else
      hourOrMin

  }

  def delZero(hourOrMin: String): String = {
    var res = hourOrMin
    if (!hourOrMin.equals("0") && hourOrMin.startsWith("0"))
      res = res.replaceAll("^0", "")
    res
  }


  def getTimeNum(time: String): Int = {
    1
  }

  def dateStrPatternOne2Two(time: String): String = {
    TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(time, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)
  }

  //Time: HH:mm:ss 秒满60进1
  def fullSecondAddMin(time: String): String = {

    val cols = time.split(":")
    val hour = cols(0)
    var min = cols(1)
    val sec = cols(2)

    if (sec.equals("60")) {
      min = addZero((delZero(min).toInt + 1).toString)
    }


    hour + ":" + min + ":00"

  }


  /** 获取当前时间所在年的周数
    *
    * @param date 当前时间
    * @return 当前时间的周数
    */
  def getWeekOfYear(date: Date): Int = {
    val ca = Calendar.getInstance
    ca.setFirstDayOfWeek(Calendar.MONDAY)
    ca.setTime(date)

    ca.get(Calendar.WEEK_OF_YEAR)
  }

  /**
    * 获取当前时间所在年的最大周数
    *
    * @param year 所在年
    * @return 所在年的最大周
    */
  def getMaxWeekNumOfYear(year: Int): Int = {
    val ca = Calendar.getInstance()
    ca.set(year, Calendar.DECEMBER, 31, 23, 59, 59)

    val calcWeekNum = getWeekOfYear(ca.getTime)
    if (calcWeekNum == 1) {
      ca.add(Calendar.DAY_OF_YEAR, -7)

      getWeekOfYear(ca.getTime)
    }
    else {
      calcWeekNum
    }
  }


  /**
    * 获取某年的第几周的开始日期
    *
    * @param year 指定的年
    * @param week 指定的周
    * @return 指定年指定周的开始日期
    */
  def getFirstDayOfWeek(year: Int, week: Int): Date = {
    getFirstDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前年当前周的当前时间
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的当前时间
    */
  def getCurrentWeekDay(year: Int, week: Int): Date = {
    //获取日期实例
    val ca = Calendar.getInstance()
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    //将当前年份设置为当前年
    ca.set(Calendar.YEAR, year)

    //获取到当前的年W周
    val currentYearWeek = dateToHalfYearWeek(ca.getTime)
    //当前年最大周度
    val maxWeek = getMaxWeekNumOfYear(year)

    //获取周数
    val currentWeek = ca.get(Calendar.WEEK_OF_YEAR)

    //若跨年,则加上最大周度
    val currentHalfYear = currentYearWeek.substring(0, 2).toInt
    val calcHalfYear = String.valueOf(year).substring(2, 4).toInt
    val actualWeek = if (currentHalfYear != calcHalfYear) maxWeek + currentWeek else currentWeek

    //获取指定周度的时间
    ca.add(Calendar.DAY_OF_MONTH, (week - actualWeek) * 7)

    ca.getTime
  }

  /**
    * 获取某年的第几周的结束日期
    *
    * @param year 指定年
    * @param week 指定周
    * @return 指定年指定周的结束日期
    */
  def getLastDayOfWeek(year: Int, week: Int): Date = {
    getLastDayOfWeek(getCurrentWeekDay(year, week))
  }

  /**
    * 获取当前时间所在周的开始日期
    *
    * @param date 当前时间
    * @return 当前时间的周开始日期
    */
  def getFirstDayOfWeek(date: Date): Date = {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.MONDAY)

    c.getTime
  }

  /**
    * 获取当前时间所在周的结束日期
    *
    * @param date 当前日期
    * @return 所在周的结束日期
    */
  def getLastDayOfWeek(date: Date): Date = {
    val c = Calendar.getInstance
    c.setFirstDayOfWeek(Calendar.MONDAY)
    c.setTime(date)
    c.set(Calendar.DAY_OF_WEEK, Calendar.SUNDAY) // Sunday

    c.getTime
  }

  /**
    * 将指定时间计算时间偏移后转换为年周格式,如16W22
    */
  def dateToHalfYearWeek(date: Date): String = {
    val ca = Calendar.getInstance()
    ca.setTime(date)
    ca.setFirstDayOfWeek(Calendar.MONDAY)

    val year = ca.get(Calendar.YEAR)
    val week = ca.get(Calendar.WEEK_OF_YEAR)

    ca.add(Calendar.DAY_OF_MONTH, -7)
    val beforeYear = ca.get(Calendar.YEAR)

    ca.add(Calendar.DAY_OF_MONTH, 14)
    val afterYear = ca.get(Calendar.YEAR)
    val afterWeek = ca.get(Calendar.WEEK_OF_YEAR)

    val calcYear = if (year == beforeYear && year == afterYear) year
    else if (year != afterYear) {
      if (afterWeek - week == 1) afterYear else year
    }
    else {
      year
    }

    calcYear.toString.substring(2) + "W" + (if (week < 10) "0" + week else "" + week)
  }

  def main(args: Array[String]) {

    //1477483944000
    //2016/10/26 15:15   2016/10/26 18:10
    //    val arr = splitTime("", 1477483944000L, 1477484004000L)
    //            val arr = splitTime("",1477484004000L, 1477490400000L)
    //2016/10/25 23:3:24 2016/10/26 1:13:0
    //    val arr = splitTime("", 1477411200000L, 1477415580000L)
    //    for (i <- 0 until arr.length) yield println(arr(i)) //将得到ArrayBuffer(2,6,4,-2,-4)
    //    splitTimeByMinute("", "00:01", "02:29")
    //    println(delZero("2109"))
    //    val res = None.toString

    //    println(addZero("1"))

//    println(isRestday("2017-06-13"))
//    println(isRestday("2017-06-17"))
//    println(isRestday("2017-06-18"))

    println(fullSecondAddMin("13:08:60"))
  }


}
package com.avcdata.vbox.util

import java.text.SimpleDateFormat
import java.util.Calendar

import com.github.nscala_time.time.Imports._
import org.joda.time.DateTime

import scala.collection.mutable.ArrayBuffer
import scala.math.BigDecimal

object TimeUtils {


  final val ONE_HOUR_MILLISECONDS = 60 * 60 * 1000

  final val SECOND_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss"

  final val DAY_DATE_FORMAT_ONE = "yyyy-MM-dd"

  final val DAY_DATE_FORMAT_TWO = "yyyyMMdd"



  //时间字符串=>时间戳
  def convertDateStr2TimeStamp(dateStr: String, pattern: String): Long = {
    new SimpleDateFormat(pattern).parse(dateStr).getTime
  }


  //时间字符串+天数=>时间戳
  def dateStrAddDays2TimeStamp(dateStr: String, pattern: String, days: Int): Long = {
    convertDateStr2Date(dateStr, pattern).plusDays(days).date.getTime
  }

  //时间字符串+天数=>时间字符串
  def dateStrAddDays(dateStr: String, pattern: String, days: Int): String = {
    DateTime.parse(dateStr).plusDays(days).toString(pattern)
  }


  //时间字符串=>日期
  def convertDateStr2Date(dateStr: String, pattern: String): DateTime = {
    new DateTime(new SimpleDateFormat(pattern).parse(dateStr))
  }


  //时间戳=>日期
  def convertTimeStamp2Date(timestamp: Long): DateTime = {
    new DateTime(timestamp)
  }

  //时间戳=>字符串
  def convertTimeStamp2DateStr(timestamp: Long, pattern: String): String = {
    new DateTime(timestamp).toString(pattern)
  }

  //时间戳=>小时数
  def convertTimeStamp2Hour(timestamp: Long): Long = {
    new DateTime(timestamp).hourOfDay().getAsString().toLong
  }


  //时间戳=>分钟数
  def convertTimeStamp2Minute(timestamp: Long): Long = {
    new DateTime(timestamp).minuteOfHour().getAsString().toLong
  }

  //时间戳=>秒数
  def convertTimeStamp2Sec(timestamp: Long): Long = {
    new DateTime(timestamp).secondOfMinute().getAsString.toLong
  }

  //TODO 对开始和结束时间分时(一天之内)
  //other_info最后加上"\t"
  def splitTimeByHour(other_info: String, launchTimeStamp: Long, exitTimeStamp: Long): ArrayBuffer[String] = {

    val arr = ArrayBuffer[String]()

    val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)

    val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)

    val diffTimeStamp = (exitTimeStamp - launchTimeStamp)

    if (exitHour - launchHour == 0) {
      //其他信息 整点 次数 时长
      arr.append(other_info + launchHour + "\t" + 1 + "\t" + diffTimeStamp / 1000)
    }

    if (exitHour - launchHour > 0) {
      val launchHour = TimeUtils.convertTimeStamp2Hour(launchTimeStamp)
      val launchMinute = TimeUtils.convertTimeStamp2Minute(launchTimeStamp)
      val launchSec = TimeUtils.convertTimeStamp2Sec(launchTimeStamp)

      val exitHour = TimeUtils.convertTimeStamp2Hour(exitTimeStamp)
      val exitMinute = TimeUtils.convertTimeStamp2Minute(exitTimeStamp)
      val exitSec = TimeUtils.convertTimeStamp2Sec(exitTimeStamp)

      arr.append(other_info + launchHour + "\t" + 1 + "\t" + (60 * 60 - (launchMinute * 60 + launchSec)))

      var h = launchHour
      //需要分时
      while (h < exitHour) {
        h = h + 1
        if (h != exitHour) {
          arr.append(other_info + h +
            "\t" + 0 + "\t" + 1 * 60 * 60)
        } else {
          arr.append(other_info + h + "\t" + 0 + "\t" + (exitMinute * 60 + exitSec))
        }
      }
    }

    arr
  }

  //对开始和结束时间分时
  def splitTimeByMinute(other_info: String, startTime: String, endTime: String): ArrayBuffer[String] = {

    val arr = ArrayBuffer[String]()
    //00:01  00:29
    val startHour = startTime.charAt(0).toString.toInt == 0 match {
      case true => startTime.charAt(1).toString.toInt
      case false => startTime.substring(0, 2).toInt
    }

    //    println("startHour:" + startHour)


    val startMin = startTime.charAt(3).toString.toInt == 0 match {
      case true => startTime.charAt(4).toString.toInt
      case false => startTime.substring(3).toInt
    }

    //    println("startMin:" + startMin)


    val endHour = endTime.charAt(0).toString.toInt == 0 match {
      case true => endTime.charAt(1).toString.toInt
      case false => endTime.substring(0, 2).toInt
    }

    //    println("endHour:" + endHour)

    val endMin = endTime.charAt(3).toString.toInt == 0 match {
      case true => endTime.charAt(4).toString.toInt
      case false => endTime.substring(3).toInt
    }

    //    println("endMin:" + endMin)

    val diffHour = endHour - startHour

    if (diffHour == 0) {

      var i = startMin

      while (i <= endMin) {
        arr.append(other_info + startHour.toString + "\t" + i.toString)
        i = i + 1
      }

    }

    if (diffHour > 0) {
      var i = startMin

      while (i <= 59) {
        arr.append(other_info + startHour.toString + "\t" + i.toString)
        i = i + 1
      }


      var h = startHour
      //需要分时
      while (h < endHour) {
        h = h + 1
        if (h != endHour) {
          for (n <- 0 to 59) {
            arr.append(other_info + h.toString + "\t" + n.toString)
          }
        } else {
          for (n <- 0 to endMin) {
            arr.append(other_info + h.toString + "\t" + n.toString)
          }
        }
      }

      //      for (i <- 1 to diffHour) {
      //
      //
      //      }


    }

    arr
  }

  def addZero(hourOrMin: String): String = {
    if (hourOrMin.toInt <= 9)
      "0" + hourOrMin
    else
      hourOrMin

  }

  def delZero(hourOrMin: String): String = {
    var res = hourOrMin
    if (!hourOrMin.equals("0") && hourOrMin.startsWith("0"))
      res = res.replaceAll("^0", "")
    res
  }


  def getTimeNum(time: String): Int = {
    1
  }

  def dateStrPatternOne2Two(time: String): String = {
    TimeUtils.convertTimeStamp2DateStr(TimeUtils.convertDateStr2TimeStamp(time, TimeUtils
      .DAY_DATE_FORMAT_ONE), TimeUtils.DAY_DATE_FORMAT_TWO)
  }

  /**
    * 获取下一个月的第一天
    *
    * @param dateStr
    * @param pattern
    * @return
    */
  def getPerFirstDayOfMonth(dateStr: String, pattern: String): String = {
    val dft = new SimpleDateFormat(pattern);
    val calendar = java.util.Calendar.getInstance()
    calendar.setTime(dft.parse(dateStr));
    calendar.add(Calendar.MONTH, 1);
    calendar.set(Calendar.DAY_OF_MONTH, calendar.getActualMinimum(Calendar.DAY_OF_MONTH));
    return dft.format(calendar.getTime());
  }


  def main(args: Array[String]) {
    val num1 = BigDecimal(2)./(BigDecimal(5))
    val num2 = BigDecimal(2)./(BigDecimal(5))

    println(num1.+(num2).toString)

    //1477483944000
    //2016/10/26 15:15   2016/10/26 18:10
    //    val arr = splitTime("", 1477483944000L, 1477484004000L)
    //            val arr = splitTime("",1477484004000L, 1477490400000L)
    //2016/10/25 23:3:24 2016/10/26 1:13:0
    //    val arr = splitTime("", 1477411200000L, 1477415580000L)
    //    for (i <- 0 until arr.length) yield println(arr(i)) //将得到ArrayBuffer(2,6,4,-2,-4)
    //    splitTimeByMinute("", "00:01", "02:29")
    //    println(delZero("2109"))
    //    val res = None.toString
    //
    //    println(addZero("1"))

    //    println(getPerFirstDayOfMonth("2017-04-27",TimeUtils.DAY_DATE_FORMAT_ONE))

  }


}



package com.avcdata.spark.job.executor

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.total._
import com.avcdata.spark.job.total.tnum._
import com.avcdata.spark.job.total.tnumpre._
import org.apache.log4j.Logger

object TnumPreTotalExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //推总前
    //overview_OC  4core 5G 4
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@OverviewOCTnumPreTotalJob start ... ")
      OverviewOCTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewOCTnumPreTotalJob end ... ")
    }

    //overview hour oc 4core 5G 4
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@OverviewHourOCTnumPreTotalJob start ... ")
      OverviewHourOCTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewHourOCTnumPreTotalJob end ... ")
    }


    //overview_live (5) 8core 10G 8
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@OverviewLiveTnumPreTotalJob start ... ")
      OverviewLiveTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewLiveTnumPreTotalJob end ... ")
    }
    

    //overview hour live 8core 10G 8
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@OverviewHourLiveTnumPreTotalJob start ... ")
      OverviewHourLiveTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewHourLiveTnumPreTotalJob end ... ")
    }



    //overview_apk (5) 4core 5G 4
    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@OverviewApkTnumPreTotalJob start ... ")
      OverviewApkTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewApkTnumPreTotalJob end ... ")
    }


    //overview hour apk 4core 5G 4
    if (executePart.charAt(5) == '1') {
      println(analysisDate + "@OverviewHourApkTnumPreTotalJob start ... ")
      OverviewHourApkTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewHourApkTnumPreTotalJob end ... ")
    }


    //live 8core 10G 8
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@LiveTnumPreTotalJob start ... ")
      LiveTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveTnumPreTotalJob end ... ")
    }

    //live hour  8core 10G 8
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@LiveHourTnumPreTotalJob start ... ")
      LiveHourTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveHourTnumPreTotalJob end ... ")
    }

    //apk 4core 5G 4
    if (executePart.charAt(8) == '1') {
      println(analysisDate + "@ApkTnumPreTotalJob start ... ")
      ApkTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkTnumPreTotalJob end ... ")
    }

    //apk hour 4core 5G 4
    if (executePart.charAt(9) == '1') {
      println(analysisDate + "@ApkHourTnumPreTotalJob start ... ")
      ApkHourTnumPreTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkHourTnumPreTotalJob end ... ")
    }

//    //play 8core 10G 8
//    if (executePart.charAt(10) == '1') {
//      println(analysisDate + "@PlayTnumPreTotalJob start ... ")
//      PlayTnumPreTotalJob.run(sc, analysisDate);
//      println(analysisDate + "@PlayTnumPreTotalJob end ... ")
//    }
//
//    // play hour  8core 10G 8
//    if (executePart.charAt(11) == '1') {
//      println(analysisDate + "@PlayHourTnumPreTotalJob start ... ")
//      PlayHourTnumPreTotalJob.run(sc, analysisDate);
//      println(analysisDate + "@PlayHourTnumPreTotalJob end ... ")
//    }




  }

}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.total.tnum2partition._
import org.apache.log4j.Logger

object Tnumtotal2partitionExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext


    //    overview 4core 5G 4
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@OverviewTnumTotal2Partition start ... ")
      OverviewTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@OverviewTnumTotal2Partition end ... ")
    }


    //    overview分时 4core 5G 4
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@OverviewHourTnumTotal2Partition start ... ")
      OverviewHourTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@OverviewHourTnumTotal2Partition end ... ")
    }

    //    直播 4core 5G 4
    //    overview分时 4core 5G 4
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@LiveTnumTotal2Partition start ... ")
      LiveTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@LiveTnumTotal2Partition end ... ")
    }

    //    直播分时  4core 5G 4
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@LiveHourTnumTotal2Partition start ... ")
      LiveHourTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@LiveHourTnumTotal2Partition end ... ")
    }

    //    apk  4core 5G 4
    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@ApkTnumTotal2Partition start ... ")
      ApkTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@ApkTnumTotal2Partition end ... ")
    }



    //    apk分时  4core 5G 4
    if (executePart.charAt(5) == '1') {
      println(analysisDate + "@ApkHourTnumTotal2Partition start ... ")
      ApkHourTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@ApkHourTnumTotal2Partition end ... ")
    }


    //    到剧  4core 5G 4
    if (executePart.charAt(6) == '1') {
      println(analysisDate + "@PlayTnumTotal2Partition start ... ")
      PlayTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@PlayTnumTotal2Partition end ... ")
    }


    //    到剧分时  4core 5G 4
    if (executePart.charAt(7) == '1') {
      println(analysisDate + "@PlayHourTnumTotal2Partition start ... ")
      PlayHourTnumTotal2Partition.run(sc, analysisDate);
      println(analysisDate + "@PlayHourTnumTotal2Partition end ... ")
    }


  }

}
package com.avcdata.spark.job.executor

import com.avcdata.spark.job.coocaa.Helper
import com.avcdata.spark.job.total.tnum._
import org.apache.log4j.Logger

/**
  * Created by avc on 2017/2/14.
  */
object TnumTotalExecutor {

  def main(args: Array[String]): Unit = {

    val log = Logger.getLogger(getClass.getName)

    val analysisDate = Helper.parseOptions(args, 0, "")

    val executePart = Helper.parseOptions(args, 1, "0000000000000000000000")

    val sc = Helper.sparkContext

    //推总
    //overview 4core 5G 4
    if (executePart.charAt(0) == '1') {
      println(analysisDate + "@OverviewTnumTotalJob start ... ")
      OverviewTnumTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewTnumTotalJob end ... ")
    }

    //overview hour 4core 5G 4
    if (executePart.charAt(1) == '1') {
      println(analysisDate + "@OverviewHourTotalJob start ... ")
      OverviewHourTotalJob.run(sc, analysisDate);
      println(analysisDate + "@OverviewHourTotalJob end ... ")
    }



    //live 4core 5G 4
    if (executePart.charAt(2) == '1') {
      println(analysisDate + "@LiveTotalJob start ... ")
      LiveTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveTotalJob end ... ")
    }



    //live_hour 4core 5G 4
    if (executePart.charAt(3) == '1') {
      println(analysisDate + "@LiveHourTotalJob start ... ")
      LiveHourTotalJob.run(sc, analysisDate);
      println(analysisDate + "@LiveHourTotalJob end ... ")
    }

    //apk 4core 5G 4
    if (executePart.charAt(4) == '1') {
      println(analysisDate + "@ApkTotalJob start ... ")
      ApkTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkTotalJob end ... ")
    }

    //apk_hour 4core 5G 4
    if (executePart.charAt(5) == '1') {
      println(analysisDate + "@ApkHourTotalJob start ... ")
      ApkHourTotalJob.run(sc, analysisDate);
      println(analysisDate + "@ApkHourTotalJob end ... ")
    }


    //到剧  4core 5G 4
    //    if (executePart.charAt(6) == '1') {
    //      println(analysisDate + "@PlayTotalJob start ... ")
    //      PlayTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@PlayTotalJob end ... ")
    //    }
    //
    //    if (executePart.charAt(7) == '1') {
    //      println(analysisDate + "@PlayHourTotalJob start ... ")
    //      PlayHourTotalJob.run(sc, analysisDate);
    //      println(analysisDate + "@PlayHourTotalJob end ... ")
    //    }


  }


}
package utils

import java.text.SimpleDateFormat
import java.util.Date

import com.alibaba.fastjson.JSON
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes
import pojo.SDKStreamToBean.{mysqlpassword, mysqlurl, mysqlusername}

import scala.util.Random
import pojo.{DataDetail, SDKStreamBean, SDKStreamToBean}

import scala.collection.mutable.ListBuffer

/**
  * Created by guxiaoyang on 2017/5/10.
  */
object Tools
{
  /*
  尝试字符串转数字
   */
  def getInt(str: String): Int =
  {
    val result = scala.util.Try(str.toInt).toOption
    if (result == None) -1 else result.get
  }

  /*
  获取随机SN
   */
  def getRandomSN(): String =
  {

    val rnd = new Random()
    val pattern = "abcdefghijklmnopqrstuvwxyz0123456789"
    var temp = pattern.charAt(rnd.nextInt(36)).toString + pattern.charAt(rnd.nextInt(36)).toString + ":" + pattern.charAt(rnd.nextInt(36)) + pattern.charAt(rnd.nextInt(36)) + ":" + pattern.charAt(rnd.nextInt(36)) + pattern.charAt(rnd.nextInt(36)) + ":" + pattern.charAt(rnd.nextInt(36)) + pattern.charAt(rnd.nextInt(36)) + ":" + pattern.charAt(rnd.nextInt(36)) + pattern.charAt(rnd.nextInt(36))
    temp
  }

  def getTimeFromUnix(timeTemp: String): String =
  {
    val sdf: SimpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val date: String = sdf.format(new Date(timeTemp.toLong * 1000l))

    date
  }

  def getJsonBean(stringTemp: String): ListBuffer[SDKStreamBean] =
  {
    val msgArr = stringTemp.split("#@#@#")
    val list = new ListBuffer[SDKStreamBean]()
    msgArr.foreach(jsonString =>
    {

      var msg = jsonString.toLowerCase.replace("""\n\"data\":[{""", "").replace(";", ",").replace("u'", "").replace("""\n""", "").replace("'", "").replace(",}", "}").replace("""}"""", "}").replace(""""{""", "{").replace("""\"""", """"""").replace("""["""", "[").replace(""""]""", "]").replace(" ", "").replace("}]}]", "}]").replace("messagetpye", "messagetype").replace(":000000000000000000000000",""":"000000000000000000000000"""").replace(""""date:""",""""date":""").replace(""""programend"1""",""""programend":1""").replace("""0"programend"""","""0,"programend"""").replace(""",duration""",""","duration""").replace("}{", "},{")


      val index1 = msg.indexOf("mac=")

      if (index1 != -1)
      {
        val left = msg.substring(0, index1)
        var right = msg.substring(index1, msg.length)
        val index2 = right.indexOf(""""data":[""")
        if (index2 != -1)
        {
          right = right.substring(index2 + 8, right.length)
        }
        msg = left + right
      }

      var jsonObj = scala.util.Try(JSON.parseObject(msg)).toOption
      if (jsonObj == None)
      {
        msg = msg + "]}"
        jsonObj = scala.util.Try(JSON.parseObject(msg)).toOption
      }
      if (jsonObj != None)
      {
        list += SDKStreamToBean.ToBean(jsonObj.get)
      }
      //不再保存错误信息
      //      else
      //      {
      //        try
      //        {
      //          LoanPattern.using(JDBCConnectionPool(mysqlurl, mysqlusername, mysqlpassword))
      //          { conn =>
      //            val exceptionStr = new String(jsonString.getBytes, "UTF-8")
      //            DBHelper.insertException(exceptionStr, conn)
      //          }
      //        }
      //        finally
      //        {
      //          list += null
      //        }
      //      }
    }
    )
    list
  }

  def getPutForStream(dataDetail: DataDetail, sn: String, manufacturer: String): Put =
  {
    //var operation = "Unit"
    //簇
    val statusFamilyCol = Bytes.toBytes("status")
    //"status".getBytes("UTF-8")
    //Bytes.toBytes("status")
    val contentFamilyCol = Bytes.toBytes("content")

    val snStatusCol = Bytes.toBytes("sn")
    //mac地址
    val dateStatusCol = Bytes.toBytes("date")
    //时间戳日期部分
    val hourStatusCol = Bytes.toBytes("hour")
    //时间戳时间部分
    val manufacturerStatusCol = Bytes.toBytes("manufacturer")
    //厂商
    val messageTypeCol = Bytes.toBytes("messagetype")
    //每个行为的入库时间
    val inserttimeCol = Bytes.toBytes("inserttime")


    val rowKey = sn + dataDetail.date + dataDetail.hour + manufacturer

    //println("rowKey:" + rowKey)
    val put = new Put(Bytes.toBytes(rowKey))
    dataDetail.messagetype match
    {
      case 1 =>
      {
        //operation = "poweron" //开机

        put.addColumn(contentFamilyCol, Bytes.toBytes("build_date"), Bytes.toBytes(dataDetail.build_date))
        put.addColumn(contentFamilyCol, Bytes.toBytes("product"), Bytes.toBytes(dataDetail.product))
        put.addColumn(contentFamilyCol, Bytes.toBytes("version_release"), Bytes.toBytes(dataDetail.version_release))
        put.addColumn(contentFamilyCol, Bytes.toBytes("name"), Bytes.toBytes(dataDetail.name))
        put.addColumn(contentFamilyCol, Bytes.toBytes("description"), Bytes.toBytes(dataDetail.description))
        put.addColumn(contentFamilyCol, Bytes.toBytes("model"), Bytes.toBytes(dataDetail.model))
      }
      case 2 =>
      {
        //operation = "heartbeat"

        put.addColumn(contentFamilyCol, Bytes.toBytes("restarttime"), Bytes.toBytes(dataDetail.restarttime))
      }
      case 3 =>
      {
        //operation = "videoondemand"

        //println("节目名称:" + programname)
        put.addColumn(contentFamilyCol, Bytes.toBytes("duration"), Bytes.toBytes(dataDetail.duration))
        put.addColumn(contentFamilyCol, Bytes.toBytes("programstart"), Bytes.toBytes(dataDetail.programstart))
        put.addColumn(contentFamilyCol, Bytes.toBytes("programtype"), Bytes.toBytes(dataDetail.programtype))
        put.addColumn(contentFamilyCol, Bytes.toBytes("provider"), Bytes.toBytes(dataDetail.provider))
        put.addColumn(contentFamilyCol, Bytes.toBytes("programend"), Bytes.toBytes(dataDetail.programend))
        put.addColumn(contentFamilyCol, Bytes.toBytes("programname"), Bytes.toBytes(dataDetail.programname))
        put.addColumn(contentFamilyCol, Bytes.toBytes("package_name"), Bytes.toBytes(dataDetail.package_name))
        put.addColumn(contentFamilyCol, Bytes.toBytes("order"), Bytes.toBytes(dataDetail.order))
      }
      case 5 =>
      {
        //operation = "maclist"

        put.addColumn(contentFamilyCol, Bytes.toBytes("ipmac"), Bytes.toBytes(dataDetail.ipmac))
      }
      case 13 =>
      {
        //operation = "province"
        //println(province + city)
        put.addColumn(contentFamilyCol, Bytes.toBytes("province"), Bytes.toBytes(dataDetail.province))
        put.addColumn(contentFamilyCol, Bytes.toBytes("city"), Bytes.toBytes(dataDetail.city))
      }

      case 20 =>
      {
        //operation = "standby"
      }
      case 21 =>
      {
        //operation = "wakeup"
      }
      case _ =>
      {

      }
    }

    //增加一个每条行为的入库时间
    put.addColumn(statusFamilyCol, inserttimeCol, Bytes.toBytes(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())))

    put.addColumn(statusFamilyCol, messageTypeCol, Bytes.toBytes(dataDetail.messagetype.toString))
    put.addColumn(statusFamilyCol, snStatusCol, Bytes.toBytes(sn))
    put.addColumn(statusFamilyCol, dateStatusCol, Bytes.toBytes(dataDetail.date))
    put.addColumn(statusFamilyCol, hourStatusCol, Bytes.toBytes(dataDetail.hour))
    put.addColumn(statusFamilyCol, manufacturerStatusCol, Bytes.toBytes(manufacturer))
    put

  }
}
package com.avcdata;

import com.avcdata.config.ConfigInfo;
import com.avcdata.core.MsgToUser;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Configuration;
import org.springframework.stereotype.Component;
import redis.clients.jedis.Jedis;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.List;

/**
 * Created by dev on 15-10-16.
 */
@Slf4j
@Configuration
@Component
public class TransferByBatch {

    @Autowired
    ConfigInfo configInfo;

    @Autowired
    MsgToUser msgToUser;

    Connection conn = null;
    PreparedStatement pst = null;
    protected ObjectMapper mapper = new ObjectMapper();
    Jedis jedis = null;

    List<String> attrVOList = new ArrayList<>();


    private final String TMP_TABLE = "urltmp";
    private final String FINAL_TABLE = "url";

    final int processCount = 30000;
    final String key1 = "jdAttributeUrlSpider:items";

    private final String title = "INSERT INTO " + TMP_TABLE + " (url,catId,platform,category1) VALUES";
    private final String SQL_DROP_TABLE = "DROP TABLE IF EXISTS " + TMP_TABLE;
    private final String SQL_CREATE_TABLE = "CREATE TABLE " + TMP_TABLE + " (id int(15) NOT NULL AUTO_INCREMENT," +
            "url varchar(500) DEFAULT NULL,catid varchar(30) DEFAULT NULL," +
            "platform varchar(50) DEFAULT NULL,category1 varchar(50) DEFAULT NULL," +
            "PRIMARY KEY (id)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8";
    private final String SQL_INSERT_FROM_MYSQL = "INSERT INTO " + TMP_TABLE + "(url,catid,platform,category1) " +
            "SELECT url,attr3,attr1,attr2 FROM " + FINAL_TABLE + " WHERE scheduler_id='2c9180a6530c084501530c08b97d0004'";

    private final String SQL_DELETE_URL_FROM_WHERE = "DELETE FROM " + FINAL_TABLE +
            " WHERE scheduler_id='2c9180a6530c084501530c08b97d0004'";
    private final String SQL_URLTMP_TO_URL = "INSERT INTO " + FINAL_TABLE +
            " SELECT REPLACE(UUID(),'-','') id,NULL crawl_end_date,NULL crawl_start_date,NULL create_by,NULL create_date," +
            "'2c9180a6530c084501530c08b97d0004' scheduler_id,'京东属性采集' scheduler_name,'2c9180a6530c084501530c08b96b0002' spider_id," +
            "'jdAttributeSpider' spider_name,url,platform,category1,catid,NULL,NULL,NULL,NULL,NULL,NULL,NULL FROM " + TMP_TABLE +
            " WHERE id IN (SELECT MIN(id) FROM " + TMP_TABLE + " GROUP BY url)";

    int sum = 0;

    private synchronized void addValue() {
        sum++;
    }

    /**
     * 处理增删改查
     *
     * @param sql
     * @return
     */
    public int processTable(String sql) {
        int result = 0;
        PreparedStatement ps = null;
        try {
            conn = this.getConnection();
            conn.setAutoCommit(true);
            ps = conn.prepareStatement(sql);
            ps.executeUpdate();
            if (!conn.getAutoCommit())
                conn.commit();
        } catch (Exception e) {
            e.printStackTrace();
            System.exit(0);
        } finally {
            try {
                if (ps != null)
                    ps.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
        return result;
    }

    public void processMessagestart() {

        //将上周采集的url放入临时表
        int dropTable = processTable(SQL_DROP_TABLE);
        log.debug("SQL_DROP_TABLE result code {} ", dropTable);
        int crateTable = processTable(SQL_CREATE_TABLE);
        log.debug("SQL_CREATE_TABLE result code {} ", crateTable);
        int insertFromMysql = processTable(SQL_INSERT_FROM_MYSQL);
        log.debug("SQL_INSERT_FROM_MYSQL result code {} ", insertFromMysql);

        //write to mysql from redis
        this.processAttr();

        //处理临时表,去重,插入url表
        log.debug("SQL_DELETE_URL_FROM_WHERE result code {} ", 2);
        int deleteUrlFromUrl = processTable(SQL_DELETE_URL_FROM_WHERE);
        log.debug("SQL_DELETE_URL_FROM_WHERE result code {} ", deleteUrlFromUrl);
        int urltmpTourl = processTable(SQL_URLTMP_TO_URL);
        log.debug("SQL_URLTMP_TO_URL result code {} ", urltmpTourl);

        this.deleteRedisKeys();
        this.distory();

    }

    private void deleteRedisKeys() {
        connectRedis();
        jedis.del("jdAttributeUrlSpider:items", "jdAttributeUrlSpider:requests", "jdAttributeUrlSpider:start_urls");
        jedis.close();
    }

    /**
     * 读取 redis 写入mysql
     */
    public void processAttr() {

        try {
            String info;
            boolean flg = true;
            int loop = 0;
            while (flg) {
                connectRedis();
                info = jedis.lpop(key1);
                if (info != null) {
                    loop = 0;
                    attrVOList.add(info);
                    if (attrVOList.size() >= processCount) {
                        jedis.close();
                        addValue();
                        processToMysql();
                        log.debug("input redis batch {}", sum);
                        attrVOList.clear();
                    }
                } else {
                    closeRedis();
                    log.debug("list size {} > loop{}", attrVOList.size(), loop);
                    loop++;
                    Thread.sleep(200000);
                    if (loop > 10) {
                        flg = false;
                    }
                }
            }
            processToMysql();
        } catch (Exception e) {
            e.printStackTrace();
            System.exit(0);
        }

    }

    /**
     * 分批次读取
     */
    private void processToMysql() {

        try {

            StringBuffer suffix = new StringBuffer();
            conn.setAutoCommit(false);
            pst = conn.prepareStatement("");
            for (String str : attrVOList) {
                AttrVO attrVO = mapper.readValue(str, AttrVO.class);
                // 构建sql后缀
                suffix.append("('" + attrVO.getUrl() + "', '" +
                        attrVO.getCatId() + "', '" +
                        attrVO.getPlatform() + "', '" +
                        attrVO.getCategory1() + "'),");
            }

            if (attrVOList.size() > 0) {
                String sql = title + suffix.substring(0, suffix.length() - 1);
                pst.addBatch(sql);
                // 执行操作
                pst.executeBatch();
                // 提交事务
                conn.commit();
            }
        } catch (Exception e) {
            e.printStackTrace();
            System.exit(0);
        }
    }

    /**
     * 连接
     *
     * @return
     * @throws Exception
     */
    private Connection getConnection() throws Exception {
        String connectionString = configInfo.getUrl();
        Class.forName(configInfo.getDriver());
        return DriverManager.getConnection(connectionString, configInfo.getUsername(), configInfo.getPassword());
    }

    public void connectRedis() {
        if (jedis == null || !jedis.isConnected())
            jedis = new Jedis(configInfo.getRedishost(), Integer.parseInt(configInfo.getRedisport()), 60000);
    }

    public void closeRedis() {
        if (jedis.isConnected())
            jedis.close();
    }

    /**
     * 断开连接
     */
    private void distory() {
        if (conn != null) {
            try {
                conn.close();
            } catch (SQLException e) {
                e.printStackTrace();
                System.exit(0);
            }
        }
    }
}package com.avcdata.etl.util.hdfs

import com.avcdata.etl.common.util.HdfsFileUtil
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

/**
  * 遍历文件夹
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/12/13 09:41
  */
object TraverseFolder
{
  def main(args: Array[String])
  {
    val conf = new Configuration()
    //conf.set("fs.defaultFS", "hdfs://nn.avcdata.com:8020")

    val fs = FileSystem.get(conf)

    val path = new Path("/user/hue/oozie/workspaces")
    val folders = fs.listStatus(path)
    folders.foreach
    { pf =>

      val childPathStr = pf.getPath.toString + "/lib"
      val childPath = new Path(childPathStr)

      if (HdfsFileUtil.exists(childPathStr, conf))
      {
        fs.listStatus(childPath).foreach
        { childFile =>

          val childPath = childFile.getPath.toString

          println(childPath)

          fs.delete(new Path(childPath), false)
        }
      }
    }
  }
}
package com.avcdata.etl.launcher.util

import com.avcdata.etl.common.udf.{ArrayUDF, AvailableTimePeriodUDF, DateTimeUDF}
import com.avcdata.etl.common.util.ChineseToEnglish
import org.apache.spark.sql.hive.HiveContext

/**
  * UDF注册相关
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 16/6/5 下午1:10
  */
object UDFUtil
{
  /**
    * 注册UDF
    *
    * @param hiveContext Hive Context
    */
  def registerUDF(hiveContext: HiveContext): Unit =
  {
    hiveContext.udf.register("timeToWeek", DateTimeUDF.timeToWeek)

    hiveContext.udf.register("timeToYearWeek", DateTimeUDF.timeToYearWeek)

    hiveContext.udf.register("timeToLastYearWeek", DateTimeUDF.timeToLastYearWeek)

    hiveContext.udf.register("timeToBeginYearWeek", DateTimeUDF.timeToBeginYearWeek)

    hiveContext.udf.register("timeToLastBeginYearWeek", DateTimeUDF.timeToLastBeginYearWeek)

    hiveContext.udf.register("dateToYearWeek", DateTimeUDF.dateToYearWeek)

    hiveContext.udf.register("yearWeek", DateTimeUDF.yearWeek)

    hiveContext.udf.register("timeFormat", DateTimeUDF.timeFormat)

    hiveContext.udf.register("fullTimeFormat", DateTimeUDF.fullTimeFormat)

    hiveContext.udf.register("timeToWeekStart", DateTimeUDF.timeToWeekStart)

    hiveContext.udf.register("timeToWeekEnd", DateTimeUDF.timeToWeekEnd)

    hiveContext.udf.register("timeToMonthStart", DateTimeUDF.timeToMonthStart)

    hiveContext.udf.register("timeToMonthEnd", DateTimeUDF.timeToMonthEnd)

    hiveContext.udf.register("availableWeek", AvailableTimePeriodUDF.availableWeek)

    hiveContext.udf.register("availableMonth", AvailableTimePeriodUDF.availableMonth)

    hiveContext.udf.register("availableDay", AvailableTimePeriodUDF.availableDay)

    hiveContext.udf.register("dbOrCoordWeek", AvailableTimePeriodUDF.dbOrCoordWeek)

    hiveContext.udf.register("dbOrCoordMonth", AvailableTimePeriodUDF.dbOrCoordMonth)

    hiveContext.udf.register("dbOrCoordDate", AvailableTimePeriodUDF.dbOrCoordDate)

    hiveContext.udf.register("dbOrCoordPeriod", AvailableTimePeriodUDF.dbOrCoordPeriod)

    hiveContext.udf.register("arrayFilterEmptyValue", ArrayUDF.arrayFilterEmptyValue)

    hiveContext.udf.register("pinyin", ChineseToEnglish.getPinYin _)
  }
}
package com.avcdata.spark.job.etl.util

import java.text.SimpleDateFormat
import java.util.Calendar

import org.apache.spark.sql.SQLContext

object UDFUtils {

  def main(args: Array[String]) {
    println(dayOfWeek("2017-06-13"))
  }


  def registerUDF(sqlContext: SQLContext, udfName: String): Unit = {
    udfName match {
      case "dayOfWeek" => sqlContext.udf.register(udfName, dayOfWeek _)
    }

  }


  def dayOfWeek(dateStr: String): Int = {
    val sdf = new SimpleDateFormat("yyyy-MM-dd")
    val date = sdf.parse(dateStr)

    //    val sdf2 = new SimpleDateFormat("EEEE")
    //    sdf2.format(date)

    val cal = Calendar.getInstance();
    cal.setTime(date);
    var w = cal.get(Calendar.DAY_OF_WEEK) - 1;

    //星期天 默认为0
    if (w <= 0)
      w = 7
    w
  }



}package com.avcdata.vbox.util

import org.apache.spark.sql.SQLContext
import java.text.SimpleDateFormat
import java.util.Calendar

object UDFUtils {

  def main(args: Array[String]) {
//    println(dayOfWeek("2017-06-13"))
    println(inet_aton("222.85.85.85"))
    //[{"w":"40.026203","j":"116.227029","p":"北京市","c":"北京市","d":"海淀区"}, {"w":"39.645216","j":"116.412542","p":"北京市","c":"北京市","d":"大兴区"}]

    //          val cols = areaInfo.split(",")


  }


  def registerUDF(sqlContext: SQLContext, udfName: String): Unit = {
    udfName match {
      case "dayOfWeek" => sqlContext.udf.register(udfName, dayOfWeek _)
      case "inet_aton" => sqlContext.udf.register(udfName, inet_aton _)
    }

  }


  def inet_aton(dateStr: String): Long = {
    val segments = dateStr.split("\\.")
    (segments(0).toLong * 16777216L + segments(1).toLong * 65536L + segments(2).toLong * 256L + segments(3).toLong)
  }

  def dayOfWeek(dateStr: String): Int = {
    val sdf = new SimpleDateFormat("yyyy-MM-dd")
    val date = sdf.parse(dateStr)

    //    val sdf2 = new SimpleDateFormat("EEEE")
    //    sdf2.format(date)

    val cal = Calendar.getInstance();
    cal.setTime(date);
    var w = cal.get(Calendar.DAY_OF_WEEK) - 1;

    //星期天 默认为0
    if (w <= 0)
      w = 7
    w
  }


}package job

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext



/**
  * Created by guxiaoyang on 2017/6/6.
  */
object usersamesource
{
  def main(args: Array[String]): Unit =
  {
    //默认配置文件读取
    val config: Config = ConfigFactory.load()


    val conf = new SparkConf()
      .setIfMissing("spark.master", config.getString("spark.master"))
      .setIfMissing("spark.app.name", config.getString("spark.app.name"))

    var countTerminal="""select count(*) from hr.tracker_oc_fact_partition a inner join hr.tracker_apk_fact_partition b on a.sn=b.dim_sn
                        |inner join hr.tracker_live_fact_partition c on a.sn=c.dim_sn """.stripMargin

    val sc=new SparkContext(conf)
    var hiveContext=new HiveContext(sc)

    val count=hiveContext.sql(countTerminal).rdd.foreach(a=>println("count:"+a(1)))

    //println(count)

  }
}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.util.{HdfsUtils, HBaseUtils}
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

/**
  * 原始表 --> 开关机时间分布-->转换为向量-->关联其他模块-->归一化--->
  */
object UserVectorAllETL {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorBehaviorETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)



    //TODO 行为特征
    val bhPair = sqlContext.sql("select sn,stat_date,period,workday_oc_dist,restday_oc_dist,workday_channel_dist," +
      "restday_channel_dist from hr.user_vector_bh where stat_date = '" + analysisDate + "'  and  period = " + recentDaysNum).rdd
      .map(line => {
        val sn = line(0)
        val stat_date = line(1)
        val period = line(2)
        val workday_oc_dist = line(3)
        val restday_oc_dist = line(4)
        val workday_channel_dist = line(5)
        val restday_channel_dist = line(6)

        (sn + "\t" + stat_date + "\t" + period, workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist)
      })


    //TODO 到剧特征
    val playPair = sqlContext.sql("select sn,stat_date,period,pg_subject_dist,pg_year_dist,pg_region_dist from hr" +
      ".user_vector_play where stat_date = '" + analysisDate + "'  and  period = " + recentDaysNum).rdd
      .map(line => {
        val sn = line(0)
        val stat_date = line(1)
        val period = line(2)
        val pg_subject_dist = line(3)
        val pg_year_dist = line(4)
        val pg_region_dist = line(5)

        (sn + "\t" + stat_date + "\t" + period, pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist)
      })

    //TODO 默认值
    val wo_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")
    val ro_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")
    val wc_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_CHANNEL_ARR, "0")
    val rc_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_CHANNEL_ARR, "0")

    val ps_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_SUBJECT_ARR, "0")
    val py_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_YEAR_ARR, "0")
    val pr_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_REGION_ARR, "0")


    //TODO 行为到剧合并
    val bhPlayPair = bhPair.fullOuterJoin(playPair).map(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)

      val stat_date = leftCols(1)
      //      val stat_date = analysisDate

      val period = leftCols(2)
      //      val period = recentDaysNum

      val bh = line._2._1.getOrElse(wo_default + "\t" + ro_default + "\t" + wc_default + "\t" + rc_default)

      val play = line._2._2.getOrElse(ps_default + "\t" + py_default + "\t" + pr_default)

      (sn, stat_date + "\t" + period + "\t" + bh + "\t" + play)

    })


    //TODO 设备特征
    val terminalPair = sqlContext.sql("select sn,brand,province,price,size from hr.user_vector_terminal").rdd
      .map(line => {
        val sn = line(0).toString
        val brand = line(1)
        val province = line(2)
        val price = line(3)
        val size = line(4)

        (sn, brand + "\t" + province + "\t" + price + "\t" + size)
      })


    //TODO 合并成行为特征表
    val resultRDD = terminalPair.leftOuterJoin(bhPlayPair)
      .map(line => {

        val sn = line._1

        val bhPlay_default = analysisDate + "\t" + recentDaysNum + "\t" + wo_default + "\t" + ro_default + "\t" + wc_default + "\t" + rc_default + "\t" + ps_default + "\t" + py_default + "\t" + pr_default

        val bhPlayCols = line._2._2.getOrElse(bhPlay_default).split("\t")

        val stat_date = bhPlayCols(0)

        val period = bhPlayCols(1)

        val workday_oc_dist = UserVectorHelper.handleNAN(bhPlayCols(2),
          wo_default)

        val restday_oc_dist = UserVectorHelper.handleNAN(bhPlayCols(3), ro_default)

        val workday_channel_dist = UserVectorHelper.handleNAN(bhPlayCols(4), wc_default)

        val restday_channel_dist = UserVectorHelper.handleNAN(bhPlayCols(5), rc_default)

        val pg_subject_dist = UserVectorHelper.handleNAN(bhPlayCols(6), ps_default)

        val pg_year_dist = UserVectorHelper.handleNAN(bhPlayCols(7),
          py_default)

        val pg_region_dist = UserVectorHelper.handleNAN(bhPlayCols(8)
          , pr_default)

        val terminalCols = line._2._1.split("\t")
        val brand = terminalCols(0)
        val province = terminalCols(1)
        val price = terminalCols(2)
        val size = terminalCols(3)

        sn + "\t" + stat_date + "\t" + period + "\t" + brand + "\t" + province + "\t" + price + "\t" + size + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist

      })

      .filter(line => {
        !line.contains("NaN")
      })


    // TODO 将未归一化的结果 写入hbase
    resultRDD.foreachPartition(items => {
      val mutator = HBaseUtils.getMutator("user_vector_all")
      try {
        items.foreach(line => {
          val cols = line.split("\t")
          var i = 0
          val sn = cols(i)
          i = i + 1

          val stat_date = cols(i)
          i = i + 1

          val period = cols(i)
          i = i + 1

          val brand = cols(i)
          i = i + 1

          val province = cols(i)
          i = i + 1

          val price = cols(i)
          i = i + 1

          val size = cols(i)
          i = i + 1

          val workday_oc_dist = cols(i)
          i = i + 1
          val restday_oc_dist = cols(i)
          i = i + 1
          val workday_channel_dist = cols(i)
          i = i + 1
          val restday_channel_dist = cols(i)
          i = i + 1

          val pg_subject_dist = cols(i)
          i = i + 1
          val pg_year_dist = cols(i)
          i = i + 1
          val pg_region_dist = cols(i)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + brand + "\t" + province + "\t" + price + "\t" + size + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist


          mutator.mutate(HBaseUtils.getPut_UserVectorAll(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

    //TODO 将归一化的结果 写入HDFS

    println("UserVectorALL-count:"+resultRDD.count())

    /////////////////test///////////////////////////////////////////////
    val outPutPath =  "/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-" +
      "UserVectorAllETL"

    //TODO 删除旧目录
    HdfsUtils.rm(outPutPath, true)


    resultRDD
      //TODO 归一化
      .map(line => {
      val cols = line.split("\t")
      var i = 0
      val sn = cols(i)
      i = i + 1

      val stat_date = cols(i)
      i = i + 1

      val period = cols(i)
      i = i + 1

      val brand = cols(i)
      i = i + 1

      val province = cols(i)
      i = i + 1

      val price = cols(i)
      i = i + 1

      val size = cols(i)
      i = i + 1

      val workday_oc_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
      i = i + 1
      val restday_oc_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
      i = i + 1
      val workday_channel_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
      i = i + 1
      val restday_channel_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
      i = i + 1

      val pg_subject_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
      i = i + 1
      val pg_year_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
      i = i + 1
      val pg_region_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))

      sn + "\t" + stat_date + "\t" + period + "\t" + brand + "\t" + province + "\t" + price + "\t" + size + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist
    })

      .saveAsTextFile(outPutPath)

    /////////////////test//////////////////////////////////////////////


  }


}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.UserVectorHelper
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object UserVectorAllETL_RDD {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorBehaviorETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

//    val sqlContext = new HiveContext(sc)
//
//
//
//    //TODO 行为特征
//    val bhPair = UserVectorBehaviorETL.run(sc,analysisDate,recentDaysNum)
//      .map(line => {
//        val sn = line(0)
//        val stat_date = analysisDate
//        val period = recentDaysNum
//        val workday_oc_dist = line(1)
//        val restday_oc_dist = line(2)
//        val workday_channel_dist = line(3)
//        val restday_channel_dist = line(4)
//
//        (sn + "\t" + stat_date + "\t" + period, workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" +
//          "" + restday_channel_dist)
//      })
//
//
//    //TODO 到剧特征
//    val playPair = UserVectorPlayETL.run(sc,analysisDate,recentDaysNum)
//      .map(line => {
//        val sn = line(0)
//        val stat_date = analysisDate
//        val period = recentDaysNum
//        val pg_subject_dist = line(1)
//        val pg_year_dist = line(2)
//        val pg_region_dist = line(3)
//
//        (sn + "\t" + stat_date + "\t" + period, pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist)
//      })
//
//
//
//    val bhPlayPair = bhPair.join(playPair).map(line => {
//      val leftCols = line._1.split("\t")
//      val sn = leftCols(0)
//      val stat_date = leftCols(1)
//      val period = leftCols(2)
//
//      (sn, stat_date + "\t" + period + "\t" + line._2._1 + "\t" + line._2._2)
//
//    })
//
//
//    //TODO 设备特征
//    val terminalPair = UserVectorTerminalETL.run(sc,analysisDate,recentDaysNum)
//      .map(line => {
//        val sn = line(0).toString
//        val brand = line(1)
//        val province = line(2)
//        val price = line(3)
//        val size = line(4)
//
//        (sn, brand + "\t" + province + "\t" + price + "\t" + size)
//      })
//
//    bhPlayPair.join(terminalPair).map(line => {
//
//      line._1 + "\t" + line._2._1 + "\t" + line._2._2
//    })
//        .map(line=>{
//          val cols = line.split("\t")
//          var i = 0
//          val sn = cols(i)
//          i=i+1
//          val stat_date = cols(i)
//          i=i+1
//          val period = cols(i)
//          i=i+1
//          val workday_oc_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val restday_oc_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val workday_channel_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val restday_channel_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val pg_subject_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val pg_year_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val pg_region_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
//          i=i+1
//          val brand = cols(i)
//          i=i+1
//          val province = cols(i)
//          i=i+1
//          val price = cols(i)
//          i=i+1
//          val size = cols(i)
//
//        sn+"\t"+stat_date+"\t"+period+"\t"+brand+"\t"+province+"\t"+price+"\t"+size+"\t"+workday_oc_dist+"\t"+restday_oc_dist+"\t"+workday_channel_dist+"\t"+restday_channel_dist+"\t"+pg_subject_dist+"\t"+pg_year_dist+"\t"+pg_region_dist
//
//        })
//
//      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
//        .currentTimeMillis + "UserVectorAllETL")
//
//    //TODO 合并成行为特征表
//    //    val resultRDD = workday_oc_pair
//    //      .join(restday_oc_pair)
//    //      .join(workday_channel_pair)
//    //      .join(restday_channel_pair)
//
//    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
//    //        .currentTimeMillis + "UserVectorBehaviorETL")
//
//    //      .map(line => {
//    //      val sn = line._1
//    //      val workday_oc_dist = line._2._1._1._1
//    //      val restday_oc_dist = line._2._1._1._2
//    //      val workday_channel_dist = line._2._1._2
//    //      val restday_channel_dist = line._2._2
//    //
//    //      sn + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist
//    //    })
//    //
//    //    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
//    //    //        .currentTimeMillis + "UserVectorBehaviorETL")
//    //
//    //
//    //    //    TODO 写入hbase
//    //    resultRDD.foreachPartition(items => {
//    //
//    //      val mutator = HBaseUtils.getMutator("user_vector_bh")
//    //
//    //      try {
//    //
//    //        items.foreach(line => {
//    //
//    //          val cols = line.split("\t")
//    //
//    //          val sn = cols(0)
//    //          val stat_date = analysisDate
//    //          val period = recentDaysNum
//    //          val workday_oc_dist = cols(1)
//    //          val restday_oc_dist = cols(2)
//    //          val workday_channel_dist = cols(3)
//    //          val restday_channel_dist = cols(4)
//    //
//    //          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" +
//    //            "" + workday_channel_dist + "\t" + restday_channel_dist
//    //
//    //          //          key	string
//    //          //         sn	string
//    //          //          stat_date	string
//    //          //          period	string
//    //          //          workday_oc_dist	string
//    //          //         restday_oc_dist	string
//    //          //         workday_channel_dist	string
//    //          //        restday_channel_dist	string
//    //
//    //          mutator.mutate(HBaseUtils.getPut_UserVectorBh(orderedLine))
//    //        })
//    //        mutator.flush()
//    //
//    //      } finally {
//    //        mutator.close()
//    //        HBaseUtils.getHbaseConn().close()
//    //      }
//    //    })
//    //
  }


}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.UserVectorHelper
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object UserVectorAllETL_ReadFromTable {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorBehaviorETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)



    //TODO 行为特征
    val bhPair = sqlContext.sql("select sn,stat_date,period,workday_oc_dist,restday_oc_dist,workday_channel_dist," +
      "restday_channel_dist from hr.user_vector_bh where stat_date = '" + analysisDate + "'  and  period = " + recentDaysNum).rdd
      .map(line => {
        val sn = line(0)
        val stat_date = line(1)
        val period = line(2)
        val workday_oc_dist = line(3)
        val restday_oc_dist = line(4)
        val workday_channel_dist = line(5)
        val restday_channel_dist = line(6)

        (sn + "\t" + stat_date + "\t" + period, workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" +
          "" + restday_channel_dist)
      })


    //TODO 到剧特征
    val playPair = sqlContext.sql("select sn,stat_date,period,pg_subject_dist,pg_year_dist,pg_region_dist from hr" +
      ".user_vector_play where stat_date = '" + analysisDate + "'  and  period = " + recentDaysNum).rdd
      .map(line => {
        val sn = line(0)
        val stat_date = line(1)
        val period = line(2)
        val pg_subject_dist = line(3)
        val pg_year_dist = line(4)
        val pg_region_dist = line(5)

        (sn + "\t" + stat_date + "\t" + period, pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist)
      })


    //TODO 行为到剧合并
    val bhPlayPair = bhPair.join(playPair).map(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      (sn, stat_date + "\t" + period + "\t" + line._2._1 + "\t" + line._2._2)

    })


    //TODO 设备特征
    val terminalPair = sqlContext.sql("select sn,brand,province,price,size from hr.user_vector_terminal").rdd
      .map(line => {
        val sn = line(0).toString
        val brand = line(1)
        val province = line(2)
        val price = line(3)
        val size = line(4)

        (sn, brand + "\t" + province + "\t" + price + "\t" + size)
      })



    //TODO 合并成行为特征表
    val resultRDD = bhPlayPair.join(terminalPair).map(line => {
      line._1 + "\t" + line._2._1 + "\t" + line._2._2
    })

    /////////////////test///////////////////////////////////////////////
    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorAllETL")
    /////////////////test//////////////////////////////////////////////



    // TODO 写入hbase
    resultRDD.foreachPartition(items => {
      val mutator = HBaseUtils.getMutator("user_vector_all")
      try {
        items.foreach(line => {
          val cols = line.split("\t")
          var i = 0
          val sn = cols(i)
          i = i + 1
          val stat_date = cols(i)
          i = i + 1
          val period = cols(i)
          i = i + 1
          val workday_oc_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val restday_oc_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val workday_channel_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val restday_channel_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val pg_subject_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val pg_year_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val pg_region_dist = UserVectorHelper.normalizArrStrDelimitedByComma(cols(i))
          i = i + 1
          val brand = cols(i)
          i = i + 1
          val province = cols(i)
          i = i + 1
          val price = cols(i)
          i = i + 1
          val size = cols(i)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + brand + "\t" + province + "\t" + price + "\t" + size + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist


          mutator.mutate(HBaseUtils.getPut_UserVectorAll(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })
  }




}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object UserVectorBehaviorETL {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorBehaviorETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")

    //TODO 工作日开机时间分布
    val workday_oc_sql = "select sn,power_on_time,sum(cnt) from hr.tracker_oc_fact_partition where date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) group by sn,power_on_time "

    println(workday_oc_sql)

    val workday_oc_rdd = sqlContext.sql(workday_oc_sql).rdd

    val workday_oc_pair = workday_oc_rdd.map(line => {

      val sn = line(0)

      val hour = line(1).toString
      val cnt = line(2).toString
      val vectorStr = UserVectorHelper.getCntDistArr(hour, UserVectorConstant.BH_OC_HOUR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })

    println(workday_oc_pair.count())


    // TODO 休息日开机时间分布
    val restday_oc_sql = "select sn,power_on_time,sum(cnt) from hr.tracker_oc_fact_partition where date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) group by sn,power_on_time "

    println(restday_oc_sql)

    val restday_oc_rdd = sqlContext.sql(restday_oc_sql).rdd

    val restday_oc_pair = restday_oc_rdd.map(line => {

      val sn = line(0)

      val hour = line(1).toString
      val cnt = line(2).toString
      val vectorStr = UserVectorHelper.getCntDistArr(hour, UserVectorConstant.BH_OC_HOUR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })



    println(restday_oc_pair.count())

    // TODO	工作日频道分布-live

    val workday_channel_sql = "SELECT dim_sn, dim_channel, SUM(fact_cnt) FROM hr.tracker_live_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) GROUP by dim_sn,dim_channel "

    println(workday_channel_sql)

    val workday_channel_rdd = sqlContext.sql(workday_channel_sql).rdd

    val workday_channel_pair = workday_channel_rdd.map(line => {

      val sn = line(0)

      var channel = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(channel)) {
        channel = UserVectorConstant.OTHER_CHANNEL_STR
      }

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(channel, UserVectorConstant.BH_CHANNEL_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(workday_channel_pair.count())

    // TODO	工作日频道分布-apk

    val workday_apk_sql = "SELECT dim_sn, ai.appname, SUM(fact_cnt) FROM hr.tracker_apk_fact_partition" +
      " join ( select " +
      " distinct packagename,appname from hr.apkinfo where onelevel = '视频' ) ai " +
      " on (dim_apk = ai.packagename )  " +
      "WHERE date > " +
      "date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) GROUP by dim_sn,ai.appname "

    println(workday_apk_sql)

    val workday_apk_rdd = sqlContext.sql(workday_apk_sql).rdd

    val workday_apk_pair = workday_apk_rdd.map(line => {

      val sn = line(0)

      //TODO 应用名称
      var apk = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(apk)) {
        apk = UserVectorConstant.OTHER_APK_STR
      }

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(apk, UserVectorConstant.BH_CHANNEL_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(workday_apk_pair.count())

  //TODO 休息日频道分布-live
    val restday_channel_sql = "SELECT dim_sn, dim_channel, SUM(fact_cnt) FROM hr.tracker_live_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) GROUP by dim_sn,dim_channel "

    println(restday_channel_sql)

    val restday_channel_rdd = sqlContext.sql(restday_channel_sql).rdd

    val restday_channel_pair = restday_channel_rdd.map(line => {

      val sn = line(0)

      var channel = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(channel)) {
        channel = UserVectorConstant.OTHER_CHANNEL_STR
      }

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(channel, UserVectorConstant.BH_CHANNEL_ARR, cnt)

      (sn, vectorStr)

    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(restday_channel_pair.count())


    //TODO 休息日频道分布-apk
    val restday_apk_sql = "SELECT dim_sn, ai.appname, SUM(fact_cnt) FROM hr.tracker_apk_fact_partition" +
      " join ( select " +
      " distinct packagename,appname from hr.apkinfo where onelevel = '视频' ) ai " +
      " on (dim_apk = ai.packagename )  " +
      " WHERE " +
      " date > " +
      " date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) " +
      " GROUP by dim_sn,ai.appname "

    println(restday_apk_sql)

    val restday_apk_rdd = sqlContext.sql(restday_apk_sql).rdd

    val restday_apk_pair = restday_apk_rdd.map(line => {

      val sn = line(0)

      var apk = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(apk)) {
        apk = UserVectorConstant.OTHER_APK_STR
      }

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(apk, UserVectorConstant.BH_CHANNEL_ARR, cnt)

      (sn, vectorStr)

    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(restday_apk_pair.count())


    //TODO 处理fullOuterJoin两边的空值


    //TODO 默认值
    val wo_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")
    val ro_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")
    val wc_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_CHANNEL_ARR, "0")
    val rc_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_CHANNEL_ARR, "0")


    //TODO 合并成行为特征表
    val resultRDD = workday_oc_pair
      .fullOuterJoin(restday_oc_pair).map(line => {
      val sn = line._1
      val wo = line._2._1.getOrElse(wo_default)
      val ro = line._2._2.getOrElse(ro_default)
      (sn, (wo, ro))
    })
      .fullOuterJoin(workday_channel_pair.union(workday_apk_pair)).map(line => {
      val sn = line._1
      val wo_ro = line._2._1.getOrElse((wo_default, ro_default))
      val wc = line._2._2.getOrElse(wc_default)
      (sn, (wo_ro, wc))
    })
      .fullOuterJoin(restday_channel_pair.union(restday_apk_pair)).map(line => {
      val sn = line._1
      val wo_ro_wc = line._2._1.getOrElse(((wo_default, ro_default), wc_default))
      val rc = line._2._2.getOrElse(rc_default)
      (sn, (wo_ro_wc, rc))
    })



      //    val resultRDD = workday_oc_pair
      //      .join(restday_oc_pair)
      //      .join(workday_channel_pair)
      //      .join(restday_channel_pair)

      //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
      //        .currentTimeMillis + "UserVectorBehaviorETL")

      .map(line => {
      val sn = line._1
      val workday_oc_dist = line._2._1._1._1
      val restday_oc_dist = line._2._1._1._2
      val workday_channel_dist = line._2._1._2
      val restday_channel_dist = line._2._2

      sn + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist
    }).filter(line => {
      !line.contains("NaN")
    })

    //    resultRDD

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorBehaviorETL")


    //    TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_vector_bh")

      try {

        items.foreach(line => {

          val cols = line.split("\t")

          val sn = cols(0)
          val stat_date = analysisDate
          val period = recentDaysNum
          val workday_oc_dist = cols(1)
          val restday_oc_dist = cols(2)
          val workday_channel_dist = cols(3)
          val restday_channel_dist = cols(4)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" +
            "" + workday_channel_dist + "\t" + restday_channel_dist

          //          key	string
          //         sn	string
          //          stat_date	string
          //          period	string
          //          workday_oc_dist	string
          //         restday_oc_dist	string
          //         workday_channel_dist	string
          //        restday_channel_dist	string

          mutator.mutate(HBaseUtils.getPut_UserVectorBh(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object UserVectorBehaviorETL01 {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorBehaviorETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")

    //TODO 工作日开机时间分布
    val workday_oc_sql = "select sn,power_on_time,sum(cnt) from hr.tracker_oc_fact_partition where date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) group by sn,power_on_time "

    println(workday_oc_sql)

    val workday_oc_rdd = sqlContext.sql(workday_oc_sql).rdd

    val workday_oc_pair = workday_oc_rdd.map(line => {

      val sn = line(0)

      val hour = line(1).toString
      val cnt = line(2).toString
      val vectorStr = UserVectorHelper.getCntDistArr(hour, UserVectorConstant.BH_OC_HOUR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })

    println(workday_oc_pair.count())


    // TODO 休息日开机时间分布
    val restday_oc_sql = "select sn,power_on_time,sum(cnt) from hr.tracker_oc_fact_partition where date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) group by sn,power_on_time "

    println(restday_oc_sql)

    val restday_oc_rdd = sqlContext.sql(restday_oc_sql).rdd

    val restday_oc_pair = restday_oc_rdd.map(line => {

      val sn = line(0)

      val hour = line(1).toString
      val cnt = line(2).toString
      val vectorStr = UserVectorHelper.getCntDistArr(hour, UserVectorConstant.BH_OC_HOUR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })



    println(restday_oc_pair.count())

    // TODO	工作日频道分布

    val workday_channel_sql = "SELECT dim_sn, dim_channel, SUM(fact_cnt) FROM hr.tracker_live_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) GROUP by dim_sn,dim_channel "

    println(workday_channel_sql)

    val workday_channel_rdd = sqlContext.sql(workday_channel_sql).rdd

    val workday_channel_pair = workday_channel_rdd.map(line => {

      val sn = line(0)

      val channel = line(1).toString

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(channel, UserVectorConstant.BH_CHANNEL_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(workday_channel_pair.count())

    // TODO 休息日频道分布

    val restday_channel_sql = "SELECT dim_sn, dim_channel, SUM(fact_cnt) FROM hr.tracker_live_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) GROUP by dim_sn,dim_channel "

    println(restday_channel_sql)

    val restday_channel_rdd = sqlContext.sql(restday_channel_sql).rdd

    val restday_channel_pair = restday_channel_rdd.map(line => {

      val sn = line(0)

      val channel = line(1).toString

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(channel, UserVectorConstant.BH_CHANNEL_ARR, cnt)

      (sn, vectorStr)

    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(restday_channel_pair.count())

    //TODO 工作日观看节目来源分布-直播

    val workday_pg_source_live_rdd = workday_channel_rdd.map(line => {
      val sn = line(0)
      sn
    }).distinct


    //TODO 工作日观看节目来源分布-点播
    val workday_pg_source_apk_sql = "SELECT distinct dim_sn FROM hr.tracker_apk_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) "

    println(workday_pg_source_apk_sql)

    val workday_pg_source_apk_rdd = sqlContext.sql(workday_pg_source_apk_sql).rdd.map(line => {
      val sn = line(0)
      sn
    }).distinct

    //TODO 转换成节目来源向量
    //TODO 只有直播行为
    val workday_pg_source_pair = workday_pg_source_live_rdd.subtract(workday_pg_source_apk_rdd).map(sn => {
      (sn, "1,0")
    }).union(
      //TODO 只有点播行为
      workday_pg_source_apk_rdd.subtract(workday_pg_source_live_rdd).map(sn => {
        (sn, "0,1")
      }))
      .union(
        //TODO 直播 点播行为都有
        workday_pg_source_live_rdd.intersection(workday_pg_source_apk_rdd).map(sn => {
          (sn, "1,1")
        }))



    println(workday_pg_source_pair.count())

    // TODO	休息日观看节目来源分布-直播
    val restday_pg_source_live_rdd = restday_channel_rdd.map(line => {
      val sn = line(0)
      sn
    }).distinct

    // TODO	休息日观看节目来源分布-点播
    val restday_pg_source_apk_sql = "SELECT distinct dim_sn FROM hr.tracker_apk_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) "

    println(restday_pg_source_apk_sql)

    val restday_pg_source_apk_rdd = sqlContext.sql(restday_pg_source_apk_sql).rdd.map(line => {
      val sn = line(0)
      sn
    })



    //TODO 转换成节目来源向量
    //TODO 只有直播行为
    val restday_pg_source_pair = restday_pg_source_live_rdd.subtract(restday_pg_source_apk_rdd).map(sn => {
      (sn, "1,0")
    }).union(
      //TODO 只有点播行为
      restday_pg_source_apk_rdd.subtract(restday_pg_source_live_rdd).map(sn => {
        (sn, "0,1")
      }))
      .union(
        //TODO 直播 点播行为都有
        restday_pg_source_live_rdd.intersection(restday_pg_source_apk_rdd).map(sn => {
          (sn, "1,1")
        }))

    println(restday_pg_source_pair.count())

    //TODO 合并成行为特征表
    val resultRDD = workday_oc_pair
      .join(restday_oc_pair)
      .join(workday_channel_pair)
      .join(restday_channel_pair)
      .join(workday_pg_source_pair)
      .join(restday_pg_source_pair)

      //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
      //        .currentTimeMillis + "UserVectorBehaviorETL")

      .map(line => {
      val sn = line._1
      val workday_oc_dist = line._2._1._1._1._1._1
      val restday_oc_dist = line._2._1._1._1._1._2
      val workday_channel_dist = line._2._1._1._1._2
      val restday_channel_dist = line._2._1._1._2
      val workday_pg_sources_dist = line._2._1._2
      val restday_pg_sources_dist = line._2._2

      sn + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + workday_pg_sources_dist + "\t" + restday_pg_sources_dist
    })

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorBehaviorETL")


    //    TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_vector_bh")

      try {

        items.foreach(line => {

          val cols = line.split("\t")

          val sn = cols(0)
          val stat_date = analysisDate
          val period = recentDaysNum
          val workday_oc_dist = cols(1)
          val restday_oc_dist = cols(2)
          val workday_channel_dist = cols(3)
          val restday_channel_dist = cols(4)
          val workday_pg_sources_dist = cols(5)
          val restday_pg_sources_dist = cols(6)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" +
            "" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + workday_pg_sources_dist + "\t" + restday_pg_sources_dist

          //          key	string
          //         sn	string
          //          stat_date	string
          //          period	string
          //          workday_oc_dist	string
          //         restday_oc_dist	string
          //         workday_channel_dist	string
          //        restday_channel_dist	string
          //        workday_pg_sources_dist	string
          //         restday_pg_sources_dist	string

          mutator.mutate(HBaseUtils.getPut_UserVectorBh(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.etl.common

import com.avcdata.spark.job.mock.RandomUtil.CategoryWeight

object UserVectorConstant {

  def main(args: Array[String]) {
    //    println(BH_PG_SUBJECT_ARR.length)
    //    println(None.isEmpty)
    //    println(BH_CHANNEL_ARR.size)

    //    for (i <- 0 until BH_OC_HOUR_ARR.length) {
    //      for (j <- 0 until this.BH_PG_SUBJECT_ARR.length) {
    //          print(BH_OC_HOUR_ARR(i)+"-"+BH_PG_SUBJECT_ARR(j)+",")
    //      }
    //    }

    //    UserVectorCrossHelper.loadConstants
    //    println(UserVectorCrossHelper.HOUR_CHANNEL_ARR)

  }


  /////////////////设备属性特征/////////////////////////
  //TODO设备品牌
  val TERMINAL_BRAND_ARR = Array("CC", "CH", "KO")

  //TODO地域
  val TERMINAL_PROVINCE_ARR = Array("安徽省", "北京市", "福建省", "甘肃省", "广东省", "广西壮族自治区", "贵州省", "海南省", "河北省", "河南省", "黑龙江省", "湖北省", "湖南省", "吉林省", "江苏省", "江西省", "辽宁省", "内蒙古自治区", "宁夏回族自治区", "青海省", "山东省", "山西省", "陕西省", "上海市", "四川省", "天津市", "新疆维吾尔自治区", "云南省", "浙江省", "重庆市", "西藏自治区")

  //TODO 价格 <0，1-1001，1001-2001，2001-3001，3001-4001，4001-5001，5001-6001，6001-7001，7001-8001，8001-9001，10001-11001，11001-12001，12001-13001，13001-14001，15001-16001，19001-20001，25001-26001，31001-32001，32001以上>
  //[1,1001) 左开右闭
  val TV_PRICE_ARR = Array[Array[Double]](
    Array(0, 0.0001),
    Array(1, 1000),
    Array(1000, 2000),
    Array(2000, 3000),
    Array(3000, 4000),
    Array(4000, 5000),
    Array(5000, 6000),
    Array(6000, 7000),
    Array(7000, 8000),
    Array(8000, 9000),
    Array(9000, 10000),
    Array(10000, 100000000)
  )


  //TODO 尺寸
  //尺寸段：32吋以下，32-40，41-45，46-50，51-55，56-60，61-65，66-70，71-75，75吋以上  共10维
  val TV_SIZE_ARR = Array(
    Array(0, 0),
    Array(1, 31),
    Array(32, 40),
    Array(41, 45),
    Array(46, 50),
    Array(51, 55),
    Array(56, 60),
    Array(61, 65),
    Array(66, 70),
    Array(71, 75),
    Array(76, 1000)
  )


  //TODO 年份
  val TERMINAL_YEAR_ARR = Array("0", "1", "2")


  /////////////////////////////////行为特征（节目无关）/////////////////////////////

  //TODO 工作日开机时间分布
  val BH_OC_HOUR_ARR = Array("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23")
  val WO_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")

  //TODO 休息日开机时间分布
  val RO_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_OC_HOUR_ARR, "0")

  //TODO 工作日频道分布
  val BH_CHANNEL_NOther_ARR = Array("CCTV-1", "CCTV-2", "CCTV-3", "CCTV-4", "CCTV-5", "CCTV-6", "CCTV-7", "CCTV-8", "CCTV-9", "CCTV-10", "CCTV-11", "CCTV-12", "CCTV-13", "CCTV-14", "CCTV-15", "安徽卫视", "北京卫视", "重庆卫视", "甘肃卫视", "广东卫视", "广西卫视", "贵州卫视", "河北卫视", "河南卫视", "黑龙江卫视", "湖北卫视", "湖南卫视", "吉林卫视", "江苏卫视", "江西卫视", "辽宁卫视", "旅游卫视", "内蒙古卫视", "宁夏卫视", "青海卫视", "厦门卫视", "山东卫视", "山西卫视", "陕西卫视", "上海东方卫视", "深圳卫视", "四川卫视", "天津卫视", "西藏卫视", "新疆卫视", "云南卫视", "浙江卫视", "银河·奇异果", "腾讯视频TV端", "CIBN环球影视", "芒果TV", "电视猫视频")

  val OTHER_CHANNEL_STR = "其他频道"
  val OTHER_APK_STR = "其它apk"

  val BH_CHANNEL_ARR = Array("CCTV-1", "CCTV-2", "CCTV-3", "CCTV-4", "CCTV-5", "CCTV-6", "CCTV-7", "CCTV-8", "CCTV-9", "CCTV-10", "CCTV-11", "CCTV-12", "CCTV-13", "CCTV-14", "CCTV-15", "安徽卫视", "北京卫视", "重庆卫视", "甘肃卫视", "广东卫视", "广西卫视", "贵州卫视", "河北卫视", "河南卫视", "黑龙江卫视", "湖北卫视", "湖南卫视", "吉林卫视", "江苏卫视", "江西卫视", "辽宁卫视", "旅游卫视", "内蒙古卫视", "宁夏卫视", "青海卫视", "厦门卫视", "山东卫视", "山西卫视", "陕西卫视", "上海东方卫视", "深圳卫视", "四川卫视", "天津卫视", "西藏卫视", "新疆卫视", "云南卫视", "浙江卫视", OTHER_CHANNEL_STR, "银河·奇异果", "腾讯视频TV端", "CIBN环球影视", "芒果TV", "电视猫视频", OTHER_APK_STR)


  val WC_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_CHANNEL_ARR, "0")

  //TODO 休息日频道分布
  val RC_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_CHANNEL_ARR, "0")


  //  //TODO 工作日节目来源分布
  //  val BH_PG_SOURCE_ARR = Array("直播", "点播")
  //
  //  //TODO 休息日节目来源分布

  ///////////////////////////////////////////行为特征（节目相关）///////////////////////

  val BH_PG_SUBJECT_ARR = Array("电视剧-爱情", "电视剧-穿越", "电视剧-传记", "电视剧-传奇", "电视剧-动作", "电视剧-都市", "电视剧-儿童", "电视剧-犯罪", "电视剧-访谈", "电视剧-宫廷", "电视剧-古代", "电视剧-故事", "电视剧-纪实", "电视剧-家庭", "电视剧-惊悚", "电视剧-剧情", "电视剧-军事", "电视剧-科幻", "电视剧-恐怖", "电视剧-栏目剧", "电视剧-历史", "电视剧-励志", "电视剧-伦理", "电视剧-冒险", "电视剧-美食", "电视剧-魔幻", "电视剧-年代", "电视剧-农村", "电视剧-偶像", "电视剧-奇幻", "电视剧-青春校园", "电视剧-情感", "电视剧-情景剧", "电视剧-人物", "电视剧-商战", "电视剧-少儿", "电视剧-社会", "电视剧-神话", "电视剧-时尚", "电视剧-网剧", "电视剧-文化", "电视剧-武侠", "电视剧-喜剧", "电视剧-悬疑", "电视剧-医院", "电视剧-音乐", "电视剧-战争", "电视剧-侦探", "电视剧-政治", "电视剧-其它", "电影-爱情", "电影-悲剧", "电影-穿越", "电影-传记", "电影-动画", "电影-动作", "电影-都市", "电影-儿童", "电影-犯罪", "电影-粉丝电影", "电影-复仇", "电影-歌舞", "电影-公益", "电影-宫廷", "电影-古代", "电影-黑色电影", "电影-纪实", "电影-家庭", "电影-惊悚", "电影-竞技", "电影-剧情", "电影-科幻", "电影-恐怖", "电影-历史", "电影-励志", "电影-伦理", "电影-冒险", "电影-魔幻", "电影-农村", "电影-偶像", "电影-奇幻", "电影-青春校园", "电影-情感", "电影-情色", "电影-热血", "电影-人物", "电影-商战", "电影-少儿", "电影-社会", "电影-神话", "电影-时尚", "电影-探索", "电影-同性", "电影-推理", "电影-网络大电影", "电影-微电影", "电影-文化", "电影-文艺", "电影-武侠", "电影-西部", "电影-喜剧", "电影-悬疑", "电影-音乐", "电影-运动", "电影-灾难", "电影-战争", "电影-侦探", "电影-真人", "电影-职场", "电影-自然", "电影-其它", "动画片-爱情", "动画片-百合", "动画片-宠物", "动画片-穿越", "动画片-催泪", "动画片-耽美", "动画片-电影", "动画片-动画", "动画片-动作", "动画片-都市", "动画片-古代", "动画片-故事", "动画片-鬼畜", "动画片-后宫", "动画片-僵尸", "动画片-教育", "动画片-惊悚", "动画片-竞技", "动画片-剧情", "动画片-卡通", "动画片-科幻", "动画片-恐怖", "动画片-励志", "动画片-猎奇", "动画片-萝莉", "动画片-漫画改编", "动画片-冒险", "动画片-美漫", "动画片-美男", "动画片-美少女", "动画片-美食", "动画片-萌系", "动画片-魔幻", "动画片-男性向", "动画片-女性向", "动画片-偶像", "动画片-奇幻", "动画片-亲子", "动画片-青春校园", "动画片-轻小说改编", "动画片-情感", "动画片-热血", "动画片-忍者", "动画片-少儿", "动画片-社会", "动画片-神话", "动画片-生活", "动画片-同人", "动画片-推理", "动画片-脱口秀", "动画片-微电影", "动画片-未来", "动画片-文艺", "动画片-武侠", "动画片-舞蹈", "动画片-吸血鬼", "动画片-喜剧", "动画片-小清新", "动画片-修仙", "动画片-悬疑", "动画片-养成", "动画片-音乐", "动画片-英雄", "动画片-游戏", "动画片-御姐", "动画片-运动", "动画片-宅男", "动画片-战争", "动画片-侦探", "动画片-真人", "动画片-治愈", "动画片-中二", "动画片-重口味", "动画片-其它", "综艺-爱情", "综艺-财经", "综艺-动作", "综艺-犯罪", "综艺-访谈", "综艺-歌舞", "综艺-婚恋", "综艺-纪实", "综艺-家庭", "综艺-家装", "综艺-教育", "综艺-惊悚", "综艺-竞技", "综艺-军情", "综艺-科幻", "综艺-恐怖", "综艺-栏目剧", "综艺-历史文化", "综艺-伦理", "综艺-旅游", "综艺-美食", "综艺-明星", "综艺-偶像", "综艺-奇幻", "综艺-亲子", "综艺-情感", "综艺-曲艺", "综艺-人物", "综艺-少儿", "综艺-社会", "综艺-生活", "综艺-时尚", "综艺-脱口秀", "综艺-晚会", "综艺-文化", "综艺-舞蹈", "综艺-喜剧", "综艺-悬疑", "综艺-选秀", "综艺-养生", "综艺-音乐", "综艺-游戏", "综艺-娱乐资讯", "综艺-运动", "综艺-早教", "综艺-战争", "综艺-侦探", "综艺-真人秀", "综艺-职场", "综艺-其它")

  val PS_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_SUBJECT_ARR, "0")


  val BH_PG_YEAR_ARR = Array("1988", "1989", "1990", "1991", "1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017")
  val PY_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_YEAR_ARR, "0")


  //TODO年份最大值
  val YEAR_MAX = 30

  val BH_PG_REGION_ARR = Array("电视剧-中国大陆", "电视剧-港台", "电视剧-欧美", "电视剧-日韩", "电视剧-其它", "电影-中国大陆", "电影-港台", "电影-欧美", "电影-日韩", "电影-其它", "综艺-中国大陆", "综艺-港台", "综艺-欧美", "综艺-日韩", "综艺-其它", "动画片-中国大陆", "动画片-港台", "动画片-欧美", "动画片-日本", "动画片-韩国", "动画片-其它")

  val PR_DEFAULT = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_REGION_ARR, "0")


  //////////////////////////////////////////////大特征向量的格式///////////////////////////

  //  "设备品牌","地域","价格","尺寸","工作日开机时间分布","休息日开机时间分布","工作日频道分布","休息日频道分布","工作日节目来源分布","休息日节目来源分布","观看过的节目题材分布","观看过的节目年份分布","观看过的节目产地分布

  /////////////////////////////////////////////向量聚类后的HDFS目录/////////////////////
  val CLUSTER_RESULT_DIR = "/user/hdfs/rsync/uservector/2017-03-15-15-ClusterResult-8"

  //////////////////////////////////////////////家庭构成标签//////////////////////////////
  val FAMILY_COMPOSE_NUM = Array("一口人", "二口人", "三口人", "四口人", "五口人", "六口人及以上")

  //  val FAMILY_COMPOSE_CATE = Array("单身", "二人世界", "夫妻和小孩", "夫妻和老人", "夫妻、老人和小孩")
  val FAMILY_COMPOSE_CLUSTER_MAP = Map(
    (1 -> "单身"),
    (2 -> "二人世界"),
    (3 -> "夫妻和小孩"),
    (4 -> "夫妻和老人"),
    (5 -> "夫妻、老人和小孩")
  )

  //category=>max_memeber_num
  val FAMILY_COMPOSE_MAX_NUM_MAP = Map(
    ("单身" -> 1),
    ("二人世界" -> 2),
    ("夫妻和小孩" -> 4),
    ("夫妻和老人" -> 4),
    ("夫妻、老人和小孩" -> 6)
  )

  val FAMILY_COMPOSE_WEIGHT = Array(
    CategoryWeight("单身", 2700),
    CategoryWeight("二人世界", 3500),
    CategoryWeight("夫妻和小孩", 5000),
    CategoryWeight("夫妻和老人", 2000),
    CategoryWeight("夫妻、老人和小孩", 1900)

  )

  def getMaxMemeberNumByCateName(cate: String): Int = {

    var result = 0
    result =
      cate match {
        case "单身" => 1;
        case "二人世界" => 2;
        case "夫妻和小孩" => 4;
        case "夫妻和老人" => 4;
        case "夫妻、老人和小孩" => 4;
      }

    result
  }

  //  单身
  //  二人世界
  //  夫妻和小孩
  //  夫妻、老人和小孩
  //  夫妻和老人

  //          凌晨 00:00-06:00
  //          早间 06:00-08:00
  //          上午 08:00-12:00
  //          午间 12:00-14:00
  //          下午 14:00-17:00
  //          傍晚 17:00-19:00
  //          黄金 19:00-22:00
  //          次黄金 22:00-24:00

  val TIME_RANGE = Array(
    "00:00-06:00",
    "06:00-08:00",
    "08:00-12:00",
    "12:00-14:00",
    "14:00-17:00",
    "17:00-19:00",
    "19:00-22:00",
    "22:00-24:00"
  )

  val TIME_RANGE_WEIGHT = Array(
    CategoryWeight("00:00-06:00", 961),
    CategoryWeight("06:00-08:00", 476),
    CategoryWeight("08:00-12:00", 1633),
    CategoryWeight("12:00-14:00", 1307),
    CategoryWeight("14:00-17:00", 1537),
    CategoryWeight("17:00-19:00", 1033),
    CategoryWeight("19:00-22:00", 2054),
    CategoryWeight("22:00-24:00", 1449)
  )


  val AGE_RANGE_MAP = Map(
    (Array(0, 13) -> "儿童"),
    (Array(14, 18) -> "少年"),
    (Array(19, 35) -> "青年"),
    (Array(36, 45) -> "中年"),
    (Array(46, 200) -> "老年")
  )

  val AGE_RANGE_WEIGHT = Array(
    CategoryWeight("儿童", 1360),
    CategoryWeight("少年", 1076),
    CategoryWeight("青年", 1218),
    CategoryWeight("中年", 5020),
    CategoryWeight("老年", 1326)
  )


  val SEX = Array("男", "女")

  val SEX_WEIGHT = Array(
    CategoryWeight("男", 5137),
    CategoryWeight("女", 4863)
  )


  def hasChild(data: String): String = {
    var res = "0"
    val hasArr = Array("三口之家")
    if (hasArr.contains(res)) {
      res = "1"
    }
    val noArr = Array("独居", "二人世界")

    res
  }

  def hasOld(): String = {
    var res = "0"
    val hasArr = Array("有老人", "空巢")
    if (hasArr.contains(res)) {
      res = "1"
    }
    val noArr = Array("三口之家")
    res
  }


}
package com.avcdata.spark.job.etl.common

import scala.collection.mutable.ArrayBuffer

object UserVectorCrossHelper {

  def main(args: Array[String]) {
    //    loadConstants()
    //    println(HOUR_CHANNEL_ARR)

    val hour = 1
    val channel = "CCTV-1"
    val cnt = "123"
    val v = UserVectorHelper.getCntDistArr(hour + "#" + channel, UserVectorCrossHelper.HOUR_CHANNEL_ARR, cnt)
    println(UserVectorHelper.arr2IntStrDelimitedByComma(v))
  }

  //TODO 分小时的维度
  //  var HOUR_CHANNEL_ARR = new ArrayBuffer[String]()
  //  var HOUR_PG_SUBJECT_ARR = new ArrayBuffer[String]()
  //  var HOUR_PG_YEAR_ARR = new ArrayBuffer[String]()

  val HOUR_CHANNEL_ARR = get_HOUR_CHANNEL_ARR(UserVectorConstant.BH_OC_HOUR_ARR, UserVectorConstant.BH_CHANNEL_ARR)
  val HOUR_PG_SUBJECT_ARR = get_HOUR_CHANNEL_ARR(UserVectorConstant.BH_OC_HOUR_ARR, UserVectorConstant.BH_PG_SUBJECT_ARR)
  val HOUR_PG_YEAR_ARR = get_HOUR_CHANNEL_ARR(UserVectorConstant.BH_OC_HOUR_ARR, UserVectorConstant.BH_PG_YEAR_ARR)


  //  def loadConstants(): Unit = {
  //    HOUR_CHANNEL_ARR = get_HOUR_CHANNEL_ARR(UserVectorConstant.BH_OC_HOUR_ARR, UserVectorConstant.BH_CHANNEL_ARR)
  //
  //    HOUR_PG_SUBJECT_ARR = get_HOUR_CHANNEL_ARR(UserVectorConstant.BH_OC_HOUR_ARR, UserVectorConstant.BH_PG_SUBJECT_ARR)
  //
  //    HOUR_PG_YEAR_ARR = get_HOUR_CHANNEL_ARR(UserVectorConstant.BH_OC_HOUR_ARR, UserVectorConstant.BH_PG_YEAR_ARR)
  //  }


  def get_HOUR_CHANNEL_ARR(arr1: Array[String], arr2: Array[String]): ArrayBuffer[String] = {
    val result = new ArrayBuffer[String](arr1.length * arr2.length)

    for (i <- 0 until arr1.length) {
      for (j <- 0 until arr2.length) {
        result.+=(arr1(i) + "#" + arr2(j))
      }
    }

    result
  }
}
package com.avcdata.spark.job.etl.common

import org.joda.time.DateTime

import scala.collection.mutable.ArrayBuffer

object UserVectorHelper {

  def main(args: Array[String]) {

    //    println(UserVectorHelper.getCntDistStr("1992", UserVectorConstant.BH_PG_YEAR_ARR, "14.0"))

    //    println(getCntDistStr("1", "CC", UserVectorConstant.TERMINAL_BRAND_ARR))

    //    println(getYearVectorStr("2016"))
    //    val res = UserVectorHelper.getVectorZeroOneStr("陕西省", UserVectorConstant.TERMINAL_PROVINCE_ARR)
    //    println(res)

    //    val res = UserVectorHelper.getVectorCntStr("动画片-御姐", "0.0", UserVectorConstant.BH_PG_SUBJECT_ARR)
    //    println(UserVectorConstant.BH_PG_SUBJECT_ARR.length)
    //    println(res.split(",").length)

    //    val cnt = "12.0"
    //    val res =Integer.parseInt(cnt.substring(0,cnt.indexOf(".")))
    //    println(res)

    //    val arr = arrCompact(Array(1, 3, 3.2), Array(3.1, 2, 1))

    //    for (i <- arr) {
    //      println(i)
    //    }

    //    println(arr2IntStrDelimitedByComma(Array(1, 3, 3.2)))

    //    val res = UserVectorHelper.getCntDistArr("综艺-欧美", UserVectorConstant.BH_PG_REGION_ARR, "3.2")
    //    println(res)

    //    val res = UserVectorHelper.getRegionDist(0, UserVectorConstant.TV_SIZE_ARR)
    //    println(res)
    //
    //    val res = UserVectorHelper.getRegionDist(0, UserVectorConstant.TV_PRICE_ARR)
    //    println(res)

    //    println(normalizArrStrDelimitedByComma("32,0,14,13,3,5,0,7,0,1,15,0,1,5,6,64,3,0,0,0,29,2,1,0,0,0,16,0,3,0,2,1,8,28,5,0,0,0,1,0,0,0,13,3,0,0,27,3,0,1,18,0,0,0,1,4,0,0,0,0,0,2,0,0,0,0,0,0,1,0,9,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,10,1,5,0,0,2,0,0,0,0,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,0,3,0,0,0,0,4,0,0,0,0,0,0,1,2,0,4,0,0,0,0,0,7,0,18,0,0,1,0,0,2,0,0,0,0,16,0,0,0,0,0,23,0,19"))
//    println(normalizArrStrDelimitedByComma("0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0"))


    var apk = "haha"

    if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(apk)) {
      apk = UserVectorConstant.OTHER_APK_STR
    }

    println(apk)

    val cnt = "0"

    val vectorStr = UserVectorHelper.getCntDistStr(apk, UserVectorConstant.BH_CHANNEL_ARR, "123")

    println(vectorStr)

    println( UserVectorConstant.BH_CHANNEL_ARR.contains(UserVectorConstant.OTHER_APK_STR))

  }

  /**
    * 当前年份的距离（倒推）
    *
    * @param ele
    * @return
    */
  def getYear(ele: String): String = {
    val currentYear = new DateTime().getYear
    val diff = currentYear - ele.trim.toInt
    diff.toString
  }


  /**
    * 获取区间分布向量(整型)
    *
    * @return
    */
  def getRegionDist(num: Int, arr: Array[Array[Int]]): String = {

    val resultArr = new Array[Int](arr.length)

    //位置
    var pos = -1

    for (i <- 0 until resultArr.length) {
      resultArr(i) = 0

      val eleArr = arr(i)
      if (eleArr.length != 2) {
        throw new Exception("arr's length must be two!")
      }
      if (num >= eleArr(0) && num <= eleArr(1)) {
        pos = i
        resultArr(i) = 1
      }
    }

    arr2IntStrDelimitedByComma(resultArr)
  }


  /**
    * 获取区间分布向量(浮点型)
    *
    * @return
    */
  def getRegionDist(num: Double, arr: Array[Array[Double]]): String = {

    val resultArr = new Array[Int](arr.length)

    //位置
    var pos = -1

    for (i <- 0 until resultArr.length) {
      resultArr(i) = 0

      val eleArr = arr(i)
      if (eleArr.length != 2) {
        throw new Exception("arr's length must be two!")
      }
      if (num >= eleArr(0) && num < eleArr(1)) {
        pos = i
        resultArr(i) = 1
      }
    }

    arr2IntStrDelimitedByComma(resultArr)
  }


  /**
    * 获取元素在数组中的次数分布数组
    *
    * @param ele
    * @param arr
    * @param cnt
    * @return
    */
  def getCntDistArr(ele: String, arr: Array[String], cnt: String): Array[Double] = {

    val resultArr = new Array[Double](arr.length)

    for (i <- 0 until resultArr.length) {
      resultArr(i) = 0
    }

    var pos = -1
    //找到位置
    for (i <- 0 until arr.length) {
      if (ele.trim.equals(arr(i))) {
        pos = i
      }
    }

    //没找到 默认为0
    if (pos == -1) {
      for (i <- 0 until arr.length) {
        resultArr(i) = 0
      }
    } else {
      //位置
      resultArr(pos) = cnt.toDouble

    }


    resultArr
  }

  /**
    * 获取元素在数组中的次数分布数组
    * ArrayBuffer 重载
    * @param ele
    * @param arr
    * @param cnt
    * @return
    */
  def getCntDistArr(ele: String, arr: ArrayBuffer[String], cnt: String): Array[Double] = {

    val resultArr = new Array[Double](arr.length)

    for (i <- 0 until resultArr.length) {
      resultArr(i) = 0
    }

    var pos = -1
    //找到位置
    for (i <- 0 until arr.length) {
      if (ele.trim.equals(arr(i))) {
        pos = i
      }
    }

    //没找到 默认为0
    if (pos == -1) {
      for (i <- 0 until arr.length) {
        resultArr(i) = 0
      }
    } else {
      //位置
      resultArr(pos) = cnt.toDouble

    }


    resultArr
  }


  /**
    * 获取元素在数组中的次数分布字符串
    *
    * @param ele
    * @param arr
    * @param cnt
    * @return
    */
  def getCntDistStr(ele: String, arr: Array[String], cnt: String): String = {
    var pos = -1
    //找到位置
    for (i <- 0 until arr.length) {
      if (ele.trim.equals(arr(i))) {
        pos = i
      }
    }
    val sb = new StringBuilder
    //没找到 默认为0
    if (pos == -1) {
      for (i <- 0 until arr.length) {
        if (i == (arr.length - 1)) {
          sb.append("0")
        } else {
          sb.append("0,")
        }
      }
    }
    else {
      //左边
      for (i <- 0 until pos) {
        sb.append("0,")
      }
      //位置
      if (pos == (arr.length - 1)) {
        if (cnt.contains(".")) {
          sb.append(Integer.parseInt(cnt.substring(0, cnt.indexOf("."))))
        } else {
          sb.append(cnt)
        }
      } else {
        if (cnt.contains(".")) {
          sb.append(Integer.parseInt(cnt.substring(0, cnt.indexOf("."))) + ",")
        } else {
          sb.append(cnt + ",")
        }

      }
      //右边
      if (pos < arr.length - 1) {
        for (i <- pos + 1 until arr.length) {
          if (i == arr.length - 1) {
            sb.append("0")
          } else {
            sb.append("0,")
          }
        }
      }
    }
    sb.toString
  }


  /**
    * 获取元素在数组中的次数分布字符串
    * 重载
    * @param ele
    * @param arr
    * @param cnt
    * @return
    */
  def getCntDistStr(ele: String, arr: ArrayBuffer[String], cnt: String): String = {
    var pos = -1
    //找到位置
    for (i <- 0 until arr.length) {
      if (ele.trim.equals(arr(i))) {
        pos = i
      }
    }
    val sb = new StringBuilder
    //没找到 默认为0
    if (pos == -1) {
      for (i <- 0 until arr.length) {
        if (i == (arr.length - 1)) {
          sb.append("0")
        } else {
          sb.append("0,")
        }
      }
    }
    else {
      //左边
      for (i <- 0 until pos) {
        sb.append("0,")
      }
      //位置
      if (pos == (arr.length - 1)) {
        if (cnt.contains(".")) {
          sb.append(Integer.parseInt(cnt.substring(0, cnt.indexOf("."))))
        } else {
          sb.append(cnt)
        }
      } else {
        if (cnt.contains(".")) {
          sb.append(Integer.parseInt(cnt.substring(0, cnt.indexOf("."))) + ",")
        } else {
          sb.append(cnt + ",")
        }

      }
      //右边
      if (pos < arr.length - 1) {
        for (i <- pos + 1 until arr.length) {
          if (i == arr.length - 1) {
            sb.append("0")
          } else {
            sb.append("0,")
          }
        }
      }
    }
    sb.toString
  }

  /**
    * 向量数组合并
    *
    * @param arr_left
    * @param arr_right
    * @return
    */
  def arrCompact(arr_left: Array[Double], arr_right: Array[Double]): Array[Double] = {
    if (arr_left.length != arr_right.length) {
      throw new Exception("two arr's length must be same!")
    }

    val resultArr = new Array[Double](arr_left.length)


    for (i <- 0 until arr_left.length) {
      resultArr(i) = (arr_left(i) + arr_right(i))
    }

    resultArr
  }


  /**
    * 整数数组转换为逗号分隔的字符串
    *
    * @param arr
    * @return
    */
  def arr2IntStrDelimitedByComma(arr: Array[Int]): String = {

    val sb = new StringBuilder

    for (i <- 0 until arr.length) {
      if (i == (arr.length - 1)) {
        sb.append(arr(i).toString)
      } else {
        sb.append(arr(i) + ",")
      }
    }

    sb.toString()
  }

  /**
    * 浮点型数组转换为逗号分隔的整数字符串
    *
    * @param arr
    * @return
    */
  def arr2IntStrDelimitedByComma(arr: Array[Double]): String = {

    val sb = new StringBuilder

    for (i <- 0 until arr.length) {
      if (i == (arr.length - 1)) {
        sb.append(Integer.parseInt(arr(i).toString.substring(0, arr(i).toString.indexOf("."))))
      } else {
        sb.append(Integer.parseInt(arr(i).toString.substring(0, arr(i).toString.indexOf("."))) + ",")
      }
    }

    sb.toString()
  }


  /**
    * 浮点型数组转换为逗号分隔的小数字符串
    *
    * @param arr
    * @return
    */
  def arr2DoubleStrDelimitedByComma(arr: Array[Double]): String = {

    val sb = new StringBuilder

    for (i <- 0 until arr.length) {
      if (i == (arr.length - 1)) {
        sb.append(arr(i))
      } else {
        sb.append(arr(i) + ",")
      }
    }

    sb.toString()
  }

  /**
    * 对数字序列字符串做归一化处理
    *
    * @param arrStr
    * @return
    */
  def normalizArrStrDelimitedByComma(arrStr: String): String = {

    val arr = arrStr.split(",")
    val doubleArr = new Array[Double](arr.length)

    for (i <- 0 until arr.length) {
      doubleArr(i) = arr(i).toDouble
    }

    val sum = doubleArr.sum
    for (i <- 0 until arr.length) {
      //      doubleArr(i) = (doubleArr(i) / sum)
      //      doubleArr(i) = String.format("%.2f", (doubleArr(i) / sum)).toDouble
      //      new java.text.DecimalFormat("#.00").format(3.1415926)
      //      new BigDecimal((doubleArr(i) / sum)).setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue()

      var num = 0.0
      if (sum != 0) {
        num = (doubleArr(i) / sum)
      }

      //保留小数点位数
      doubleArr(i) = f"$num%1.3f".toDouble
    }
    arr2DoubleStrDelimitedByComma(doubleArr)
  }




  //  灵活
  //  def getYearVectorStr(ele: String): String = {
  //
  //    val currentYear = new DateTime().getYear
  //    val diff = currentYear - ele.trim.toInt
  //
  //    var pos = -1;
  //    for (i <- 0 to UserVectorConstant.YEAR_MAX) {
  //      if (i == diff) {
  //        pos = i
  //      }a
  //    }
  //
  //    val sb = new StringBuilder
  //    if (pos == -1) {
  //      for (i <- 0 until UserVectorConstant.YEAR_MAX) {
  //        sb.append("0")
  //      }
  //    } else {
  //      for (i <- 0 until pos) {
  //        sb.append("0")
  //      }
  //      sb.append("1")
  //      for (i <- pos + 1 until UserVectorConstant.YEAR_MAX) {
  //        sb.append("0")
  //      }
  //    }
  //    str2StrDelimitedByComma(sb.toString)
  //
  //
  //  }


  ///

  //  def str2StrDelimitedByComma(str: String): String = {
  //
  //    val sb = new StringBuilder()
  //
  //    for (i <- 0 to (str.length - 1)) {
  //      val str_ele = str.charAt(i)
  //      if (i != str.length - 1) {
  //        sb.append(str_ele + ",")
  //      } else {
  //        sb.append(str_ele)
  //      }
  //    }
  //
  //    sb.toString
  //  }

  //  /**
  //    * 获取字符串在数组中的分布（001000...的形式）
  //    * param ele
  //    * ,param arr
  //    * ,return
  //    */
  //  def getVectorZeroOneStr(ele: String, arr: Array[String]): String = {
  //
  //    //位置
  //    var pos = -1;
  //    for (i <- 0 until arr.length) {
  //      if (ele.trim.equals(arr(i))) {
  //        pos = i
  //      }
  //    }
  //    val sb = new StringBuilder
  //    if (pos == -1) {
  //      for (i <- 0 until arr.length) {
  //        sb.append("0")
  //      }
  //    } else {
  //      for (i <- 0 until pos) {
  //        sb.append("0")
  //      }
  //      sb.append("1")
  //      for (i <- pos + 1 until arr.length) {
  //        sb.append("0")
  //      }
  //    }
  //    sb.toString
  //  }
  //
  //  def addNumDist(cntStr: String, index: Int, length: Int): String = {
  //    if (index == length - 1) {
  //      cntStr
  //    }
  //    cntStr + ","
  //  }
  //
  //  /**
  //    * 获取字符串在数组中的频率分布（0000n000...的形式）
  //    *
  //    * ,param ele
  //    * ,param arr
  //    * ,return
  //    */
  //  def getVectorCntStr(ele: String, cnt: String, arr: Array[String]): String = {
  //    val defaultCntStr = "0"
  //    //位置
  //    var pos = -1
  //    for (i <- 0 until arr.length) {
  //      if (ele.trim.equals(arr(i))) {
  //        pos = i
  //      }
  //    }
  //    val sb = new StringBuilder
  //    if (pos == -1) {
  //      for (i <- 0 until arr.length) {
  //        sb.append(addNumDist(defaultCntStr, i, arr.length))
  //      }
  //    } else {
  //      for (i <- 0 until pos) {
  //        sb.append(addNumDist(defaultCntStr, i, arr.length))
  //      }
  //      sb.append(addNumDist(Integer.parseInt(cnt.substring(0, cnt.indexOf("."))).toString, pos, arr.length))
  //
  //      for (i <- pos + 1 until arr.length) {
  //        sb.append("0")
  //      }
  //    }
  //    sb.toString
  //  }


  def handleNAN(data: String, default: String): String = {
    var res = data
    if (data.contains("NaN")) {
      res = default
    }

    res
  }
}
package com.avcdata.spark.job.clean.cross

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorCrossHelper, UserVectorHelper}
import com.avcdata.spark.job.etl.util.UDFUtils
import com.avcdata.spark.job.util.HdfsUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


/**
  * @author zhangyongtian
  * @define 分小时的观看行为及到剧特征的向量清洗
  */
object UserVectorHourCrossETL {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorHourCrossETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {


    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")
    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)

    //TODO 注册UDF函数
    UDFUtils.registerUDF(sqlContext, "dayOfWeek")
    sqlContext.sql("use hr")



    // TODO	工作日频道分布-live
    val workday_channel_sql = "SELECT dim_sn,dim_channel,dim_hour,SUM(fact_cnt) FROM hr.tracker_live_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) GROUP by dim_sn,dim_channel,dim_hour "

    println(workday_channel_sql)

    val workday_channel_rdd = sqlContext.sql(workday_channel_sql).rdd

    val workday_channel_pair = workday_channel_rdd.map(line => {

      val sn = line(0)

      var channel = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(channel)) {
        channel = UserVectorConstant.OTHER_CHANNEL_STR
      }

      val hour = line(2).toString

      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(hour + "#" + channel, UserVectorCrossHelper.HOUR_CHANNEL_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(workday_channel_pair.count())

    // TODO	工作日频道分布-apk

    val workday_apk_sql = "SELECT dim_sn, ai.appname,dim_hour,SUM(fact_cnt) FROM hr.tracker_apk_fact_partition" +
      " join ( select " +
      " distinct packagename,appname from hr.apkinfo where onelevel = '视频' ) ai " +
      " on (dim_apk = ai.packagename )  " +
      "WHERE date > " +
      "date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (1,2,3,4,5) GROUP by dim_sn,ai.appname,dim_hour "

    println(workday_apk_sql)

    val workday_apk_rdd = sqlContext.sql(workday_apk_sql).rdd

    val workday_apk_pair = workday_apk_rdd.map(line => {

      val sn = line(0)

      //TODO 应用名称
      var apk = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(apk)) {
        apk = UserVectorConstant.OTHER_APK_STR
      }

      val hour = line(2).toString
      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(hour + "#" + apk, UserVectorCrossHelper.HOUR_CHANNEL_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(workday_apk_pair.count())

    //TODO 休息日频道分布-live
    val restday_channel_sql = "SELECT dim_sn, dim_channel,dim_hour,SUM(fact_cnt) FROM hr.tracker_live_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) GROUP by dim_sn,dim_channel,dim_hour"

    println(restday_channel_sql)

    val restday_channel_rdd = sqlContext.sql(restday_channel_sql).rdd

    val restday_channel_pair = restday_channel_rdd.map(line => {

      val sn = line(0)

      var channel = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(channel)) {
        channel = UserVectorConstant.OTHER_CHANNEL_STR
      }

      val hour = line(2).toString

      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(hour + "#" + channel, UserVectorCrossHelper.HOUR_CHANNEL_ARR, cnt)

      (sn, vectorStr)

    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(restday_channel_pair.count())


    //TODO 休息日频道分布-apk
    val restday_apk_sql = "SELECT dim_sn,ai.appname,dim_hour,SUM(fact_cnt) FROM hr.tracker_apk_fact_partition" +
      " join ( select " +
      " distinct packagename,appname from hr.apkinfo where onelevel = '视频' ) ai " +
      " on (dim_apk = ai.packagename )  " +
      " WHERE " +
      " date > " +
      " date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' and dayOfWeek(date) in (6,7) " +
      " GROUP by dim_sn,ai.appname,dim_hour "

    println(restday_apk_sql)

    val restday_apk_rdd = sqlContext.sql(restday_apk_sql).rdd

    val restday_apk_pair = restday_apk_rdd.map(line => {

      val sn = line(0)

      var apk = line(1).toString

      if (!UserVectorConstant.BH_CHANNEL_NOther_ARR.contains(apk)) {
        apk = UserVectorConstant.OTHER_APK_STR
      }

      val hour = line(2).toString
      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(hour + "#" + apk, UserVectorCrossHelper.HOUR_CHANNEL_ARR, cnt)


      (sn, vectorStr)

    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    println(restday_apk_pair.count())


    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    //TODO 观看过的节目题材分布
    //TODO 点播+直播
    val pg_subject_sql =
      """
                       SELECT
                       		 sn,model,plot,dim_hour,sum(cnt)
                       FROM
                         (
                       SELECT pp.dim_sn AS sn ,
                                pp.dim_model AS model ,
                                fp.plot AS plot,
                                pp.dim_hour as dim_hour,
                                SUM(vv) AS cnt
                       FROM
                       (SELECT dim_sn,
                                dim_awcid,
                                dim_model,
                                dim_hour,
                                SUM(fact_vv) AS vv
                           FROM hr.tracker_player_fact_partition
                           WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                              AND date <= '""" + analysisDate +
        """' GROUP BY  dim_sn,dim_awcid,dim_hour,dim_model ) pp
                          JOIN hr.film_plot fp
                          ON pp.dim_awcid = fp.id
                          GROUP BY  pp.dim_sn, pp.dim_model, pp.dim_hour,fp.plot
                                                                                                                                                                      UNION  ALL

                      SELECT lp.dim_sn AS sn,
                             fp.model AS model,
                             fp.plot AS plot,
                             lp.dim_hour AS dim_hour,
                             SUM(lp.fact_cnt) AS cnt
                             FROM (SELECT * FROM hr.tracker_live_fact_partition
                             WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                              AND date <= '""" + analysisDate +
        """') lp
                             JOIN hr.epg epg
                             ON (lp.dim_date = epg.tv_date AND lp.dim_channel = epg.channel AND lp.dim_hour = epg.tv_hour AND lp.dim_min = epg.tv_min)
                             JOIN hr.epg_film ef
                              ON epg.pg = ef.pg
                              JOIN hr.film_plot fp
                              ON ef.id = fp.id
                              GROUP BY  lp.dim_sn, fp.model, fp.plot,lp.dim_hour
                       ) res
                       GROUP BY sn,model,plot,dim_hour
        """.stripMargin


    println(pg_subject_sql)

    val pg_subject_rdd = sqlContext.sql(pg_subject_sql).rdd

    val pg_subject_pair = pg_subject_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1).toString
      val plot = (dim_model + "-" + line(2).toString).trim
      val hour = line(3).toString
      val cnt = line(4).toString
      val vectorStr = UserVectorHelper.getCntDistArr(hour + "#" + plot, UserVectorCrossHelper.HOUR_PG_SUBJECT_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))

    })


    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO 观看过的节目年份分布
    //点播+直播
    val pg_year_sql =
      """
                        SELECT
                               sn,year,dim_hour,sum(cnt)
                            FROM
                             ( select
                       		    dim_sn AS sn,
                       			dim_year AS year,
                            dim_hour As dim_hour,
                       			SUM(fact_vv) AS cnt
                       		from hr.tracker_player_fact_partition
                       		WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                             AND date <= '""" + analysisDate +
        """'
               				group by dim_sn,dim_year,dim_hour
                               UNION  ALL
                               SELECT lp.dim_sn AS sn ,
                                        fp.year AS year,
                                        dim_hour AS dim_hour,
                                        SUM(lp.fact_cnt) AS cnt
                             FROM
                               (SELECT *
                               FROM hr.tracker_live_fact_partition
                          WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                      AND date <= '""" + analysisDate +
        """') lp
                           JOIN hr.epg epg
                           ON (lp.dim_date = epg.tv_date
                           AND lp.dim_channel = epg.channel
                           AND lp.dim_hour = epg.tv_hour
                           AND lp.dim_min = epg.tv_min)

                           JOIN hr.epg_film ef
                           ON epg.pg = ef.pg
                           JOIN hr.film_properties fp
                           ON ef.id = fp.id
                           GROUP BY  lp.dim_sn, fp.year,lp.dim_hour ) res
                           GROUP BY sn,year,dim_hour
        """.stripMargin

    println(pg_year_sql)

    val pg_year_rdd = sqlContext.sql(pg_year_sql).rdd

    val pg_year_pair = pg_year_rdd.map(line => {

      val sn = line(0)

      val dim_year = line(1).toString.trim

      val hour = line(2).toString

      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(hour + "#" + dim_year, UserVectorCrossHelper.HOUR_PG_YEAR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    //TODO 处理fullOuterJoin两边的空值


    //TODO 默认值
    val wc_default = UserVectorHelper.getCntDistStr("0", UserVectorCrossHelper.HOUR_CHANNEL_ARR, "0")
    val rc_default = UserVectorHelper.getCntDistStr("0", UserVectorCrossHelper.HOUR_CHANNEL_ARR, "0")
    val ps_default = UserVectorHelper.getCntDistStr("0", UserVectorCrossHelper.HOUR_PG_SUBJECT_ARR, "0")
    val py_default = UserVectorHelper.getCntDistStr("0", UserVectorCrossHelper.HOUR_PG_YEAR_ARR, "0")


    //TODO 合并成行为特征表


    val resultPair =
      workday_channel_pair.union(workday_apk_pair)
        .fullOuterJoin(restday_channel_pair.union(restday_apk_pair)).map(line => {
        val sn = line._1
        val wc = line._2._1.getOrElse(wc_default)
        val rc = line._2._2.getOrElse(rc_default)
        (sn, (wc, rc))
      })
        .fullOuterJoin(pg_subject_pair).map(line => {
        val sn = line._1
        val wc_rc = line._2._1.getOrElse((wc_default, rc_default))
        val ps = line._2._2.getOrElse(ps_default)
        (sn, (wc_rc, ps))
      })
        .fullOuterJoin(pg_year_pair).map(line => {
        val sn = line._1
        val wc_rc_ps = line._2._1.getOrElse(((wc_default, rc_default), ps_default))
        val py = line._2._2.getOrElse(py_default)
        (sn, (wc_rc_ps, py))
      })

        .map(line => {
          val sn = line._1
          val workday_channel_dist = line._2._1._1._1
          val restday_channel_dist = line._2._1._1._2
          val pg_subject_dist = line._2._1._2
          val pg_year_dist = line._2._2
          sn + "\t" + workday_channel_dist + "\t" + restday_channel_dist + "\t" + pg_subject_dist + "\t" + pg_year_dist
        }).filter(line => {
        !line.contains("NaN")
      })

    val outPutPath = "/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
      .currentTimeMillis + "UserVectorHourCrossETL"

    //TODO 删除旧数据
    HdfsUtils.rm(outPutPath, true)

    resultPair.saveAsTextFile(outPutPath)


    //    val resultRDD = workday_oc_pair
    //      .fullOuterJoin(restday_oc_pair).map(line => {
    //      val sn = line._1
    //      val wo = line._2._1.getOrElse(wo_default)
    //      val ro = line._2._2.getOrElse(ro_default)
    //      (sn, (wo, ro))
    //    })
    //      .fullOuterJoin(workday_channel_pair.union(workday_apk_pair)).map(line => {
    //      val sn = line._1
    //      val wo_ro = line._2._1.getOrElse((wo_default, ro_default))
    //      val wc = line._2._2.getOrElse(wc_default)
    //      (sn, (wo_ro, wc))
    //    })
    //      .fullOuterJoin(restday_channel_pair.union(restday_apk_pair)).map(line => {
    //      val sn = line._1
    //      val wo_ro_wc = line._2._1.getOrElse(((wo_default, ro_default), wc_default))
    //      val rc = line._2._2.getOrElse(rc_default)
    //      (sn, (wo_ro_wc, rc))
    //    })


    //    val resultRDD = workday_oc_pair
    //      .join(restday_oc_pair)
    //      .join(workday_channel_pair)
    //      .join(restday_channel_pair)

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorBehaviorETL")

    //      .map(line => {
    //      val sn = line._1
    //      val workday_oc_dist = line._2._1._1._1
    //      val restday_oc_dist = line._2._1._1._2
    //      val workday_channel_dist = line._2._1._2
    //      val restday_channel_dist = line._2._2
    //
    //      sn + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" + workday_channel_dist + "\t" + restday_channel_dist
    //    }).filter(line => {
    //      !line.contains("NaN")
    //    })

    //    resultRDD

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorBehaviorETL")


    //    TODO 写入hbase
    //    resultRDD.foreachPartition(items => {
    //
    //      val mutator = HBaseUtils.getMutator("user_vector_bh")
    //
    //      try {
    //
    //        items.foreach(line => {
    //
    //          val cols = line.split("\t")
    //
    //          val sn = cols(0)
    //          val stat_date = analysisDate
    //          val period = recentDaysNum
    //          val workday_oc_dist = cols(1)
    //          val restday_oc_dist = cols(2)
    //          val workday_channel_dist = cols(3)
    //          val restday_channel_dist = cols(4)
    //
    //          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + workday_oc_dist + "\t" + restday_oc_dist + "\t" +
    //            "" + workday_channel_dist + "\t" + restday_channel_dist
    //
    //          //          key	string
    //          //         sn	string
    //          //          stat_date	string
    //          //          period	string
    //          //          workday_oc_dist	string
    //          //         restday_oc_dist	string
    //          //         workday_channel_dist	string
    //          //        restday_channel_dist	string
    //
    //          mutator.mutate(HBaseUtils.getPut_UserVectorBh(orderedLine))
    //        })
    //        mutator.flush()
    //
    //      } finally {
    //        mutator.close()
    //        HBaseUtils.getHbaseConn().close()
    //      }
    //    })

  }


}
package com.avcdata.spark.job.etl.common


object UserVectorParser {

  case class UserVector(
                         brand: String,
                         province: String,
                         price: String,
                         size: String,
                         workday_oc_dist: String,
                         restday_oc_dist: String,
                         workday_channel_dist: String,
                         restday_channel_dist: String,
                         pg_subject_dist: String,
                         pg_year_dist: String,
                         pg_region_dist: String
                       )

  def main(args: Array[String]) {
    println(getEleByVectorStrSplitByComma("0,0,1,0,0,0,0,0,0,0,0,0", UserVectorConstant.TV_PRICE_ARR))
  }

  def main1(args: Array[String]) {

    val userVector = UserVector(
      "0,1,0",

      "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0",

      "0,0,0,0,1,0,0,0,0,0,0,0",

      "0,0,0,0,0,1,0,0,0,0,0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.067,0.0,0.0,0.067,0.067,0.067,0.2,0.067,0.0,0.267,0.0,0.133,0.067,0.0,0.0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0," +
        "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0," +
        "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0," +
        "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0," +
        "0.0,0.0",

      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0"
    )
    println(getEleMapByVectorStrSplitByComma(userVector.brand, UserVectorConstant.TERMINAL_BRAND_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.province, UserVectorConstant.TERMINAL_PROVINCE_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.price, UserVectorConstant.TV_PRICE_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.size, UserVectorConstant.TV_SIZE_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.workday_oc_dist, UserVectorConstant.BH_OC_HOUR_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.restday_oc_dist, UserVectorConstant.BH_OC_HOUR_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.workday_channel_dist, UserVectorConstant.BH_CHANNEL_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.restday_channel_dist, UserVectorConstant.BH_CHANNEL_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.pg_subject_dist, UserVectorConstant.BH_PG_SUBJECT_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.pg_year_dist, UserVectorConstant.BH_PG_YEAR_ARR))

    println(getEleMapByVectorStrSplitByComma(userVector.pg_region_dist, UserVectorConstant.BH_PG_REGION_ARR))

  }


  /**
    * 根据逗号分隔向量获取特征
    * 适用于单个向量区间特征的向量00000000100000000000000
    *
    * @param vectorStr 向量字符串 以逗号分隔
    * @param arr       特征集
    * @return
    */
  def getEleMapByVectorStrSplitByComma(vectorStr: String, arr: Array[String]): scala.collection.mutable.Map[String,
    String] = {

    //    val result = new scala.collection.mutable.ArrayBuffer[String]

    val result = scala.collection.mutable.HashMap[String, String]()

    val vectorArr = vectorStr.split(",")

    if (vectorArr.length != arr.length) {
      throw new Exception("two arr's length must be same!")
    }

    for (i <- 0 until vectorArr.length) {
      //      if (!vectorArr(i).equals("0") && !vectorArr(i).equals("0.0")) {
      result.+=((arr(i), vectorArr(i)))
      //      }
    }

    //    if (result.isEmpty) {
    //      result.append("NaN,")
    //    }

    result
  }

  /**
    * 根据逗号分隔向量获取特征
    * 适用于单个向量区间特征的向量00000000100000000000000
    *
    * @param vectorStr 向量字符串 以逗号分隔
    * @param arr       区间特征集
    * @return
    */
  def getEleMapByVectorStrSplitByComma(vectorStr: String, arr: Array[Array[Int]]): scala.collection.mutable.Map[String,
    String] = {

    //    val result = new scala.collection.mutable.ArrayBuffer[String]

    val result = scala.collection.mutable.HashMap[String, String]()

    val vectorArr = vectorStr.split(",")

    if (vectorArr.length != arr.length) {
      throw new Exception("two arr's length must be same!")
    }

    for (i <- 0 until vectorArr.length) {
      //      if (!vectorArr(i).equals("0") && !vectorArr(i).equals("0.0")) {
      result.+=(((arr(i)(0).toString + "-" + (arr(i)(1).toString), vectorArr(i))))
      //      }
    }

    //    if (result.isEmpty) {
    //      result.append("NaN,")
    //    }

    result
  }

  /**
    * 根据逗号分隔向量获取特征
    * 适用于单个向量区间特征的向量00000000100000000000000
    *
    * @param vectorStr 向量字符串 以逗号分隔
    * @param arr       区间特征集
    * @return
    */
  def getEleMapByVectorStrSplitByComma(vectorStr: String, arr: Array[Array[Double]]): scala.collection.mutable
  .Map[String, String] = {

    //    val result = new scala.collection.mutable.ArrayBuffer[String]

    val result = scala.collection.mutable.HashMap[String, String]()

    val vectorArr = vectorStr.split(",")

    if (vectorArr.length != arr.length) {
      throw new Exception("two arr's length must be same!")
    }

    for (i <- 0 until vectorArr.length) {
      //      if (!vectorArr(i).equals("0") && !vectorArr(i).equals("0.0")) {
      result.+=(((arr(i)(0).toString + "-" + (arr(i)(1).toString), vectorArr(i))))
      //      }
    }

    //    if (result.isEmpty) {
    //      result.append("NaN,")
    //    }

    result
  }


  def getEleByVectorStrSplitByComma(vectorStr: String, arr: Array[Array[Double]]): String = {

    //    val result = new scala.collection.mutable.ArrayBuffer[String]

    var result = "#"
    val vectorArr = vectorStr.split(",")

    if (vectorArr.length != arr.length) {
      throw new Exception("two arr's length must be same!")
    }

    for (i <- 0 until vectorArr.length) {

      if (!vectorArr(i).equals("0")) {
        result = arr(i)(0).toString + "-" + arr(i)(1).toString
      }

    }
    result
  }


  def getEleByVectorStrSplitByComma(vectorStr: String, arr: Array[Array[Int]]): String = {

    //    val result = new scala.collection.mutable.ArrayBuffer[String]

    var result = "#"
    val vectorArr = vectorStr.split(",")

    if (vectorArr.length != arr.length) {
      throw new Exception("two arr's length must be same!")
    }

    for (i <- 0 until vectorArr.length) {

      if (!vectorArr(i).equals("0")) {
        result = arr(i)(0).toString + "-" + arr(i)(1).toString
      }

    }
    result
  }



  def getEleByVectorStrSplitByComma(vectorStr: String, arr: Array[String]): String = {

    //    val result = new scala.collection.mutable.ArrayBuffer[String]

    var result = "#"
    val vectorArr = vectorStr.split(",")

    if (vectorArr.length != arr.length) {
      throw new Exception("two arr's length must be same!")
    }

    for (i <- 0 until vectorArr.length) {

      if (!vectorArr(i).equals("0")) {
        result = arr(i)
      }

    }
    result
  }


  //  /**
  //    * 根据逗号分隔向量获取特征
  //    * 适用于单个向量只有一个维度特征的向量00000000100000000000000
  //    *
  //    * @param vectorStr 向量字符串 以逗号分隔
  //    * @param arr       特征集
  //    * @return
  //    */
  //  def getEleByVectorStrSplitByComma(vectorStr: String, arr: Array[String]): String = {
  //
  //    //    val result = new scala.collection.mutable.ArrayBuffer[String]
  //
  //    val result = new StringBuilder
  //
  //    val vectorArr = vectorStr.split(",")
  //
  //    if (vectorArr.length != arr.length) {
  //      throw new Exception("two arr's length must be same!")
  //    }
  //
  //    for (i <- 0 until vectorArr.length) {
  //      //      if (!vectorArr(i).equals("0") && !vectorArr(i).equals("0.0")) {
  //      result.append(arr(i) + ",")
  //      //      }
  //    }
  //
  //    if (result.isEmpty) {
  //      result.append("NaN,")
  //    }
  //
  //    result.substring(0, result.lastIndexOf(",")).toString
  //  }
  //
  //  /**
  //    * 根据逗号分隔向量获取特征
  //    * 适用于单个向量只有一个维度区间特征的向量00000000100000000000000
  //    *
  //    * @param vectorStr 向量字符串 以逗号分隔
  //    * @param arr       整数区间特征集
  //    * @return
  //    */
  //  def getEleByVectorStrSplitByComma(vectorStr: String, arr: Array[Array[Int]]): String = {
  //
  //    //    val result = new scala.collection.mutable.ArrayBuffer[String]
  //
  //    val result = new StringBuilder
  //
  //    val vectorArr = vectorStr.split(",")
  //
  //    if (vectorArr.length != arr.length) {
  //      throw new Exception("two arr's length must be same!")
  //    }
  //
  //    for (i <- 0 until vectorArr.length) {
  //      //      if (!vectorArr(i).equals("0") && !vectorArr(i).equals("0.0")) {
  //      result.append(arr(i)(0) + "-" + arr(i)(1) + ",")
  //      //      }
  //    }
  //
  //    if (result.isEmpty) {
  //      result.append("NaN,")
  //    }
  //    result.substring(0, result.lastIndexOf(",")).toString
  //  }
  //
  //  /**
  //    * 根据逗号分隔向量获取特征
  //    * 适用于单个向量只有一个维度区间特征的向量00000000100000000000000
  //    *
  //    * @param vectorStr 向量字符串 以逗号分隔
  //    * @param arr       小数区间特征集
  //    * @return
  //    */
  //  def getEleByVectorStrSplitByComma(vectorStr: String, arr: Array[Array[Double]]): String = {
  //
  //    //    val result = new scala.collection.mutable.ArrayBuffer[String]
  //
  //    val result = new StringBuilder
  //
  //    val vectorArr = vectorStr.split(",")
  //
  //    if (vectorArr.length != arr.length) {
  //      throw new Exception("two arr's length must be same!")
  //    }
  //
  //    for (i <- 0 until vectorArr.length) {
  //      //      if (!vectorArr(i).equals("0") && !vectorArr(i).equals("0.0")) {
  //      result.append(arr(i)(0) + "-" + arr(i)(1) + ",")
  //      //      }
  //    }
  //    if (result.isEmpty) {
  //      result.append("NaN,")
  //    }
  //    result.substring(0, result.lastIndexOf(",")).toString
  //  }

}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


object UserVectorPlayETL {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorPlayETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)


    //TODO 观看过的节目题材分布
    //TODO 点播+直播
    val pg_subject_sql =
      """
                       SELECT
                       		 sn,model,plot,sum(cnt)
                       FROM
                         (
                       SELECT pp.dim_sn AS sn ,
                                pp.dim_model AS model ,
                                fp.plot AS plot,
                                SUM(vv) AS cnt
                       FROM
                       (SELECT dim_sn,
                                dim_awcid,
                                dim_model,
                                SUM(fact_vv) AS vv
                           FROM hr.tracker_player_fact
                           WHERE dim_date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                              AND dim_date <= '""" + analysisDate +
        """' GROUP BY  dim_sn,dim_awcid,dim_model ) pp
                          JOIN hr.film_plot fp
                          ON pp.dim_awcid = fp.id
                          GROUP BY  pp.dim_sn, pp.dim_model, fp.plot
                                                                                                                                                                      UNION  ALL

                      SELECT lp.dim_sn AS sn,
                             fp.model AS model,
                             fp.plot AS plot,
                             SUM(lp.fact_cnt) AS cnt
                             FROM (SELECT * FROM hr.tracker_live_fact_partition
                             WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                              AND date <= '""" + analysisDate +
        """') lp
                             JOIN hr.epg epg
                             ON (lp.dim_date = epg.tv_date AND lp.dim_channel = epg.channel AND lp.dim_hour = epg.tv_hour AND lp.dim_min = epg.tv_min)
                             JOIN hr.epg_film ef
                              ON epg.pg = ef.pg
                              JOIN hr.film_plot fp
                              ON ef.id = fp.id
                              GROUP BY  lp.dim_sn, fp.model, fp.plot
                       ) res
                       GROUP BY  sn,model,plot
        """.stripMargin


    println(pg_subject_sql)

    val pg_subject_rdd = sqlContext.sql(pg_subject_sql).rdd

    val pg_subject_pair = pg_subject_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1).toString
      val plot = (dim_model + "-" + line(2).toString).trim
      val cnt = line(3).toString
      val vectorStr = UserVectorHelper.getCntDistArr(plot, UserVectorConstant.BH_PG_SUBJECT_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))

    })


    // TODO 观看过的节目年份分布
    //点播+直播
    val pg_year_sql =
      """
                        SELECT
                               sn,year,sum(cnt)
                            FROM
                             ( select
                       		    dim_sn AS sn,
                       			dim_year AS year,
                       			SUM(fact_vv) AS cnt
                       		from hr.tracker_player_fact
                       		WHERE dim_date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                             AND dim_date <= '""" + analysisDate +
        """'
               				group by dim_sn,dim_year
                               UNION  ALL
                               SELECT lp.dim_sn AS sn ,
                                        fp.year AS year,
                                        SUM(lp.fact_cnt) AS cnt
                             FROM
                               (SELECT *
                               FROM hr.tracker_live_fact_partition
                          WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                      AND date <= '""" + analysisDate +
        """') lp
                           JOIN hr.epg epg
                           ON (lp.dim_date = epg.tv_date
                           AND lp.dim_channel = epg.channel
                           AND lp.dim_hour = epg.tv_hour
                           AND lp.dim_min = epg.tv_min)

                           JOIN hr.epg_film ef
                           ON epg.pg = ef.pg
                           JOIN hr.film_properties fp
                           ON ef.id = fp.id
                           GROUP BY  lp.dim_sn, fp.year ) res
                           GROUP BY sn,year
        """.stripMargin

    println(pg_year_sql)

    val pg_year_rdd = sqlContext.sql(pg_year_sql).rdd

    //TODO 直播
    //Live_fact   EPG  epg_film   film_properties

    val pg_year_pair = pg_year_rdd.map(line => {

      val sn = line(0)

      val dim_year = line(1).toString.trim

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(dim_year, UserVectorConstant.BH_PG_YEAR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })




    // TODO	观看过的产地分布

    //TODO 点播+直播
    val pg_region_sql =
      """
                      SELECT
                            sn,model,region,sum(cnt)
                               FROM
                                 (
                       		select
                       			dim_sn as sn,
                       			dim_model as model,
                       			dim_region as region,
                       			SUM(fact_vv) as cnt
                       		FROM hr.tracker_player_fact
                            WHERE dim_date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                            AND dim_date <= '""" + analysisDate +
        """'
                            group by dim_sn,dim_model,dim_region

                            UNION  ALL

                            SELECT
                                lp.dim_sn AS sn,
                                fp.model AS model,
                                fp.region AS region,
                                SUM(lp.fact_cnt) AS cnt
                            FROM (SELECT * FROM hr.tracker_live_fact_partition
                                WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                  AND date <= '""" + analysisDate +
        """') lp

                            JOIN hr.epg epg
                              ON (lp.dim_date = epg.tv_date
                              AND lp.dim_channel = epg.channel
                              AND lp.dim_hour = epg.tv_hour
                              AND lp.dim_min = epg.tv_min)

                              JOIN hr.epg_film ef
                              ON epg.pg = ef.pg

                              JOIN hr.film_properties fp
                              ON ef.id = fp.id
                              GROUP BY  lp.dim_sn,fp.model,fp.region
                          ) res
                          GROUP BY sn,model,region
        """.stripMargin

    println(pg_region_sql)

    val pg_region_rdd = sqlContext.sql(pg_region_sql).rdd


    val pg_region_rdd_pair = pg_region_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1)

      val dim_region = (dim_model + "-" + line(2).toString).trim

      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(dim_region, UserVectorConstant.BH_PG_REGION_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    //TODO 默认值
    val ps_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_SUBJECT_ARR, "0")
    val py_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_YEAR_ARR, "0")
    val pr_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_REGION_ARR, "0")



    //TODO 合并成行为特征表
    val resultRDD = pg_subject_pair
      .fullOuterJoin(pg_year_pair).map(line => {
      val sn = line._1
      val ps = line._2._1.getOrElse(ps_default)
      val py = line._2._2.getOrElse(py_default)
      (sn, (ps, py))
    })

      .fullOuterJoin(pg_region_rdd_pair).map(line => {
      val sn = line._1
      val ps_py = line._2._1.getOrElse((ps_default, py_default))
      val pr = line._2._2.getOrElse(pr_default)
      (sn, (ps_py, pr))
    })

      .map(line => {
        val sn = line._1
        val pg_subject_dist = line._2._1._1
        val pg_year_dist = line._2._1._2
        val pd_region_dist = line._2._2
        sn + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pd_region_dist
      }).filter(line => {
      !line.contains("NaN")
    })

    //    resultRDD

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" +
    //        analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorPlayETL")


    //    TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_vector_play")

      try {

        items.foreach(line => {

          //        1		key	string	from deserializer
          //        2		sn	string	from deserializer
          //        3		stat_date	string	from deserializer
          //        4		period	string	from deserializer
          //        5		pg_subject_dist	string	from deserializer
          //        6		pg_year_dist	string	from deserializer
          //        7		pg_region_dist 	string	from deserializer

          val cols = line.split("\t")

          val sn = cols(0)

          val stat_date = analysisDate

          val period = recentDaysNum

          val pg_subject_dist = cols(1)

          val pg_year_dist = cols(2)

          val pg_region_dist = cols(3)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist

          mutator.mutate(HBaseUtils.getPut_UserVectorPlay(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

object UserVectorPlayETL01 {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorPlayETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()
  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)


    //TODO 观看过的节目题材分布
    val pg_subject_sql = "SELECT pp.dim_sn,pp.dim_model,fp.plot, SUM(vv) FROM (SELECT dim_sn, dim_awcid, dim_model," +
      "SUM(fact_vv) AS vv FROM hr.tracker_player_fact_partition WHERE date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' GROUP BY dim_sn,dim_awcid,dim_model ) pp JOIN hr.film_plot fp ON pp.dim_awcid = fp.id GROUP BY pp.dim_sn,pp.dim_model,fp.plot  "

    println(pg_subject_sql)

    val pg_subject_rdd = sqlContext.sql(pg_subject_sql).rdd

    val pg_subject_pair = pg_subject_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1).toString
      val plot = (dim_model + "-" + line(2).toString).trim
      val cnt = line(3).toString
      val vectorStr = UserVectorHelper.getCntDistArr(plot, UserVectorConstant.BH_PG_SUBJECT_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    // TODO 观看过的节目年份分布
    val pg_year_sql = "select distinct dim_sn,dim_year,SUM(fact_vv)  from hr.tracker_player_fact_partition where date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' group by dim_sn,dim_year"

    println(pg_year_sql)

    val pg_year_rdd = sqlContext.sql(pg_year_sql).rdd

    val pg_year_pair = pg_year_rdd.map(line => {

      val sn = line(0)

      val dim_year = line(1).toString.trim

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(dim_year, UserVectorConstant.BH_PG_YEAR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })




    // TODO	观看过的产地分布

    val pg_region_sql = "select distinct dim_sn,dim_model,dim_region,SUM(fact_vv)  from hr.tracker_player_fact_partition where date > date_sub('" + analysisDate + "'," + recentDaysNum + ") and date <= '" + analysisDate + "' group by dim_sn," +
      "dim_model,dim_region"

    println(pg_region_sql)

    val pg_region_rdd = sqlContext.sql(pg_region_sql).rdd

    val pg_region_rdd_pair = pg_region_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1)

      val dim_region = (dim_model + "-" + line(2).toString).trim

      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(dim_region, UserVectorConstant.BH_PG_REGION_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })



    //TODO 合并成行为特征表
    val resultRDD = pg_subject_pair.join(pg_year_pair).join(pg_region_rdd_pair)
      .map(line => {
        val sn = line._1
        val pg_subject_dist = line._2._1._1
        val pg_year_dist = line._2._1._2
        val pd_region_dist = line._2._2
        sn + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pd_region_dist
      })

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" +
    //        analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorPlayETL")


    //    TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_vector_play")

      try {

        items.foreach(line => {

          //        1		key	string	from deserializer
          //        2		sn	string	from deserializer
          //        3		stat_date	string	from deserializer
          //        4		period	string	from deserializer
          //        5		pg_subject_dist	string	from deserializer
          //        6		pg_year_dist	string	from deserializer
          //        7		pg_region_dist 	string	from deserializer

          val cols = line.split("\t")

          val sn = cols(0)

          val stat_date = analysisDate

          val period = recentDaysNum

          val pg_subject_dist = cols(1)

          val pg_year_dist = cols(2)

          val pg_region_dist = cols(3)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist

          mutator.mutate(HBaseUtils.getPut_UserVectorPlay(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.etl.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext


object UserVectorPlayETL_PlayerPartition {


  def main(args: Array[String]) {
    //    val conf = new SparkConf()
    //      .setMaster("local[1]")
    //      .setAppName("UserVectorPlayETL")
    //    val sc = new SparkContext(conf)
    //    run(sc, "2017-03-31","30")
    //    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {

    ///////////test tool///////////////
    //    oCDF.rdd.saveAsTextFile("/user/hdfs/rsync/uservector/"+analysisDate+"-"+recentDaysNum)
    //    println("/user/hdfs/rsync/uservector")

    //    oCDF.foreach(println(_))

    val sqlContext = new HiveContext(sc)


    //TODO 观看过的节目题材分布
    //TODO 点播+直播
    val pg_subject_sql =
      """
                       SELECT
                       		 sn,model,plot,sum(cnt)
                       FROM
                         (
                       SELECT pp.dim_sn AS sn ,
                                pp.dim_model AS model ,
                                fp.plot AS plot,
                                SUM(vv) AS cnt
                       FROM
                       (SELECT dim_sn,
                                dim_awcid,
                                dim_model,
                                SUM(fact_vv) AS vv
                           FROM hr.tracker_player_fact_partition
                           WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                              AND date <= '""" + analysisDate +
        """' GROUP BY  dim_sn,dim_awcid,dim_model ) pp
                          JOIN hr.film_plot fp
                          ON pp.dim_awcid = fp.id
                          GROUP BY  pp.dim_sn, pp.dim_model, fp.plot
                                                                                                                                                                      UNION  ALL

                      SELECT lp.dim_sn AS sn,
                             fp.model AS model,
                             fp.plot AS plot,
                             SUM(lp.fact_cnt) AS cnt
                             FROM (SELECT * FROM hr.tracker_live_fact_partition
                             WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                              AND date <= '""" + analysisDate +
        """') lp
                             JOIN hr.epg epg
                             ON (lp.dim_date = epg.tv_date AND lp.dim_channel = epg.channel AND lp.dim_hour = epg.tv_hour AND lp.dim_min = epg.tv_min)
                             JOIN hr.epg_film ef
                              ON epg.pg = ef.pg
                              JOIN hr.film_plot fp
                              ON ef.id = fp.id
                              GROUP BY  lp.dim_sn, fp.model, fp.plot
                       ) res
                       GROUP BY  sn,model,plot
        """.stripMargin


    println(pg_subject_sql)

    val pg_subject_rdd = sqlContext.sql(pg_subject_sql).rdd

    val pg_subject_pair = pg_subject_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1).toString
      val plot = (dim_model + "-" + line(2).toString).trim
      val cnt = line(3).toString
      val vectorStr = UserVectorHelper.getCntDistArr(plot, UserVectorConstant.BH_PG_SUBJECT_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))

    })


    // TODO 观看过的节目年份分布
    //点播+直播
    val pg_year_sql =
      """
                        SELECT
                               sn,year,sum(cnt)
                            FROM
                             ( select
                       		    dim_sn AS sn,
                       			dim_year AS year,
                       			SUM(fact_vv) AS cnt
                       		from hr.tracker_player_fact_partition
                       		WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                             AND date <= '""" + analysisDate +
        """'
               				group by dim_sn,dim_year
                               UNION  ALL
                               SELECT lp.dim_sn AS sn ,
                                        fp.year AS year,
                                        SUM(lp.fact_cnt) AS cnt
                             FROM
                               (SELECT *
                               FROM hr.tracker_live_fact_partition
                          WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                      AND date <= '""" + analysisDate +
        """') lp
                           JOIN hr.epg epg
                           ON (lp.dim_date = epg.tv_date
                           AND lp.dim_channel = epg.channel
                           AND lp.dim_hour = epg.tv_hour
                           AND lp.dim_min = epg.tv_min)

                           JOIN hr.epg_film ef
                           ON epg.pg = ef.pg
                           JOIN hr.film_properties fp
                           ON ef.id = fp.id
                           GROUP BY  lp.dim_sn, fp.year ) res
                           GROUP BY sn,year
        """.stripMargin

    println(pg_year_sql)

    val pg_year_rdd = sqlContext.sql(pg_year_sql).rdd

    //TODO 直播
    //Live_fact   EPG  epg_film   film_properties

    val pg_year_pair = pg_year_rdd.map(line => {

      val sn = line(0)

      val dim_year = line(1).toString.trim

      val cnt = line(2).toString

      val vectorStr = UserVectorHelper.getCntDistArr(dim_year, UserVectorConstant.BH_PG_YEAR_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })




    // TODO	观看过的产地分布

    //TODO 点播+直播
    val pg_region_sql =
      """
                      SELECT
                            sn,model,region,sum(cnt)
                               FROM
                                 (
                       		select
                       			dim_sn as sn,
                       			dim_model as model,
                       			dim_region as region,
                       			SUM(fact_vv) as cnt
                       		FROM hr.tracker_player_fact_partition
                            WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                            AND date <= '""" + analysisDate +
        """'
                            group by dim_sn,dim_model,dim_region

                            UNION  ALL

                            SELECT
                                lp.dim_sn AS sn,
                                fp.model AS model,
                                fp.region AS region,
                                SUM(lp.fact_cnt) AS cnt
                            FROM (SELECT * FROM hr.tracker_live_fact_partition
                                WHERE date > date_sub('""" + analysisDate +
        """',""" + recentDaysNum +
        """)
                                  AND date <= '""" + analysisDate +
        """') lp

                            JOIN hr.epg epg
                              ON (lp.dim_date = epg.tv_date
                              AND lp.dim_channel = epg.channel
                              AND lp.dim_hour = epg.tv_hour
                              AND lp.dim_min = epg.tv_min)

                              JOIN hr.epg_film ef
                              ON epg.pg = ef.pg

                              JOIN hr.film_properties fp
                              ON ef.id = fp.id
                              GROUP BY  lp.dim_sn,fp.model,fp.region
                          ) res
                          GROUP BY sn,model,region
        """.stripMargin

    println(pg_region_sql)

    val pg_region_rdd = sqlContext.sql(pg_region_sql).rdd


    val pg_region_rdd_pair = pg_region_rdd.map(line => {

      val sn = line(0)

      val dim_model = line(1)

      val dim_region = (dim_model + "-" + line(2).toString).trim

      val cnt = line(3).toString

      val vectorStr = UserVectorHelper.getCntDistArr(dim_region, UserVectorConstant.BH_PG_REGION_ARR, cnt)

      (sn, vectorStr)
    }).reduceByKey((prex, post) => {
      UserVectorHelper.arrCompact(prex, post)
    }).map(line => {
      (line._1, UserVectorHelper.arr2IntStrDelimitedByComma(line._2))
    })


    //TODO 默认值
    val ps_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_SUBJECT_ARR, "0")
    val py_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_YEAR_ARR, "0")
    val pr_default = UserVectorHelper.getCntDistStr("0", UserVectorConstant.BH_PG_REGION_ARR, "0")



    //TODO 合并成行为特征表
    val resultRDD = pg_subject_pair
      .fullOuterJoin(pg_year_pair).map(line => {
      val sn = line._1
      val ps = line._2._1.getOrElse(ps_default)
      val py = line._2._2.getOrElse(py_default)
      (sn, (ps, py))
    })

      .fullOuterJoin(pg_region_rdd_pair).map(line => {
      val sn = line._1
      val ps_py = line._2._1.getOrElse((ps_default, py_default))
      val pr = line._2._2.getOrElse(pr_default)
      (sn, (ps_py, pr))
    })

      .map(line => {
        val sn = line._1
        val pg_subject_dist = line._2._1._1
        val pg_year_dist = line._2._1._2
        val pd_region_dist = line._2._2
        sn + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pd_region_dist
      }).filter(line => {
      !line.contains("NaN")
    })

    //    resultRDD

    //      .saveAsTextFile("/user/hdfs/rsync/uservector/" +
    //        analysisDate + "-" + recentDaysNum + System
    //        .currentTimeMillis + "UserVectorPlayETL")


    //    TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_vector_play")

      try {

        items.foreach(line => {

          //        1		key	string	from deserializer
          //        2		sn	string	from deserializer
          //        3		stat_date	string	from deserializer
          //        4		period	string	from deserializer
          //        5		pg_subject_dist	string	from deserializer
          //        6		pg_year_dist	string	from deserializer
          //        7		pg_region_dist 	string	from deserializer

          val cols = line.split("\t")

          val sn = cols(0)

          val stat_date = analysisDate

          val period = recentDaysNum

          val pg_subject_dist = cols(1)

          val pg_year_dist = cols(2)

          val pg_region_dist = cols(3)

          val orderedLine = sn + "\t" + stat_date + "\t" + period + "\t" + pg_subject_dist + "\t" + pg_year_dist + "\t" + pg_region_dist

          mutator.mutate(HBaseUtils.getPut_UserVectorPlay(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}
package com.avcdata.spark.job.clean

import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorHelper}
import com.avcdata.spark.job.until.ValidateUtils
import com.avcdata.spark.job.util.HBaseUtils
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object UserVectorTerminalETL {


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("UserVectorTerminalETL")
    val sc = new SparkContext(conf)
    run(sc, "2017-01-04", "30")

    sc.stop()


  }

  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    //TODO 从sample_termianl_three  &  live_termianl中获取终端信息
    val sqlContext = new HiveContext(sc)

    //TODO 获取价格匹配数据
    val pricePair = sc.textFile("/user/hdfs/rsync/userdata/terminal-price.csv").map(line => {
      val cols = line.split(",")
      val brand = cols(0)
      val model = cols(1)
      val size = cols(2)
      val price = cols(3)
      (brand + "\t" + model, size + "\t" + price)
    })

    //TODO 获取终端数据
    sqlContext.sql("use hr")

    val sampleThreeDF = sqlContext.sql("SELECT tr.sn, tr.brand, tr.province, tm.model, tm.size FROM hr.sample_terminal_three tr JOIN hr.terminal tm ON (tr.sn = tm.sn)")
    val liveTerminalDF = sqlContext.sql("SELECT tr.sn, tr.brand, tr.province, tm.model, tm.size FROM hr.live_terminal tr JOIN hr.terminal tm ON (tr.sn = tm.sn) ")

    val unionDF = sampleThreeDF.unionAll(liveTerminalDF).distinct

    val terminalPair = unionDF.rdd.map(line => {

      val sn = line(0)
      val brand = line(1).toString
      val province = line(2).toString

      var model = line(3).toString
      var size = line(4).toString

      if (model.trim.isEmpty) {
        model = "#"
      }

      if (size.trim.isEmpty || (!ValidateUtils.isNumber(size))) {
        size = "0"
      }

      (brand + "\t" + model, sn + "\t" + province + "\t" + size)
    })

    //TODO 关联
    val allRDD = terminalPair.leftOuterJoin(pricePair).map(line => {
      //      (k, (v, None))
      val leftCols = line._1.split("\t")

      val brand = leftCols(0)

      val rightTerminalCols = line._2._1.split("\t")

      val sn = rightTerminalCols(0)

      val province = rightTerminalCols(1)

      var size = rightTerminalCols(2)

      var price = "0"

      //关联上的
      if (!line._2._2.isEmpty) {
        val rightPriceCols = line._2._2.get.toString.split("\t")
        size = rightPriceCols(0)
        price = rightPriceCols(1)
      }

      sn + "\t" + brand + "\t" + province + "\t" + size + "\t" + price
    })



    //TODO 转换成向量格式
    val resultRDD = allRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val brandVector = UserVectorHelper.getCntDistStr(cols(1).toString, UserVectorConstant.TERMINAL_BRAND_ARR, "1")

      val provinceVector = UserVectorHelper.getCntDistStr(cols(2).toString, UserVectorConstant
        .TERMINAL_PROVINCE_ARR, "1")


      val size = UserVectorHelper.getRegionDist(cols(3).trim.toInt, UserVectorConstant.TV_SIZE_ARR)

      val price = UserVectorHelper.getRegionDist(cols(4).trim.toDouble, UserVectorConstant.TV_PRICE_ARR)

      //距离当前年份的数（倒推）转换
      val pro_year = UserVectorHelper.getYear("2016")

      sn + "\t" + brandVector + "\t" + provinceVector + "\t" + price + "\t" + size + "\t" + pro_year
    }).filter(line => {
      !line.contains("NaN")
    })


    //      .saveAsTextFile("/user/hdfs/rsync/uservector/analysisDate")

    //    // TODO 写入hbase
    resultRDD.foreachPartition(items => {

      val mutator = HBaseUtils.getMutator("user_vector_terminal")

      try {

        items.foreach(line => {

          val cols = line.split("\t")

          val sn = cols(0)
          val brand = cols(1)
          val province = cols(2)
          val price = cols(3)
          val size = cols(4)

          val orderedLine = sn + "\t" + brand + "\t" + province + "\t" + price + "\t" + size

          mutator.mutate(HBaseUtils.getPut_UserVectorTerminal(orderedLine))
        })
        mutator.flush()

      } finally {
        mutator.close()
      }
    })

  }


}

package com.avcdata.spark.job.until

import java.util.regex.Pattern

import com.avcdata.spark.job.util.Constant
import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils {



  /**
    * 判断是否是数字
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]", "").replaceAll("\\d|\\w", "")
    println("res:" + res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  def main(args: Array[String]): Unit = {

    var res = true
    res = isContainsMessyCode("??????asdfasdf")

    //    res = isContainsMessyCode("呵呵呵呵呵呵")
    //    Array([2020,2020,动画片,4, ,少儿, ], [蜜蜜和丽莎的魔幻旅程,蜜蜜和丽莎的魔幻旅程,动画片,21670, ,少儿, ], [『剧集』蜜蜜与莉莎的魔幻旅程,蜜蜜与莉莎的魔幻旅程,动画片,21671,2015,少儿,欧美], [“做张贺卡送祖国”fun秀进校园,“做张贺卡送祖国”fun秀进校园,动画片,23, ,少儿, ], [面包超人 咪嘉与魔法灯,面包超人：咪嘉与魔法灯,动画片,21694, ,少儿, ], [『剧集』面粉镇的节日,面粉镇的节日,动画片,21706,2008,少儿,中国大陆], [1001个玩意儿,1001个玩意儿,动画片,42, ,少儿, ], [面具熊,面具熊,动画片,21709, ,少儿, ], [面具战士,面具战士,动画片,21710, ,少儿,中国大陆], [『剧集』面具战士,面具战士,动画片,21710,2014,少儿,中国大陆], [喵星人V5动作戏,喵星人V5动作戏,动画片,21717, ,少儿, ], [喵星人的那些小破事儿,喵星人的那些小破事儿,动画片,21718, ,少儿, ], [妙趣森林,妙趣森林,动画片,21744, ,少儿, ], [妙音动漫系列,妙音动漫系列,动画片,21755, ,少儿, ], [『剧集』1到2岁绘本故事,1到2岁绘本故事,动画片,114,2013,少儿,中国大陆], [『剧集』1至2岁宝宝好习惯,1至2岁宝宝好习惯,动画片,115,2013,少儿,中国大陆], [『剧集』名画神剪历险记,名画神剪历险记,动画片,21783,2013,少儿,中国大陆], [名人爆料童年趣事,名人爆料童年趣事,动画片,21797, ,少儿, ]


    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.until

import java.util.regex.Pattern

import com.avcdata.spark.job.util.Constant
import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils {



  /**
    * 判断是否是数字
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]", "").replaceAll("\\d|\\w", "")
    println("res:" + res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  def main(args: Array[String]): Unit = {
    println(isNumber("0.1"))

//    var res = true
//    res = isContainsMessyCode("??????asdfasdf")

    //    res = isContainsMessyCode("呵呵呵呵呵呵")
    //    Array([2020,2020,动画片,4, ,少儿, ], [蜜蜜和丽莎的魔幻旅程,蜜蜜和丽莎的魔幻旅程,动画片,21670, ,少儿, ], [『剧集』蜜蜜与莉莎的魔幻旅程,蜜蜜与莉莎的魔幻旅程,动画片,21671,2015,少儿,欧美], [“做张贺卡送祖国”fun秀进校园,“做张贺卡送祖国”fun秀进校园,动画片,23, ,少儿, ], [面包超人 咪嘉与魔法灯,面包超人：咪嘉与魔法灯,动画片,21694, ,少儿, ], [『剧集』面粉镇的节日,面粉镇的节日,动画片,21706,2008,少儿,中国大陆], [1001个玩意儿,1001个玩意儿,动画片,42, ,少儿, ], [面具熊,面具熊,动画片,21709, ,少儿, ], [面具战士,面具战士,动画片,21710, ,少儿,中国大陆], [『剧集』面具战士,面具战士,动画片,21710,2014,少儿,中国大陆], [喵星人V5动作戏,喵星人V5动作戏,动画片,21717, ,少儿, ], [喵星人的那些小破事儿,喵星人的那些小破事儿,动画片,21718, ,少儿, ], [妙趣森林,妙趣森林,动画片,21744, ,少儿, ], [妙音动漫系列,妙音动漫系列,动画片,21755, ,少儿, ], [『剧集』1到2岁绘本故事,1到2岁绘本故事,动画片,114,2013,少儿,中国大陆], [『剧集』1至2岁宝宝好习惯,1至2岁宝宝好习惯,动画片,115,2013,少儿,中国大陆], [『剧集』名画神剪历险记,名画神剪历险记,动画片,21783,2013,少儿,中国大陆], [名人爆料童年趣事,名人爆料童年趣事,动画片,21797, ,少儿, ]


//    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.vbox.util

import java.util.regex.Pattern


/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils {



  /**
    * 判断是否是数字
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]", "").replaceAll("\\d|\\w", "")
    println("res:" + res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }

  /**
    * 判断是否包含字母
    * @param str
    * @return
    */
  def isContainsLetter(str: String): Boolean = {
    val p = Pattern.compile("[a-zA-Z]")
    val m = p.matcher(str)

    m.find()
  }



  /**
    * 判断是否包含特殊字符
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  def main(args: Array[String]): Unit = {


    println(isContainsLetter("haha"))
    println(isContainsLetter("123"))

//    var res = true
//    res = isContainsMessyCode("??????asdfasdf")

    //    res = isContainsMessyCode("呵呵呵呵呵呵")
    //    Array([2020,2020,动画片,4, ,少儿, ], [蜜蜜和丽莎的魔幻旅程,蜜蜜和丽莎的魔幻旅程,动画片,21670, ,少儿, ], [『剧集』蜜蜜与莉莎的魔幻旅程,蜜蜜与莉莎的魔幻旅程,动画片,21671,2015,少儿,欧美], [“做张贺卡送祖国”fun秀进校园,“做张贺卡送祖国”fun秀进校园,动画片,23, ,少儿, ], [面包超人 咪嘉与魔法灯,面包超人：咪嘉与魔法灯,动画片,21694, ,少儿, ], [『剧集』面粉镇的节日,面粉镇的节日,动画片,21706,2008,少儿,中国大陆], [1001个玩意儿,1001个玩意儿,动画片,42, ,少儿, ], [面具熊,面具熊,动画片,21709, ,少儿, ], [面具战士,面具战士,动画片,21710, ,少儿,中国大陆], [『剧集』面具战士,面具战士,动画片,21710,2014,少儿,中国大陆], [喵星人V5动作戏,喵星人V5动作戏,动画片,21717, ,少儿, ], [喵星人的那些小破事儿,喵星人的那些小破事儿,动画片,21718, ,少儿, ], [妙趣森林,妙趣森林,动画片,21744, ,少儿, ], [妙音动漫系列,妙音动漫系列,动画片,21755, ,少儿, ], [『剧集』1到2岁绘本故事,1到2岁绘本故事,动画片,114,2013,少儿,中国大陆], [『剧集』1至2岁宝宝好习惯,1至2岁宝宝好习惯,动画片,115,2013,少儿,中国大陆], [『剧集』名画神剪历险记,名画神剪历险记,动画片,21783,2013,少儿,中国大陆], [名人爆料童年趣事,名人爆料童年趣事,动画片,21797, ,少儿, ]


//    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils01 {





  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    var result = "#"
    filmInfoArr.foreach(row => {
      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】
      //完全匹配
      if ("电影".equals(module) && log_dim_title.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去掉版本匹配
      for (version <- versionArr) {
        if (log_dim_title.endsWith(version)) {
          if ("电影".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
            result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }

      //提取书名号中的内容
      if ("电影".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //数字转换匹配
      var log_dim_title_num = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
      )

      if ("电影".equals(module) && log_dim_title_num.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去特殊符号匹配
      var log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！", "")
      if ("电影".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      //完全匹配
      if (!"^(\\d{8})(.+)[([ ]\\d)|(_\\d)|(\\d)|(-第\\d集)|(第\\d集)|(\\(第\\d集\\))|(大结局)|(先导集)]{1}$".r.findFirstMatchIn(log_dim_title).isEmpty) {
        if ("电视剧".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("电视剧".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("电视剧".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        val luomaNumMap = Constant.luomaNumMap
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        val zhNumMap = Constant.zhNumMap
        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("电视剧".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        val log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("电视剧".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

      }

      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      ///头部 动画名称 版本 集数 无关字段
      if (!"^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r.findFirstMatchIn(log_dim_title)
        .isEmpty) {
        if ("动画片".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("动画片".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("动画片".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        val luomaNumMap = Constant.luomaNumMap
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        val zhNumMap = Constant.zhNumMap
        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("动画片".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        val log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("动画片".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

      }

      ////////////////////////////////////////////////////////////////////////////////////////////
      if (!"^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季|期)|第\\d(集|季|期)|\\(第\\d(集|季|期)\\)|大结局|先导集){1})([^集季]*)$".r.findFirstMatchIn(log_dim_title).isEmpty) {
        if ("综艺".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }


        //提取书名号中的内容
        if ("综艺".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        val luomaNumMap = Constant.luomaNumMap
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        val zhNumMap = Constant.zhNumMap
        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("综艺".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        val log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("综艺".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }
      }

    })


    //        var apkPackage = cols(5)
    //        if (apkPackage.equals("com.tianci.movieplatform")) {
    //          snLicenseArrBV.value.foreach(ele => {
    //            if (sn.equals(ele.get(0))) {
    //              if (ele.get(1).equals("tencent")) {
    //                apkPackage = "腾讯launcher"
    //              }
    //
    //              if (ele.get(1).equals("yinhe")) {
    //                apkPackage = "爱奇艺launcher"
    //              }
    //            }
    //
    //          })
    //        }


    //完全匹配 原始名称 original_name

    //匹配上  返回

    //数字转换 中文数字、罗马数字 -->阿拉伯数字
    //匹配

    //提取书名号里面的内容
    //匹配







    result
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    *
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]","").replaceAll("\\d|\\w","")
    println("res:"+res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    *
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    *
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    *
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  /**
    * 提取视频名称中的集数（酷开）
    */
  def extractVideoPartOFCooCaa(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    var matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(result)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }
    result

  }


  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }

  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }


  def main(args: Array[String]): Unit = {
    var res = true
    res = isContainsMessyCode("??????asdfasdf")
//    res = isContainsMessyCode("呵呵呵呵呵呵")
    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils01 {





  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    var result = "#"
    filmInfoArr.foreach(row => {
      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】
      //完全匹配
      if ("电影".equals(module) && log_dim_title.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去掉版本匹配
      for (version <- versionArr) {
        if (log_dim_title.endsWith(version)) {
          if ("电影".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
            result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }

      //提取书名号中的内容
      if ("电影".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //数字转换匹配
      var log_dim_title_num = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
      )

      if ("电影".equals(module) && log_dim_title_num.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去特殊符号匹配
      var log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！", "")
      if ("电影".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      //完全匹配
      if (!"^(\\d{8})(.+)[([ ]\\d)|(_\\d)|(\\d)|(-第\\d集)|(第\\d集)|(\\(第\\d集\\))|(大结局)|(先导集)]{1}$".r.findFirstMatchIn(log_dim_title).isEmpty) {
        if ("电视剧".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("电视剧".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("电视剧".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        val luomaNumMap = Constant.luomaNumMap
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        val zhNumMap = Constant.zhNumMap
        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("电视剧".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        val log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("电视剧".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

      }

      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      ///头部 动画名称 版本 集数 无关字段
      if (!"^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r.findFirstMatchIn(log_dim_title)
        .isEmpty) {
        if ("动画片".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("动画片".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("动画片".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        val luomaNumMap = Constant.luomaNumMap
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        val zhNumMap = Constant.zhNumMap
        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("动画片".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        val log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("动画片".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

      }

      ////////////////////////////////////////////////////////////////////////////////////////////
      if (!"^(\\d{8})(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集|季|期)|第\\d(集|季|期)|\\(第\\d(集|季|期)\\)|大结局|先导集){1})([^集季]*)$".r.findFirstMatchIn(log_dim_title).isEmpty) {
        if ("综艺".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }


        //提取书名号中的内容
        if ("综艺".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        val luomaNumMap = Constant.luomaNumMap
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        val zhNumMap = Constant.zhNumMap
        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("综艺".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        val log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("综艺".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }
      }

    })


    //        var apkPackage = cols(5)
    //        if (apkPackage.equals("com.tianci.movieplatform")) {
    //          snLicenseArrBV.value.foreach(ele => {
    //            if (sn.equals(ele.get(0))) {
    //              if (ele.get(1).equals("tencent")) {
    //                apkPackage = "腾讯launcher"
    //              }
    //
    //              if (ele.get(1).equals("yinhe")) {
    //                apkPackage = "爱奇艺launcher"
    //              }
    //            }
    //
    //          })
    //        }


    //完全匹配 原始名称 original_name

    //匹配上  返回

    //数字转换 中文数字、罗马数字 -->阿拉伯数字
    //匹配

    //提取书名号里面的内容
    //匹配







    result
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    *
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]","").replaceAll("\\d|\\w","")
    println("res:"+res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    *
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    *
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    *
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  /**
    * 提取视频名称中的集数（酷开）
    */
  def extractVideoPartOFCooCaa(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    var matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(result)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }
    result

  }


  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }

  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }


  def main(args: Array[String]): Unit = {
    var res = true
    res = isContainsMessyCode("??????asdfasdf")
//    res = isContainsMessyCode("呵呵呵呵呵呵")
    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils02 {





  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    var result = "#"
    filmInfoArr.foreach(row => {
      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】
      //完全匹配
      if ("电影".equals(module) && log_dim_title.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去掉版本匹配
      for (version <- versionArr) {
        if (log_dim_title.endsWith(version)) {
          if ("电影".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
            result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }

      //提取书名号中的内容
      if ("电影".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //数字转换匹配
      var log_dim_title_num = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
      )

      if ("电影".equals(module) && log_dim_title_num.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去特殊符号匹配
      var log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！", "")
      if ("电影".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      //完全匹配
        if ("电视剧".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("电视剧".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("电视剧".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("电视剧".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("电视剧".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }


      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      ///头部 动画名称 版本 集数 无关字段
      if (!"^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r.findFirstMatchIn(log_dim_title)
        .isEmpty) {
        if ("动画片".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("动画片".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("动画片".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("动画片".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("动画片".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

      }

      ////////////////////////////////////////////////////////////////////////////////////////////
        if ("综艺".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }


        //提取书名号中的内容
        if ("综艺".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("综艺".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("综艺".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

    })


    //        var apkPackage = cols(5)
    //        if (apkPackage.equals("com.tianci.movieplatform")) {
    //          snLicenseArrBV.value.foreach(ele => {
    //            if (sn.equals(ele.get(0))) {
    //              if (ele.get(1).equals("tencent")) {
    //                apkPackage = "腾讯launcher"
    //              }
    //
    //              if (ele.get(1).equals("yinhe")) {
    //                apkPackage = "爱奇艺launcher"
    //              }
    //            }
    //
    //          })
    //        }


    //完全匹配 原始名称 original_name

    //匹配上  返回

    //数字转换 中文数字、罗马数字 -->阿拉伯数字
    //匹配

    //提取书名号里面的内容
    //匹配


    result
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    *
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]","").replaceAll("\\d|\\w","")
    println("res:"+res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    *
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    *
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    *
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  /**
    * 提取视频名称中的集数（酷开）
    */
  def extractVideoPartOFCooCaa(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    var matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(result)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }
    result

  }


  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }

  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }


  def main(args: Array[String]): Unit = {
    var res = true
    res = isContainsMessyCode("??????asdfasdf")
//    res = isContainsMessyCode("呵呵呵呵呵呵")
    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils02 {





  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    var result = "#"
    filmInfoArr.foreach(row => {
      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))
      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】
      //完全匹配
      if ("电影".equals(module) && log_dim_title.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去掉版本匹配
      for (version <- versionArr) {
        if (log_dim_title.endsWith(version)) {
          if ("电影".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
            result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }

      //提取书名号中的内容
      if ("电影".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //数字转换匹配
      var log_dim_title_num = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
      )

      if ("电影".equals(module) && log_dim_title_num.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      //去特殊符号匹配
      var log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！", "")
      if ("电影".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
        result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
      }

      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      //完全匹配
        if ("电视剧".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("电视剧".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("电视剧".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("电视剧".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("电视剧".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }


      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      ///头部 动画名称 版本 集数 无关字段
      if (!"^(\\d+)(.+)(([ ]\\d|_\\d|\\(\\d\\)|-第\\d(集)|第\\d(集)|\\(第\\d(集)\\)|大结局|先导集){1})([^集]*)$".r.findFirstMatchIn(log_dim_title)
        .isEmpty) {
        if ("动画片".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去掉版本匹配
        for (version <- versionArr) {
          if (log_dim_title.contains(version)) {
            if ("动画片".equals(module) && log_dim_title.substring(0, log_dim_title.indexOf(version)).equals(original_name)) {
              result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
            }
          }
        }

        //提取书名号中的内容
        if ("动画片".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("动画片".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("动画片".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

      }

      ////////////////////////////////////////////////////////////////////////////////////////////
        if ("综艺".equals(module) && log_dim_title.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }


        //提取书名号中的内容
        if ("综艺".equals(module) && extractByBookMark(log_dim_title).equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //数字转换匹配
        //var log_dim_title_num = log_dim_title
        luomaNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, luomaNumMap.get(i).get)
        )

        zhNumMap.keys.foreach(i =>
          log_dim_title_num = log_dim_title_num.replaceAll(i, zhNumMap.get(i).get)
        )

        if ("综艺".equals(module) && log_dim_title_num.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

        //去特殊符号匹配
        log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")
        if ("综艺".equals(module) && log_dim_title_spec.trim.equals(original_name)) {
          result = standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
        }

    })


    //        var apkPackage = cols(5)
    //        if (apkPackage.equals("com.tianci.movieplatform")) {
    //          snLicenseArrBV.value.foreach(ele => {
    //            if (sn.equals(ele.get(0))) {
    //              if (ele.get(1).equals("tencent")) {
    //                apkPackage = "腾讯launcher"
    //              }
    //
    //              if (ele.get(1).equals("yinhe")) {
    //                apkPackage = "爱奇艺launcher"
    //              }
    //            }
    //
    //          })
    //        }


    //完全匹配 原始名称 original_name

    //匹配上  返回

    //数字转换 中文数字、罗马数字 -->阿拉伯数字
    //匹配

    //提取书名号里面的内容
    //匹配


    result
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    *
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]","").replaceAll("\\d|\\w","")
    println("res:"+res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    *
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    *
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    *
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  /**
    * 提取视频名称中的集数（酷开）
    */
  def extractVideoPartOFCooCaa(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    var matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(result)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }
    result

  }


  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }

  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }


  def main(args: Array[String]): Unit = {
    var res = true
    res = isContainsMessyCode("??????asdfasdf")
//    res = isContainsMessyCode("呵呵呵呵呵呵")
    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils03 {

  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))


      //去掉版本匹配
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容
      val delBookMarkTitle = extractByBookMark(log_dim_title)

      //数字转换匹配
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去特殊符号匹配
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      //去剧集匹配
      var delPartTitle = log_dim_title.trim

      if(delPartTitle.contains("第")){
        delPartTitle= delPartTitle.trim.substring(0,delPartTitle.indexOf("第")).trim
      }

      if(log_dim_title.contains("_")){
        delPartTitle= delPartTitle.trim.substring(0,delPartTitle.indexOf("_")).trim
      }

      val titleArr = Array[String](log_dim_title, delVersionTitle, delBookMarkTitle, changeNumTitle,
        log_dim_title_spec,delPartTitle)


      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】
      if(module.equals("电影")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }


      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if(module.equals("电视剧")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }

      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      if(module.equals("动画片")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }


      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////
      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if(module.equals("综艺")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }


    })

    return "#"
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    *
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]", "").replaceAll("\\d|\\w", "")
    println("res:" + res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    *
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    *
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    *
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  /**
    * 提取视频名称中的集数（酷开）
    */
  def extractVideoPartOFCooCaa(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    var matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(result)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }
    result

  }


  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }

  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }


  def main(args: Array[String]): Unit = {

    var res = true
    res = isContainsMessyCode("??????asdfasdf")

    //    res = isContainsMessyCode("呵呵呵呵呵呵")
//    Array([2020,2020,动画片,4, ,少儿, ], [蜜蜜和丽莎的魔幻旅程,蜜蜜和丽莎的魔幻旅程,动画片,21670, ,少儿, ], [『剧集』蜜蜜与莉莎的魔幻旅程,蜜蜜与莉莎的魔幻旅程,动画片,21671,2015,少儿,欧美], [“做张贺卡送祖国”fun秀进校园,“做张贺卡送祖国”fun秀进校园,动画片,23, ,少儿, ], [面包超人 咪嘉与魔法灯,面包超人：咪嘉与魔法灯,动画片,21694, ,少儿, ], [『剧集』面粉镇的节日,面粉镇的节日,动画片,21706,2008,少儿,中国大陆], [1001个玩意儿,1001个玩意儿,动画片,42, ,少儿, ], [面具熊,面具熊,动画片,21709, ,少儿, ], [面具战士,面具战士,动画片,21710, ,少儿,中国大陆], [『剧集』面具战士,面具战士,动画片,21710,2014,少儿,中国大陆], [喵星人V5动作戏,喵星人V5动作戏,动画片,21717, ,少儿, ], [喵星人的那些小破事儿,喵星人的那些小破事儿,动画片,21718, ,少儿, ], [妙趣森林,妙趣森林,动画片,21744, ,少儿, ], [妙音动漫系列,妙音动漫系列,动画片,21755, ,少儿, ], [『剧集』1到2岁绘本故事,1到2岁绘本故事,动画片,114,2013,少儿,中国大陆], [『剧集』1至2岁宝宝好习惯,1至2岁宝宝好习惯,动画片,115,2013,少儿,中国大陆], [『剧集』名画神剪历险记,名画神剪历险记,动画片,21783,2013,少儿,中国大陆], [名人爆料童年趣事,名人爆料童年趣事,动画片,21797, ,少儿, ]


    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.spark.job.util

import java.util.regex.Pattern

import org.apache.spark.sql.Row

/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils03 {

  /**
    * 酷开到剧剧名清洗
    *
    * @param log_dim_title
    * @return
    */
  def extractVideoNameOFCooCaa(log_dim_title: String, filmInfoArr: Array[Row]): String = {

    val versionArr = Array[String](
      "未删减版", "[未删减版]", "完整版", "全集", "合集", "完全版", "[TV版]", "精华版", "国语", "（国语）", "（国语版）", "国语版", "国语中字", "（英语版）", "英语中字", "（英语）", "[英语版]", "英语版", "[英语]", "粤语版", "粤语", "（粤语版）", "（粤语）", "[粤语版]", "[粤语]", "日语版", "（日语版）", "日语", "中文版", "TV中文版", "（中文版）", "韩语版", "[韩语版]", "韩语中字", "四川话版", "云南话版", "东北话版", "天津话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "日配版", "法语版", "卫视版", "央视版", "TVB版", "浙江卫视版", "湖南卫视版", "东方卫视版", "安徽卫视版", "深圳卫视版", "旅游卫视版", "江西卫视版", "DVD版", "网络版", "电视版", "版权版", "OVA", "标准版", "原版", "未删剪原版", "4K版", "（4K）", "VR版", "（VR）", "3D版", "【3D版】", "（新3D版）", "（3D）", "3D", "标清版", "_标清", "蓝光真高清", "（蓝光真高清）", "（清晰版）", "高清版", "高清字幕版", "【高清】", "[高清版]", "春节贺岁版", "纯享版", "精简版", "加长版", "（加长版）", "（加长重映版）", "精编版", "重制版", "字幕版", "双语字幕版", "（双语字幕版）", "高清无字幕版", "完整字幕版", "中英字幕版", "免费版", "（免费版）", "[免费版]", "（原声）", "原声", "原声高清版", "英文原声高清版", "特别版", "生肖特别版", "圣诞特别版", "完全版"
    )

    //TODO 判断 module
    //从书名号中提取 数字 特殊符号清除 [空 国语版]
    //通过 ****版本 判断是电影

    filmInfoArr.foreach(row => {

      //original_name,model,id,year,crowd,region
      val original_name = isNullorEmptyHandle(row.getString(0))
      val standard_name = isNullorEmptyHandle(row.getString(1))
      val module = isNullorEmptyHandle(row.getString(2))
      val id = isNullorEmptyHandle(row.getString(3))
      val year = isNullorEmptyHandle(row.getString(4))
      val crowd = isNullorEmptyHandle(row.getString(5))
      val region = isNullorEmptyHandle(row.getString(6))


      //去掉版本匹配
      var delVersionTitle = log_dim_title
      for (version <- versionArr) {
        if (log_dim_title.contains(version)) {
          delVersionTitle = log_dim_title.substring(0, log_dim_title.indexOf(version))
        }
      }

      //提取书名号中的内容
      val delBookMarkTitle = extractByBookMark(log_dim_title)

      //数字转换匹配
      var changeNumTitle = log_dim_title
      val luomaNumMap = Constant.luomaNumMap
      luomaNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, luomaNumMap.get(i).get)
      )

      val zhNumMap = Constant.zhNumMap
      zhNumMap.keys.foreach(i =>
        changeNumTitle = changeNumTitle.replaceAll(i, zhNumMap.get(i).get)
      )

      //去特殊符号匹配
      var log_dim_title_spec = log_dim_title.trim
      log_dim_title_spec = log_dim_title.replaceAll(":|,|!|。|：|，|！|•   |.", "")

      //去剧集匹配
      var delPartTitle = log_dim_title.trim

      if(delPartTitle.contains("第")){
        delPartTitle= delPartTitle.trim.substring(0,delPartTitle.indexOf("第")).trim
      }

      if(log_dim_title.contains("_")){
        delPartTitle= delPartTitle.trim.substring(0,delPartTitle.indexOf("_")).trim
      }

      val titleArr = Array[String](log_dim_title, delVersionTitle, delBookMarkTitle, changeNumTitle,
        log_dim_title_spec,delPartTitle)


      ///////////////////////////////////////////////////////////////////////////
      //电影 【电影名称】【版本】
      if(module.equals("电影")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }


      /////////////////////////////////////////////////////////////////////////////////
      //电视剧：8为数字/空 书名号 数字 去符号 版本 集数
      if(module.equals("电视剧")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }

      //////////////////////////////////////////////////////////////////////////////////
      //动画片
      if(module.equals("动画片")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }


      ///头部 动画名称 版本 集数 无关字段
      // //数字 名称  版本  集数
      ////////////////////////////////////////////////////////////////////////////////////////////
      //综艺 //8位数字 书名号 去空格 版本 期数 之  排除：爸爸去哪儿_02 爸爸去哪儿_05
      if(module.equals("综艺")){
        for (title <- titleArr) {
          if (title.equals(original_name)) {
            return standard_name + "\t" + module + "\t" + id + "\t" + year + "\t" + crowd + "\t" + region
          }
        }
      }


    })

    return "#"
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    *
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]", "").replaceAll("\\d|\\w", "")
    println("res:" + res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    *
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }


  /**
    * 判断是否包含特殊字符
    *
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    *
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  /**
    * 提取视频名称中的集数（酷开）
    */
  def extractVideoPartOFCooCaa(videoName: String): String = {

    var result = "unknow"
    //    println(videoName)

    if (videoName.contains("大结局")) {
      result = "大结局"
    }

    if (videoName.contains("先导集")) {
      result = "1"
    }

    var matched = "\\_[\\s\\S]*".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("\\_", "").trim


    //////////////////////////////////////////////////////////////////

    matched = "第[\\s\\S]*集".r.findFirstMatchIn(videoName)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("第|集|_", "").trim


    ////////////////////////////////////////////////////////////

    matched = "季[\\s\\S]{0,10}".r.findFirstMatchIn(result)

    if (!matched.isEmpty)
      result = matched.get.toString().replaceAll("季|_", "").trim

    //20160905企鹅爱地球(17)
    //regex4
    val luomaNumMap = Constant.luomaNumMap
    luomaNumMap.keys.foreach(i =>
      result = result.replaceAll(i, luomaNumMap.get(i).get)
    )

    val zhNumMap = Constant.zhNumMap
    zhNumMap.keys.foreach(i =>
      result = result.replaceAll(i, zhNumMap.get(i).get)
    )


    //regex5

    //regex6


    //    if (!matched.isEmpty) {
    //      result = matched.get.toString().replaceAll("第", "").replaceAll("集", "").trim
    //
    //      if (result.contains("季")) {
    //        result = result.substring(result.indexOf("季") + 1).trim
    //      } else {
    //        result = result.substring(1)
    //      }
    //
    //    }
    result

  }


  def convertTitle2Keyword(title: String): String = {
    val keywordArr = Array[String](
      "国语版", "英语版", "粤语版", "日语版", "中文版", "韩语版", "四川话版", "东北话版", "天津话版", "日配版", "云南话版", "兰州话版", "潮汕话版", "陕西话版", "闽南语版", "上海话版", "中配版", "法语版", "多语言版", "话混搭版", "卫视版", "湖南卫视版", "DVD版", "网络版", "央视版", "浙江卫视版", "东方卫视版", "TVB版", "安徽卫视版", "旅游卫视版", "web版", "江西卫视版", "Q版", "OVA版", "FLASH版", "未删减版", "完整版", "全集版", "完全版", "标准版", "真人版", "特别版", "原版", "原声版", "清正版", "重制版", "高清版", "国际版", "独家抢鲜版", "免费版", "短剧版", "搜狐版", "字幕版", "特效重制版", "终极版", "明星版", "配音版", "重映版", "晚间版", "影院版", "新编集版", "分集版", "长篇版", "粉丝定制版", "现场版", "儿歌版", "夜间版", "普通版", "导演版", "抢鲜版", "整合版", "高清正版", "无悔版", "超长版", "现实版", "古代版", "演示版", "国画版", "影像版", "水墨版", "预告版", "翻唱版", "精华短剧版", "阿狸版", "旧版", "合唱版", "口琴版", "舔屏版", "沙画版", "短篇版", "世界版", "合集版", "三次元版", "大陆版", "美国版", "韩国版", "中国版", "英国版", "香港版", "浙江版", "海外版", "内地版", "台湾版", "哥伦比亚版", "伊朗版", "潮汕版", "西班牙版", "意大利版", "希腊版", "四川版", "德国版", "泰国版", "新加坡版", "电影版", "动漫版", "电视剧版", "精编版", "加长版", "纪念版", "经典版", "精简版", "纯享版", "定制版", "混剪版", "贺岁版", "典藏版", "教学版", "独家未播版", "周末版", "周间版", "日播版", "周播版", "清晰版", "蓝光版", "标清版", "3D版", "VR版", "4K版"
    )

    var result = title

    for (keyword <- keywordArr) {
      result = result.replaceAll(keyword, "")
    }

    result
  }

  def isNullorEmptyHandle(str: String): String = {
    var result = str
    if (str == null || str.trim.isEmpty) {
      result = "unknow"
    }
    result
  }

  /**
    * 从书名号中提取书名
    *
    * @param str
    * @return
    */
  def extractByBookMark(str: String): String = {
    ///\《([^》《]*)\》/ig
    val p = Pattern.compile("《(.+?)》")
    val m = p.matcher(str)

    while (m.find()) {
      m.group(1)
    }

    "unknow"
  }


  def main(args: Array[String]): Unit = {

    var res = true
    res = isContainsMessyCode("??????asdfasdf")

    //    res = isContainsMessyCode("呵呵呵呵呵呵")
//    Array([2020,2020,动画片,4, ,少儿, ], [蜜蜜和丽莎的魔幻旅程,蜜蜜和丽莎的魔幻旅程,动画片,21670, ,少儿, ], [『剧集』蜜蜜与莉莎的魔幻旅程,蜜蜜与莉莎的魔幻旅程,动画片,21671,2015,少儿,欧美], [“做张贺卡送祖国”fun秀进校园,“做张贺卡送祖国”fun秀进校园,动画片,23, ,少儿, ], [面包超人 咪嘉与魔法灯,面包超人：咪嘉与魔法灯,动画片,21694, ,少儿, ], [『剧集』面粉镇的节日,面粉镇的节日,动画片,21706,2008,少儿,中国大陆], [1001个玩意儿,1001个玩意儿,动画片,42, ,少儿, ], [面具熊,面具熊,动画片,21709, ,少儿, ], [面具战士,面具战士,动画片,21710, ,少儿,中国大陆], [『剧集』面具战士,面具战士,动画片,21710,2014,少儿,中国大陆], [喵星人V5动作戏,喵星人V5动作戏,动画片,21717, ,少儿, ], [喵星人的那些小破事儿,喵星人的那些小破事儿,动画片,21718, ,少儿, ], [妙趣森林,妙趣森林,动画片,21744, ,少儿, ], [妙音动漫系列,妙音动漫系列,动画片,21755, ,少儿, ], [『剧集』1到2岁绘本故事,1到2岁绘本故事,动画片,114,2013,少儿,中国大陆], [『剧集』1至2岁宝宝好习惯,1至2岁宝宝好习惯,动画片,115,2013,少儿,中国大陆], [『剧集』名画神剪历险记,名画神剪历险记,动画片,21783,2013,少儿,中国大陆], [名人爆料童年趣事,名人爆料童年趣事,动画片,21797, ,少儿, ]


    println(res)
    //    regxpTest("12341234哈哈")
    //    extractByBookMark("asdfasdfasdfasdf嘿嘿《哈哈asdfasdf》呵呵")

    //    println(extractVideoPartOFCooCaa("2005托马斯和他的朋友们_第3季_Ⅰ集"))
    //    println("2005托马斯和他的朋友们_第3季_三集集版".replaceAll("集版", ""))
    //    println(filterTitle("2005托马斯和他的朋友们_第3季_三集合集版"))

    //    val str =
    //      "[INFO]-[20:00:00.791] RequestBuilder4cupd:160 [loggerNo: 20150720195316712874<?xml version='1.0' encoding='gbk'?><trans><send_header><tran_code>011232</tran_code><tran_date>2015-07-20</tran_date><tran_time>200000</tran_time><code>011232</code><rcv_code>000000</rcv_code>";
    //    val info = getLoggerInfo(str, "INFO", false);
    //    println(info);
    //    val loggerNo = getLoggerInfo(str, "loggerNo", false);
    //    println(loggerNo);
    //    val tran_code = getLoggerInfo(str, "tran_code", false);
    //    println(tran_code);
    //    val code = getLoggerInfo(str, "code", false);
    //    println(code);
    //    val rcv_code = getLoggerInfo(str, "rcv_code", false);
    //    println(rcv_code);
  }


}
package com.avcdata.etl.common.util

import java.util.regex.Pattern

/**
  * 变量替换
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/5/25 15:22
  */
class VariableSubstitution(variableValues: Map[String, String])
{
  private val pattern = Pattern.compile("[#\\$]\\{[^#\\}\\$]+\\}") //Pattern.compile("""[#\$]\{\s*(\w+)\s*\}""")

  /**
    * Given a query, does variable substitution and return the result.
    */
  def substitute(input: String): String =
  {
    // Note that this function is mostly copied from Hive's SystemVariables, so the style is
    // very Java/Hive like.
    if (input eq null)
    {
      return null
    }

    var eval = input
    //conf.variableSubstituteDepth
    val depth = 20
    val builder = new StringBuilder
    val m = pattern.matcher("")

    var s = 0
    while (s <= depth)
    {
      m.reset(eval)
      builder.setLength(0)

      var prev = 0
      var found = false
      while (m.find(prev))
      {
        val group = m.group()
        var substitute = substituteVariable(group.substring(2, group.length - 1))
        if (substitute.isEmpty)
        {
          substitute = group
        }
        else
        {
          if (group.startsWith("#"))
          {
            substitute = "'" + substitute + "'"
          }

          found = true
        }

        builder.append(eval.substring(prev, m.start())).append(substitute)

        prev = m.end()
      }

      if (!found)
      {
        return eval
      }

      builder.append(eval.substring(prev))
      eval = builder.toString

      s += 1
    }

    if (s > depth)
    {
      throw new RuntimeException("Variable substitution depth is deeper than " + depth + " for input " + input)
    }
    else
    {
      eval
    }
  }

  /**
    * Given a variable, replaces with the substitute value (default to "").
    */
  private def substituteVariable(variable: String): String =
  {
    var value: String = null

    if (variable.startsWith("system:"))
    {
      value = System.getProperty(variable.substring("system:".length()))
    }

    if (value == null && variable.startsWith("env:"))
    {
      value = System.getenv(variable.substring("env:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hiveconf:") && variableValues.contains(variable.substring("hiveconf:".length())))
    {
      value = variableValues(variable.substring("hiveconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hivevar:") && variableValues.contains(variable.substring("hivevar:".length())))
    {
      value = variableValues(variable.substring("hivevar:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("sparkconf:") && variableValues.contains(variable.substring("sparkconf:".length())))
    {
      value = variableValues(variable.substring("sparkconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("spark:") && variableValues.contains(variable.substring("spark:".length())))
    {
      value = variableValues(variable.substring("spark:".length()))
    }

    if (value == null && variableValues != null && variableValues.contains(variable))
    {
      value = variableValues(variable)
    }

    if (value == null)
    {
      value = ""
    }

    value
  }
}
package com.avcdata.etl.common.util

import java.util.regex.Pattern

/**
  * 变量替换
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/5/25 15:22
  */
class VariableSubstitution(variableValues: Map[String, String])
{
  private val pattern = Pattern.compile("[#\\$]\\{[^#\\}\\$]+\\}") //Pattern.compile("""[#\$]\{\s*(\w+)\s*\}""")

  /**
    * Given a query, does variable substitution and return the result.
    */
  def substitute(input: String): String =
  {
    // Note that this function is mostly copied from Hive's SystemVariables, so the style is
    // very Java/Hive like.
    if (input eq null)
    {
      return null
    }

    var eval = input
    //conf.variableSubstituteDepth
    val depth = 20
    val builder = new StringBuilder
    val m = pattern.matcher("")

    var s = 0
    while (s <= depth)
    {
      m.reset(eval)
      builder.setLength(0)

      var prev = 0
      var found = false
      while (m.find(prev))
      {
        val group = m.group()
        var substitute = substituteVariable(group.substring(2, group.length - 1))
        if (substitute.isEmpty)
        {
          substitute = group
        }
        else
        {
          if (group.startsWith("#"))
          {
            substitute = "'" + substitute + "'"
          }

          found = true
        }

        builder.append(eval.substring(prev, m.start())).append(substitute)

        prev = m.end()
      }

      if (!found)
      {
        return eval
      }

      builder.append(eval.substring(prev))
      eval = builder.toString

      s += 1
    }

    if (s > depth)
    {
      throw new RuntimeException("Variable substitution depth is deeper than " + depth + " for input " + input)
    }
    else
    {
      eval
    }
  }

  /**
    * Given a variable, replaces with the substitute value (default to "").
    */
  private def substituteVariable(variable: String): String =
  {
    var value: String = null

    if (variable.startsWith("system:"))
    {
      value = System.getProperty(variable.substring("system:".length()))
    }

    if (value == null && variable.startsWith("env:"))
    {
      value = System.getenv(variable.substring("env:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hiveconf:") && variableValues.contains(variable.substring("hiveconf:".length())))
    {
      value = variableValues(variable.substring("hiveconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hivevar:") && variableValues.contains(variable.substring("hivevar:".length())))
    {
      value = variableValues(variable.substring("hivevar:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("sparkconf:") && variableValues.contains(variable.substring("sparkconf:".length())))
    {
      value = variableValues(variable.substring("sparkconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("spark:") && variableValues.contains(variable.substring("spark:".length())))
    {
      value = variableValues(variable.substring("spark:".length()))
    }

    if (value == null && variableValues != null && variableValues.contains(variable))
    {
      value = variableValues(variable)
    }

    if (value == null)
    {
      value = ""
    }

    value
  }
}
package com.avcdata.etl.common.util

import java.util.regex.Pattern

/**
  * 变量替换
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/5/25 15:22
  */
class VariableSubstitution(variableValues: Map[String, String])
{
  private val pattern = Pattern.compile("[#\\$]\\{[^#\\}\\$]+\\}") //Pattern.compile("""[#\$]\{\s*(\w+)\s*\}""")

  /**
    * Given a query, does variable substitution and return the result.
    */
  def substitute(input: String): String =
  {
    // Note that this function is mostly copied from Hive's SystemVariables, so the style is
    // very Java/Hive like.
    if (input eq null)
    {
      return null
    }

    var eval = input
    //conf.variableSubstituteDepth
    val depth = 20
    val builder = new StringBuilder
    val m = pattern.matcher("")

    var s = 0
    while (s <= depth)
    {
      m.reset(eval)
      builder.setLength(0)

      var prev = 0
      var found = false
      while (m.find(prev))
      {
        val group = m.group()
        var substitute = substituteVariable(group.substring(2, group.length - 1))
        if (substitute.isEmpty)
        {
          substitute = group
        }
        else
        {
          if (group.startsWith("#"))
          {
            substitute = "'" + substitute + "'"
          }

          found = true
        }

        builder.append(eval.substring(prev, m.start())).append(substitute)

        prev = m.end()
      }

      if (!found)
      {
        return eval
      }

      builder.append(eval.substring(prev))
      eval = builder.toString

      s += 1
    }

    if (s > depth)
    {
      throw new RuntimeException("Variable substitution depth is deeper than " + depth + " for input " + input)
    }
    else
    {
      eval
    }
  }

  /**
    * Given a variable, replaces with the substitute value (default to "").
    */
  private def substituteVariable(variable: String): String =
  {
    var value: String = null

    if (variable.startsWith("system:"))
    {
      value = System.getProperty(variable.substring("system:".length()))
    }

    if (value == null && variable.startsWith("env:"))
    {
      value = System.getenv(variable.substring("env:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hiveconf:") && variableValues.contains(variable.substring("hiveconf:".length())))
    {
      value = variableValues(variable.substring("hiveconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hivevar:") && variableValues.contains(variable.substring("hivevar:".length())))
    {
      value = variableValues(variable.substring("hivevar:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("sparkconf:") && variableValues.contains(variable.substring("sparkconf:".length())))
    {
      value = variableValues(variable.substring("sparkconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("spark:") && variableValues.contains(variable.substring("spark:".length())))
    {
      value = variableValues(variable.substring("spark:".length()))
    }

    if (value == null && variableValues != null && variableValues.contains(variable))
    {
      value = variableValues(variable)
    }

    if (value == null)
    {
      value = ""
    }

    value
  }
}
package com.avcdata.etl.common.util

import java.util.regex.Pattern

/**
  * 变量替换
  * <p/>
  * Author   : wangxp
  * <p/>
  * DateTime : 2016/5/25 15:22
  */
class VariableSubstitution(variableValues: Map[String, String])
{
  private val pattern = Pattern.compile("[#\\$]\\{[^#\\}\\$]+\\}") //Pattern.compile("""[#\$]\{\s*(\w+)\s*\}""")

  /**
    * Given a query, does variable substitution and return the result.
    */
  def substitute(input: String): String =
  {
    // Note that this function is mostly copied from Hive's SystemVariables, so the style is
    // very Java/Hive like.
    if (input eq null)
    {
      return null
    }

    var eval = input
    //conf.variableSubstituteDepth
    val depth = 20
    val builder = new StringBuilder
    val m = pattern.matcher("")

    var s = 0
    while (s <= depth)
    {
      m.reset(eval)
      builder.setLength(0)

      var prev = 0
      var found = false
      while (m.find(prev))
      {
        val group = m.group()
        var substitute = substituteVariable(group.substring(2, group.length - 1))
        if (substitute.isEmpty)
        {
          substitute = group
        }
        else
        {
          if (group.startsWith("#"))
          {
            substitute = "'" + substitute + "'"
          }

          found = true
        }

        builder.append(eval.substring(prev, m.start())).append(substitute)

        prev = m.end()
      }

      if (!found)
      {
        return eval
      }

      builder.append(eval.substring(prev))
      eval = builder.toString

      s += 1
    }

    if (s > depth)
    {
      throw new RuntimeException("Variable substitution depth is deeper than " + depth + " for input " + input)
    }
    else
    {
      eval
    }
  }

  /**
    * Given a variable, replaces with the substitute value (default to "").
    */
  private def substituteVariable(variable: String): String =
  {
    var value: String = null

    if (variable.startsWith("system:"))
    {
      value = System.getProperty(variable.substring("system:".length()))
    }

    if (value == null && variable.startsWith("env:"))
    {
      value = System.getenv(variable.substring("env:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hiveconf:") && variableValues.contains(variable.substring("hiveconf:".length())))
    {
      value = variableValues(variable.substring("hiveconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("hivevar:") && variableValues.contains(variable.substring("hivevar:".length())))
    {
      value = variableValues(variable.substring("hivevar:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("sparkconf:") && variableValues.contains(variable.substring("sparkconf:".length())))
    {
      value = variableValues(variable.substring("sparkconf:".length()))
    }

    if (value == null && variableValues != null && variable.startsWith("spark:") && variableValues.contains(variable.substring("spark:".length())))
    {
      value = variableValues(variable.substring("spark:".length()))
    }

    if (value == null && variableValues != null && variableValues.contains(variable))
    {
      value = variableValues(variable)
    }

    if (value == null)
    {
      value = ""
    }

    value
  }
}
package com.avcdata.spark.job.etl.stat.cross

import com.avcdata.spark.job.common.Helper
import com.avcdata.spark.job.etl.common.{UserVectorConstant, UserVectorParser}
import com.avcdata.spark.job.util.JdbcUtils
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}


object WorkdayOcTimeChannel {

  case class WorkdayOcTimeDistCnt(
                                   sn: String,
                                   stat_date: String,
                                   period: String,
                                   dim_hour: String,
                                   cnt: String,
                                   cluster_id: String
                                 )


  def main(args: Array[String]) {
    val conf = new SparkConf()
      .setMaster("local[1]")
      .setAppName("BhWorkdayOcTimeDistCnt")
    val sc = new SparkContext(conf)
    run(sc, "2017-03-15", "15")
    sc.stop()

  }


  def run(sc: SparkContext, analysisDate: String, recentDaysNum: String) = {
    testRun(sc, analysisDate, recentDaysNum, 4)
    testRun(sc, analysisDate, recentDaysNum, 8)
    testRun(sc, analysisDate, recentDaysNum, 12)
    testRun(sc, analysisDate, recentDaysNum, 16)
  }


  def testRun(sc: SparkContext, analysisDate: String, recentDaysNum: String, clusterNum: Int) = {
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._

    val initRDD = sc.textFile("/user/hdfs/rsync/uservector/" + analysisDate + "-" + recentDaysNum + "-ClusterResult-" + clusterNum)

    val snCidRDD = initRDD.map(line => {
      val cols = line.split("\t")
      val sn = cols(0)
      val stat_date = cols(1)
      val period = cols(2)
      val brand = cols(3)
      val province = cols(4)
      val price = cols(5)
      val size = cols(6)
      val workday_oc_dist = cols(7)
      val restday_oc_dist = cols(8)
      val workday_channel_dist = cols(9)
      val restday_channel_dist = cols(10)
      val pg_subject_dist = cols(11)
      val pg_yeay_dist = cols(12)
      val pg_region_dist = cols(13)
      val cluster_id = cols(14)

      (sn + "\t" + stat_date + "\t" + period, cluster_id)

    })

    val snWorkdayOCDistRDD = sqlContext.sql("select sn,stat_date,period,workday_oc_dist from hr.user_vector_all").rdd.map(line => {
      val sn = line(0)
      val stat_date = line(1)
      val period = line(2)
      val workday_oc_dist_map = UserVectorParser.getEleMapByVectorStrSplitByComma(line(3).toString, UserVectorConstant
        .BH_OC_HOUR_ARR)

      (sn + "\t" + stat_date + "\t" + period, workday_oc_dist_map)

    })

    val workdayOcTimeDistCntDF = snCidRDD.join(snWorkdayOCDistRDD).flatMap(line => {
      val leftCols = line._1.split("\t")
      val sn = leftCols(0)
      val stat_date = leftCols(1)
      val period = leftCols(2)

      val cluster_id = line._2._1
      val workday_oc_dist_map = line._2._2

      val resultArr = new Array[WorkdayOcTimeDistCnt](workday_oc_dist_map.size)

      val hour_arr = workday_oc_dist_map.keySet.toArray

      for (i <- 0 until resultArr.length) {
        val wotd = WorkdayOcTimeDistCnt(
          sn,
          stat_date,
          period,
          hour_arr(i),
          workday_oc_dist_map.get(hour_arr(i)).get.toString,
          cluster_id
        )

        resultArr(i) = wotd

      }

      resultArr
    }).toDF

    //    workdayOcTimeDistCntDF.rdd.map(line=>{
    //
    //      line(4)
    //    }).saveAsTextFile("/tmp/botd")
    //    //TODO 注册临时表
    workdayOcTimeDistCntDF.registerTempTable("workdayOcTimeDistCnt")

    val resultDF = sqlContext.sql(
      """
        select
        a.stat_date,a.period,a.cluster_id,a.dim_hour,a.cnt,a.cnt/b.c_cnt as percent
        from
        (
        select stat_date,period,cluster_id,dim_hour,sum(cnt) as cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id,dim_hour
        ) a
         join
        (
        select stat_date,period,cluster_id,sum(cnt) as c_cnt  from workdayOcTimeDistCnt group by stat_date,period,cluster_id
        ) b
        on
        (a.stat_date = b.stat_date and a.period=b.period and a.cluster_id = b.cluster_id)
      """.stripMargin)


    //TODO 写入Mysql
    JdbcUtils.writeDF2Mysql(sc, resultDF, Helper.mysqlConf, "personas", "stat_workday_oc_time_dist_cnt_k"+ clusterNum, false,
      SaveMode.Append)


  }
}
package stores

import kafka.common.TopicAndPartition
import org.I0Itec.zkclient.ZkClient
import org.I0Itec.zkclient.exception.{ZkNoNodeException, ZkNodeExistsException}
import org.I0Itec.zkclient.serialize.SerializableSerializer
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.kafka.HasOffsetRanges
import org.apache.zookeeper.data.Stat

class ZooKeeperOffsetsStore(zkHosts: String, zkPath: String) extends OffsetsStore {

  private val zkClient = new ZkClient(zkHosts, 10000, 10000, new SerializableSerializer())

  def readDataMaybeNull(client: ZkClient, path: String): (Option[String], Stat) = {
    val stat: Stat = new Stat()
    val dataAndStat = try {
      (Some(client.readData(path, stat)), stat)
    } catch {
      case e: ZkNoNodeException =>
        (None, stat)
      case e2: Throwable => throw e2
    }
    dataAndStat
  }

  def createParentPath(client: ZkClient, path: String): Unit = {
    val parentDir = path.substring(0, path.lastIndexOf('/'))
    if (parentDir.length != 0)
      client.createPersistent(parentDir, true)
  }

  def updatePersistentPath(client: ZkClient, path: String, data: String) = {
    try {
      client.writeData(path, data)
    } catch {
      case e: ZkNoNodeException => {
        createParentPath(client, path)
        try {
          client.createPersistent(path, data)
        } catch {
          case e: ZkNodeExistsException =>
            client.writeData(path, data)
          case e2: Throwable => throw e2
        }
      }
      case e2: Throwable => throw e2
    }
  }

  // Read the previously saved offsets from Zookeeper
  override def readOffsets(topic: String): Option[Map[TopicAndPartition, Long]] = {

    val (offsetsRangesStrOpt, _) = readDataMaybeNull(zkClient,zkPath)

    offsetsRangesStrOpt match {
      case Some(offsetsRangesStr) =>

        val offsets = offsetsRangesStr.split(",")
          .map(s => s.split(":"))
          .map { case Array(partitionStr, offsetStr) => (TopicAndPartition(topic, partitionStr.toInt) -> offsetStr.toLong) }
          .toMap

        Some(offsets)
      case None =>
        None
    }

  }

  override def saveOffsets(topic: String, rdd: RDD[_]): Unit = {

    val offsetsRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges

    val offsetsRangesStr = offsetsRanges.map(offsetRange => s"${offsetRange.partition}:${offsetRange.fromOffset}")
      .mkString(",")
    updatePersistentPath(zkClient, zkPath, offsetsRangesStr)


  }

}