

导语：本文介绍的项目主要解决 check 和 opinion2 张历史数据表（历史数据是指当业务发生过程中的完整中间流程和结果数据）的在线查询。原实现基于 Oracle 提供存储查询服务，随着数据量的不断增加，在写入和读取过程中面临性能问题，且历史数据仅供业务查询参考，并不影响实际流程，从系统结构上来说，放在业务链条上游比较重。该项目将其置于下游数据处理 Hadoop 分布式平台来实现此需求。
背景介绍
本项目主要解决 check 和 opinion2 张历史数据表（历史数据是指当业务发生过程中的完整中间流程和结果数据）的在线查询。原实现基于 Oracle 提供存储查询服务，随着数据量的不断增加，在写入和读取过程中面临性能问题，且历史数据仅供业务查询参考，并不影响实际流程，从系统结构上来说，放在业务链条上游比较重。本项目将其置于下游数据处理 Hadoop 分布式平台来实现此需求。下面列一些具体的需求指标：



数据量：目前 check 表的累计数据量为 5000w+ 行，11GB；opinion 表的累计数据量为 3 亿 +，约 100GB。每日增量约为每张表 50 万 + 行，只做 insert，不做 update。

查询要求：check 表的主键为 id（Oracle 全局 id），查询键为 check_id，一个 check_id 对应多条记录，所以需返回对应记录的 list； opinion 表的主键也是 id，查询键是 bussiness_no 和 buss_type，同理返回 list。单笔查询返回 List 大小约 50 条以下，查询频率为 100 笔 / 天左右，查询响应时间 2s。

技术选型
从数据量及查询要求来看，分布式平台上具备大数据量存储，且提供实时查询能力的组件首选 HBase。根据需求做了初步的调研和评估后，大致确定 HBase 作为主要存储组件。将需求拆解为写入和读取 HBase 两部分。

读取 HBase 相对来说方案比较确定，基本根据需求设计 RowKey，然后根据 HBase 提供的丰富 API（get，scan 等）来读取数据，满足性能要求即可。

写入 HBase 的方法大致有以下几种：

1、 Java 调用 HBase 原生 API，HTable.add(List(Put))。

2、 MapReduce 作业，使用 TableOutputFormat 作为输出。

3、 Bulk Load，先将数据按照 HBase 的内部数据格式生成持久化的 HFile 文件，然后复制到合适的位置并通知 RegionServer ，即完成海量数据的入库。其中生成 Hfile 这一步可以选择 MapReduce 或 Spark。

本文采用第 3 种方式，Spark + Bulk Load 写入 HBase。该方法相对其他 2 种方式有以下优势：

① BulkLoad 不会写 WAL，也不会产生 flush 以及 split。

②如果我们大量调用 PUT 接口插入数据，可能会导致大量的 GC 操作。除了影响性能之外，严重时甚至可能会对 HBase 节点的稳定性造成影响，采用 BulkLoad 无此顾虑。

③过程中没有大量的接口调用消耗性能。

④可以利用 Spark 强大的计算能力。

图示如下：



设计
环境信息
Hadoop 2.5-2.7HBase 0.98.6Spark 2.0.0-2.1.1Sqoop 1.4.6
表设计
本段的重点在于讨论 HBase 表的设计，其中 RowKey 是最重要的部分。为了方便说明问题，我们先来看看数据格式。以下以 check 举例，opinion 同理。

check 表（原表字段有 18 个，为方便描述，本文截选 5 个字段示意）



如上图所示，主键为 id，32 位字母和数字随机组成，业务查询字段 check_id 为不定长字段（不超过 32 位），字母和数字组成，同一 check_id 可能对应多条记录，其他为相关业务字段。众所周知，HBase 是基于 RowKey 提供查询，且要求 RowKey 是唯一的。RowKey 的设计主要考虑的是数据将怎样被访问。初步来看，我们有 2 种设计方法。

① 拆成 2 张表，一张表 id 作为 RowKey，列为 check 表对应的各列；另一张表为索引表，RowKey 为 check_id，每一列对应一个 id。查询时，先找到 check_id 对应的 id list，然后根据 id 找到对应的记录。均为 HBase 的 get 操作。

②将本需求可看成是一个范围查询，而不是单条查询。将 check_id 作为 RowKey 的前缀，后面跟 id。查询时设置 Scan 的 startRow 和 stopRow，找到对应的记录 list。

第一种方法优点是表结构简单，RowKey 容易设计，缺点为 1）数据写入时，一行原始数据需要写入到 2 张表，且索引表写入前需要先扫描该 RowKey 是否存在，如果存在，则加入一列，否则新建一行，2）读取的时候，即便是采用 List, 也至少需要读取 2 次表。第二种设计方法，RowKey 设计较为复杂，但是写入和读取都是一次性的。综合考虑，我们采用第二种设计方法。

RowKey 设计
热点问题

HBase 中的行是以 RowKey 的字典序排序的，其热点问题通常发生在大量的客户端直接访问集群的一个或极少数节点。默认情况下，在开始建表时，表只会有一个 region，并随着 region 增大而拆分成更多的 region，这些 region 才能分布在多个 regionserver 上从而使负载均分。对于我们的业务需求，存量数据已经较大，因此有必要在一开始就将 HBase 的负载均摊到每个 regionserver，即做 pre-split。常见的防治热点的方法为加盐，hash 散列，自增部分（如时间戳）翻转等。

RowKey 设计

Step1：确定预分区数目，创建 HBase Table

不同的业务场景及数据特点确定数目的方式不一样，我个人认为应该综合考虑数据量大小和集群大小等因素。比如 check 表大小约为 11G，测试集群大小为 10 台机器，hbase.hregion.max.filesize=3G（当 region 的大小超过这个数时，将拆分为 2 个），所以初始化时尽量使得一个 region 的大小为 1~2G（不会一上来就 split），region 数据分到 11G/2G=6 个，但为了充分利用集群资源，本文中 check 表划分为 10 个分区。如果数据量为 100G，且不断增长，集群情况不变，则 region 数目增大到 100G/2G=50 个左右较合适。Hbase check 表建表语句如下：



其中，Column Family =‘f’，越短越好。

COMPRESSION => 'SNAPPY'，HBase 支持 3 种压缩 LZO, GZIP and Snappy。GZIP 压缩率高，但是耗 CPU。后两者差不多，Snappy 稍微胜出一点，cpu 消耗的比 GZIP 少。一般在 IO 和 CPU 均衡下，选择 Snappy。

DATA_BLOCK_ENCODING => 'FAST_DIFF'，本案例中 RowKey 较为接近，通过以下命令查看 key 长度相对 value 较长。





Step2：RowKey 组成

Salt

让数据均衡的分布到各个 Region 上，结合 pre-split，我们对查询键即 check 表的 check_id 求 hashcode 值，然后 modulus(numRegions) 作为前缀，注意补齐数据。



说明：如果数据量达上百 G 以上，则 numRegions 自然到 2 位数，则 salt 也为 2 位。

Hash 散列

因为 check_id 本身是不定长的字符数字串，为使数据散列化，方便 RowKey 查询和比较，我们对 check_id 采用 SHA1 散列化，并使之 32 位定长化。



唯一性

以上 salt+hash 作为 RowKey 前缀，加上 check 表的主键 id 来保障 RowKey 唯一性。综上，check 表的 RowKey 设计如下：（check_id=A208849559）



为增强可读性，中间还可以加上自定义的分割符，如’+’,’|’等。



以上设计能保证对每次查询而言，其 salt+hash 前缀值是确定的，并且落在同一个 region 中。需要说明的是 HBase 中 check 表的各列同数据源 Oracle 中 check 表的各列存储。

WEB 查询设计
RowKey 设计与查询息息相关，查询方式决定 RowKey 设计，反之基于以上 RowKey 设计，查询时通过设置 Scan 的 [startRow，stopRow], 即可完成扫描。以查询 check_id=A208849559 为例，根据 RowKey 的设计原则，对其进行 salt+hash 计算，得前缀。



代码实现关键流程
Spark write to HBase
Step0: prepare work

因为是从上游系统承接的业务数据，存量数据采用 sqoop 抽到 hdfs；增量数据每日以文件的形式从 ftp 站点获取。因为业务数据字段中包含一些换行符，且 sqoop1.4.6 目前只支持单字节，所以本文选择’0x01’作为列分隔符，’0x10’作为行分隔符。

Step1: Spark read hdfs text file



SparkContext.textfile() 默认行分隔符为”
”，此处我们用“0x10”，需要在 Configuration 中配置。应用配置，我们调用 newAPIHadoopFile 方法来读取 hdfs 文件，返回 JavaPairRDD，其中 LongWritable 和 Text 分别为 Hadoop 中的 Long 类型和 String 类型（所有 Hadoop 数据类型和 java 的数据类型都很相像，除了它们是针对网络序列化而做的特殊优化）。我们需要的数据文件放在 pairRDD 的 value 中，即 Text 指代。为后续处理方便，可将 JavaPairRDD转换为 JavaRDD< String >。

Step2: Transfer and sort RDD

① 将 avaRDD< String>转换成 JavaPairRDD<tuple2,String>，其中参数依次表示为，RowKey，col，value。做这样转换是因为 HBase 的基本原理是基于 RowKey 排序的，并且当采用 bulk load 方式将数据写入多个预分区（region）时，要求 Spark 各 partition 的数据是有序的，RowKey，column family（cf），col name 均需要有序。在本案例中因为只有一个列簇，所以将 RowKey 和 col name 组织出来为 Tuple2格式的 key。请注意原本数据库中的一行记录（n 个字段），此时会被拆成 n 行。</tuple2

② 基于 JavaPairRDD<tuple2,String>进行 RowKey，col 的二次排序。如果不做排序，会报以下异常：</tuple2



③ 将数据组织成 HFile 要求的 JavaPairRDDhfileRDD。

Step3：create hfile and bulk load to HBase

①主要调用 saveAsNewAPIHadoopFile 方法：



② hfilebulk load to HBase



注：如果集群开启了 kerberos，step4 需要放置在 ugi.doAs（）方法中，在进行如下验证后实现



访问 HBase 集群的 60010 端口 web，可以看到 region 分布情况。



Read from HBase
本文基于 spring boot 框架来开发 web 端访问 HBase 内数据。

use connection pool(使用连接池)

创建连接是一个比较重的操作，在实际 HBase 工程中，我们引入连接池来共享 zk 连接，meta 信息缓存，region server 和 master 的连接。



也可以通过以下方法，覆盖默认线程池。



process query

Step1: 根据查询条件，确定 RowKey 前缀

根据 3.3 RowKey 设计介绍，HBase 的写和读都遵循该设计规则。此处我们采用相同的方法，将 web 调用方传入的查询条件，转化成对应的 RowKey 前缀。例如，查询 check 表传递过来的 check_id=A208849559，生成前缀 7+7c9498b4a83974da56b252122b9752bf。

Step2：确定 scan 范围

A208849559 对应的查询结果数据即在 RowKey 前缀为 7+7c9498b4a83974da56b252122b9752bf 对应的 RowKey 及 value 中。



Step3：查询结果组成返回对象

遍历 ResultScanner 对象，将每一行对应的数据封装成 table entity，组成 list 返回。

测试
从原始数据中随机抓取 1000 个 check_id，用于模拟测试，连续发起 3 次请求数为 2000（200 个线程并发，循环 10 次），平均响应时间为 51ms，错误率为 0。





如上图，经历 N 次累计测试后，各个 region 上的 Requests 数较为接近，符合负载均衡设计之初。

踩坑记录
1、kerberos 认证问题
如果集群开启了安全认证，那么在进行 Spark 提交作业以及访问 HBase 时，均需要进行 kerberos 认证。

本文采用 yarn cluster 模式，像提交普通作业一样，可能会报以下错误。



定位到 HbaseKerberos.java:18，代码如下：



这是因为 executor 在进行 HBase 连接时，需要重新认证，通过 --keytab 上传的 tina.keytab 并未被 HBase 认证程序块获取到，所以认证的 keytab 文件需要另外通过 --files 上传。示意如下



其中 tina.keytab.hbase 是将 tina.keytab 复制并重命名而得。因为 Spark 不允许同一个文件重复上传。

2、序列化


解决方法一：

如果 sc 作为类的成员变量，在方法中被引用，则加 transient 关键字，使其不被序列化。



解决方法二：

将 sc 作为方法参数传递，同时使涉及 RDD 操作的类 implements Serializable。 代码中采用第二种方法。详见代码。

3、批量请求测试


或者



查看下面 issue 以及一次排查问题的过程，可能是 open file 超过限制。

https://github.com/spring-projects/spring-boot/issues/1106

http://mp.weixin.qq.com/s/34GVlaYDOdY1OQ9eZs-iXg

使用 ulimit-a 查看每个用户默认打开的文件数为 1024。

在系统文件 /etc/security/limits.conf 中修改这个数量限制，在文件中加入以下内容, 即可解决问题。

soft nofile 65536

hard nofile 65536

作者介绍
汪婷，中国民生银行大数据开发工程师，专注于 Spark 大规模数据处理和 Hbase 系统设计。

参考文献

http://hbase.apache.org/book.html#perf.writing

http://www.opencore.com/blog/2016/10/efficient-bulk-load-of-hbase-using-spark/

http://hbasefly.com/2016/03/23/hbase_writer/

https://github.com/spring-projects/spring-boot/issues/1106

http://mp.weixin.qq.com/s/34GVlaYDOdY1OQ9eZs-iXg

推荐阅读：

1，Hbase源码系列之源码前奏hbase:meta表相关详细介绍

2，Hbase源码系列之regionserver应答数据请求服务设计

3，Hbase源码系列之scan源码解析及调优

4，Hbase源码系列之BufferedMutator的Demo和源码解析








Hbase是运行在Hadoop上的NoSQL数据库，它是一个分布式的和可扩展的大数据仓库，也就是说HBase能够利用HDFS的分布式处理模

式，并从Hadoop的MapReduce程序模型中获益。这意味着在一组商业硬件上存储许多具有数十亿行和上百万列的大表。除去Hadoop的优

势，HBase本身就是十分强大的数据库，它能够融合key/value存储模式带来实时查询的能力，以及通过MapReduce进行离线处理或者批处理

的能力。

HBase不是一个关系型数据库，它需要不同的方法定义你的数据模型，HBase实际上定义了一个四维数据模型，下面就是每一维度的定义：

行键：每行都有唯一的行键，行键没有数据类型，它内部被认为是一个字节数组。

列簇：数据在行中被组织成列簇，每行有相同的列簇，但是在行之间，相同的列簇不需要有相同的列修饰符。在引擎中，HBase将列簇存储在它自己的数据文件中，所以，它们需要事先被定义，此外，改变列簇并不容易。

列修饰符：列簇定义真实的列，被称之为列修饰符，你可以认为列修饰符就是列本身。

硬件
存储
网络
操作系统调优
hadoop调优
hbase调优
负载不均调优

系统要求
hbase单机安装
虚拟机中的Hbase
本地与VM
故障排除
分布式

表设计
数据转换
HFile校验
批量加载
数据索引
数据检索


hbase作为记录系统
摄取 预处理
处理/服务
用户体验

近实时实现事件处理

Hbase作为主数据管理工具
摄取
处理

主数据管理工具HBase实现
MapReduce spark
spark hbase
spark hbase

文档存储
数据服务
数据摄取
清理

文档存储的实现
MoB
数据一致性

问题
region过多

列族过多

热点

超时

垃圾回收

HBCK和不一致
HBase文件系统布局
查看META表
在HDFS上查看HBase
HBCK概述
使用HBCK


1.2 关系数据库系统的问题
1.3 非关系型数据库系统Not-Only-SQL（简称NoSQL）
1.3.1 维度
1.3.2 可扩展性
1.3.3 数据库的范式化和反范式化
1.4 结构
1.4.1 背景
1.4.2 表、行、列和单元格
1.4.3 自动分区
1.4.4 存储API
1.4.5 实现
1.4.6 小结
1.5 HBase：Hadoop数据库
1.5.1 历史
1.5.2 命名

2.2 必备条件
2.2.1 硬件
2.2.2 软件
2.3 HBase使用的文件系统
2.3.1 本地模式
2.3.2 HDFS
2.3.3 S
2.3.4 其他文件系统
2.4 安装选项
2.4.1 Apache二进制发布包
2.4.2 编译源码
2.5 运行模式
2.5.1 单机模式
2.5.2 分布式模式
2.6.1 hbase-site.xml与hbase-default.xml
2.6.2 hbase-env.sh
2.6.3 regionserver
2.6.4 log4j.properties
2.6.5 配置示例
2.6.6 客户端配置
2.7 部署
2.7.1 基于脚本
2.7.2 Apache Whirr
2.7.3 Puppet与Chef
2.8 操作集群
2.8.1 确定安装运行
2.8.2 Web UI介绍
2.8.3 Shell介绍
2.8.4 关闭集群

3.2 CRUD操作
3.2.1 put方法
3.2.2 get方法
3.2.3 删除方法
3.3 批量处理操作
3.4 行锁
3.5 扫描
3.5.2 ResultScanner类
3.5.3 缓存与批量处理
3.6 各种特性
3.6.1 HTable的实用方法
3.6.2 Bytes类

4.1 过滤器
4.1.1 过滤器简介
4.1.2 比较过滤器
4.1.3 专用过滤器
4.1.4 附加过滤器
4.1.5 FilterList
4.1.6 自定义过滤器
4.1.7 过滤器总结
4.2 计数器
4.2.1 计数器简介
4.2.2 单计数器
4.2.3 多计数器
4.3 协处理器
4.3.1 协处理器简介
4.3.2 Coprocessor类
4.3.3 协处理器加载
4.3.4 RegionObserver类
4.3.5 MasterObserver类
4.3.6 endpoint
4.4 HTablePool
4.5 连接管理

第5章 客户端API：管理功能
5.1 模式定义
5.1.1 表
5.1.2 表属性
5.1.3 列族
5.2 HBaseAdmin
5.2.1 基本操作
5.2.2 表操作
5.2.3 模式操作
5.2.4 集群管理
5.2.5 集群状态信息

第6章 可用客户端
6.1 REST、Thrift和Avro的介绍
6.2 交互客户端
6.2.1 原生Java
6.2.2 REST
6.2.3 Thrift
6.2.4 Avro
6.2.5 其他客户端
6.3 批处理客户端
6.3.1 MapReduce
6.3.2 Hive
6.3.3 Pig
6.3.4 Cascading
6.4 Shell
6.4.1 基础
6.4.2 命令
6.4.3 脚本
6.5 基于Web的UI
6.5.1 master的UI
6.5.2 region服务器的UI
6.5.3 共享页面

第7章 与MapReduce集成
7.1 框架
7.1.1 MapReduce介绍
7.1.2 类
7.1.3 支撑类
7.1.4 MapReduce的执行地点
7.1.5 表拆分
7.2 在HBase之上的MapReduce
7.2.1 准备
7.2.2 数据流向
7.2.3 数据源
7.2.4 数据源与数据流向
7.2.5 自定义处理

第8章 架构
8.1 数据查找和传输
8.1.1 B+树
8.1.2 LSM树
8.2 存储
8.2.1 概览
8.2.2 写路径
8.2.3 文件
8.2.4 HFile格式
8.2.5 KeyValue格式
8.3 WAL
8.3.1 概述
8.3.2 HLog类
8.3.3 HLogKey类
8.3.4 WALEdit类
8.3.5 LogSyncer类
8.3.6 LogRoller类
8.3.7 回放
8.3.8 持久性
8.4 读路径
8.5 region查找
8.6 region生命周期
8.7 ZooKeeper
8.8 复制
8.8.1 Log Edit的生命周期
8.8.2 内部机制

9.1 行键设计
9.1.1 概念
9.1.2 高表与宽表
9.1.3 部分键扫描
9.1.4 分页
9.1.5 时间序列
9.1.6 时间顺序关系
9.2 高级模式
9.3 辅助索引
9.4 搜索集成
9.5 事务
9.6 布隆过滤器
9.7 版本管理
9.7.1 隐式版本控制
9.7.2 自定义版本控制

第10章 集群监控
10.1 介绍
10.2 监控框架
10.2.1 上下文、记录和监控指标
10.2.2 master监控指标
10.2.3 region服务器监控指标
10.2.4 RPC监控指标
10.2.5 JVM监控指标
10.2.6 info监控指标
10.3 Ganglia
10.3.1 安装
10.3.2 用法
10.4 JMX
10.4.1 JConsole
10.4.2 JMX远程API
10.5 Nagios

第11章 性能优化
11.1 垃圾回收优化
11.2 本地memstore分配缓冲区
11.3 压缩
11.3.1 可用的编解码器
11.3.2 验证安装
11.3.3 启用压缩
11.4 优化拆分和合并
11.4.1 管理拆分
11.4.2 region热点
11.4.3 预拆分region
11.5 负载均衡
11.6 合并region
11.7 客户端API：最佳实践
11.8 配置
11.9 负载测试
11.9.1 性能评价
11.9.2 YCSB

第12章 集群管理
12.1 运维任务
12.1.1 减少节点
12.1.2 滚动重启
12.1.3 新增服务器
12.2 数据任务
12.2.1 导入/导出
12.2.2 CopyTable工具
12.2.3 批量导入
12.2.4 复制
12.3 额外的任务
12.3.1 集群共存
12.3.2 端口要求
12.4 改变日志级别
12.5 故障处理
12.5.1 HBase Fsck
12.5.2 日志分析
12.5.3 常见问题

附录A HBase配置属性
附录B 计划
附录C 版本升级
附录D 分支
附录E Hush SQL Schema
附录F 对比HBase和BigTable









1.1 海量数据与NoSQL 1
1.1.1 关系型数据库的极限 1
1.1.2 CAP理论 1
1.1.3 NoSQL 2
1.2 HBase是怎么来的 3
1.3 为什么要用HBase 3
1.4 你必须懂的基本概念 4
1.4.1 部署架构 4
1.4.2 存储架构 7
1.4.3 跟关系型数据库的对比 9
第2章 让HBase跑起来 11
2.1 本书测试环境 12
2.2 配置服务器名 12
2.3 配置SSH免密登录 13
2.4 安装Hadoop 15
2.4.1 安装Hadoop单机模式 15
2.4.2 安装Hadoop集群模式 20
2.4.3 ZooKeeper 23
2.4.4 配置Hadoop HA 27
2.4.5 让Hadoop可以开机自启动 35
2.4.6 最终配置文件 41
2.5 安装HBase 43
2.5.1 单机模式 45
2.5.2 伪分布式模式 47
2.5.3 关于ZooKeeper不得不说的事 51
2.5.4 完全分布式模式 52
2.5.5 HBase Web控制台（UI） 58
2.5.6 让HBase可以开机自启动 58
2.5.7 启用数据块编码（可选） 60
2.5.8 启用压缩器（可选） 65
2.5.9 数据块编码还是压缩器（可选） 70
第3章 HBase基本操作 71
3.1 hbase shell的使用 71
3.1.1 用create命令建表 72
3.1.2 用list命令来查看库中有哪些表 73
3.1.3 用describe命令来查看表属性 73
3.1.4 用put命令来插入数据 74
3.1.5 用scan来查看表数据 76
3.1.6 用get来获取单元格数据 77
3.1.7 用delete来删除数据 77
3.1.8 用deleteall删除整行记录 79
3.1.9 用disable来停用表 80
3.1.10 用drop来删除表 80
3.1.11 shell命令列表 81
3.2 使用Hue来查看HBase数据 121
3.2.1 准备工作 121
3.2.2 安装Hue 124
3.2.3 配置Hue 127
3.2.4 使用Hue来查看HBase 132
第4章 客户端API入门 134
4.1 10分钟教程 134
4.2 30分钟教程 141
4.3 CRUD一个也不能少 147
4.3.1 HTable类和Table接口 147
4.3.2 put方法 148
4.3.3 append方法 155
4.3.4 increment方法 157
4.3.5 get方法 158
4.3.6 exists 方法 162
4.3.7 delete方法 162
4.3.8 mutation方法 164
4.4 批量操作 166
4.4.1 批量put操作 167
4.4.2 批量get操作 167
4.4.3 批量delete操作 168
4.5 BufferedMutator（可选） 168
4.6 Scan扫描 170
4.6.1 用法 170
4.6.2 缓存 173
4.7 HBase支持什么数据格式 174
4.8 总结 175
第5章 HBase内部探险 176
5.1 数据模型 176
5.2 HBase是怎么存储数据的 178
5.2.1 宏观架构 178
5.2.2 预写日志 181
5.2.3 MemStore 183
5.2.4 HFile 184
5.2.5 KeyValue类 186
5.2.6 增删查改的真正面目 186
5.2.7 数据单元层次图 187
5.3 一个KeyValue的历险 187
5.3.1 写入 188
5.3.2 读出 188
5.4 Region的定位 189
第6章 客户端API的高阶用法 193
6.1 过滤器 193
6.1.1 过滤器快速入门 194
6.1.2 比较运算快速入门 198
6.1.3 分页过滤器 201
6.1.4 过滤器列表 203
6.1.5 行键过滤器 208
6.1.6 列过滤器 214
6.1.7 单元格过滤器 227
6.1.8 装饰过滤器 228
6.1.9 自定义过滤器 231
6.1.10 如何在hbase shell中使用过滤器 248
6.2 协处理器 249
6.2.1 协处理器家族 249
6.2.2 快速入门 251
6.2.3 如何加载 254
6.2.4 协处理器核心类 256
6.2.5 观察者 259
6.2.6 终端程序 276
第7章 客户端API的管理功能 290
7.1 列族管理 290
7.2 表管理 296
7.3 Region管理 299
7.4 快照管理 304
7.5 维护工具管理 307
7.5.1 均衡器 307
7.5.2 规整器 308
7.5.3 目录管理器 310
7.6 集群状态以及负载（ClusterStatus & ServerLoad） 311
7.7 Admin的其他方法 315
7.8 可见性标签管理 319
7.8.1 快速入门 321
7.8.2 可用标签 328
7.8.3 用户标签 329
7.8.4 单元格标签 329

第8章 再快一点 331
8.1 Master和RegionServer的JVM调优 331
8.1.1 先调大堆内存 331
8.1.2 可怕的Full GC 333
8.1.3 Memstore的专属JVM策略MSLAB 335
8.2 Region的拆分 340
8.2.1 Region的自动拆分 341
8.2.2 Region的预拆分 345
8.2.3 Region的强制拆分 347
8.2.4 推荐方案 347
8.2.5 总结 347
8.3 Region的合并 348
8.3.1 通过Merge类合并Region 348
8.3.2 热合并 348
8.4 WAL的优化 349
8.5 BlockCache的优化 351
8.5.1 LRUBlockCache 352
8.5.2 SlabCache 353
8.5.3 BucketCache 354
8.5.4 组合模式 356
8.5.5 总结 357
8.6 Memstore的优化 357
8.6.1 读写中的Memstore 358
8.6.2 Memstore的刷写 358
8.6.3 总结 361
8.7 HFile的合并 361
8.7.1 合并的策略 361
8.7.2 compaction的吞吐量限制参数 374
8.7.3 合并的时候HBase做了什么 377
8.7.4 Major Compaction 378
8.7.5 总结 380
8.8 诊断手册 380
8.8.1 阻塞急救 380
8.8.2 朱丽叶暂停 381
8.8.3 读取性能调优 384
8.8.4 案例分析 385
第9章 当HBase遇上MapReduce 389
9.1 为什么要用MapReduce 389
9.2 快速入门 389
9.3 慢速入门：编写自己的MapReduce 391
9.3.1 准备数据 391
9.3.2 新建项目 392
9.3.3 建立MapReduce类 393
9.3.4 建立驱动类 396
9.3.5 打包、部署、运行 400
9.4 相关类介绍 402
9.4.1 TableMapper 402
9.4.2 TableReducer 403
9.4.3 TableMapReduceUtil 403
1.1 数据管理系统：速成
1.1.1 你好，大数据
1.1.2 数据创新
1.1.3 HBase的崛起
1.2 HBase使用场景和成功案例
1.2.1 典型互联网搜索问题：BigTable发明的原因
1.2.2 抓取增量数据
1.2.3 内容服务
1.2.4 信息交换
1.3 你好HBase
1.3.1 快速安装
1.3.2 HBase Shell命令行交互
1.3.3 存储数据
1.4 小结

第2章 入门
2.1 从头开始
2.1.1 创建表
2.1.2 检查表模式
2.1.3 建立连接
2.1.4 连接管理
2.2 数据操作
2.2.1 存储数据
2.2.2 修改数据
2.2.3 工作机制：HBase写路径
2.2.4 读数据
2.2.5 工作机制：HBase读路径
2.2.6 删除数据
2.2.7 合并：HBase的后台工作
2.2.8 有时间版本的数据
2.2.9 数据模型概括
2.3 数据坐标
2.4 小结
2.5 数据模型
2.5.1 逻辑模型：有序映射的映射集合
2.5.2 物理模型：面向列族
2.6 表扫描
2.6.1 设计用于扫描的表
2.6.2 执行扫描
2.6.3 扫描器缓存
2.6.4 使用过滤器
2.7 原子操作
2.8 ACID语义
2.9 小结

第3章 分布式的HBase、HDFS和MapReduce
3.1 一个MapReduce的例子
3.1.1 延迟与吞吐量
3.1.2 串行计算吞吐量有限
3.1.3 并行计算提高吞吐量
3.1.4 MapReduce：用布式计算最大化吞吐量
3.2 Hadoop MapReduce概览
3.2.1 MapReduce数据流介绍
3.2.2 MapReduce内部机制
3.3 分布式模式的HBase
3.3.1 切分和分配大表
3.3.2 如何找到region
3.3.3 如何找到-ROOT-表
3.4 HBase和MapReduce
3.4.1 使用HBase作为数据源
3.4.2 使用HBase接收数据
3.4.3 使用HBase共享资源
3.5 信息汇总
3.5.1 编写MapReduce应用
3.5.2 运行MapReduce应用
3.6 大规模条件下的可用性和可靠性
3.6.1 HDFS作为底层存储
3.7 小结

第二部分 高级概念
第4章 HBase表设计
4.1 如何开始模式设计
4.1.1 问题建模
4.1.2 需求定义：提前多做准备工作总是有好处的
4.1.3 均衡分布数据和负载的建模方法
4.1.4 目标数据访问
4.2 反规范化是HBase世界里的词语
4.3 相同表里的混杂数据
4.4 行键设计策略
4.5 IO考虑
4.5.1 为写优化
4.5.2 为读优化
4.5.3 基数和行键结构
4.6 从关系型到非关系型
4.6.1 一些基本概念
4.6.2 嵌套实体
4.6.3 没有映射到的一些东西
4.7 列族高级配置
4.7.1 可配置的数据块大小
4.7.2 数据块缓存
4.7.3 激进缓存
4.7.4 布隆过滤器
4.7.5 生存时间（TTL）
4.7.6 压缩
4.7.7 单元时间版本
4.8 过滤数据
4.8.1 实现一个过滤器
4.8.2 预装过滤器
4.9 小结

第5章 使用协处理器扩展HBase
5.1 两种协处理器
5.1.1 Observer协处理器
5.1.2 endpoint协处理器
5.2 实现一个observer
5.2.1 修改模式
5.2.2 从HBase开始
5.2.3 安装observer
5.2.4 其他安装选项
5.3 实现一个endpoint
5.3.1 为endpoint定义接口
5.3.2 实现endpoint服务器
5.3.3 实现endpoint客户端
5.3.4 部署endpoint服务器
5.3.5 试运行
5.4 小结

第6章 其他的HBase客户端选择
6.1 在UNIX里使用HBase Shell脚本
6.1.1 准备HBase Shell
6.1.2 使用UNIX Shell脚本创建表模式
6.2 使用JRuby进行HBase Shell编程
6.2.1 准备HBase Shell
6.2.2 访问TwitBase的users表
6.3 通过REST访问HBase
6.3.1 启动HBase REST服务
6.3.2 访问TwitBase的users表
6.4 通过Python使用HBase Thrift网关
6.4.1 生成Python语言的HBase Thrift客户端库
6.4.2 启动HBase Thrift服务
6.4.3 扫描TwitBaseuser表
6.5 asynchbase：另外一种HBase Java客户端
6.5.1 创建一个asynchbase项目
6.5.2 改变TwitBase的密码策略
6.5.3 试运行
6.6 小结

第三部分 应用系统实例
第7章 通过实例学习HBase：OpenTSDB
7.1 OpenTSDB概述
7.1.1 挑战：基础设施监控
7.1.2 数据：时间序列
7.1.3 存储：HBase
7.2 设计一个HBase应用系统
7.2.1 模式设计
7.2.2 应用架构
7.3 实现一个HBase应用系统
7.3.1 存储数据
7.3.2 查询数据
7.4 小结

第8章 在HBase上查询地理信息系统
8.1 运用地理数据
8.2 设计一个空间索引
8.2.1 从复合行键开始
8.2.2 介绍geohash
8.2.3 理解geohash
8.2.4 在有空间感知特性的行键里使用geohash
8.3 实现最近邻居查询
8.4 把计算工作推往服务器端
8.4.1 基于查询多边形创建一次geohash扫描
8.4.2 区域内查询第一幕：客户端
8.4.3 区域内查询第二幕：WithinFilter
8.5 小结

第四部分 让HBase运转起来
第9章 部署HBase
9.1 规划集群
9.1.1 原型集群
9.1.2 小型生产集群（10～20台服务器）
9.1.3 中型生产集群（50台以下服务器）
9.1.4 大型生产集群（超过50台服务器）
9.1.5 Hadoop Master节点
9.1.6 HBase Master
9.1.7 Hadoop DataNode和HBase RegionServer
9.1.8 ZooKeeper
9.1.9 采用云服务怎么样？
9.2 部署软件
9.2.1 Whirr：在云端部署
9.3 发行版本
9.3.1 使用原生Apache发行版本
9.3.2 使用Cloudera的CDH发行版本
9.4 配置
9.4.1 HBase配置
9.4.2 与HBase有关的Hadoop配置参数
9.4.3 操作系统配置
9.5 管理守护进程
9.6 小结

第10章 运维
10.1 监控你的集群
10.1.1 HBase如何输出监控指标
10.1.2 收集监控指标和图形展示
10.1.3 HBase输出的监控指标
10.1.4 应用端监控
10.2 HBase集群的性能
10.2.1 性能测试
10.2.2 什么影响了HBase的性能
10.2.3 优化支撑系统
10.2.4 优化HBase
10.3 集群管理
10.3.1 启动和停止HBase
10.3.2 优雅停止和让节点退役
10.3.3 增加节点
10.3.4 滚动重启和升级
10.3.5 bin/hbase和HbaseShell
10.3.6 维护一致性——hbck
10.3.7 查看HFile和HLog
10.3.8 预先拆分表
10.4 备份和复制
10.4.1 集群间复制
10.4.2 使用MapReduce作业进行备份
10.4.3 备份根目录
10.5 小结

附录A 探索HBase系统
附录B 更多关于HDFS的工作原理
1．1　背景　1
1．2　NoSQL与传统RDBMS　2
1．3　应用场景　3
1．3．1　Facebook用户交互数据　3
1．3．2　淘宝TLog等　3
1．3．3　小米云服务　4
1．3．4　用户行为数据存储　4
第2章　HBase安装　5
2．1　单机部署　5
2．1．1　前置条件　5
2．1．2　下载HBase　6
2．1．3　配置HBase　6
2．1．4　启动HBase　6
2．1．5　HBase初体验　7
2．2　分布式部署　8
2．2．1　环境准备　9
2．2．2　ZooKeeper安装　11
2．2．3　Hadoop安装　12
2．2．4　HBase安装　23
2．2．5　启动集群　28
2．3　集群增删节点　29
2．3．1　增加节点　29
2．3．2　删除节点　30
第3章　HBase数据模型　33
3．1　逻辑模型　33
3．2　物理模型　35
第4章　HBase shell　39
4．1　数据定义语言　39
4．1．1　创建表　39
4．1．2　查看所有表　40
4．1．3　查看建表　40
4．1．4　修改表　41
4．2　数据操纵语言　41
4．2．1　Put　41
4．2．2　Get　42
4．2．3　Scan　43
4．2．4　删除数据　45
4．3　其他常用shell　46
4．3．1　复制状态查看　46
4．3．2　分区拆分　47
4．3．3　分区主压缩　47
4．3．4　负载均衡开关　48
4．3．5　分区手动迁移　48
第5章　模式设计　49
5．1　行键设计　50
5．2　规避热点区间　52
5．3　高表与宽表　54
5．4　微信朋友圈设计　55
5．4．1　需求定义　55
5．4．2　问题建模　55
第6章　客户端API　61
6．1　Java客户端使用　61
6．2　数据定义语言　64
6．2．1　表管理　64
6．2．2　分区管理　66
6．3　数据操纵语言　68
6．3．1　Put　68
6．3．2　Get　70
6．3．3　Scan　72
6．3．4　Delete　74
6．3．5　Increment　76
6．4　过滤器　78
6．4．1　过滤器简介　78
6．4．2　过滤器使用　81
6．5　事务　94
6．5．1　原子性　95
6．5．2　隔离性　95
第7章　架构实现　101
7．1　存储　101
7．1．1　B+树　101
7．1．2　LSM树　102
7．1．3　WAL　104
7．2　数据写入读取　107
7．2．1　定位分区服务器　107
7．2．2　数据修改流程　108
7．2．3　数据查询流程　113
第8章　协处理器　115
8．1　观察者类型协处理器　115
8．2　端点类型协处理器　132
8．3　装载/卸载协处理器　136
8．3．1　静态装载/卸载　136
8．3．2　动态装载/卸载　137
第9章　HBase性能调优　141
9．1　客户端调优　141
9．1．1　设置客户端写入缓存　141
9．1．2　设置合适的扫描缓存　143
9．1．3　跳过WAL写入　143
9．1．4　设置重试次数与间隔　144
9．1．5　选用合适的过滤器　144
9．2　服务端调优　145
9．2．1　建表DDL优化　145
9．2．2　禁止分区自动拆分与压缩　150
9．2．3　开启机柜感知　151
9．2．4　开启Short Circuit Local Reads　153
9．2．5　开启补偿重试读　154
9．2．6　JVM内存调优　155
第10章　集群间数据复制　163
10．1　复制　164
10．1．1　集群拓扑　165
10．1．2　配置集群复制　166
10．1．3　验证复制数据　169
10．1．4　复制详解　171
10．2　快照　175
10．2．1　配置快照　176
10．2．2　管理快照　176
10．3　导出和导入　178
10．3．1　导出　178
10．3．2　导入　180
10．4　复制表　180
第11章　监控　183
11．1　Hadoop监控　183
11．1．1　Web监控页面　184
11．1．2　JMX监控　185
11．2　HBase监控　188
11．2．1　Web监控页面　188
11．2．2　JMX监控　190
11．2．3　API监控　192

1．1 面向行和面向列存储对比
1．1．1 面向行存储的数据库
1．1．2 面向列存储的数据库
1．1．3 两种存储方式的对比
1．2 HDFS分布式存储的特点
1．3 HBase的使用场景
1．4 本章小结

第2章 HBase模型和系统架构
2．1 HBase的相关概念
2．2 HBase的逻辑模型
2．3 HBase的物理模型
2．4 HBase的特点
2．5 HBase的系统架构
2．5．1 Client
2．5．2 ZooKeepel
2．5．3 HMaster
2．5．4 HRegionServer
2．5．5 HRegion
2．6 本章小结

第3章 HBase数据读写流程
3．1 HRegionServer详解
3．1．1 WAL
3．1．2 MemStore
3．1．3 BlockCache
3．1．4 HFile
3．1．5 HRegionServer的恢复
3．1．6 HRegionServer的上线下线
3．2 HRegion
3．2．1 HRegion分配
3．2．2 HRegion Split
3．2．3 HRegion Compact
3．3 HMaster上线
3．4 数据读流程
3．5 数据写流程
3．6 删除数据流程
3．7 本章小结

第4章 HBase环境搭建
4．1 ZooKeeper的安装
4．2 HBase的安装
4．3 本章小结

第5章 HBase Shell
5．1 HBase Shell启动
5．2 表的管理
5．3 表数据的增删改查
5．4 HBase数据迁移的importtsv的使用
5．5 本章小结

第6章 HBase程序开发
6．1 表的相关操作
6．2 创建Configuration对象
6．3 创建表
6．3．1 开发环境配置
6．3．2 创建表
6．4 数据插入
6．5 数据查询
6．6 数据删除
6．7 Scan查询
6．8 Filter过滤
6．9 行数统计
6．1 0NameSpace开发
6．1 1计数器
6．1 2协处理器
6．1 3HBase快照
6．1 4本章小结

第7章 HBase高级特性
7．1 HBase表设计
7．2 列族设计优化
7．3 写性能优化策略
7．4 读性能优化策略
7．4．1 HBase客户端优化
7．4．2 HBase服务器端优化
7．4．3 HDFS相关优化
7．5 HBase集群规划
7．5．1 集群业务规划
7．5．2 集群容量规划
7．5．3 Region规划
7．5．4 内存规划
7．6 本章小结

第8章 MapReduce On HBase
8．1 HBase MapReduce
8．2 编程实例
8．2．1 使用MapReduce操作HBase
8．2．2 从HBase获取数据上传至HDFS
8．2．3 MapReduce生成HFile入库到HBase
8．2．4 同时写入多张表
8．2．5 从多个表读取数据
8．2．6 通过读取HBase表删除Hbase数据
8．2．7 通过读取HBase表数据复制到另外一张表
8．2．8 建立HBase表索引
8．2．9 将MapReduce输出结果到MySQL
8．2．10 利用MapReduce完成MySQL数据读写




1.2.1 准备
1.2.2 操作步骤
1.2.3 运行原理
1.3 Amazon EC2的安装及准备
1.3.1 准备
1.3.2 操作步骤
1.3.3 运行原理
1.4 安装Hadoop
1.4.1 准备
1.4.2 操作步骤
1.4.3 运行原理
1.5 ZooKeeper安装
1.5.1 准备
1.5.2 操作步骤
1.5.3 运行原理
1.5.4 补充说明
1.6 修改内核参数设置
1.6.1 准备
1.6.2 操作步骤
1.6.3 运行原理
1.6.4 参考章节
1.7 HBase安装
1.7.1 准备
1.7.2 操作步骤
1.7.3 运行原理
1.8 Hadoop/ZooKeeper/HBase基本配置
1.8.1 操作步骤
1.8.2 运行原理
1.8.3 参考章节
1.9 安装多个高可用性（HA）的主节点
1.9.1 准备
1.9.2 操作步骤
1.9.3 运行原理
1.9.4 补充说明
第2章 数据迁移
2.1 简介
2.2 通过客户端程序导入MySQL数据
2.2.1 准备
2.2.2 操作步骤
2.2.3 运行原理
2.3 使用批量加载工具导入TSV文件的数据
2.3.1 准备
2.3.2 操作步骤
2.3.3 运行原理
2.3.4 补充说明
2.4 编写自定义MapReduce任务来导入数据
2.4.1 准备
2.4.2 操作步骤
2.4.3 运行原理
2.4.4 补充说明
2.4.5 参考章节
2.5 在数据移入HBase前预创建区域
2.5.1 准备
2.5.2 操作步骤
2.5.3 运行原理
2.5.4 参考章节
第3章 使用管理工具
3.1 简介
3.2 HBase主Web界面
3.2.1 准备
3.2.2 操作步骤
3.2.3 运行原理
3.3 使用HBase Shell管理表
3.3.1 准备
3.3.2 操作步骤
3.3.3 运行原理
3.3.4 补充说明
3.4 使用HBase Shell访问HBase中的数据
3.4.1 准备
3.4.2 操作步骤
3.4.3 运行原理
3.4.4 参考章节
3.5 使用HBase Shell管理集群
3.5.1 准备
3.5.2 操作步骤
3.5.3 运行原理
3.5.4 参考章节
3.6 在HBase Shell中执行Java方法
3.6.1 准备
3.6.2 操作步骤
3.6.3 运行原理
3.6.4 补充说明
3.7 行计数器
3.7.1 准备
3.7.2 操作步骤
3.7.3 运行原理
3.7.4 补充说明
3.8 WAL工具——手动分割和转储WAL
3.8.1 准备
3.8.2 操作步骤
3.8.3 运行原理
3.8.4 参考章节
3.9 HFile工具——以文本方式查看HFile的内容
3.9.1 准备
3.9.2 操作步骤
3.9.3 运行原理
3.9.4 补充说明
3.10 HBase hbck——检查HBase集群的一致性
3.10.1 准备
3.10.2 操作步骤
3.10.3 运行原理
3.10.4 参考章节
3.11 HBase Hive——使用类SQL语言查询HBase中的数据
3.11.1 准备
3.11.2 操作步骤
3.11.3 运行原理
第4章 HBase数据备份及恢复
4.1 简介
4.2 使用distcp进行关机全备份
4.2.1 准备
4.2.2 操作步骤
4.2.3 运行原理
4.3 使用CopyTable在表间复制数据
4.3.1 准备
4.3.2 操作步骤
4.3.3 运行原理
4.4 将HBase表导出为HDFS上的转储文件
4.4.1 准备
4.4.2 操作步骤
4.4.3 运行原理
4.4.4 扩展介绍
4.4.5 参考章节
4.5 通过从HDFS导入转储文件来恢复HBase数据
4.5.1 准备
4.5.2 操作步骤
4.5.3 运行原理
4.5.4 补充说明
4.5.5 参考章节
4.6 备份NameNode元数据
4.6.1 准备
4.6.2 操作步骤
4.6.3 运行原理
4.6.4 补充说明
4.7 备份区域开始键
4.7.1 准备
4.7.2 操作步骤
4.7.3 运行原理
4.7.4 参考章节
4.8 集群复制
4.8.1 准备
4.8.2 操作步骤
4.8.3 运行原理
4.8.4 补充说明
第5章 监控与诊断
5.1 简介
5.2 显示HBase表的磁盘利用率
5.2.1 准备
5.2.2 操作步骤
5.2.3 运行原理
5.2.4 补充说明
5.3 安装Ganglia来监控HBase集群
5.3.1 准备
5.3.2 操作步骤
5.3.3 运行原理
5.3.4 补充说明
5.3.5 参考章节
5.4 OpenTSDB——使用HBase监控HBase集群
5.4.1 准备
5.4.2 操作步骤
5.4.3 运行原理
5.4.4 补充说明
5.5 安装Nagios来监控HBase进程
5.5.1 准备
5.5.2 操作步骤
5.5.3 运行原理
5.5.4 补充说明
5.6 使用Nagios检查Hadoop/HBase日志
5.6.1 准备
5.6.2 操作步骤
5.6.3 运行原理
5.6.4 补充说明
5.6.5 参考章节
5.7 使用一些简单脚本来报告集群状态
5.7.1 准备
5.7.2 操作步骤
5.7.3 运行原理
5.7.4 补充说明
5.7.5 参考章节
5.8 热点区域——诊断写操作
5.8.1 准备
5.8.2 操作步骤
5.8.3 运行原理
5.8.4 补充说明
5.8.5 参考章节
第6章 维护和安全
6.1 简介
6.2 启用HBase RPC的DEBUG级日志功能
6.2.1 准备
6.2.2 操作步骤
6.2.3 运行原理
6.2.4 补充说明
6.3 平稳节点停机
6.3.1 准备
6.3.2 操作步骤
6.3.3 运行原理
6.3.4 补充说明
6.3.5 参考章节
6.4 为集群添加节点
6.4.1 准备
6.4.2 操作步骤
6.4.3 运行原理
6.4.4 补充说明
6.5 滚动重启
6.5.1 准备
6.5.2 操作步骤
6.5.3 运行原理
6.5.4 补充说明
6.6 管理HBase进程的简单脚本
6.6.1 准备
6.6.2 操作步骤
6.6.3 运行原理
6.7 简化部署的简单脚本
6.7.1 准备
6.7.2 操作步骤
6.7.3 运行原理
6.7.4 补充说明
6.8 对Hadoop和HBase进行Kerberos身份认证
6.8.1 准备
6.8.2 操作步骤
6.8.3 运行原理
6.8.4 补充说明
6.8.5 参考章节
6.9 配置HDFS使用Kerberos安全保护机制
6.9.1 准备
6.9.2 操作步骤
6.9.3 运行原理
6.9.4 补充说明
6.10 HBase的安全保护配置
6.10.1 准备
6.10.2 操作步骤
6.10.3 运行原理
6.10.4 补充说明
第7章 故障排查
7.1 简介
7.2 故障排查工具介绍
7.2.1 准备
7.2.2 操作步骤
7.2.3 运行原理
7.2.4 参考章节
7.3 处理XceiverCount错误
7.3.1 准备
7.3.2 操作步骤
7.3.3 运行原理
7.4 处理“打开的文件过多”的错误
7.4.1 准备
7.4.2 操作步骤
7.4.3 运行原理
7.4.4 补充说明
7.4.5 参考章节
7.5 处理“无法创建新本地线程”错误
7.5.1 准备
7.5.2 操作步骤
7.5.3 运行原理
7.5.4 补充说明
7.5.5 参考章节
7.6 处理“HBase忽略了HDFS的客户端配置”问题
7.6.1 准备
7.6.2 操作步骤
7.6.3 运行原理
7.7 处理ZooKeeper客户端的连接错误
7.7.1 准备
7.7.2 操作步骤
7.7.3 运行原理
7.7.4 补充说明
7.8 处理ZooKeeper会话过期错误
7.8.1 准备
7.8.2 操作步骤
7.8.3 运行原理
7.8.4 参考章节
7.9 处理EC2上HBase的启动错误
7.9.1 准备
7.9.2 操作步骤
7.9.3 运行原理
7.9.4 补充说明
7.9.5 参考章节
第8章 基本性能调整
8.1 简介
8.2 设置Hadoop分散磁盘I/O
8.2.1 准备
8.2.2 操作步骤
8.2.3 运行原理
8.2.4 补充说明
8.3 使用网络拓扑结构脚本使Hadoop可感知机架
8.3.1 准备
8.3.2 操作步骤
8.3.3 运行原理
8.4 以noatime和nodiratime方式装载磁盘
8.4.1 准备
8.4.2 操作步骤
8.4.3 运行原理
8.4.4 补充说明
8.5 将vm.swappiness设为0以避免交换
8.5.1 准备
8.5.2 操作步骤
8.5.3 运行原理
8.5.4 参考章节
8.6 Java GC和HBase堆的设置
8.6.1 准备
8.6.2 操作步骤
8.6.3 运行原理
8.6.4 补充说明
8.6.5 参考章节
8.7 使用压缩
8.7.1 准备
8.7.2 操作步骤
8.7.3 运行原理
8.7.4 补充说明
8.8 管理合并
8.8.1 准备
8.8.2 操作步骤
8.8.3 运行原理
8.8.4 补充说明
8.9 管理区域分割
8.9.1 准备
8.9.2 操作步骤
8.9.3 运行原理
8.9.4 补充说明
8.9.5 参考章节
第9章 高级配置和调整
9.1 简介
9.2 使用YCSB对HBase集群进行基准测试
9.2.1 准备
9.2.2 操作步骤
9.2.3 运行原理
9.2.4 补充说明
9.3 增加区域服务器的处理线程数
9.3.1 准备
9.3.2 操作步骤
9.3.3 运行原理
9.3.4 参考章节
9.4 使用自定义算法预创建区域
9.4.1 准备
9.4.2 操作步骤
9.4.3 运行原理
9.4.4 补充说明
9.4.5 参考章节
9.5 避免写密集集群中的更新阻塞
9.5.1 准备
9.5.2 操作步骤
9.5.3 工作原理
9.5.4 参考章节
9.6 调节MemStore内存大小
9.6.1 准备
9.6.2 操作步骤
9.6.3 运行原理
9.6.4 补充说明
9.6.5 参考章节
9.7 低延迟系统的客户端调节
9.7.1 准备
9.7.2 操作步骤
9.7.3 运行原理
9.7.4 补充说明
9.8 配置列族的块缓存
9.8.1 准备
9.8.2 操作步骤
9.8.3 运行原理
9.8.4 补充说明
9.8.5 参考章节
9.9 调高读密集集群的块缓存大小
9.9.1 准备
9.9.2 操作步骤
9.9.3 运行原理
9.9.4 参考章节
9.10 客户端扫描类的设置
9.10.1 准备
9.10.2 操作步骤
9.10.3 工作原理
9.10.4 补充说明
9.10.5 参考章节
9.11 调整块大小来提高寻道性能
9.11.1 准备
9.11.2 操作步骤
9.11.3 运行原理
9.11.4 补充说明
9.11.5 参考章节
9.12 启用Bloom过滤器提高整体吞吐量
9.12.1 准备
9.12.2 操作步骤
9.12.3 运行原理
9.12.4 补充说明

前言
第一部分 基础篇
第1章 认识HBase
1.1 理解大数据背景
1.2 HBase是什么
1.3 HBase与Hadoop的关系
1.4 HBase的核心功能模块
1.5 HBase的使用场景和经典案例
1.6 
第2章 HBase安装与配置
2.1 先决条件
2.2 HBase运行模式
2.3 HBase的Web UI
2.4 HBase Shell工具使用
2.5 停止HBase集群
2.6 
第3章 数据模型
3.1 两类数据模型
3.2 数据模型的重要概念
3.3 数据模型的操作
3.4 数据模型的特殊属性
3.5 CAP原理与最终一致性
3.6 
第4章 HBase表结构设计
4.1 模式创建
4.2 Rowkey设计
4.3 列族定义
4.4 模式设计实例
4.5 
第5章 HBase客户端
5.1 精通原生Java客户端
5.2 使用HBase Shell工具操作HBase
5.3 使用Thrift客户端访问HBase
5.4 通过REST客户端访问HBase
5.5 使用MapReduce批量操作HBase
5.6 通过Web UI工具查看HBase状态
5.7 其他客户端
5.8 
第二部分 实战篇
第6章 整合SQL引擎层
6.1 NoSQL背景知识
6.2 Hive整合HBase的实现
6.3 查询引擎Phoenix
6.4 对象映射框架Kundera
6.5 分布式SQL引擎Lealone
6.6 
第7章 构建音乐站用户属性库
7.1 案例背景
7.2 概要设计
7.3 表结构设计
7.4 数据加载
7.5 数据检索
7.6 后台查询
7.7 
第8章 构建广告实时计算系统
8.1 理解广告数据和流处理框架
8.2 概要设计
8.3 详细设计
8.4 核心功能实现
8.5 
第三部分 高级篇
第9章 核心概念
9.1 核心结构
9.2 底层持久化
9.3 预写日志
9.4 写入流程
9.5 查询流程
9.6 数据备份
9.7 数据压缩
9.8 
第10章 HBase高级特性
10.1 过滤器
10.2 计数器
10.3 协处理器
10.4 Schema设计要点
10.5 二级索引
10.6 布隆过滤器
10.7 负载均衡
10.8 批量加载
10.9 
第11章 集群运维管理
11.1 HBase常用工具
11.2 Region和RegionServer管理
11.3 性能指标Metrics
11.4 监控系统Ganglia
11.5 HBase管理扩展JMX
11.6 报警工具Nagios
11.7 故障处理
11.8 集群备份
11.9 
第12章 性能调优
12.1 硬件和操作系统调优
12.2 网络通信调优
12.3 JVM优化
12.4 HBase查询优化
12.5 HBase写入优化
12.6 HBase基本核心服务优化
12.7 HBase配置参数优化
12.8 分布式协调系统ZooKeeper优化
12.9 表设计优化
12.10 其他优化
12.11 性能测试
12.12 
附录A HBase配置参数介绍
附录B Phoenix SQL语法详解
附录C YCSB编译安装


https://help.aliyun.com/document_detail/95995.html?spm=a2c4g.11186623.6.585.55e64f68Qdktia

规格容量评估
更新时间：2019-03-20 10:50:10

编辑 ·
 · 我的收藏
本页目录
适用磁盘类型
磁盘容量评估
集群规格评估
shard评估
在使用阿里云ES之前，需要优先评估阿里云ES资源容量。我们根据实际测试结果和用户使用经验，提供了相对通用的评估方法，可以作为参考。

适用磁盘类型
本文档主要适用于购买存储类型为SSD云盘的阿里云ES实例进行参考。

磁盘容量评估
阿里云ES集群磁盘空间大小影响因素：

副本数量，至少1个副本。
索引开销，通常比源数据大10%（_all 等未计算）。
操作系统预留，默认操作系统会保留5%的文件系统供用户处理关键流程，系统恢复，磁盘碎片等。
ES内部开销，段合并，日志等内部操作，预留20%。
安全阈值，通常至少预留15%的安全阈值。
最小磁盘总大小 = 源数据 * 3.4

磁盘总大小 = 源数据 * (1 + 副本数量) * (1 + 索引开销) / (1 - Linux预留空间) / (1 - ES开销) / (1 - 安全阈值)
= 源数据 * (1 + 副本数量) * 1.7
= 源数据 * 3.4
对于 _all 这项参数，如果在业务使用上没有必要，我们通常的建议是禁止或者有选择性的添加。

对于需要开启这个参数的索引，其开销也会随之增大。根据我们的测试结果和使用经验，建议在上述评估的基础上额外增加一半的空间：

磁盘总大小 = 源数据 * (1 + 副本数) * 1.7 * (1 + 0.5)
= 源数据 * 5.1
集群规格评估
阿里云ES的单机规格在一定程度上是限制了集群能力的，这里我们根据测试结果和使用经验给出如下建议。

集群最大节点数 = 单节点CPU * 5
使用场景不同，单节点最大承载数据量也会不同，具体如下：

数据加速、查询聚合等场景

单节点最大数据量 = 单节点Mem(G) * 10
日志写入、离线分析等场景

单节点最大数据量 = 单节点Mem(G) * 50
通常情况

单节点最大数据量 = 单节点Mem(G) * 30
参考列表
规格	最大节点数	单节点最大磁盘（查询）	单节点最大磁盘（日志）	单节点最大磁盘（通常）
2C 4G	10	40 GB	200 GB	100 GB
2C 8G	10	80 GB	400 GB	200 GB
4C 16G	20	160 GB	800 GB	512 GB
8C 32G	40	320 GB	1.5 TB	1 TB
16C 64G	50	640 GB	2 TB	2 TB
shard评估
shard大小和个数是影响ES集群稳定性和性能的重要因素之一。ES集群中任何一个索引都需要有一个合理的shard规划，很多情况下都会有一个更好的策略来替换ES默认的5个shard。

建议在小规格节点下单shard大小不要超过30GB。更高规格的节点单shard大小不要超过50GB。
对于日志分析场景或者超大索引，建议单shard大小不要超过100GB。
shard的个数（包括副本）要尽可能匹配节点数，等于节点数，或者是节点数的整数倍。
通常我们建议单节点上同一索引的shard个数不要超5个。
说明
由于不同用户在数据结构，查询复杂度，数据量大小，性能要求，数据的变化等等诸多方面是不同的，所以本文的评估绝不是每一个用户的最佳建议。

在条件允许的情况下，还是希望您可以通过实际的数据和使用场景测试出适合自己的最佳实践。得益于阿里云elasticsearch提供的弹性扩容功能，您可以参考本文的内容做初始规划。在实际使用时根据情况随时增加磁盘大小、扩容节点个数、升级节点规格。

压缩算法
目前阿里云平台支持压缩算法有： LZO ZSTD GZ LZ4 SNAPPY NONE，其中NONE就代表不开启压缩

不同压缩算法在不同场景的压缩比，及解压速度对比如下，都是来自线上真实场景：

业务类型	无压缩表大小	LZO（压缩率/解压速度MB/s）	ZSTD（压缩率/解压速度MB/s）	LZ4（压缩率/解压速度MB/s）
监控类	419.75T	5.82/372	13.09/256	5.19/463.8
日志类	77.26T	4.11/333	6.0/287	4.16/496.1
风控类	147.83T	4.29/297.7	5.93/270	4.19/441.38
消费记录	108.04T	5.93/316.8	10.51/288.3	5.55/520.3
我们建议在：

对rt要求极高，建议使用 lz4 压缩算法
对rt要求不高，特别是 监控、物联网等场景，建议使用 zstd 压缩算法
编码
hbase很早就支持了DataBlockEncoding，也就是是通过减少hbase keyvalue中重复的部分来压缩数据。我们推荐DATA_BLOCK_ENCODING使用diff

操作
修改压缩编码的步骤：
1、修改表的属性，此为压缩编码
alter 'test', {NAME => 'f', COMPRESSION => 'lz4', DATA_BLOCK_ENCODING =>'DIFF'}
2、压缩编码并不会立即生效，需要major_compact，此会耗时较长，注意在业务低峰期进行
major_compact 'test'





看我72变，阿里HBase数据压缩编码探索
国际顶级盛会HBaseCon Asia 2018将于8月在北京举行，目前正免费开放申请中，更多详情参考https://yq.aliyun.com/promotion/631
如果你对大数据存储、分布式数据库、HBase等感兴趣，欢迎加入我们，一起做最好的大数据在线存储，职位参考及联系方式
前言
你可曾遇到这种需求，只有几百qps的冷数据缓存，却因为存储水位要浪费几十台服务器？你可曾遇到这种需求，几百G的表，必须纯cache命中，性能才能满足业务需求？你可曾遇到，几十M的小表，由于qps过高，必须不停的split，balance，利用多台服务器来抗热点？
面对繁杂的场景，Ali-HBase团队一直致力于为业务提供更多的选择和更低的成本。本文主要介绍了hbase目前两种提高压缩率的主要方法：压缩和DataBlockEncoding。

无损压缩:更小，更快，更省资源
通用压缩是数据库减少存储的重要手段，在hbase中也存在广泛应用。通常数据库都存在数据块的概念，针对每个块做压缩和解压。块越大，压缩率越高，scan throughput增加；块越小，latency越小。作为一种Tradeoff，线上hbase通常采用64K块大小，在cache中不做压缩，仅在落盘和读盘时做压缩和解压操作。

开源hbase通常使用的LZO压缩或者Snappy压缩。这两种压缩的共同特点是都追求较高的压缩解压速度，并实现合理的压缩率。然而，随着业务的快速增涨，越来越多的业务因为因为存储水位问题而扩容。hbase针对这一情况，采用了基于跨集群分区恢复技术的副本数优化、机型升级等优化手段，但依然无法满足存储量的快速膨胀，我们一直致力于寻找压缩更高的压缩方式。

新压缩（zstd、lz4）上线
Zstandard（缩写为Zstd）是一种新的无损压缩算法，旨在提供快速压缩，并实现高压缩比。它既不像LZMA和ZPAQ那样追求尽可能高的压缩比，也不像LZ4那样追求极致的压缩速度。这种算法的压缩速度超过200MB/s, 解压速度超过400MB/s，基本可以满足目前hbase对吞吐量的需求。经验证，Zstd的数据压缩率相对于Lzo基本可以提高25%-30%，对于存储型业务，这就意味着三分之一到四分之一的的成本减少。

而在另一种情况下，部分表存储量较小，但qps大，对rt要求极高。针对这种场景，我们引入了lz4压缩，其解压速度在部分场景下可以达到lzo的两倍以上。一旦读操作落盘需要解压缩，lz4解压的rt和cpu开销都明显小于lzo压缩。

我们通过一张图片直观的展示各种压缩算法的性能:
ZSTD.png | center | 704x419
解压缩.png | center | 704x422

以线上几种典型数据场景为例，看看几种压缩的实际压缩率和单核解压速度（以下数据均来自于线上）

业务类型	无压缩表大小	LZO（压缩率/解压速度MB/s）	ZSTD（压缩率/解压速度MB/s）	LZ4（压缩率/解压速度MB/s）
监控类	419.75T	5.82/372	13.09/256	5.19/463.8
日志类	77.26T	4.11/333	6.0/287	4.16/ 496.1
风控类	147.83T	4.29/297.7	5.93/270	4.19/441.38
消费记录	108.04T	5.93/316.8	10.51/288.3	5.55/520.3
目前，2017年双11，ZSTD已经在线上全面铺开，已累计优化存储超过2.5PB，预计全面推开后，节约存储空间超过15PB。LZ4也已经在部分读要求较高业务上线。
下图为某监控类应用zstd压缩算法后，集群整体存储量的下降情况。数据量由100+T减少到75T。

监控.png | center | 704x307

编码技术：针对结构化数据的即查即解压
hbase作为一种schema free的数据库，相当于传统的关系型数据库更加灵活，用户无需设计好表的结构，也可以在同一张表内写入不同schema的数据。然而，由于缺少数据结构的支持，hbase需要很多额外的数据结构来标注长度信息，且无法针对不同的数据类型采用不同的压缩方式。针对这一问题，hbase提出了编码功能，用来降低存储开销。由于编码对cpu开销较小，而效果较好，通常cache中也会开启编码功能。

旧DIFF Encoding介绍
hbase很早就支持了DataBlockEncoding，也就是是通过减少hbase keyvalue中重复的部分来压缩数据。 以线上最常见的DIFF算法为例，某kv压缩之后的结果：

一个字节的flag（这个flag的作用后面解释）
如果和上个KV的键长不一样，则写入1~5个字节的长度
如果和上个KV的值长不一样，则写入1~5个字节的长度
记录和上个KV键相同的前缀长度，1~5个字节
非前缀部分的row key
如果是第一条KV,写入列族名
非前缀部分的的列名
写入1~8字节的timestamp或者与上个KV的timestamp的差(是原值还是写与上个KV的差，取决于哪个字节更小)
如果和上个KV的type不一样，则写入1字节的type（Put，Delete）
Value内容
那么在解压缩时，怎么判断和上个KV的键长是否一样，值长是否一样，写入的时间戳究竟是是原值还是差值呢？这些都是通过最早写入的1个字节的flag来实现的，
这个字节中的8位bit，含义是：

第0位，如果为1，键长与上个kv相等
第1位，如果为1，值长与上个kv相等
第2位，如果为1，type与上个kv一样
第3位，如果为1，则写入的timestamp是差值，否则为原值
第456位，这3位组合起来的值（能表示0~7），表示写入的时间戳的长度
第7位，如果为1，表示写入的timestamp差值为负数，取了绝对值。
小凡1 (2).png | center | 704x327
DIFF 编码之后，对某个文件的seek包含以下两步：

通过index key找到对应的datablock
从第一个完整KV开始，顺序查找，不断decode下一个kv，直到找到目标kv为止。
通过线上数据验证，DIFF encoding可以减少2-5倍的数据量。

新Indexable Delta Encoding上线
从性能角度考虑，hbase通常需要将Meta信息装载进block cache。如果将block大小较小，Meta信息较多，会出现Meta无法完全装入Cache的情况, 性能下降。如果block大小较大，DIFF Encoding顺序查询的性能会成为随机读的性能瓶颈。针对这一情况，我们开发了Indexable Delta Encoding，在block内部也可以通过索引进行快速查询，seek性能有了较大提高。Indexable Delta Encoding原理如图所示：
小凡2 (1).png | center | 704x320
在通过BlockIndex找到对应的数据块后，我们从数据块末尾找到每个完整KV的offset，并利用二分查找快速定位到符合查询条件的完整kv，再顺序decode每一个Diff kv，直到找到目标kv位置。

通过Indexable Delta Encoding， HFile的随机seek性能相对于使用之前翻了一倍，以64K block为例，随机seek性能基本与不做encoding相近.在全cache命中的随机Get场景下，相对于Diff encoding rt下降50%，但存储开销仅仅提高3-5%。Indexable Delta Encoding目前已在线上多个场景应用，经受了双十一的考验。以风控集群为例，双集群双十一高峰期访问量接近，但已经上线Indexable Delta Encoding的集群在访问量稍多的情况下，平均读rt减少10%-15%。

gtj.png | center | 704x299


未开启集群
em14.png | center | 704x299


开启集群
未来
性能优化和用户体验，一直是阿里hbase的团队不懈的追求。我们一直在不断丰富自己的武器库，努力做最好的海量数据在线存储产品。欢迎对hbase实现，使用有任何问题的同学联系我们.

本文为云栖社区原创内容，未经允许不得转载，如需转载请发送邮件至yqeditor@list.alibaba-inc.com；如果您发现本社区中有涉嫌抄袭的内容，欢迎发送邮件至：yqgroup@service.aliyun.com 进行举报，并提供相关证据，一经查实，本社区将立刻删除涉嫌侵权内容。




read读取优化
更新时间：2018-03-29 19:39:23


其实HBase还是比较灵活的，关键看你是否使用得当，以下主要列举一些读的优化。HBase在生产中往往会遇到Full GC、进程OOM、RIT问题、读取延迟较大等一些问题，使用更好的硬件往往可以解决一部分问题，但是还是需要使用的方式。

我们把优化分为：

客户端优化、服务端优化、平台优化(ApsaraDB for HBase平台完成)

客户端优化
get请求是否可以使用批量请求
这样可以成倍减小客户端与服务端的rpc次数，显著提高吞吐量

 Result[] re= table.get(List<Get> gets)
大scan缓存是否设置合理
scan一次性需求从服务端返回大量的数据，客户端发起一次请求，客户端会分多批次返回客户端，这样的设计是避免一次性传输较多的数据给服务端及客户端有较大的压力。目前 数据会加载到本地的缓存中，默认100条数据大小。 一些大scan需要获取大量的数据，传输数百次甚至数万的rpc请求。 我们建议可以 适当放开 缓存的大小。

scan.setCaching(int caching) //大scan可以设置为1000
请求指定列族或者列名
HBase是列族数据库，同一个列族的数据存储在一块，不同列族是分开的，为了减小IO，建议指定列族或者列名

离线计算访问Hbase建议禁止缓存
当离线访问HBase时，往往就是一次性的读取，此时读取的数据没有必要存放在blockcache中，建议在读取时禁止缓存

scan.setBlockCache(false)
可干预服务端优化
请求是否均衡
读取的压力是否都在一台或者几台之中，在业务高峰期间可以查看下，可以到 HBase管控平台查看HBase的ui。如果有明显的热点，一劳永逸是重新设计rowkey，短期是 把热点region尝试拆分

BlockCache是否合理
BlockCache作为读缓存，对于读的性能比较重要，如果读比较多，建议内存使用1:4的机器，比如：8cpu32g或者16pu64g的机器。当前可以调高 BlockCache 的数值，降低 Memstore 的数值。

在ApsaraDB for HBase控制台可以完成：hfile.block.cache.size 改为0.5 ； hbase.regionserver.global.memstore.size 改为0.3；再重启

读取优化

HFile文件数目
因为读取需要频繁打开HFile，如果文件越多，IO次数就越多，读取的延迟就越高此 需要主要定时做 major compaction，如果晚上的业务压力不大，可以在晚上做major compaction

Compaction是否消耗较多的系统资源
compaction主要是将HFile的小文件合并成大文件，提高后续业务的读性能，但是也会带来较大的资源消耗。Minor Compaction一般情况下不会带来大量的系统资源消耗，除非因为配置不合理。 切勿在高峰期间做 major compaction。 建议在业务低峰期做major compaction。

Bloomfilter设置是否合理
Bloomfilter主要用来在查询时，过滤HFile的，避免不需要的IO操作。Bloomfilter能提高读取的性能，一般情况下创建表，都会默认设置为：BLOOMFILTER => ‘ROW’

平台端优化
数据本地率是否太低?（平台已经优化）
Hbase 的HFile，在本地是否有文件，如果有文件可以走Short-Circuit Local Read目前平台在重启、磁盘扩容时，都会自动拉回移动出去的region，不降低数据本地率；另外 定期做major compaction也有益于提高本地化率

Short-Circuit Local Read （已经默认开启）
当前HDFS读取数据需要经过DataNode，开启Short-Circuit Local Read后，客户端可以直接读取本地数据

Hedged Read （已经默认开启）
优先会通过Short-Circuit Local Read功能尝试本地读。但是在某些特殊情况下，有可能会出现因为磁盘问题或者网络问题引起的短时间本地读取失败，为了应对这类问题，开发出了Hedged Read。该机制基本工作原理为：客户端发起一个本地读，一旦一段时间之后还没有返回，客户端将会向其他DataNode发送相同数据的请求。哪一个请求先返回，另一个就会被丢弃

关闭swap区（已经默认关闭）
swap是当物理内存不足时，拿出部分的硬盘空间当做swap使用，解决内存不足的情况。但是会有较大的延迟的问题，所以我们HBase平台默认关闭。 但是关闭swap导致anon-rss很高，page reclaim没办法reclaim足够的page，可能导致内核挂住，平台已经采取相关隔离措施避免此情况


write写入优化
更新时间：2018-03-29 19:39:23


HBase基于LSM模式，写是写HLOG及Memory的，也就是基本没有随机的IO，所以在写链路上性能高校还比较平稳。很多时候，写都是用可靠性来换取性能。



=======================

客户端优化
批量写
也是为了减少rpc的次数

HTable.put(List<Put>)
Auto Flush
autoflush=false可以提升几倍的写性能，但是还是要注意，直到数据超过2M(hbase.client.write.buffer决定)或用户执行了hbase.flushcommits()时才向regionserver提交请求。需要注意并不是写到了远端。

HTable.setWriteBufferSize(writeBufferSize) 可以设置buffer的大小

服务端优化
WAL Flag
不写WAL可以成倍提升性能，因为不需要写HLog，减少3次IO，写MemStore是内存操作

是以数据可靠性为代价的，在数据导入时，可以关闭WAL

增大memstore的内存
当前可以调高Memstore 的数值，降低 BlockCache 的数，跟读优化的思路正好相反

大量的HFile产生
如果写很快，很容易带来大量的HFile，因为此时HFile合并的速度还没有写入的速度快

需要在业务低峰期做majorcompaction，充分利用系统资源；如果HFile降低不下来，则需要添加节点

================
读写分离
更新时间：2018-06-05 17:26:13


HBase有三个典型的API : read(get、scan)、write ，我们有时候希望这三个访问尽可能的互相不影响，可以参考如下配置：（线上默认没有配置读写分离）

场景
写请求与读请求都比较高，业务往往接受：写请求慢点可以，读请求越快越好，最好有单独的资源保障
scan与get都比较多，业务希望scan不影响get（因为scan比较消耗资源）
相关配置：
hbase.ipc.server.callqueue.read.ratio
hbase.ipc.server.callqueue.scan.ratio
具体含义：
hbase.ipc.server.callqueue.read.ratio 设置为0.5，代表有50%的线程数处理读请求
如果再设置hbase.ipc.server.callqueue.scan.ratio 设置为0.5，则代表在50%的读线程之中，再有50%的线程处理scan，也就是全部线程的25%
操作步骤
打开HBase控制台，找到实例，点击进去，找到 - 参数设置
修改配置，按照业务读写情况
不重启不会生效，请在业务低峰期重启集群，重启不会中断业务，可能会有一些抖动
修改配置

请根据实际的业务配置以上数值，默认情况下是没有配置的，也就是读写都共享。

 上一篇：write写入优化

===============
预分区
更新时间：2018-05-05 17:19:51


初次接触HBase的客户，在创建HBase表的时候，不指分区的数目，另外就是rowkey设计不合理，导致热点。

最为常见的建表语句为：

create ‘t3’,’f1’, { NUMREGIONS => 50, SPLITALGO => ‘HexStringSplit’ , COMPRESSION => ‘snappy’}

其中 NUMREGIONS 为 region的个数，一般取10-500左右，集群规模大，可以取大一些，
SPLITALGO 为 rowkey分割的算法：Hbase自带了两种pre-split的算法，分别是 HexStringSplit 和 UniformSplit，HexStringSplit 如果我们的row key是十六进制的字符串作为前缀的，就比较适合用HexStringSplit，关于rowkey的设计可以参考：RowKey设计
COMPRESSION压缩算法，参考：数据压缩与编码
 上一篇：读写分离
下一篇：Rowkey设计
相关文档
Rowkey设计
读写分离
write写入优化
数据压缩与编码
read读取优化
schema设计原则
相关产品
云数据库 HBase 版
云数据库 HBase 版（ApsaraDB for HBase）是基于 Hadoop 的一个分布式数据库，支持海量的PB级的大数据存储，适用于高吞吐的随机读写的场景。目前在阿里内部有数百个集群，10000台左右规模的集群，服务数百个业务线，在订单存储、消息存储、物联网、轨迹、wifi、安全风控、搜索等领域有较多的在线应用。 阿里云特别提供HBase产品化方案服务广大的中小型客户。
E-MapReduce
阿里云 Elastic MapReduce（E-MapReduce） 是一种大数据处理的系统解决方案。构建于阿里云云服务器 ECS 上，基于开源的 Apache Hadoop 和 Apache Spark，让用户可以方便地使用Hadoop和Spark生态系统中的其他周边系统（如 Apache Hive、Apache Pig、HBase 等）来分析和处理自己的数据。用户还可以通过E-MapReduce将数据非常方便的导入和导出到阿里云其他的云数据存储系统和数据库系统中，如阿里云 OSS、阿里云 RDS 等。
表格存储
表格存储（Table
Rowkey设计
更新时间：2018-04-10 07:42:24


HBase的rowkey设计可以说是使用HBase最为重要的事情，直接影响到HBase的性能，常见的RowKey的设计问题及对应访问为：

Hotspotting

的行由行键按字典顺序排序，这样的设计优化了扫描，允许存储相关的行或者那些将被一起读的邻近的行。然而，设计不好的行键是导致 hotspotting 的常见原因。当大量的客户端流量（ traffic ）被定向在集群上的一个或几个节点时，就会发生 hotspotting。这些流量可能代表着读、写或其他操作。流量超过了承载该region的单个机器所能负荷的量，这就会导致性能下降并有可能造成region的不可用。在同一 RegionServer 上的其他region也可能会受到其不良影响，因为主机无法提供服务所请求的负载。设计使集群能被充分均匀地使用的数据访问模式是至关重要的。


为了防止在写操作时出现 hotspotting ，设计行键时应该使得数据尽量同时往多个region上写，而避免只向一个region写，除非那些行真的有必要写在一个region里。


下面介绍了集中常用的避免 hotspotting 的技巧，它们各有优劣：

Salting
Salting 从某种程度上看与加密无关，它指的是将随机数放在行键的起始处。进一步说，salting给每一行键随机指定了一个前缀来让它与其他行键有着不同的排序。所有可能前缀的数量对应于要分散数据的region的数量。如果有几个“hot”的行键模式，而这些模式在其他更均匀分布的行里反复出现，salting就能到帮助。下面的例子说明了salting能在多个RegionServer间分散负载，同时也说明了它在读操作时候的负面影响。

假设行键的列表如下，表按照每个字母对应一个region来分割。前缀‘a’是一个region，‘b’就是另一个region。在这张表中，所有以‘f’开头的行都属于同一个region。这个例子关注的行和键如下：

foo0001
foo0002
foo0003
foo0004
现在，假设想将它们分散到不同的region上，就需要用到四种不同的 salts ：a，b，c，d。在这种情况下，每种字母前缀都对应着不同的一个region。用上这些salts后，便有了下面这样的行键。由于现在想把它们分到四个独立的区域，理论上吞吐量会是之前写到同一region的情况的吞吐量的四倍。

a-foo0003
b-foo0001
c-foo0004
d-foo0002
如果想新增一行，新增的一行会被随机指定四个可能的salt值中的一个，并放在某条已存在的行的旁边。

a-foo0003
b-foo0001
c-foo0003
c-foo0004
d-foo0002
由于前缀的指派是随机的，因而如果想要按照字典顺序找到这些行，则需要做更多的工作。从这个角度上看，salting增加了写操作的吞吐量，却也增大了读操作的开销。

Hashing
可用一个单向的 hash 散列来取代随机指派前缀。这样能使一个给定的行在“salted”时有相同的前缀，从某种程度上说，这在分散了RegionServer间的负载的同时，也允许在读操作时能够预测。确定性hash（ deterministic hash ）能让客户端重建完整的行键，以及像正常的一样用Get操作重新获得想要的行。

考虑和上述salting一样的情景，现在可以用单向hash来得到行键foo0003，并可预测得‘a’这个前缀。然后为了重新获得这一行，需要先知道它的键。可以进一步优化这一方法，如使得将特定的键对总是在相同的region。


Reversing the Key(反转键)
第三种预防hotspotting的方法是反转一段固定长度或者可数的键，来让最常改变的部分（最低显著位， the least significant digit ）在第一位，这样有效地打乱了行键，但是却牺牲了行排序的属性



单调递增行键/时序数据
在一个集群中，一个导入数据的进程锁住不动，所有的client都在等待一个region(因而也就是一个单个节点)，过了一会后，变成了下一个region… 如果使用了单调递增或者时序的key便会造成这样的问题。使用了顺序的key会将本没有顺序的数据变得有顺序，把负载压在一台机器上。所以要尽量避免时间戳或者序列(e.g. 1, 2, 3)这样的行键。

如果需要导入时间顺序的文件(如log)到HBase中，可以学习OpenTSDB的做法。它有一个页面来描述它的HBase模式。OpenTSDB的Key的格式是[metric_type][event_timestamp]，乍一看，这似乎违背了不能将timestamp做key的建议，但是它并没有将timestamp作为key的一个关键位置，有成百上千的metric_type就足够将压力分散到各个region了。因此，尽管有着连续的数据输入流，Put操作依旧能被分散在表中的各个region中

简化行和列
在HBase中，值是作为一个单元(Cell)保存在系统的中的，要定位一个单元，需要行，列名和时间戳。通常情况下，如果行和列的名字要是太大(甚至比value的大小还要大)的话，可能会遇到一些有趣的情况。在HBase的存储文件（ storefiles ）中，有一个索引用来方便值的随机访问，但是访问一个单元的坐标要是太大的话，会占用很大的内存，这个索引会被用尽。要想解决这个问题，可以设置一个更大的块大小，也可以使用更小的行和列名 。压缩也能得到更大指数。

大部分时候，细微的低效不会影响很大。但不幸的是，在这里却不能忽略。无论是列族、属性和行键都会在数据中重复上亿次。

列族
尽量使列族名小，最好一个字符。（如：f 表示）

属性
详细属性名 (如:”myVeryImportantAttribute”) 易读，最好还是用短属性名 (e.g., “via”) 保存到HBase.

行键长度
让行键短到可读即可，这样对获取数据有帮助(e.g., Get vs. Scan)。短键对访问数据无用，并不比长键对get/scan更好。设计行键需要权衡

字节模式
long类型有8字节。8字节内可以保存无符号数字到18,446,744,073,709,551,615。 如果用字符串保存——假设一个字节一个字符——需要将近3倍的字节数。


下面是示例代码，可以自己运行一下:

// long
//
long l = 1234567890L;
byte[] lb = Bytes.toBytes(l);
System.out.println("long bytes length: " + lb.length);   // returns 8
String s = String.valueOf(l);
byte[] sb = Bytes.toBytes(s);
System.out.println("long as string length: " + sb.length);    // returns 10
// hash
//
MessageDigest md = MessageDigest.getInstance("MD5");
byte[] digest = md.digest(Bytes.toBytes(s));
System.out.println("md5 digest bytes length: " + digest.length);    // returns 16
String sDigest = new String(digest);
byte[] sbDigest = Bytes.toBytes(sDigest);
System.out.println("md5 digest as string length: " + sbDigest.length);    // returns 26
不幸的是，用二进制表示会使数据在代码之外难以阅读。下例便是当需要增加一个值时会看到的shell：

hbase(main):001:0> incr 't', 'r', 'f:q', 1
COUNTER VALUE = 1
hbase(main):002:0> get 't', 'r'
COLUMN                                        CELL
 f:q                                          timestamp=1369163040570, value=\x00\x00\x00\x00\x00\x00\x00\x01
1 row(s) in 0.0310 seconds
这个shell尽力在打印一个字符串，但在这种情况下，它决定只将进制打印出来。当在region名内行键会发生相同的情况。如果知道储存的是什么，那自是没问题，但当任意数据都可能被放到相同单元的时候，这将会变得难以阅读。这是最需要权衡之处。

倒序时间戳
一个数据库处理的通常问题是找到最近版本的值。采用倒序时间戳作为键的一部分可以对此特定情况有很大帮助。该技术包含追加( Long.MAX_VALUE - timestamp ) 到key的后面，如 [key][reverse_timestamp] 。

表内[key]的最近的值可以用[key]进行Scan，找到并获取第一个记录。由于HBase行键是排序的，该键排在任何比它老的行键的前面，所以是第一个。

该技术可以用于代替版本数，其目的是保存所有版本到“永远”(或一段很长时间) 。同时，采用同样的Scan技术，可以很快获取其他版本。

行键和列族
行键在列族范围内。所以同样的行键可以在同一个表的每个列族中存在而不会冲突。

行键不可改
行键不能改变。唯一可以“改变”的方式是删除然后再插入。这是一个常问问题，所以要注意开始就要让行键正确(且/或在插入很多数据之前)。

行键和region split的关系
如果已经 pre-split （预裂）了表，接下来关键要了解行键是如何在region边界分布的。为了说明为什么这很重要，可考虑用可显示的16位字符作为键的关键位置（e.g., “0000000000000000” to “ffffffffffffffff”）这个例子。通过 Bytes.split来分割键的范围（这是当用 Admin.createTable(byte[] startKey, byte[] endKey, numRegions) 创建region时的一种拆分手段），这样会分得10个region。

48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48                                // 0
54 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10                 // 6
61 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -68                 // =
68 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -126  // D
75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 72                                // K
82 18 18 18 18 18 18 18 18 18 18 18 18 18 18 14                                // R
88 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -44                 // X
95 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -102                // _
102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102                // f


但问题在于，数据将会堆放在前两个region以及最后一个region，这样就会导致某几个region由于数据分布不均匀而特别忙。为了理解其中缘由，需要考虑ASCII Table的结构。根据ASCII表，“0”是第48号，“f”是102号；但58到96号是个巨大的间隙，考虑到在这里仅[0-9]和[a-f]这些值是有意义的，因而这个区间里的值不会出现在键空间（ keyspace ），进而中间区域的region将永远不会用到。为了pre-split这个例子中的键空间，需要自定义拆分。

教程1:预裂表（ pre-splitting tables ） 是个很好的实践，但pre-split时要注意使得所有的region都能在键空间中找到对应。尽管例子中解决的问题是关于16位键的键空间，但其他任何空间也是同样的道理。

教程2:16位键（通常用到可显示的数据中）尽管通常不可取，但只要所有的region都能在键空间找到对应，它依旧能和预裂表配合使用。

一下case说明了如何16位键预分区

public static boolean createTable(Admin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;
  }
}
public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i < numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}



阿里云hbase客户端
更新时间：2019-03-07 17:01:16


说明
阿里云HBase的服务端是定制的代码，客户端是完全兼容社区的。但是我们会根据阿里云产品特点提供专属的HBase 客户端并开源，且maven jar同步到maven中央仓库，这就意味着，任何地方都能下载并使用。我们不会改变任何语法，主要是提供一些特性、运维的便利等。

tar包：

wget http://public-hbase.oss-cn-hangzhou.aliyuncs.com/installpackage/alihbase-1.1.3-bin.tar.gz
maven可以引用

<dependency>
  <groupId>com.aliyun.hbase</groupId>
  <artifactId>alihbase-client</artifactId>
  <version>{version}</version>
</dependency>
{version}可以参考如下：（高版本包括低版本的功能）

1.1.1
使用1.8编译
支持混合访问的需求，比如：公网访问、经典网络访问VPC内的环境。需要配置阿里云HBase提供的域名
1.1.2

使用1.7编译
hbase-clinet可以同时支持guava12.0.1、guava22.0的包，目前hbase-client默认依赖是guava12.0.1，可以采取以下方式去掉依赖，后再指定guava依赖

<dependency>
  <groupId>com.aliyun.hbase</groupId>
  <artifactId>alihbase-client</artifactId>
  <version>1.1.2</version>
  <exclusions>
      <exclusion>
          <artifactId>com.google.guava</artifactId>
          <groupId>guava</groupId>
      </exclusion>
  </exclusions>
</dependency>
<dependency>
  <artifactId>com.google.guava</artifactId>
  <groupId>guava</groupId>
  <version>22.0</version>
</dependency>
1.1.3
使用1.7编译
优化了公网访问功能，支持单进程公网访问多集群，如copytable对两个公网hbase集群
直接互相导数据。修改了phoenix接口依赖兼容问题。
1.1.3.2
使用1.7编译
基于最新非安全的1.1.3版本之上，增加支持了HBase for Solr客户端功能模块
注意此版本只支持 wget tar包下载，用于HBase for solr的命令行操作，其他HBase api操作依然使用依赖1.1.3版本的jar即可
1.1.4
使用1.7编译
增加了数据导出到OSS的支持
注意此版本只支持 wget tar包下载
1.1.5
使用1.7编译
支持HAS安全功能，因为HAS安全只支持jdk8，使用安全时应该使用jdk8运行环境
注意此版本只支持 wget tar包下载
1.1.6
使用1.7编译
添加了HBase for Solr索引管理模块
注意此版本目前也只能wget tar包下载
2.0.0
使用1.8编译
同样支持云HBase2.0.0的公网访问
2.0.1
使用1.8编译
支持HAS安全功能
注意此版本只支持 wget tar包下载
2.0.2
使用1.8编译
不再支持HAS安全功能
针对冷热分离场景做了特殊处理，使得自动同步的过程对应用透明



多语言支持(thrift)
更新时间：2018-11-28 11:04:07


1、 产品支持
Thrift 提供多语言访问HBase的能力，支持的语言包从Thrift官网看括: C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, OCaml , Delphi 以及别的语言。主要流程是用户thrift Client 通过Thrift协议访问HBase的thriftserver,thriftserver请求转发给HBase的存储服务.大概架构图如下：

​  架构图

开通HBase thriftserver服务：
​ 管控页面点击 开通thriftserver服务化（高可用版本thriftserver） ，会得到一个 host:port的访问入口;

thriftserver

2、使用说明
2.1 用户Thrift client访问：
一般客户常见的访问方式是python及php ，这里给出php的访问方式；

2.1.1 以php走thrift访问HBase：
我们云HBase的thrift环境是0.9.0，所以建议客户客户端的版本也为 0.9.0，可以从这里下载thrift的0.9.0 版本，下载的源码包我们后面会用到，这里需要先安装thrift编译环境，对于源码安装可以参考thrift官网；

通过如下命令可以看出安装thrift的版本信息；

thrift --version
2.1.2. 生成thrift访问client的访问文件；
​ 我们从这里下载云HBase的Hbase.thrift文件，云HBase使用的是thrift1协议

​ 编译命令如下：

 thrift --gen <language> Hbase.thrift
​ 上述 language 是语言的缩写，常见的有如下：

thrift --gen php Hbase.thrift
thrift --gen cpp Hbase.thrift
thrift --gen py Hbase.thrift
​ 执行thrift —gen php Hbase.thrift ，在目录下得到gen-php ，这个是需要的函数包文件；

thrift git:(last_dev) ✗ ll
total 56
-rw-r--r--  1 xuanling.gc  staff    24K  3  5 15:06 Hbase.thrift
drwxr-xr-x  3 xuanling.gc  staff    96B  8  1 16:03 gen-php
​ 将下载到的Thrift源码文件夹下的/lib/php/lib下面的Thrift文件夹以及gen-php一起放在我们的业务逻辑代码一个src目录下面，加上我们自己的client.php的代码，目录结果如下所示：

[root@xxxxxxxxxxx thrift_client]# ll
total 12
-rw-r--r--  1 zookeeper games 2743 Aug  2 11:16 client.php
drwxr-xr-x  3 zookeeper games 4096 Aug  2 01:22 gen-php
drwxr-xr-x 12 zookeeper games 4096 Aug  2 01:22 Thrift
php访问代码编写；
​ 上述的Thrift文件夹以及gen-php文件夹，可以随自己项目以及个人风格命名，这里方便大家搞清目录结构，就保留原来风格；下面贴出php的代码，下面的程序是在HBase 建了一张表”new”：

<?php
ini_set('display_errors', E_ALL);
$GLOBALS['THRIFT_ROOT'] = "/root/thrift_client";
/* Dependencies. In the proper order. */
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Transport/TTransport.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Transport/TSocket.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Protocol/TProtocol.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Protocol/TBinaryProtocol.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Protocol/TBinaryProtocolAccelerated.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Transport/TBufferedTransport.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Type/TMessageType.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Factory/TStringFuncFactory.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/StringFunc/TStringFunc.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/StringFunc/Core.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Type/TType.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Exception/TException.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Exception/TTransportException.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/Thrift/Exception/TProtocolException.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/gen-php/Hbase/Types.php';
require_once $GLOBALS['THRIFT_ROOT'] . '/gen-php/Hbase/Hbase.php';
use Thrift\Protocol\TBinaryProtocol;
use Thrift\Transport\TBufferedTransport;
use Thrift\Transport\TSocket;
use Hbase\HbaseClient;
use Hbase\ColumnDescriptor;
use Hbase\Mutation;
$host='hb-bp12pt6alr1788y35-001.hbase.rds.aliyuncs.com';
$port=9099;
$socket = new TSocket($host, $port);
$socket->setSendTimeout(10000); // 发送超时，单位毫秒
$socket->setRecvTimeout(20000); // 接收超时，单位毫秒
$transport = new TBufferedTransport($socket);
$protocol = new TBinaryProtocol($transport);
$client = new HbaseClient($protocol);
$transport->open();
####列出表####
echo "----list tables----\n";
$tables = $client->getTableNames();
foreach ($tables as $name) {
    var_dump($tables);
}
$tablename='new';
####写数据####
echo "----write data----\n";
$row = 'key';
$value = 'value';
$atrribute = array();
$mutations = array(
    new Mutation(array(
        'column' => 'info:cn1',
        'value' => $value
    )),
);
try {
    $client->mutateRow($tablename, $row, $mutations, $atrribute);
} catch (Exception $e) {
    var_dump($e);//这里自己打log
}
###读数据####
echo "---read data---\n";
$result = $client->getRow($tablename, $row, $atrribute);
var_dump($result);
###删数据####
echo "---delete data---\n";
$client->deleteAllRow($tablename, $row, $atrribute);
echo "---get data---\n";
$result = $client->getRow($tablename, $row, $atrribute);
var_dump($result);
?>
​ 代码执行结果如下：

[root@xxxxxxxxxxx thrift_client]# php client.php
----list tables----
array(1) {
  [0]=>
  string(3) "new"
}
----write data----
---read data---
array(1) {
  [0]=>
  object(Hbase\TRowResult)#8 (3) {
    ["row"]=>
    string(3) "key"
    ["columns"]=>
    array(1) {
      ["info:cn1"]=>
      object(Hbase\TCell)#10 (2) {
        ["value"]=>
        string(5) "value"
        ["timestamp"]=>
        int(1533179795969)
      }
    }
    ["sortedColumns"]=>
    NULL
  }
}
---delete data---
---get data---
array(0) {
}
2.2.python访问流程；
​ 此外还有常见的python的客户，对于python的话，有happybase这种python的第三方包含thrift的库，我们见过一些客户使用Happybase进行访问HBase thrift，参见文章；此外，python 有丰富的库，我们通过pip可以安装thrift，以及访问HBase的thrift库；执行流程如下,假设用户已经安装python以及pip：

pip install thrift //安装thrift默认最新版本
pip install hbase-thrift //安装hbase thrift接口库
​ 上面2步执行完成以后，既可以编写访问HBase的代码：

import sys
import time
import os
from thrift import Thrift
from thrift.transport import TSocket, TTransport
from thrift.protocol import TBinaryProtocol
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated
from hbase import ttypes
from hbase.Hbase import Client, ColumnDescriptor, Mutation
def printRow(entry):
  print "row: " + entry.row + ", cols:",
  for k in sorted(entry.columns):
    print k + " => " + entry.columns[k].value,
  print
transport = TSocket.TSocket('hb-bp12pt6alr1788y35-001.hbase.rds.aliyuncs.com', 9099)
transport = TTransport.TBufferedTransport(transport)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Client(protocol)
transport.open()
print "---list table--"
print client.getTableNames()
table="new"
row="key"
print "---write data---"
mutations = [Mutation(column="info:cn1", value="value")]
client.mutateRow(table, row, mutations)
print "---get data----"
printRow(client.getRow(table, row)[0])
print "---delete data---"
client.deleteAllRow(table, row)
print "---end----"
transport.close()
​ 对应上述的程序执行的结果如下：

[root@Test ~]# python Hbase_client.py
---list table--
['new']
---write data---
---get data----
row: key, cols: info:cn1 => value
---delete data---
---end----
Go语言访问HBase
下载优化后的thrift访问压缩包（也可以从https://github.com/sdming/goh 下载原始版本），解压后放到$GOPATH/src下

wget http://public-hbase.oss-cn-hangzhou.aliyuncs.com/thrift/goh.tar.gz
tar -xvzf goh.tar.gz
mv github.com $GOPATH/src
示例代码请参考 $GOPATH/src/github.com/sdming/goh/demo/client.go 包含了DDL以及数据读写的代码示例

访问HBase HDFS
更新时间：2018-03-21 12:12:59


在一些场景下，比如需要bulkload导入数据，需要打开HBase集群的HDFS端口。

注意：hdfs端口打开后，因误操作hdfs导致的数据丢失等问题客户自身承担，客户需要对hdfs的操作比较了解
开通HDFS端口
首先联系云HBase答疑(钉钉号)，开通HDFS（ 由于hdfs的开放可能造成用户的恶意攻击，引起集群不稳定甚至造成破坏。因此此功能暂时不直接开放给用户，当用户特别需要的情况下，我们通过云HBase答疑后台开通，随后客户使用完成，再关闭）
验证
检查端口是否可以正常使用通过一个hdfs client访问云hbase上的hdfs（目标集群）
创建一个hadoop客户端配置目录conf(如果使用客户端已存在这个目录则不需要另行创建)
添加以下两个hdfs配置到hadoop客户端conf目录({hbase-header-1-host}和{hbase-header-1-host}可以咨询 云HBase答疑

core-site.xml
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://hbase-cluster</value>
</property>
</configuration>
hdfs-site.xml
<configuration>
<property>
    <name>dfs.nameservices</name>
    <value>hbase-cluster</value>
</property>
<property>
<name>dfs.client.failover.proxy.provider.hbase-cluster</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<property>
<name>dfs.ha.automatic-failover.enabled.hbase-cluster</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.http-address.hbase-cluster.nn1</name>
<value>{hbase-header-1-host}:50070</value>
</property>
<property>
<name>dfs.namenode.http-address.hbase-cluster.nn2</name>
<value>{hbase-header-2-host}:50070</value>
</property>
<property>
    <name>dfs.ha.namenodes.hbase-cluster</name>
    <value>nn1,nn2</value>
</property>
<property>
    <name>dfs.namenode.rpc-address.hbase-cluster.nn1</name>
    <value>{hbase-header-1-host}:8020</value>
</property>
<property>
    <name>dfs.namenode.rpc-address.hbase-cluster.nn2</name>
    <value>{hbase-header-2-host}:8020</value>
</property>
</configuration>
添加conf到hadoop 客户端classpath中

读写验证hdfs端口能否正常访问
echo "hdfs port test"  >/tmp/test
hadoop dfs -put /tmp/test  /
hadoop dfs -cat /test

Hive 读写 HBase 指南
更新时间：2019-02-26 11:10:57


本页目录
环境准备
修改配置
在Hive中读写HBase表
云 HBase 支持使用 Hive 读写里面的数据，配置起来也很简单。

环境准备
将 Hive 所在的 Hadoop 集群所有的节点的IP加入到云HBase白名单；
获取云HBase的zookeeper访问地址，可在云HBase控制台查看。
修改配置
进入hive配置目录 /etc/ecm/hive-conf/
修改 hbase-site.xml，将 hbase.zookeeper.quorum 修改为云HBase的zookeeper访问连接，如下：
<property>
  <name>hbase.zookeeper.quorum</name>
  <value>hb-xxx-001.hbase.rds.aliyuncs.com,hb-xxx-002.hbase.rds.aliyuncs.com,hb-xxx-003.hbase.rds.aliyuncs.com</value>
</property>
在Hive中读写HBase表
如果HBase表不存在，可在Hive中直接创建云HBase关联表

进入hive cli命令行Hive CLI
创建HBase表
CREATE TABLE hive_hbase_table(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")
TBLPROPERTIES ("hbase.table.name" = "hive_hbase_table", "hbase.mapred.output.outputtable" = "hive_hbase_table");
Hive中向hbase插入数据
insert into hive_hbase_table values(212,'bab');
hive
查看云HBase表,hbase表已创建，数据也已写入hivehive
在HBase中写入数据，并在Hive中查看hive

在Hive中查看：

hive

Hive删除表，HBase表也删除

hive

查看hbase表，报错不存在表hive

如果HBase表已存在，可在Hive中HBase外表进行关联，外部表在删除时不影响HBase已创建表

云hbase中创建hbase表，并put测试数据hive
Hive中创建HBase外部关联表，并查看数据hive
删除Hive表不影响HBase已存在表

hive

hive

Hive更多操作HBase步骤，可参考https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration

如果使用ECS自建mr集群的 Hive时，操作步骤跟EMR操作类似，需要注意的是自建Hive的hbase-site.xml部分配置项可能与云HBase不一致，简单来说网络和端口开放后，只保留hbase.zookeeper.quorum即可与云Hbase进行关联。


HBase版本选择
更新时间：2019-04-12 12:00:10


本页目录
当前云HBase提供的版本
HBase1.1版本
HBase2.0版本
1.1与2.0版本对比
当前云HBase提供的版本
目前云HBase提供1.1版本和2.0版本供用户选择。相比开源产品的优势，可以参照这篇文章。

HBase1.1版本
1.1版本基于HBase社区1.1.2版本开发，在此基础上做了大量的改进，包括性能优化，稳定性优化和运维能力增强。该版本是阿里内部使用的主力版本，拥有1w+节点部署规模，多次成功支撑阿里双十一高并发和海量数据存储。在此基础上，1.1版本又集成了Phoenix(介绍与使用说明)与冷存储功能(介绍与使用说明)，满足不同业务的需求。云HBase1.1版本100%兼容社区HBase协议。

HBase2.0版本
2.0版本是基于社区2018年发布的HBase2.0.0版本开发的全新版本。同样，在此基础上，做了大量的改进和优化，吸收了众多阿里内部成功经验，比社区HBase版本具有更好的稳定性和性能，同时具备了HBase2.0提供的全新能力。HBase2.0提供的新功能介绍可以参照这篇文章。我们建议有以下需求的业务，可以直接选用HBase2.0版本。

对毛刺敏感，延迟要求高的业务，可以直接选用云HBase2.0版本，HBase2.0版本具有全链路offheap功能，能够大大地降低GC停顿，减少毛刺，具体的测试数据可以参见这篇文章《消灭毛刺！HBase2.0全链路offheap效果拔群》
使用HBase存储小对象的用户，如音视频文件，文档，二进制数据，典型大小在100KB~10MB左右。云HBase2.0中对这类小对象存储有专门的优化，能够减少写放大，优化性能。
需要用到一些只有在云HBase2.0上才有的功能，如全异步客户端
1.1与2.0版本对比
功能	HBase1.1	HBase2.0
HBase基本接口	支持	支持
企业级安全认证	支持	支持
主备同步，双活	即将上线	支持
Phoenix与二级索引	支持	支持
冷存储	支持	即将上线
RegionServer Group	支持	支持
Region Replica	不支持	支持
读写链路offheap	不支持	支持
小对象存储MOB	不支持	支持
In memory Compaction	不支持	支持
异步客户端	不支持	支持



使用 Shell 访问
更新时间：2018-09-28 21:40:26


使用 HBase Shell 访问
准备访问用ECS

因为云HBase只提供了内网的访问，所以如果要访问HBase，需要在相同的Region内准备一台ECS。如果已经有ECS了那么请继续下一步。如果还没有，您可以在ECS的购买页面上购买一台按量的ECS进行测试。

设置请参考这里

下载HBase Shell

参考：阿里云开源HBase客户端，下载对应最新HBase包，如果购买的为1.x版本的HBase集群，可以下载1.1.x系列的HBase包，如：

 wget http://public-hbase.oss-cn-hangzhou.aliyuncs.com/installpackage/alihbase-1.1.4-bin.tar.gz .
如果购买的是2.x版本的HBase集群，可以下载2.0.x系列的HBase包,如：

 wget http://public-hbase.oss-cn-hangzhou.aliyuncs.com/installpackage/alihbase-2.0.0-bin.tar.gz .
下载完成以后，解压缩（以1.x版本为例）

 tar -zxf alihbase-1.1.4-bin.tar.gz
配置 ZK 地址 解压后，修改 conf/hbase-site.xml 文件，添加集群的 ZK 地址，如下所示：

<configuration>
     <property>
         <name>hbase.zookeeper.quorum</name>
         <value>$ZK_IP1,$ZK_IP2,$ZK_IP3</value>
     </property>
</configuration>
其中的$ZK_IP1,$ZK_IP2,$ZK_IP3请参考上一节中的获取集群的 ZK 连接地址

访问集群

通过如下命令就可以访问集群了。

 bin/hbase shell





HBase Shell 入门
更新时间：2017-06-07 13:26:11


HBase Shell 入门
本篇会介绍最基本的HBase Shell的使用命令。

连接到HBase

请参考HBase Shell的配置来进行基本环境的配置。

运行HBase目录下bin下的如下命令进入HBase shell

 $ ./bin/hbase shell
成功后会进入到如下的界面，这个时候我们就可以输入各种命令来执行了

hbase(main):001:0>

展示HBase Shell的帮助信息

help命令提供了很多基本的命令和对应的使用方法，当你忘记一些基本用法的时候，记得输入help来查看。

 hbase(main):001:0>help
创建表

使用create命令来创建一张新的表，在创建的时候你必须输入表的名称和ColumnFamily的名称

 hbase(main):001:0> create 'test', 'cf'
 0 row(s) in 0.4170 seconds
 => Hbase::Table - test
查询表信息

使用list命令来查询表

 hbase(main):002:0> list 'test'
 TABLE
 test
 1 row(s) in 0.0180 seconds
 => ["test"]
往表里面插入记录

在hbase中，往表里面写一行记录的命令叫做put

 hbase(main):003:0> put 'test', 'row1', 'cf:a', 'value1'
 0 row(s) in 0.0850 seconds
 hbase(main):004:0> put 'test', 'row2', 'cf:b', 'value2'
 0 row(s) in 0.0110 seconds
 hbase(main):005:0> put 'test', 'row3', 'cf:c', 'value3'
 0 row(s) in 0.0100 seconds
这里我们写入了三条数据，前面的rowx，代表写入的表的rowkey，也就是主键。后面的cf:x，是我们的自定义列，可以有无数多个，这里写了3个。我们叫这个a，b，c为qualifier，也就是列名。

查询表中的所有数据

scan是一种访问HBase数据的方式，它非常的灵活，你可以用它来扫描全表，也可以用它查询固定范围。相应的它的速度会比查询单条的get略慢一些，这里因为我们的demo数据库数据并不多，所以我们全部取出来。

 hbase(main):006:0> scan 'test'
 ROW                                      COLUMN+CELL
  row1                                    column=cf:a, timestamp=1421762485768, value=value1
  row2                                    column=cf:b, timestamp=1421762491785, value=value2
  row3                                    column=cf:c, timestamp=1421762496210, value=value3
 3 row(s) in 0.0230 seconds
查询单条记录

使用get来查询单条记录

 hbase(main):007:0> get 'test', 'row1'
 COLUMN                                   CELL
  cf:a                                    timestamp=1421762485768, value=value1
 1 row(s) in 0.0350 seconds
禁用一张表

如果你想要删除一张表，或者改变一张表的设置，或者其他类似的场景。你需要先禁用这张表，使用disable命令能够禁用一张表，使用enable命令能够取消禁用，恢复禁用的表。

 hbase(main):008:0> disable 'test'
 0 row(s) in 1.1820 seconds
 hbase(main):009:0> enable 'test'
 0 row(s) in 0.1770 seconds
删除一张表

要删除一张表，使用drop命令，这是一个危险的操作，使用的时候请务必小心。

 hbase(main):011:0> drop 'test'
 0 row(s) in 0.1370 seconds
退出HBase Shell

输入quit命令就可以离开HBase Shell环境了。

As told in HBase introduction, HBase provides Extensible jruby-based (JIRB) shell as a feature to execute some commands(each command represents one functionality).

HBase shell commands are mainly categorized into 6 parts

1) General  HBase shell commands

status	Show cluster status. Can be ‘summary’, ‘simple’, or ‘detailed’. The
default is ‘summary’.
hbase> status
hbase> status ‘simple’
hbase> status ‘summary’
hbase> status ‘detailed’

version	Output this HBase versionUsage:
hbase> version

whoami	Show the current hbase user.Usage:
hbase> whoami

2) Tables Management commands

alter	Alter column family schema; pass table name and a dictionary
specifying new column family schema. Dictionaries are described
on the main help command output. Dictionary must include name
of column family to alter.For example, to change or add the ‘f1’ column family in table ‘t1’ from
current value to keep a maximum of 5 cell VERSIONS, do:
hbase> alter ‘t1’, NAME => ‘f1’, VERSIONS => 5

You can operate on several column families:

hbase> alter ‘t1’, ‘f1’, {NAME => ‘f2’, IN_MEMORY => true}, {NAME => ‘f3’, VERSIONS => 5}

To delete the ‘f1’ column family in table ‘t1’, use one of:hbase> alter ‘t1’, NAME => ‘f1’, METHOD => ‘delete’
hbase> alter ‘t1’, ‘delete’ => ‘f1’

You can also change table-scope attributes like MAX_FILESIZE, READONLY,
MEMSTORE_FLUSHSIZE, DEFERRED_LOG_FLUSH, etc. These can be put at the end;
for example, to change the max size of a region to 128MB, do:

hbase> alter ‘t1’, MAX_FILESIZE => ‘134217728’

You can add a table coprocessor by setting a table coprocessor attribute:

hbase> alter ‘t1’,
‘coprocessor’=>’hdfs:///foo.jar|com.foo.FooRegionObserver|1001|arg1=1,arg2=2’

Since you can have multiple coprocessors configured for a table, a
sequence number will be automatically appended to the attribute name
to uniquely identify it.

The coprocessor attribute must match the pattern below in order for
the framework to understand how to load the coprocessor classes:

[coprocessor jar file location] | class name | [priority] | [arguments]

You can also set configuration settings specific to this table or column family:

hbase> alter ‘t1’, CONFIGURATION => {‘hbase.hregion.scan.loadColumnFamiliesOnDemand’ => ‘true’}
hbase> alter ‘t1’, {NAME => ‘f2’, CONFIGURATION => {‘hbase.hstore.blockingStoreFiles’ => ’10’}}

You can also remove a table-scope attribute:

hbase> alter ‘t1’, METHOD => ‘table_att_unset’, NAME => ‘MAX_FILESIZE’

hbase> alter ‘t1’, METHOD => ‘table_att_unset’, NAME => ‘coprocessor$1’

There could be more than one alteration in one command:

hbase> alter ‘t1’, { NAME => ‘f1’, VERSIONS => 3 },
{ MAX_FILESIZE => ‘134217728’ }, { METHOD => ‘delete’, NAME => ‘f2’ },
OWNER => ‘johndoe’, METADATA => { ‘mykey’ => ‘myvalue’ }

create	Create table; pass table name, a dictionary of specifications per
column family, and optionally a dictionary of table configuration.
hbase> create ‘t1’, {NAME => ‘f1’, VERSIONS => 5}
hbase> create ‘t1’, {NAME => ‘f1’}, {NAME => ‘f2’}, {NAME => ‘f3’}
hbase> # The above in shorthand would be the following:
hbase> create ‘t1’, ‘f1’, ‘f2’, ‘f3’
hbase> create ‘t1’, {NAME => ‘f1’, VERSIONS => 1, TTL => 2592000, BLOCKCACHE => true}
hbase> create ‘t1’, {NAME => ‘f1’, CONFIGURATION => {‘hbase.hstore.blockingStoreFiles’ => ’10’}}

Table configuration options can be put at the end.

describe	Describe the named table.
hbase> describe ‘t1’

disable	Start disable of named table
hbase> disable ‘t1’

disable_all	Disable all of tables matching the given regex
hbase> disable_all ‘t.*’

is_disabled	verifies Is named table disabled
hbase> is_disabled ‘t1’

drop 	Drop the named table. Table must first be disabled
hbase> drop ‘t1’

drop_all	Drop all of the tables matching the given regex
hbase> drop_all ‘t.*’

enable	Start enable of named table
hbase> enable ‘t1’

enable_all	Enable all of the tables matching the given regex
hbase> enable_all ‘t.*’

is_enabled	verifies Is named table enabled
hbase> is_enabled ‘t1’

exists	Does the named table exist
hbase> exists ‘t1’

list	List all tables in hbase. Optional regular expression parameter could
be used to filter the output
hbase> list
hbase> list ‘abc.*’

show_filters	Show all the filters in hbase.
hbase> show_filters

alter_status	Get the status of the alter command. Indicates the number of regions of the table that have received the updated schema Pass table name.
hbase> alter_status ‘t1’

alter_async	Alter column family schema, does not wait for all regions to receive the
schema changes. Pass table name and a dictionary specifying new column
family schema. Dictionaries are described on the main help command output.
Dictionary must include name of column family to alter.
To change or add the ‘f1’ column family in table ‘t1’ from defaults
to instead keep a maximum of 5 cell VERSIONS, do:hbase> alter_async ‘t1’, NAME => ‘f1’, VERSIONS => 5To delete the ‘f1’ column family in table ‘t1’, do:

hbase> alter_async ‘t1’, NAME => ‘f1’, METHOD => ‘delete’or a shorter version:hbase> alter_async ‘t1’, ‘delete’ => ‘f1’

You can also change table-scope attributes like MAX_FILESIZE
MEMSTORE_FLUSHSIZE, READONLY, and DEFERRED_LOG_FLUSH.

For example, to change the max size of a family to 128MB, do:

hbase> alter ‘t1’, METHOD => ‘table_att’, MAX_FILESIZE => ‘134217728’

There could be more than one alteration in one command:

hbase> alter ‘t1’, {NAME => ‘f1’}, {NAME => ‘f2’, METHOD => ‘delete’}

To check if all the regions have been updated, use alter_status <table_name>

3) Data Manipulation commands

count	Count the number of rows in a table. Return value is the number of rows.
This operation may take a LONG time (Run ‘$HADOOP_HOME/bin/hadoop jar
hbase.jar rowcount’ to run a counting mapreduce job). Current count is shown
every 1000 rows by default. Count interval may be optionally specified. Scan
caching is enabled on count scans by default. Default cache size is 10 rows.
If your rows are small in size, you may want to increase this
parameter. Examples:hbase> count ‘t1’
hbase> count ‘t1’, INTERVAL => 100000
hbase> count ‘t1’, CACHE => 1000
hbase> count ‘t1’, INTERVAL => 10, CACHE => 1000
The same commands also can be run on a table reference. Suppose you had a reference
t to table ‘t1’, the corresponding commands would be:hbase> t.count
hbase> t.count INTERVAL => 100000
hbase> t.count CACHE => 1000
hbase> t.count INTERVAL => 10, CACHE => 1000

delete	Put a delete cell value at specified table/row/column and optionally
timestamp coordinates. Deletes must match the deleted cell’s
coordinates exactly. When scanning, a delete cell suppresses older
versions. To delete a cell from ‘t1’ at row ‘r1’ under column ‘c1’
marked with the time ‘ts1’, do:
hbase> delete ‘t1’, ‘r1’, ‘c1’, ts1

The same command can also be run on a table reference. Suppose you had a reference
t to table ‘t1’, the corresponding command would be:hbase> t.delete ‘r1’, ‘c1’, ts1

deleteall	Delete all cells in a given row; pass a table name, row, and optionally
a column and timestamp. Examples:hbase> deleteall ‘t1’, ‘r1’
hbase> deleteall ‘t1’, ‘r1’, ‘c1’
hbase> deleteall ‘t1’, ‘r1’, ‘c1’, ts1
The same commands also can be run on a table reference. Suppose you had a reference
t to table ‘t1’, the corresponding command would be:hbase> t.deleteall ‘r1’
hbase> t.deleteall ‘r1’, ‘c1’
hbase> t.deleteall ‘r1’, ‘c1’, ts1

get	Get row or cell contents; pass table name, row, and optionally
a dictionary of column(s), timestamp, timerange and versions. Examples:
hbase> get ‘t1’, ‘r1’
hbase> get ‘t1’, ‘r1’, {TIMERANGE => [ts1, ts2]}
hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’}
hbase> get ‘t1’, ‘r1’, {COLUMN => [‘c1’, ‘c2’, ‘c3’]}
hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’, TIMESTAMP => ts1}
hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’, TIMERANGE => [ts1, ts2], VERSIONS => 4}
hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’, TIMESTAMP => ts1, VERSIONS => 4}
hbase> get ‘t1’, ‘r1’, {FILTER => “ValueFilter(=, ‘binary:abc’)”}
hbase> get ‘t1’, ‘r1’, ‘c1’
hbase> get ‘t1’, ‘r1’, ‘c1’, ‘c2’
hbase> get ‘t1’, ‘r1’, [‘c1’, ‘c2’]

Besides the default ‘toStringBinary’ format, ‘get’ also supports custom formatting by
column. A user can define a FORMATTER by adding it to the column name in the get
specification. The FORMATTER can be stipulated:1. either as a org.apache.hadoop.hbase.util.Bytes method name (e.g, toInt, toString)
2. or as a custom class followed by method name: e.g. ‘c(MyFormatterClass).format’.Example formatting cf:qualifier1 and cf:qualifier2 both as Integers:
hbase> get ‘t1’, ‘r1’ {COLUMN => [‘cf:qualifier1:toInt’,
‘cf:qualifier2:c(org.apache.hadoop.hbase.util.Bytes).toInt’] }

Note that you can specify a FORMATTER by column only (cf:qualifer). You cannot specify
a FORMATTER for all columns of a column family.The same commands also can be run on a reference to a table (obtained via get_table or
create_table). Suppose you had a reference t to table ‘t1’, the corresponding commands
would be:

hbase> t.get ‘r1’
hbase> t.get ‘r1’, {TIMERANGE => [ts1, ts2]}
hbase> t.get ‘r1’, {COLUMN => ‘c1’}
hbase> t.get ‘r1’, {COLUMN => [‘c1’, ‘c2’, ‘c3’]}
hbase> t.get ‘r1’, {COLUMN => ‘c1’, TIMESTAMP => ts1}
hbase> t.get ‘r1’, {COLUMN => ‘c1’, TIMERANGE => [ts1, ts2], VERSIONS => 4}
hbase> t.get ‘r1’, {COLUMN => ‘c1’, TIMESTAMP => ts1, VERSIONS => 4}
hbase> t.get ‘r1’, {FILTER => “ValueFilter(=, ‘binary:abc’)”}
hbase> t.get ‘r1’, ‘c1’
hbase> t.get ‘r1’, ‘c1’, ‘c2’
hbase> t.get ‘r1’, [‘c1’, ‘c2’]

get_counter	Return a counter cell value at specified table/row/column coordinates.
A cell cell should be managed with atomic increment function oh HBase
and the data should be binary encoded. Example:
hbase> get_counter ‘t1’, ‘r1’, ‘c1’

The same commands also can be run on a table reference. Suppose you had a reference
t to table ‘t1’, the corresponding command would be:

hbase> t.get_counter ‘r1’, ‘c1’

incr	Increments a cell ‘value’ at specified table/row/column coordinates.
To increment a cell value in table ‘t1’ at row ‘r1’ under column
‘c1’ by 1 (can be omitted) or 10 do:
hbase> incr ‘t1’, ‘r1’, ‘c1’
hbase> incr ‘t1’, ‘r1’, ‘c1’, 1
hbase> incr ‘t1’, ‘r1’, ‘c1’, 10

The same commands also can be run on a table reference. Suppose you had a reference
t to table ‘t1’, the corresponding command would be:hbase> t.incr ‘r1’, ‘c1’
hbase> t.incr ‘r1’, ‘c1’, 1
hbase> t.incr ‘r1’, ‘c1’, 10

put	Put a cell ‘value’ at specified table/row/column and optionally
timestamp coordinates. To put a cell value into table ‘t1’ at
row ‘r1’ under column ‘c1’ marked with the time ‘ts1’, do:
hbase> put ‘t1’, ‘r1’, ‘c1’, ‘value’, ts1

The same commands also can be run on a table reference. Suppose you had a reference
t to table ‘t1’, the corresponding command would be:

hbase> t.put ‘r1’, ‘c1’, ‘value’, ts1

scan	Scan a table; pass table name and optionally a dictionary of scanner
specifications. Scanner specifications may include one or more of:
TIMERANGE, FILTER, LIMIT, STARTROW, STOPROW, TIMESTAMP, MAXLENGTH,
or COLUMNS, CACHEIf no columns are specified, all columns will be scanned.
To scan all members of a column family, leave the qualifier empty as in
‘col_family:’.The filter can be specified in two ways:
1. Using a filterString – more information on this is available in the
Filter Language document attached to the HBASE-4176 JIRA
2. Using the entire package name of the filter.Some examples:hbase> scan ‘.META.’
hbase> scan ‘.META.’, {COLUMNS => ‘info:regioninfo’}
hbase> scan ‘t1’, {COLUMNS => [‘c1’, ‘c2’], LIMIT => 10, STARTROW => ‘xyz’}
hbase> scan ‘t1’, {COLUMNS => ‘c1’, TIMERANGE => [1303668804, 1303668904]}
hbase> scan ‘t1’, {FILTER => “(PrefixFilter (‘row2’) AND
(QualifierFilter (>=, ‘binary:xyz’))) AND (TimestampsFilter ( 123, 456))”}
hbase> scan ‘t1’, {FILTER =>
org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}
For experts, there is an additional option — CACHE_BLOCKS — which
switches block caching for the scanner on (true) or off (false). By
default it is enabled. Examples:hbase> scan ‘t1’, {COLUMNS => [‘c1’, ‘c2’], CACHE_BLOCKS => false}

Also for experts, there is an advanced option — RAW — which instructs the
scanner to return all cells (including delete markers and uncollected deleted
cells). This option cannot be combined with requesting specific COLUMNS.
Disabled by default. Example:

hbase> scan ‘t1’, {RAW => true, VERSIONS => 10}

Besides the default ‘toStringBinary’ format, ‘scan’ supports custom formatting
by column. A user can define a FORMATTER by adding it to the column name in
the scan specification. The FORMATTER can be stipulated:

1. either as a org.apache.hadoop.hbase.util.Bytes method name (e.g, toInt, toString)
2. or as a custom class followed by method name: e.g. ‘c(MyFormatterClass).format’.

Example formatting cf:qualifier1 and cf:qualifier2 both as Integers:
hbase> scan ‘t1’, {COLUMNS => [‘cf:qualifier1:toInt’,
‘cf:qualifier2:c(org.apache.hadoop.hbase.util.Bytes).toInt’] }

Note that you can specify a FORMATTER by column only (cf:qualifer). You cannot
specify a FORMATTER for all columns of a column family.

Scan can also be used directly from a table, by first getting a reference to a
table, like such:

hbase> t = get_table ‘t’
hbase> t.scan

Note in the above situation, you can still provide all the filtering, columns,
options, etc as described above.

truncate	Disables, drops and recreates the specified table.
Examples:
hbase>truncate ‘t1’
4) HBase surgery tools

assign	Assign a region. Use with caution. If region already assigned,
this command will do a force reassign. For experts only.
Examples:
hbase> assign ‘REGION_NAME’
balancer	Trigger the cluster balancer. Returns true if balancer ran and was able to
tell the region servers to unassign all the regions to balance (the re-assignment itself is async).
Otherwise false (Will not run if regions in transition).
Examples:
hbase> balancer
balance_switch	Enable/Disable balancer. Returns previous balancer state.
Examples:
hbase> balance_switch true
hbase> balance_switch false

close_region	Close a single region. Ask the master to close a region out on the cluster
or if ‘SERVER_NAME’ is supplied, ask the designated hosting regionserver to
close the region directly. Closing a region, the master expects ‘REGIONNAME’
to be a fully qualified region name. When asking the hosting regionserver to
directly close a region, you pass the regions’ encoded name only. A region
name looks like this:TestTable,0094429456,1289497600452.527db22f95c8a9e0116f0cc13c680396.The trailing period is part of the regionserver name. A region’s encoded name
is the hash at the end of a region name; e.g. 527db22f95c8a9e0116f0cc13c680396
(without the period). A ‘SERVER_NAME’ is its host, port plus startcode. For
example: host187.example.com,60020,1289493121758 (find servername in master ui
or when you do detailed status in shell). This command will end up running
close on the region hosting regionserver. The close is done without the
master’s involvement (It will not know of the close). Once closed, region will
stay closed. Use assign to reopen/reassign. Use unassign or move to assign
the region elsewhere on cluster. Use with caution. For experts only.
Examples:hbase> close_region ‘REGIONNAME’
hbase> close_region ‘REGIONNAME’, ‘SERVER_NAME’
compact	Compact all regions in passed table or pass a region row
to compact an individual region. You can also compact a single column
family within a region.
Examples:
Compact all regions in a table:
hbase> compact ‘t1’
Compact an entire region:
hbase> compact ‘r1’
Compact only a column family within a region:
hbase> compact ‘r1’, ‘c1’
Compact a column family within a table:
hbase> compact ‘t1’, ‘c1’
flush	Flush all regions in passed table or pass a region row to
flush an individual region. For example:hbase> flush ‘TABLENAME’
hbase> flush ‘REGIONNAME’
major_compact	Run major compaction on passed table or pass a region row
to major compact an individual region. To compact a single
column family within a region specify the region name
followed by the column family name.
Examples:
Compact all regions in a table:
hbase> major_compact ‘t1’
Compact an entire region:
hbase> major_compact ‘r1’
Compact a single column family within a region:
hbase> major_compact ‘r1’, ‘c1’
Compact a single column family within a table:
hbase> major_compact ‘t1’, ‘c1’
move	Move a region. Optionally specify target regionserver else we choose one
at random. NOTE: You pass the encoded region name, not the region name so
this command is a little different to the others. The encoded region name
is the hash suffix on region names: e.g. if the region name were
TestTable,0094429456,1289497600452.527db22f95c8a9e0116f0cc13c680396. then
the encoded region name portion is 527db22f95c8a9e0116f0cc13c680396
A server name is its host, port plus startcode. For example:
host187.example.com,60020,1289493121758
Examples:hbase> move ‘ENCODED_REGIONNAME’
hbase> move ‘ENCODED_REGIONNAME’, ‘SERVER_NAME’
split	Split entire table or pass a region to split individual region. With the
second parameter, you can specify an explicit split key for the region.
Examples:
split ‘tableName’
split ‘regionName’ # format: ‘tableName,startKey,id’
split ‘tableName’, ‘splitKey’
split ‘regionName’, ‘splitKey’
unassign	Unassign a region. Unassign will close region in current location and then
reopen it again. Pass ‘true’ to force the unassignment (‘force’ will clear
all in-memory state in master before the reassign. If results in
double assignment use hbck -fix to resolve. To be used by experts).
Use with caution. For expert use only. Examples:hbase> unassign ‘REGIONNAME’
hbase> unassign ‘REGIONNAME’, true
hlog_roll	Roll the log writer. That is, start writing log messages to a new file.
The name of the regionserver should be given as the parameter. A
‘server_name’ is the host, port plus startcode of a regionserver. For
example: host187.example.com,60020,1289493121758 (find servername in
master ui or when you do detailed status in shell)
hbase>hlog_roll

zk_dump	Dump status of HBase cluster as seen by ZooKeeper. Example:
hbase>zk_dump
5) Cluster replication tools

add_peer	Add a peer cluster to replicate to, the id must be a short and
the cluster key is composed like this:
hbase.zookeeper.quorum:hbase.zookeeper.property.clientPort:zookeeper.znode.parent
This gives a full path for HBase to connect to another cluster.
Examples:hbase> add_peer ‘1’, “server1.cie.com:2181:/hbase”
hbase> add_peer ‘2’, “zk1,zk2,zk3:2182:/hbase-prod”
remove_peer	Stops the specified replication stream and deletes all the meta
information kept about it. Examples:
hbase> remove_peer ‘1’

list_peers	List all replication peer clusters.
hbase> list_peers
enable_peer	Restarts the replication to the specified peer cluster,
continuing from where it was disabled.Examples:
hbase> enable_peer ‘1’

disable_peer	Stops the replication stream to the specified cluster, but still
keeps track of new edits to replicate.Examples:
hbase> disable_peer ‘1’

start_replication	Restarts all the replication features. The state in which each
stream starts in is undetermined.
WARNING:
start/stop replication is only meant to be used in critical load situations.
Examples:
hbase> start_replication

stop_replication	Stops all the replication features. The state in which each
stream stops in is undetermined.
WARNING:
start/stop replication is only meant to be used in critical load situations.
Examples:
hbase> stop_replication

6) Security tools

grant	Grant users specific rights.
Syntax : grantpermissions is either zero or more letters from the set “RWXCA”.
READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)For example:hbase> grant ‘bobsmith’, ‘RWXCA’
hbase> grant ‘bobsmith’, ‘RW’, ‘t1’, ‘f1’, ‘col1’
revoke	Revoke a user’s access rights.
Syntax : revoke
For example:
hbase> revoke ‘bobsmith’, ‘t1’, ‘f1’, ‘col1’

user_permission	Show all permissions for the particular user.
Syntax : user_permission
For example:hbase> user_permission
hbase> user_permission ‘table1’
Share this:

