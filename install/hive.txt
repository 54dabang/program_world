
#!/usr/bin/bash
HADOOP_HOME="/opt/module/cdh-5.3.6-ha/hadoop-2.5.0-cdh5.3.6"
HIVE_HOME='/opt/module/cdh-5.3.6-ha/hive-0.13.1-cdh5.3.6'
SQOOP_HOME='/opt/module/cdh-5.3.6-ha/sqoop-1.4.5-cdh5.3.6'
MYSQL_HOME='/opt/module/mysql-5.6'
####### execute hive ######
sql=$(cat <<!EOF
use wordcount;

-- Hive实现wordCount程序
drop table if exists wc_src_data;
create  table wc_src_data(line string) row format delimited fields terminated by '\n' stored as textfile;

load data inpath '/data/wc.input'  overwrite into table wc_src_data;

drop table if exists wc_tmp;
-- 这里需要用到一个hive的内置表生成函数（UDTF）：explode(array)，参数是array，其实就是行变多列：
create table wc_tmp(word string);
insert overwrite table wc_tmp select explode(split(line, " ")) as word from wc_src_data;

drop table if exists wc_res;
create table wc_res (word string,num bigint);
insert overwrite table wc_res select word,count(*) from wc_tmp group by word;

select * from wc_res limit 10;

!EOF)
############  execute begin   ###########

######上传jar包到hdfs##
# cd $HADOOP_HOME
# bin/hdfs dfs -test -e /hive/warehouse/funcs/hive-01-0.0.1-SNAPSHOT.jar
# if [ $? -ne 0 ] ;then
	# bin/hdfs dfs -put $HIVE_HOME/auxlib/hive-01-0.0.1-SNAPSHOT.jar /hive/warehouse/funcs
# fi

#echo $sql
cd $HIVE_HOME
bin/hive -e "$sql"

cd $HADOOP_HOME
# bin/hdfs dfs -mkdir -p /hive/warehouse/aiso_log.db/dept_part/day=20150914
# bin/hdfs dfs -put /opt/data/dept.txt /hive/warehouse/aiso_log.db/dept_part/day=20150914

cd $HIVE_HOME
# bin/hive -e "use aiso_log;"
# bin/hive -f /opt/data/hivef.sql > /opt/data/hivef-res.txt


exitCode=$?
if [ $exitCode -ne 0 ];then
         echo "[ERROR] hive execute failed!"
         exit $exitCode
fi

cd $HIVE_HOME
#bin/hive -e "drop table aiso_log.bf_log_visit_temp;"

sudo cp execute-hql.sh wordcount/wordcount-hive.sh










#!/usr/bin/bash

# 通过编写shell脚本，将addhiveserver添加到系统服务里，这样以后就可以类似启动mysql那样通过
# service hiveserver start
# service hiveserver stop
# service hiveserver restart
# 去启动hiveserver服务了，

# 把hiveserver添加到系统服务里，还可以通过chkconfig hiveserver on设置hiveserver服务随系统开机自动启动了。

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# .  /etc/profile


# Paths to configuration, binaries, etc

HADOOP_HOME="/opt/module/cdh-5.3.6-ha/hadoop-2.5.0-cdh5.3.6"
HIVE_HOME='/opt/module/cdh-5.3.6-ha/hive-0.13.1-cdh5.3.6'

echo "HADOOP_HOME: $HADOOP_HOME"
echo "HIVE_HOME: $HIVE_HOME"

prog="$0"
port="$3"
echo "port: $port"
HIVE_BASE=$HIVE_HOME
HIVE_BIN="$HIVE_BASE/bin/hive"
HIVE_ARGS="--service hiveserver"
HIVE_LOG="$HIVE_BASE/logs/hive-server.log"
HIVE_USER="root"

if [ "$2" != "" ] && [ "$2" != "-p" ]; then
  echo "Please use the -p parameter to specify the port number."
  exit 1
fi

if [ ! -f $HIVE_BIN ]; then
  echo "Can't found: $HIVE_BIN"
  exit 1
fi



# pid file for /sbin/runuser
pidfile=${PIDFILE-/tmp/hive-server.pid}
RETVAL=0

checkpid() {
    [ -z "$PORT" ] && port=10000
	echo "tem_port: $tem_port"
    if [ -z "`netstat -tanp | grep java | grep $port | awk '{print $7}' | awk -F / '{print $1}'`" ]; then
        return 0;
	else
	    return 1;
    fi
}

echo_success() {
	echo "The Hive Server has been started successfully."
	return 0;
}

echo_failure() {
	echo "Start the Service Hive Server failure."
	return 1;
}

start() {
  # check to see if hive is already running by looking at the pid file and grepping
  # the process table.
  if [ -f $pidfile ] && checkpid `cat $pidfile`; then
    echo "hive-server is already running or hive server not normally closed at the last time,please delete the hive-server.pid file in the /tmp directory and then try again."
    exit 0
  fi
  echo "Starting $prog......"

  if [ -n "$port" ] && [ "$port" -gt 0 ]; then
	/sbin/runuser -s /bin/sh -c "$HIVE_BIN $HIVE_ARGS -p $port" $HIVE_USER >> $HIVE_LOG 2>&1 &
  else
    /sbin/runuser -s /bin/sh -c "$HIVE_BIN $HIVE_ARGS" $HIVE_USER >> $HIVE_LOG 2>&1 &
  fi

  runuser_pid=$!
  echo "parent pid: $runuser_pid"
  usleep 500000
  hiveserver_pid=`ps -eo pid,ppid,fname | grep $runuser_pid | grep -v runuser | awk '{print $1}'`
  echo $hiveserver_pid > $pidfile
  echo "hiveserver pid: $hiveserver_pid"
  disown -ar
  return_val=`ps aux | grep $hiveserver_pid | grep -v grep`
  if [ -n "$return_val" ]
  then
	  echo_success
  else
      echo_failure
  fi
  RETVAL=$?
  return $RETVAL
}

stop() {
  # check if the process is already stopped by seeing if the pid file exists.
  if [ ! -f $pidfile ]; then
    echo "hive-server had been already stopped"
    exit 0
  fi
  echo "Stopping $prog......"
  if kill -9 `cat $pidfile`; then
    RETVAL=0
    echo "The Hive Server has been stopped successfully."
  else
    RETVAL=1
    echo "Stop the Service Hive Server failure."
  fi
  echo
  [ $RETVAL = 0 ] && rm -f ${pidfile}
}

status_fn() {
  if [ -f $pidfile ] && checkpid `cat $pidfile`; then
    echo "hive-server is running now."
    exit 0
  else
    echo "hive-server had been stopped."
    exit 1
  fi
}

case "$1" in
  start)
    start
    ;;
  stop)
    stop
    ;;
  status)
    status_fn
    ;;
  restart)
    stop
    start
    ;;
  *)
    echo $1 "Usage: $prog {start|stop|restart|status}"
    RETVAL=3
esac

exit $RETVAL


# 2.赋予当前用户拥有执行hiveserver脚本文件权限
 chmod u+x $HIVE_HOME/shell/hive2server.sh

# 3.创建日志文件
# HIVE_LOG=$HIVE_HOME/logs/hive-server.log
# 同时要赋予当前登录用户拥有对hive-server.log日志文件的写权限
 chmod u+w $HIVE_HOME/logs/hive-server.log

# 4.创建hiveserver系统服务
CHKCONFIG --ADD HIVESERVER
CHKCONFIG--LIST|GREPHIVESERVER
# 查看服务是否添加成功

# 5.将hiveserver系统服务设置为随系统开机自动启动
chkconfig hiveserver on
# 设置随系统开机自动启动是可选的

# service hiveserver stop
# service hiveserver start
# service hiveserver start -p 10002
# service hiveserver restart

# 可以指定启动端口号,不指定默认端口号是10000

# hiveserver服务的进程id文件保存在/tmp/hive-server.pid文件里


#!/bin/bash

HIVE_HOME='/opt/module/cdh-5.3.6-ha/hive-0.13.1-cdh5.3.6'
MYSQL_HOME='/opt/module/mysql-5.6'
HADOOP_HOME='/opt/module/cdh-5.3.6-ha/hadoop-2.5.0-cdh5.3.6'


#Hive
# cp $HIVE_HOME/conf/hive-env.sh.template $HIVE_HOME/conf/hive-env.sh
# cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml


#log
if [ ! -d "$HIVE_HOME/logs" ]; then
  mkdir -p $HIVE_HOME/logs
fi


#mysql
 if [ ! -f "$HIVE_HOME/lib/mysql-connector-java-5.1.27-bin.jar" ]; then
	cp  $MYSQL_HOME/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar $HIVE_HOME/lib
 fi

$HADOOP_HOME/bin/hdfs dfs -mkdir -p /hive/warehouse
$HADOOP_HOME/bin/hdfs dfs -chmod g+w /tmp
$HADOOP_HOME/bin/hdfs dfs -chmod g+w /hive/warehouse


# hive-env.sh
		# export JAVA_HOME=/opt/modules/jdk1.7.0_67
		# HADOOP_HOME=/opt/modules/hadoop-2.5.0
		# export HIVE_CONF_DIR=/opt/modules/hive/apache-hive-0.13.1-bin/conf



# hive-site.xml
	# <property>
		# <name>hive.metastore.warehouse.dir</name>
		# <value>/user/hive/warehouse</value>
	# </property>
# <property>
	  # <name>javax.jdo.option.ConnectionURL</name>
	  # <value>jdbc:mysql://hadoop01.ibeifeng.com:3306/metastore?createDatabaseIfNotExist=true</value>
	  # <description>JDBC connect string for a JDBC metastore</description>
	# </property>

	# <property>
	  # <name>javax.jdo.option.ConnectionDriverName</name>
	  # <value>com.mysql.jdbc.Driver</value>
	  # <description>Driver class name for a JDBC metastore</description>
	# </property>

	# <property>
	  # <name>javax.jdo.option.ConnectionUserName</name>
	  # <value>root</value>
	  # <description>username to use against metastore database</description>
	# </property>

	# <property>
	  # <name>javax.jdo.option.ConnectionPassword</name>
	  # <value>zrr123</value>
	  # <description>password to use against metastore database</description>
	# </property>

	# <property>
	# <name>hive.cli.print.header</name>
	# <value>true</value>
	# <description>Whether to print the names of the columns in query output.</description>
	# </property>

	# <property>
	  # <name>hive.cli.print.current.db</name>
	  # <value>true</value>
	  # <description>Whether to include the current database in the Hive prompt.</description>
	# </property>

# <property>
  # <name>hive.metastore.warehouse.dir</name>
  # <value>/user/hive/warehouse</value>
  # <description>location of default database for the warehouse</description>
# </property>

# hive-log4j.properties
# hive.root.logger=INFO,DRFA
	# hive.log.dir=/opt/modules/hive/apache-hive-0.13.1-bin/logs
	# hive.log.file=hive.log


####################hwi####################
















==========================
Hive 1.2.1安装

spark 2.0，默认是跟hive 1.2.1进行整合的，所以之前我们安装的是hive 0.13.1是不Ok的，实际跑的时候会出现hive 0.13支持的一些操作，spark 2.0会用自己内置的hive 1.2.1 lib去操作和访问我们的hive 0.13（包括metastore service），出现版本不一致的问题

1、将/usr/local/hive删除
2、将apache-hive-1.2.1-bin.tar.gz使用WinSCP上传到spark1的/usr/local目录下。
3、解压缩hive安装包：tar -zxvf apache-hive-1.2.1-bin.tar.gz。
4、重命名hive目录：mv apache-hive-1.2.1-bin hive
5、cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib


mv hive-default.xml.template hive-site.xml
vi hive-site.xml
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://spark2upgrade01:3306/hive_metadata?createDatabaseIfNotExist=true</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://spark2upgrade01:9083</value>
</property>

把hive-site.xml中所有${system:java.io.tmpdir}全部替换为/usr/local/hive/iotmp
把hive-site.xml中所有${system:user.name}全部替换为root

rm -rf /usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar
cp /usr/local/hive/lib/jline-2.12.jar /usr/local/hadoop/share/hadoop/yarn/lib

mv hive-env.sh.template hive-env.sh
vi /usr/local/hive/bin/hive-config.sh
export JAVA_HOME=/usr/java/latest
export HIVE_HOME=/usr/local/hive
export HADOOP_HOME=/usr/local/hadoop

1、将hive-site.xml放置到spark的conf目录下
2、启动hive metastore service
hive --service metastore &

1、创建一份文件，students.txt，每行是一个学生的信息
2、CREATE TABLE students(name string, age int, score double)
3、LOAD DATA LOCAL INPATH '/usr/local/test_data/students.txt' INTO TABLE students
4、spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m
5、在spark-shell中，运行针对hive的sql语句
spark.sql(“select * from students”).show();
spark.sql(“select name from students where score>=90;”).show();
spark.sql(“select name from students where age<=15;”).show();
6、观察是否有正确的结果
7、在spark web ui中检查是否有运行的作业记录





=================

Spark从入门到精通
Spark 2.0-开发环境搭建：Eclipse+Maven+Scala+Spark

Maven
scala 2.11

Eclipse
安装eclipse scala插件：http://download.scala-ide.org/sdk/helium/e38/scala211/stable/site
安装eclipse m2e connector插件：http://download.eclipse.org/technology/m2e/releases
add scala library

Spark 2.0 Dependency




手动创建一个课程的工程，作为演示
maven project

org.scala-tools.archetypes
scala-archetype-simple
1.2

<inceptionYear>2015</inceptionYear>
<properties>
    <scala.version>2.11.7</scala.version>
</properties>






CentOS 6.5集群搭建
虚拟机安装

1、使用课程提供的CentOS 6.5镜像即可，CentOS-6.5-i386-minimal.iso。
2、创建虚拟机：打开Virtual Box，点击“新建”按钮，点击“下一步”，输入虚拟机名称为spark2upgrade01，选择操作系统为Linux，选择版本为Red Hat，分配4096MB内存，后面的选项全部用默认，在Virtual Disk File location and size中，一定要自己选择一个目录来存放虚拟机文件，最后点击“create”按钮，开始创建虚拟机。
3、设置虚拟机网卡：选择创建好的虚拟机，点击“设置”按钮，在网络一栏中，连接方式中，选择“Bridged Adapter”。
4、安装虚拟机中的CentOS 6.5操作系统：选择创建好的虚拟机，点击“开始”按钮，选择安装介质（即本地的CentOS 6.5镜像文件），选择第一项开始安装-Skip-欢迎界面Next-选择默认语言-Baisc Storage Devices-Yes, discard any data-主机名:spark2upgrade01-选择时区-设置初始密码为hadoop-Replace Existing Linux System-Write changes to disk-CentOS 6.5自己开始安装。
5、安装完以后，CentOS会提醒你要重启一下，就是reboot，你就reboot就可以了。


CentOS 6.5集群搭建
配置网络

vi /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=dhcp
service network restart
ifconfig

BOOTPROTO=static
IPADDR=192.168.0.X
NETMASK=255.255.255.0
GATEWAY=192.168.0.1
service network restart

vi /etc/hosts
配置本机的hostname到ip地址的映射

此时就可以使用SecureCRT从本机连接到虚拟机进行操作了

CentOS 6.5集群搭建
关闭防火墙
service iptables stop
service ip6tables stop
chkconfig iptables off
chkconfig ip6tables off

vi /etc/selinux/config
SELINUX=disabled

关闭windows的防火墙





CentOS 6.5集群搭建
安装yum


yum clean all
yum makecache
yum install telnet

CentOS 6.5集群搭建
安装JDK 1.7


1、将jdk-7u60-linux-i586.rpm通过WinSCP上传到虚拟机中
2、安装JDK：rpm -ivh jdk-7u65-linux-i586.rpm
3、配置jdk相关的环境变量
vi .bashrc
export JAVA_HOME=/usr/java/latest
export PATH=$PATH:$JAVA_HOME/bin
source .bashrc
4、测试jdk安装是否成功：java -version




CentOS 6.5集群搭建
安装另外两台虚拟机

1、按照上述步骤，再安装两台一模一样环境的虚拟机，唯一的区别是内存为1024MB。
2、另外两台机器的hostname分别设置为spark2upgrade02和spark2upgrade03即可。
3、在安装的时候，另外两台虚拟机的centos镜像文件必须重新拷贝一份，使用自己的镜像文件。
4、虚拟机的硬盘文件也重新选择一个新的目录。
5、安装好之后，要在三台机器的/etc/hosts文件中，配置全三台机器的ip地址到hostname的映射
6、在windows的hosts文件中也要配置全三台机器的ip地址到hostname的映射。




CentOS 6.5集群搭建
配置集群的SSH免密码通信

1、首先在三台机器上配置对本机的ssh免密码登录
ssh-keygen -t rsa
生成本机的公钥，过程中不断敲回车即可，ssh-keygen命令默认会将公钥放在/root/.ssh目录下
cd /root/.ssh
cp id_rsa.pub authorized_keys
将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了

2、接着配置三台机器互相之间的ssh免密码登录
使用ssh-copy-id -i spark命令将本机的公钥拷贝到指定机器的authorized_keys文件中



Hadoop 2.4集群搭建
部署hadoop安装包
1、使用课程提供的hadoop-2.4.1.tar.gz，使用WinSCP上传到CentOS的/usr/local目录下。
2、将hadoop包进行解压缩：tar -zxvf hadoop-2.4.1.tar.gz
3、对hadoop目录进行重命名：mv hadoop-2.4.1 hadoop
4、配置hadoop相关环境变量
vi .bashrc
export HADOOP_HOME=/usr/local/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source .bashrc
5、在/usr/local目录下创建data目录




Hadoop 2.4集群搭建
修改core-site.xml配置文件



<property>
  <name>fs.default.name</name>
  <value>hdfs://spark2upgrade01:9000</value>
</property>

修改hdfs-site.xml配置文件

<property>
  <name>dfs.name.dir</name>
  <value>/usr/local/data/namenode</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/usr/local/data/datanode</value>
</property>
<property>
  <name>dfs.tmp.dir</name>
  <value>/usr/local/data/tmp</value>
</property>
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>



Hadoop 2.4集群搭建
修改mapred-site.xml配置文件
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

Hadoop 2.4集群搭建
修改yarn-site.xml配置文件

<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>spark2upgrade01</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>


Hadoop 2.4集群搭建
修改slaves配置文件
spark2upgrade01
spark2upgrade02
spark2upgrade03



Hadoop 2.4集群搭建
在另外两台机器上部署hadoop

1、使用scp命令将spark2upgrade01上面的hadoop安装包和.bashrc配置文件都拷贝过去。
2、要记得对.bashrc文件进行source，以让它生效。
3、记得在另外两台机器的/usr/local目录下创建data目录。





Hadoop 2.4集群搭建
启动hdfs集群

1、格式化namenode：在spark2upgrade01上执行以下命令hdfs namenode -format
2、启动hdfs集群：start-dfs.sh3、验证启动是否成功：jps、50070端口
spark2upgrade01：namenode、datanode、secondarynamenode
spark2upgrade02：datanode
spark2upgrade03：datanode




Hadoop 2.4集群搭建
启动yarn集群
1、启动yarn集群：start-yarn.sh2、验证启动是否成功：jps、8088端口
spark2upgrade01：resourcemanager、nodemanager
spark2upgrade02：nodemanager
spark2upgrade03：nodemanager







Hive 0.13搭建
部署hive安装包

1、将课程提供的apache-hive-0.13.1-bin.tar.gz使用WinSCP上传到spark1的/usr/local目录下。
2、解压缩hive安装包：tar -zxvf apache-hive-0.13.1-bin.tar.gz。
3、重命名hive目录：mv apache-hive-0.13.1-bin hive
4、配置hive相关的环境变量
vi .bashrc
export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin
source .bashrc






安装mysql


1、在spark2upgrade01上安装mysql。
2、使用yum安装mysql server。
yum install -y mysql-server
service mysqld start
chkconfig mysqld on
3、使用yum安装mysql connector
yum install -y mysql-connector-java
4、将mysql connector拷贝到hive的lib包中
cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib
5、在mysql上创建hive元数据库，并对hive进行授权
create database if not exists hive_metadata;
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'spark2upgrade01' identified by 'hive';
flush privileges;
use hive_metadata;






修改hive-site.xml配置文件

mv hive-default.xml.template hive-site.xml
vi hive-site.xml
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://spark2upgrade01:3306/hive_metadata?createDatabaseIfNotExist=true</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>



配置hive-env.sh和hive-config.sh

mv hive-env.sh.template hive-env.sh

vi /usr/local/hive/bin/hive-config.sh
export JAVA_HOME=/usr/java/latest
export HIVE_HOME=/usr/local/hive
export HADOOP_HOME=/usr/local/hadoop




验证安装是否成功


直接输入hive命令，可以进入hive命令行


Spark 2.0集群搭建
安装scala 2.11

1、将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到spark1的/usr/local目录下。
2、对scala-2.11.4.tgz进行解压缩：tar -zxvf scala-2.11.4.tgz。
3、对scala目录进行重命名：mv scala-2.11.4 scala
4、配置scala相关的环境变量
vi .bashrc
export SCALA_HOME=/usr/local/scala
export PATH=$SCALA_HOME/bin
source .bashrc
5、查看scala是否安装成功：scala -version
6、在另外两台机器上都安装scala，使用scp将scala和.bashrc拷贝到过去即可。


部署spark 2.0安装包


1、将spark-2.0.0-bin-hadoop2.4.tgz使用WinSCP上传到/usr/local目录下。自己在http://spark.apache.org/downloads.html上下载即可。
2、解压缩spark包：tar zxvf spark-2.0.0-bin-hadoop2.4.tgz。
3、更改spark目录名：mv spark-2.0.0-bin-hadoop2.4.tgz spark
4、设置spark环境变量
vi .bashrc
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib
source .bashrc

配置spark-env.sh
1、cd /usr/local/spark/conf
2、cp spark-env.sh.template spark-env.sh
3、vi spark-env.sh
export JAVA_HOME=/usr/java/latest
export SCALA_HOME=/usr/local/scala
export SPARK_MASTER_HOST=spark2upgrade01
export SPARK_WORKER_MEMORY=500m
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop




配置slaves
spark2upgrade01
spark2upgrade02
spark2upgrade03





在另外两台机器上部署spark 2.0


在另外两个节点进行一模一样的配置，使用scp将spark和.bashrc拷贝到过去即可。




配置spark可以使用hive

1、将hive-site.xml放置到spark的conf目录下
2、修改spark/conf和hive/conf下的hive-site.xml
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://spark2upgrade01:9083</value>
</property>
3、启动hive metastore service
hive --service metastore &
4、cp hive/lib/mysql-connector-java-5.1.17.jar spark/jars/
5、hdfs dfs -chmod 777 /tmp/hive-root



启动spark集群
1、在spark目录下的sbin目录
2、执行./start-all.sh
3、使用jsp和8080端口可以检查集群是否启动成功
4、进入spark-shell查看是否正常




检查spark集群能否与hdfs整合使用


1、使用 spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m启动
2、手工创建一份文件，上传到hdfs上去，放在/test_data/wordcount.txt中
3、在spark-shell中，基于hdfs上的文件，编写与运行一个wordcount程序
val lines = sc.textFile(“hdfs://spark2upgrade01:9000/test_data/wordcount.txt”)
val words = lines.flatMap(_.split(“ ”))
val pairs = words.map((_, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.collect().foreach(println(_))
4、观察是否有正确的结果
5、在spark web ui中检查是否有运行的作业记录




检查spark集群能否与hive整合使用
1、创建一份文件，students.txt，每行是一个学生的信息
2、CREATE TABLE students(name string, age int, score double)
3、LOAD DATA LOCAL INPATH '/usr/local/test_data/students.txt' INTO TABLE students
4、spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m
5、在spark-shell中，运行针对hive的sql语句
spark.sql(“select * from students”).show();
spark.sql(“select name from students where score>=90;”).show();
spark.sql(“select name from students where age<=15;”).show();
6、观察是否有正确的结果
7、在spark web ui中检查是否有运行的作业记录



































