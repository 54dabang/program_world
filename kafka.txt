第13章Kafka设计理念与基本架构
13.1Kafka产生的背景
13.2消息队列系统
13.2.1概述
13.2.2常用的消息队列系统对比
13.2.3Kafka特点及特性
13.2.4Kafka系统应用场景
13.3Kafka设计理念
13.3.1专业术语解析
13.3.2消息存储与缓存设计
13.3.3消费者与生产者模型
13.3.4Push与Pull机制
13.3.5镜像机制
13.4Kafka整体架构
13.4.1Kafka基本组成结构
13.4.2Kafka工作流程
13.5Kafka性能分析及优化
13.6Kafka未来研究方向
13.7小结
第14章Kafka核心组件及核心特性剖析
14.1Kafka核心组件剖析
14.1.1Producers
14.1.2Consumers
14.1.3Low Level Consumer
14.1.4High Level Consumer
14.2Kafka核心特性剖析
14.2.1Topic、Partitions
14.2.2Replication和Leader Election
14.2.3Consumer Rebalance
14.2.4消息传送机制
14.2.5Kafka的可靠性
14.2.6Kafka的高效性
14.3Kafka即将发布版本核心组件及特性剖析
14.3.1重新设计的Consumer
14.3.2Coordinator Rebalance
14.4小结
第15章Kafka应用实践
15.1Kafka开发环境搭建及运行环境部署
15.1.1Kafka开发环境配置
15.1.2Kafka运行环境安装与部署
15.2基于Kafka客户端开发
15.2.1消息生产者（Producer）设计
15.2.2消息消费者（Consumer）设计
15.2.3Kafka消费者与生产者配置

．1　基本概念
1．2　安装与配置
1．3　生产与消费
1．4　服务端参数配置
1．5　总结
第2章　生产者
2．1　客户端开发
2．1．1　必要的参数配置
2．1．2　消息的发送
2．1．3　序列化
2．1．4　分区器
2．1．5　生产者拦截器
2．2　原理分析
2．2．1　整体架构
2．2．2　元数据的更新
2．3　重要的生产者参数
2．4　总结
第3章　消费者
3．1　消费者与消费组
3．2　客户端开发
3．2．1　必要的参数配置
3．2．2　订阅主题与分区
3．2．3　反序列化
3．2．4　消息消费
3．2．5　位移提交
3．2．6　控制或关闭消费
3．2．7　指定位移消费
3．2．8　再均衡
3．2．9　消费者拦截器
3．2．10　多线程实现
3．2．11　重要的消费者参数
3．3　总结
第4章　主题与分区
4．1　主题的管理
4．1．1　创建主题
4．1．2　分区副本的分配
4．1．3　查看主题
4．1．4　修改主题
4．1．5　配置管理
4．1．6　主题端参数
4．1．7　删除主题
4．2　初识KafkaAdminClient
4．2．1　基本使用
4．2．2　主题合法性验证
4．3　分区的管理
4．3．1　优先副本的选举
4．3．2　分区重分配
4．3．3　复制限流
4．3．4　修改副本因子
4．4　如何选择合适的分区数
4．4．1　性能测试工具
4．4．2　分区数越多吞吐量就越高吗
4．4．3　分区数的上限
4．4．4　考量因素
4．5　总结
第5章　日志存储
5．1　文件目录布局
5．2　日志格式的演变
5．2．1　v0版本
5．2．2　v1版本
5．2．3　消息压缩
5．2．4　变长字段
5．2．5　v2版本
5．3　日志索引
5．3．1　偏移量索引
5．3．2　时间戳索引
5．4　日志清理
5．4．1　日志删除
5．4．2　日志压缩
5．5　磁盘存储
5．5．1　页缓存
5．5．2　磁盘I/O流程
5．5．3　零拷贝
5．6　总结
第6章　深入服务端
6．1　协议设计
6．2　时间轮
6．3　延时操作
6．4　控制器
6．4．1　控制器的选举及异常恢复
6．4．2　优雅关闭
6．4．3　分区leader的选举
6．5　参数解密
6．5．1　broker．id
6．5．2　bootstrap．servers
6．5．3　服务端参数列表
6．6　总结
第7章　深入客户端
7．1　分区分配策略
7．1．1　RangeAssignor分配策略
7．1．2　RoundRobinAssignor分配策略
7．1．3　StickyAssignor分配策略
7．1．4　自定义分区分配策略
7．2　消费者协调器和组协调器
7．2．1　旧版消费者客户端的问题
7．2．2　再均衡的原理
7．3　__consumer_offsets剖析
7．4　事务
7．4．1　消息传输保障
7．4．2　幂等
7．4．3　事务
7．5　总结
第8章　可靠性探究
8．1　副本剖析
8．1．1　失效副本
8．1．2　ISR的伸缩
8．1．3　LEO与HW
8．1．4　Leader Epoch的介入
8．1．5　为什么不支持读写分离
8．2　日志同步机制
8．3　可靠性分析
8．4　总结
第9章　Kafka应用
9．1　命令行工具
9．1．1　消费组管理
9．1．2　消费位移管理
9．1．3　手动删除消息
9．2　Kafka Connect
9．2．1　独立模式
9．2．2　REST API
9．2．3　分布式模式
9．3　Kafka Mirror Maker
9．4　Kafka Streams
9．5　总结
第10章　Kafka监控
10．1　监控数据的来源
10．1．1　OneMinuteRate
10．1．2　获取监控指标
10．2　消费滞后
10．3　同步失效分区
10．4　监控指标说明
10．5　监控模块
10．6　总结
第11章　高级应用
11．1　过期时间（TTL）
11．2　延时队列
11．3　死信队列和重试队列
11．4　消息路由
11．5　消息轨迹
11．6　消息审计
11．7　消息代理
11．7．1　快速入门
11．7．2　REST API介绍及示例
11．7．3　服务端配置及部署
11．7．4　应用思考
11．8　消息中间件选型
11．8．1　各类消息中间件简述
11．8．2　选型要点概述
11．8．3　消息中间件选型误区探讨
11．9　总结
第12章　Kafka与Spark的集成
12．1　Spark的安装及简单应用
12．2　Spark编程模型
12．3　Spark的运行结构
12．4　Spark Streaming简介
12．5　Kafka与Spark Streaming的整合
12．6　Spark SQL
12．7　Structured Streaming
12．8　Kafka与Structured Streaming的整合

val df= spark                                                                                                                

.readStream

.format("kafka")

.option("kafka.bootstrap.servers","host1:port1,host2:port2")

.option("subscribe","topic1")

.load()

df.selectExpr("CAST(key AS STRING)","CAST(value AS STRING)")

.as[(String,String)]

// Subscribe to multiple topics

val df= spark

.readStream

.format("kafka")

.option("kafka.bootstrap.servers","host1:port1,host2:port2")

.option("subscribe","topic1,topic2")

.load()

df.selectExpr("CAST(key AS STRING)","CAST(value AS STRING)")

.as[(String,String)]

// Subscribe to a pattern

val df= spark

.readStream

.format("kafka")

.option("kafka.bootstrap.servers","host1:port1,host2:port2")

.option("subscribePattern","topic.*")

.load()

df.selectExpr("CAST(key AS STRING)","CAST(value AS STRING)")

.as[(String,String)]


valschema=StructType(Seq(

StructField("schema",StringType, true),

StructField("payload",StringType, true)

))

 
 
valdf= ds1.selectExpr("cast (value as string) as json")

.select(from_json($"json",schema=schema).as("data"))

.select("data.payload")


package com.test

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession

object CommonStructuedKafka {
  def main(args: Array[String]): Unit = {

    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    Logger.getLogger("org.apache.kafka").setLevel(Level.WARN)

    // 读取配置文件信息
    val masterUrl = Props.get("master", "local")
    val appName = Props.get("appName", "Test7")
    val className = Props.get("className", "")
    val kafkaBootstrapServers = Props.get("kafka.bootstrap.servers", "localhost:9092")
    val subscribe = Props.get("subscribe", "test")
    val tmpTable = Props.get("tmpTable", "tmp")
    val sparksql = Props.get("sparksql", "select * from tmp")


    val spark = SparkSession.builder()
      .master(masterUrl)
      .appName(appName)
      .getOrCreate()


    // 读取kafka数据
    val lines = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", kafkaBootstrapServers)
      .option("subscribe", subscribe)
      .load()

    //隐式转换
    import spark.implicits._

    val values = lines.selectExpr("cast(value as string)").as[String]

    val res = values.map { value =>
      // 将json数据解析成list集合
      val list = Tools.parseJson(value, className)
      // 将List转成元组
      Tools.list2Tuple7(list)
    }

    res.createOrReplaceTempView(tmpTable)

    val result = spark.sql(sparksql)

    val query = result.writeStream
      .format("console")
      .outputMode("append")
      .start()

    query.awaitTermination()
  }
}


/** 
  * Created by dongyunlong on 2018/1/5. 
  * {"a":"1","b":"2","c":"2018-01-08","d":[23.9,45]} 
  */  
object SparkStructuredStreaming {  
   
  /** 
    * 创建SparkSession 
    * @return 
    */  
  def getSparkSession={  
    SparkSession  
      .builder()  
      .config(new SparkConf().setMaster("local[2]"))  
      .appName(getClass.getName)  
      .getOrCreate()  
  }  
   
  /** 
    * 解析kafka json数据，并将其映射为spark临时表 
    * @param spark 
    * @param kafkaTopic 
    * @param sourceName 
    */  
  def createOrReplaceTempView(spark:SparkSession, kafkaTopic:String, sourceName:String): Unit ={  
    import spark.implicits._  
    val df = spark  
      .readStream  
      .format("kafka")  
      .option("kafka.bootstrap.servers", "XX.XX.XX.XX:9092")  
      .option("subscribe", kafkaTopic)  
      .option("startingOffsets", "earliest")  
    .load()  
//    val schema = SocSchemaCollection.getSchemaBySourceName(sourceName) //从数据库加载json schema  
    val schema = StructType(mutable.Seq(  
      StructField("a", DataTypes.StringType),  
      StructField("b", DataTypes.StringType),  
      StructField("c", DataTypes.StringType),  
      StructField("d", DataTypes.createArrayType(DataTypes.StringType))  
    ))  
    if(schema != null){  
      val jsonDf = df.selectExpr("CAST(key AS STRING)", "cast (value as string) as json")  
          .select(from_json($"json", schema=schema).as("data"))  
      jsonDf.select("data.*").createOrReplaceTempView(sourceName)  
    }else{  
      println("error,schema is null")  
    }  
  }  
   
  /** 
    * 输出spark sql的查询结果 
    * @param spark 
    * @param sql 
    * @return 
    */  
  def sqlWriteStream(spark:SparkSession, sql:String): StreamingQuery ={  
    val query = spark.sql(sql)  
      .writeStream  
      .outputMode("append")  
      .format("console")  
      .start()  
    query  
  }  
   
  /** 
    * 注册spark临时表，执行sql语句，注意这里每一个sql都是一个writeStream，最后使用spark.streams.awaitAnyTermination()等待所有查询 
    * @param spark 
    */  
  def sparkReadKafka(spark:SparkSession): Unit ={  
    createOrReplaceTempView(spark, "dyl_test01", "dyl_test")  
    val sqls = Array("select * from dyl_test","select *,'2' as e from dyl_test")  
    val querys = mutable.ListBuffer[StreamingQuery]()  
    for(sql <- sqls){  
      println(sql)  
      querys += sqlWriteStream(spark, sql)  
    }  
  }  
   
  /** 
    * 主函数 
    * @param args 
    */  
  def main(args: Array[String]) {  
    println("hello world")  
    val spark = getSparkSession  
    sparkReadKafka(spark)  
    spark.streams.awaitAnyTermination()  
  }  
}  


import java.io.{FileInputStream, InputStream}
import java.nio.file.{Files, Paths}
import java.util.Properties

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

object Props {
  private val prop = new Properties()

  prop.load(getPropertyFileInputStream)

  /**
    * 在spark-submit中加入--driver-java-options -DPropPath=/home/spark/prop.properties的参数后，
    * 使用System.getProperty("PropPath")就能获取路径：/home/spark/prop.properties如果spark-submit中指定了
    * prop.properties文件的路径，那么使用prop.properties中的属性，否则使用该类中定义的属性
    */
  private def getPropertyFileInputStream: InputStream = {
    var is: InputStream = null
    val filePath = System.getProperty("PropPath")
    if (filePath != null && filePath.length > 0) {
      if (Files.exists(Paths.get(filePath))) {
        is = new FileInputStream(filePath)
      } else {
        println(s"在本地未找到config文件$filePath，尝试在HDFS上获取文件")
        val fs = FileSystem.get(new Configuration())
        if (fs.exists(new Path(filePath))) {
          val fis = fs.open(new Path(filePath))
          is = fis.getWrappedStream
        } else {
          println(s"在HDFS上找不到config文件$filePath，加载失败...")
        }
      }
    } else {
      println(s"未设置配置文件PropPath")
    }
    is
  }


  def get(propertyName: String, defaultValue: String): String = {
    prop.getProperty(propertyName, defaultValue)
  }


  def get(): Properties = {
    println("prop:" + this.prop)
    this.prop
  }


  def reload(): Properties = {
    prop.load(getPropertyFileInputStream)
    prop
  }
}


import com.google.gson.Gson

import scala.collection.mutable


object Tools {

  def main(args: Array[String]): Unit = {
    val tools = new Tools()
    val res = tools.parse("{'name':'caocao','age':'32','sex':'male'}", "com.test.People")
    println(res)
  }

  def parseJson(json: String, className: String): List[String] = {
    val tools = new Tools()
    tools.parse(json, className)
  }

  // 将List转成Tuple7元组类，这里仅仅是定义7个字段，可以定义更多字段。（ps：这种处理方式很不雅，一时也没想到好办法）
  def list2Tuple7(list: List[String]): (String, String, String, String, String, String, String) = {
    val t = list match {
      case List(a) => (a, "", "", "", "", "", "")
      case List(a, b) => (a, b, "", "", "", "", "")
      case List(a, b, c) => (a, b, c, "", "", "", "")
      case List(a, b, c, d) => (a, b, c, d, "", "", "")
      case List(a, b, c, d, e) => (a, b, c, d, e, "", "")
      case List(a, b, c, d, e, f) => (a, b, c, d, e, f, "")
      case List(a, b, c, d, e, f, g) => (a, b, c, d, e, f, g)
      case _ => ("", "", "", "", "", "", "")
    }
    t
  }
}


class Tools {
  // 通过传进来的Bean的全类名，进行反射，解析json，返回一个List()
  def parse(json: String, className: String): List[String] = {
    val list = mutable.ListBuffer[String]()
    val gson = new Gson()
    val clazz = Class.forName(className)
    val obj = gson.fromJson(json, clazz)
    val aClass = obj.getClass
    val fields = aClass.getDeclaredFields
    fields.foreach { f =>
      val fName = f.getName
      val m = aClass.getDeclaredMethod(fName)
      val value = m.invoke(obj).toString
      list.append(value)
    }
    list.toList
  }
}






12．9　总结
附录A　Kafka源码环境搭建

1.1.1 下载并解压缩Kafka二进制代码压缩包文件
1.1.2 启动服务器
1.1.3 创建topic
1.1.4 发送消息
1.1.5 消费消息
1.2 消息引擎系统
1.2.1 消息设计
1.2.2 传输协议设计
1.2.3 消息引擎范型
1.2.4 Java消息服务
1.3 Kafka概要设计
1.3.1 吞吐量/延时
1.3.2 消息持久化
1.3.3 负载均衡和故障转移
1.3.4 伸缩性
1.4 Kafka基本概念与术语
1.4.1 消息
1.4.2 topic和partition
1.4.3 offset
1.4.4 replica
1.4.5 leader和follower
1.4.6 ISR
1.5 Kafka使用场景
1.5.1 消息传输
1.5.2 网站行为日志追踪
1.5.3 审计数据收集
1.5.4 日志收集
1.5.5 Event Sourcing
1.5.6 流式处理
1.6 本章小结
第2章 Kafka发展历史
2.1 Kafka的历史
2.1.1 背景
2.1.2 Kafka横空出世
2.1.3 Kafka开源
2.2 Kafka版本变迁
2.2.1 Kafka的版本演进
2.2.2 Kafka的版本格式
2.2.3 新版本功能简介
2.2.4 旧版本功能简介
2.3 如何选择Kafka版本
2.3.1 根据功能场景
2.3.2 根据客户端使用场景
2.4 Kafka与Confluent
2.5 本章小结
第3章 Kafka线上环境部署
3.1 集群环境规划
3.1.1 操作系统的选型
3.1.2 磁盘规划
3.1.3 磁盘容量规划
3.1.4 内存规划
3.1.5 CPU规划
3.1.6 带宽规划
3.1.7 典型线上环境配置
3.2 伪分布式环境安装
3.2.1 安装Java
3.2.2 安装ZooKeeper
3.2.3 安装单节点Kafka集群
3.3 多节点环境安装
3.3.1 安装多节点ZooKeeper集群
3.3.2 安装多节点Kafka
3.4 验证部署
3.4.1 测试topic创建与删除
3.4.2 测试消息发送与消费
3.4.3 生产者吞吐量测试
3.4.4 消费者吞吐量测试
3.5 参数设置
3.5.1 broker端参数
3.5.2 topic级别参数
3.5.3 GC参数
3.5.4 JVM参数
3.5.5 OS参数
3.6 本章小结
第4章 producer开发
4.1 producer概览
4.2 构造producer
4.2.1 producer程序实例
4.2.2 producer主要参数
4.3 消息分区机制
4.3.1 分区策略
4.3.2 自定义分区机制
4.4 消息序列化
4.4.1 默认序列化
4.4.2 自定义序列化
4.5 producer拦截器
4.6 无消息丢失配置
4.6.1 producer端配置
4.6.2 broker端配置
4.7 消息压缩
4.7.1 Kafka支持的压缩算法
4.7.2 算法性能比较与调优
4.8 多线程处理
4.9 旧版本producer
4.10 本章小结
第5章 consumer开发
5.1 consumer概览
5.1.1 消费者（consumer）
5.1.2 消费者组（consumer group）
5.1.3 位移（offset）
5.1.4 位移提交
5.1.5__consumer_offsets
5.1.6 消费者组重平衡（consumer group rebalance）
5.2 构建consumer
5.2.1 consumer程序实例
5.2.2 consumer脚本命令
5.2.3 consumer主要参数
5.3 订阅topic
5.3.1 订阅topic列表
5.3.2 基于正则表达式订阅topic
5.4 消息轮询
5.4.1 poll内部原理
5.4.2 poll使用方法
5.5 位移管理
5.5.1 consumer位移
5.5.2 新版本consumer位移管理
5.5.3 自动提交与手动提交
5.5.4 旧版本consumer位移管理
5.6 重平衡（rebalance）
5.6.1 rebalance概览
5.6.2 rebalance触发条件
5.6.3 rebalance分区分配
5.6.4 rebalance generation
5.6.5 rebalance协议
5.6.6 rebalance流程
5.6.7 rebalance监听器
5.7 解序列化
5.7.1 默认解序列化器
5.7.2 自定义解序列化器
5.8 多线程消费实例
5.8.1 每个线程维护一个KafkaConsumer
5.8.2 单KafkaConsumer实例+多worker线程
5.8.3 两种方法对比
5.9 独立consumer
5.10 旧版本consumer
5.10.1 概览
5.10.2 high-level consumer
5.10.3 low-level consumer
5.11 本章小结
第6章 Kafka设计原理
6.1 broker端设计架构
6.1.1 消息设计
6.1.2 集群管理
6.1.3 副本与ISR设计
6.1.4 水印（watermark）和leader epoch
6.1.5 日志存储设计
6.1.6 通信协议（wire protocol）
6.1.7 controller设计
6.1.8 broker请求处理
6.2 producer端设计
6.2.1 producer端基本数据结构
6.2.2 工作流程
6.3 consumer端设计
6.3.1 consumer group状态机
6.3.2 group管理协议
6.3.3 rebalance场景剖析
6.4 实现精确一次处理语义
6.4.1 消息交付语义
6.4.2 幂等性producer（idempotent producer）
6.4.3 事务（transaction）
6.5 本章小结
第7章 管理Kafka集群
7.1 集群管理
7.1.1 启动broker
7.1.2 关闭broker
7.1.3 设置JMX端口
7.1.4 增加broker
7.1.5 升级broker版本
7.2 topic管理
7.2.1 创建topic
7.2.2 删除topic
7.2.3 查询topic列表
7.2.4 查询topic详情
7.2.5 修改topic
7.3 topic动态配置管理
7.3.1 增加topic配置
7.3.2 查看topic配置
7.3.3 删除topic配置
7.4 consumer相关管理
7.4.1 查询消费者组
7.4.2 重设消费者组位移
7.4.3 删除消费者组
7.4.4 kafka-consumer-offset-checker
7.5 topic分区管理
7.5.1 preferred leader选举
7.5.2 分区重分配
7.5.3 增加副本因子
7.6 Kafka常见脚本工具
7.6.1 kafka-console-producer脚本
7.6.2 kafka-console-consumer脚本
7.6.3 kafka-run-class脚本
7.6.4 查看消息元数据
7.6.5 获取topic当前消息数
7.6.6 查询__consumer_offsets
7.7 API方式管理集群
7.7.1 服务器端API管理topic
7.7.2 服务器端API管理位移
7.7.3 客户端API管理topic
7.7.4 客户端API查看位移
7.7.5 0.11.0.0版本客户端API
7.8 MirrorMaker
7.8.1 概要介绍
7.8.2 主要参数
7.8.3 使用实例
7.9 Kafka安全
7.9.1 SASL+ACL
7.9.2 SSL加密
7.10 常见问题
7.11 本章小结
第8章 监控Kafka集群
8.1 集群健康度检查
8.2 MBean监控
8.2.1 监控指标
8.2.2 指标分类
8.2.3 定义和查询JMX端口
8.3 broker端JMX监控
8.3.1 消息入站/出站速率
8.3.2 controller存活JMX指标
8.3.3 备份不足的分区数
8.3.4 leader分区数
8.3.5 ISR变化速率
8.3.6 broker I/O工作处理线程空闲率
8.3.7 broker网络处理线程空闲率
8.3.8 单个topic总字节数
8.4 clients端JMX监控
8.4.1 producer端JMX监控
8.4.2 consumer端JMX监控
8.5 JVM监控
8.5.1 进程状态
8.5.2 GC性能
8.6 OS监控
8.7 主流监控框架
8.7.1 JmxTool
8.7.2 kafka-manager
8.7.3 Kafka Monitor
8.7.4 Kafka Offset Monitor
8.7.5 CruiseControl
8.8 本章小结
第9章 调优Kafka集群
9.1 引言
9.2 确定调优目标
9.3 集群基础调优
9.3.1 禁止atime更新
9.3.2 文件系统选择
9.3.3 设置swapiness
9.3.4 JVM设置
9.3.5 其他调优
9.4 调优吞吐量
9.5 调优延时
9.6 调优持久性
9.7 调优可用性
9.8 本章小结
第10章 Kafka Connect与Kafka Streams
10.1 引言
10.2 Kafka Connect
10.2.1 概要介绍
10.2.2 standalone Connect
10.2.3 distributed Connect
10.2.4 开发connector
10.3 Kafka Streams
10.3.1 流处理
10.3.2 Kafka Streams核心概念
10.3.3 Kafka Streams与其他框架的异同
10.3.4 Word Count实例
10.3.5 Kafka Streams应用开发
10.3.6 Kafka Streams状态查询
第1章　初识 Kafka
1.1　发布与订阅消息系统
1.2　Kafka登场
1.3　为什么选择Kafka
1.4　数据生态系统
1.5　起源故事
1.6　开始Kafka之旅
第2章　安装 Kafka
2.1　要事先行
2.2　安装Kafka Broker
2.3　broker配置
2.4　硬件的选择
2.5　云端的Kafka
2.6　Kafka集群
2.7　生产环境的注意事项
2.8　总结
第3章　Kafka 生产者——向 Kafka 写入数据
3.1　生产者概览
3.2　创建Kafka生产者
3.3　发送消息到Kafka
3.4　生产者的配置
3.5　序列化器
3.6　分区
3.7　旧版的生产者API
3.8　总结
第4章　Kafka 消费者——从 Kafka 读取数据
4.1　KafkaConsumer概念
4.2　创建Kafka消费者
4.3　订阅主题
4.4　轮询
4.5　消费者的配置
4.6　提交和偏移量
4.7　再均衡监听器
4.8　从特定偏移量处开始处理记录
4.9　如何退出
4.10　反序列化器
4.11　独立消费者——为什么以及怎样使用没有群组的消费者
4.12　旧版的消费者API
4.13　总结
第5章　深入 Kafka
5.1　集群成员关系
5.2　控制器
5.3　复制
5.4　处理请求
5.5　物理存储
5.6　总结
第6章　可靠的数据传递
6.1　可靠性保证
6.2　复制
6.3　broker配置
6.4　在可靠的系统里使用生产者
6.5　在可靠的系统里使用消费者
6.6　验证系统可靠性
6.7　总结
第7章　构建数据管道
7.1　构建数据管道时需要考虑的问题
7.2　如何在Connect API和客户端API之间作出选择
7.3　Kafka Connect
7.4　Connect之外的选择
7.5　总结
第8章　跨集群数据镜像
8.1　跨集群镜像的使用场景
8.2　多集群架构
8.3　Kafka的MirrorMaker
8.4　其他跨集群镜像方案
8.5　总结
第9章　管理 Kafka
9.1　主题操作
9.2　消费者群组
9.3　动态配置变更
9.4　分区管理
9.5　消费和生产
9.6　客户端ACL
9.7　不安全的操作
9.8　总结
第10章　监控 Kafka
10.1　度量指标基础
10.2　broker的度量指标
10.3　客户端监控
10.4　延时监控
10.5　端到端监控
10.6　总结
第11章　流式处理
11.1　什么是流式处理
11.2　流式处理的一些概念
11.3　流式处理的设计模式
11.4　Streams示例
11.5　Kafka Streams的架构概览
11.6　流式处理使用场景
11.7　如何选择流式处理框架
11.8　总结
附录A　在其他操作系统上安装 Kafka
A.1　在Windows上安装Kafka
A.2　在MacOS上安装Kafka

1．1　基本概念
1．2　安装与配置
1．3　生产与消费
1．4　服务端参数配置
1．5　总结
第2章　生产者
2．1　客户端开发
2．1．1　必要的参数配置
2．1．2　消息的发送
2．1．3　序列化
2．1．4　分区器
2．1．5　生产者拦截器
2．2　原理分析
2．2．1　整体架构
2．2．2　元数据的更新
2．3　重要的生产者参数
2．4　总结
第3章　消费者
3．1　消费者与消费组
3．2　客户端开发
3．2．1　必要的参数配置
3．2．2　订阅主题与分区
3．2．3　反序列化
3．2．4　消息消费
3．2．5　位移提交
3．2．6　控制或关闭消费
3．2．7　指定位移消费
3．2．8　再均衡
3．2．9　消费者拦截器
3．2．10　多线程实现
3．2．11　重要的消费者参数
3．3　总结
第4章　主题与分区
4．1　主题的管理
4．1．1　创建主题
4．1．2　分区副本的分配
4．1．3　查看主题
4．1．4　修改主题
4．1．5　配置管理
4．1．6　主题端参数
4．1．7　删除主题
4．2　初识KafkaAdminClient
4．2．1　基本使用
4．2．2　主题合法性验证
4．3　分区的管理
4．3．1　优先副本的选举
4．3．2　分区重分配
4．3．3　复制限流
4．3．4　修改副本因子
4．4　如何选择合适的分区数
4．4．1　性能测试工具
4．4．2　分区数越多吞吐量就越高吗
4．4．3　分区数的上限
4．4．4　考量因素
4．5　总结
第5章　日志存储
5．1　文件目录布局
5．2　日志格式的演变
5．2．1　v0版本
5．2．2　v1版本
5．2．3　消息压缩
5．2．4　变长字段
5．2．5　v2版本
5．3　日志索引
5．3．1　偏移量索引
5．3．2　时间戳索引
5．4　日志清理
5．4．1　日志删除
5．4．2　日志压缩
5．5　磁盘存储
5．5．1　页缓存
5．5．2　磁盘I/O流程
5．5．3　零拷贝
第6章　深入服务端
6．1　协议设计
6．2　时间轮
6．3　延时操作
6．4　控制器
6．4．1　控制器的选举及异常恢复
6．4．2　优雅关闭
6．4．3　分区leader的选举
6．5　参数解密
6．5．1　broker．id
6．5．2　bootstrap．servers
6．5．3　服务端参数列表
6．6　总结
第7章　深入客户端
7．1　分区分配策略
7．1．1　RangeAssignor分配策略
7．1．2　RoundRobinAssignor分配策略
7．1．3　StickyAssignor分配策略
7．1．4　自定义分区分配策略
7．2　消费者协调器和组协调器
7．2．1　旧版消费者客户端的问题
7．2．2　再均衡的原理
7．3　__consumer_offsets剖析
7．4　事务
7．4．1　消息传输保障
7．4．2　幂等
7．4．3　事务
第8章　可靠性探究
8．1　副本剖析
8．1．1　失效副本
8．1．2　ISR的伸缩
8．1．3　LEO与HW
8．1．4　Leader Epoch的介入
8．1．5　为什么不支持读写分离
8．2　日志同步机制
8．3　可靠性分析
第9章　Kafka应用
9．1　命令行工具
9．1．1　消费组管理
9．1．2　消费位移管理
9．1．3　手动删除消息
9．2　Kafka Connect
9．2．1　独立模式
9．2．2　REST API
9．2．3　分布式模式
9．3　Kafka Mirror Maker
9．4　Kafka Streams
第10章　Kafka监控
10．1　监控数据的来源
10．1．1　OneMinuteRate
10．1．2　获取监控指标
10．2　消费滞后
10．3　同步失效分区
10．4　监控指标说明
10．5　监控模块
第11章　高级应用
11．1　过期时间（TTL）
11．2　延时队列
11．3　死信队列和重试队列
11．4　消息路由
11．5　消息轨迹
11．6　消息审计
11．7　消息代理
11．7．1　快速入门
11．7．2　REST API介绍及示例
11．7．3　服务端配置及部署
11．7．4　应用思考
11．8　消息中间件选型
11．8．1　各类消息中间件简述
11．8．2　选型要点概述
11．8．3　消息中间件选型误区探讨
11．9　总结
第12章　Kafka与Spark的集成
12．1　Spark的安装及简单应用
12．2　Spark编程模型
12．3　Spark的运行结构
12．4　Spark Streaming简介
12．5　Kafka与Spark Streaming的整合
12．6　Spark SQL
12．7　Structured Streaming
12．8　Kafka与Structured Streaming的整合
附录A　Kafka源码环境搭建

1.1　发布与订阅消息系统　　1
1.1.1　如何开始　　2
1.1.2　独立的队列系统　　3
1.2　Kafka登场　　4
1.2.1　消息和批次　　4
1.2.2　模式　　4
1.2.3　主题和分区　　5
1.2.4　生产者和消费者　　5
1.2.5　broker和集群　　6
1.2.6　多集群　　7
1.3　为什么选择Kafka　　8
1.3.1　多个生产者　　8
1.3.2　多个消费者　　8
1.3.3　基于磁盘的数据存储　　9
1.3.4　伸缩性　　9
1.3.5　高性能　　9
1.4　数据生态系统　　9
1.5　起源故事　　11
1.5.1　LinkedIn的问题　　11
1.5.2　Kafka的诞生　　12
1.5.3　走向开源　　12
1.5.4　命名　　13
1.6　开始Kafka之旅　　13
第2章　安装Kafka　　14
2.1　要事先行　　14
2.1.1　选择操作系统　　14
2.1.2　安装Java　　14
2.1.3　安装Zookeeper　　15
2.2　安装Kafka Broker　　17
2.3　broker配置　　18
2.3.1　常规配置　　18
2.3.2　主题的默认配置　　19
2.4　硬件的选择　　23
2.4.1　磁盘吞吐量　　23
2.4.2　磁盘容量　　23
2.4.3　内存　　23
2.4.4　网络　　24
2.4.5　CPU　　24
2.5　云端的Kafka　　24
2.6　Kafka集群　　24
2.6.1　需要多少个broker　　25
2.6.2　broker配置　　25
2.6.3　操作系统调优　　26
2.7　生产环境的注意事项　　28
2.7.1　垃圾回收器选项　　28
2.7.2　数据中心布局　　29
2.7.3　共享Zookeeper　　29
2.8　总结　　30
第3章　Kafka生产者——向Kafka写入数据　　31
3.1　生产者概览　　32
3.2　创建Kafka生产者　　33
3.3　发送消息到Kafka　　34
3.3.1　同步发送消息　　35
3.3.2　异步发送消息　　35
3.4　生产者的配置　　36
3.5　序列化器　　39
3.5.1　自定义序列化器　　39
3.5.2　使用Avro序列化　　41
3.5.3　在Kafka里使用Avro　　42
3.6　分区　　45
3.7　旧版的生产者API　　46
3.8　总结　　47
第4章　Kafka消费者——从Kafka读取数据　　48
4.1　KafkaConsumer概念　　48
4.1.1　消费者和消费者群组　　48
4.1.2　消费者群组和分区再均衡　　51
4.2　创建Kafka 消费者　　52
4.3　订阅主题　　53
4.4　轮询　　53
4.5　消费者的配置　　55
4.6　提交和偏移量　　57
4.6.1　自动提交　　58
4.6.2　提交当前偏移量　　59
4.6.3　异步提交　　59
4.6.4　同步和异步组合提交　　61
4.6.5　提交特定的偏移量　　61
4.7　再均衡监听器　　62
4.8　从特定偏移量处开始处理记录　　64
4.9　如何退出　　66
4.10　反序列化器　　67
4.11　独立消费者——为什么以及怎样使用没有群组的消费者　　71
4.12　旧版的消费者API　　71
4.13　总结　　72
第5章　深入Kafka　　73
5.1　集群成员关系　　73
5.2　控制器　　74
5.3　复制　　74
5.4　处理请求　　76
5.4.1　生产请求　　78
5.4.2　获取请求　　78
5.4.3　其他请求　　80
5.5　物理存储　　81
5.5.1　分区分配　　81
5.5.2　文件管理　　82
5.5.3　文件格式　　83
5.5.4　索引　　84
5.5.5　清理　　84
5.5.6　清理的工作原理　　84
5.5.7　被删除的事件　　86
5.5.8　何时会清理主题　　86
5.6　总结　　86
第6章　可靠的数据传递　　87
6.1　可靠性保证　　87
6.2　复制　　88
6.3　broker配置　　89
6.3.1　复制系数　　89
6.3.2　不完全的首领选举　　90
6.3.3　最少同步副本　　91
6.4　在可靠的系统里使用生产者　　92
6.4.1　发送确认　　92
6.4.2　配置生产者的重试参数　　93
6.4.3　额外的错误处理　　94
6.5　在可靠的系统里使用消费者　　94
6.5.1　消费者的可靠性配置　　95
6.5.2　显式提交偏移量　　95
6.6　验证系统可靠性　　97
6.6.1　配置验证　　98
6.6.2　应用程序验证　　98
6.6.3　在生产环境监控可靠性　　99
6.7　总结　　100
第7章　构建数据管道　　101
7.1　构建数据管道时需要考虑的问题　　102
7.1.1　及时性　　102
7.1.2　可靠性　　102
7.1.3　高吞吐量和动态吞吐量　　103
7.1.4　数据格式　　103
7.1.5　转换　　104
7.1.6　安全性　　104
7.1.7　故障处理能力　　104
7.1.8　耦合性和灵活性　　105
7.2　如何在Connect API和客户端API之间作出选择　　105
7.3　Kafka Connect　　106
7.3.1　运行Connect　　106
7.3.2　连接器示例——文件数据源和文件数据池　　107
7.3.3　连接器示例——从MySQL到ElasticSearch　　109
7.3.4　深入理解Connect　　114
7.4　Connect之外的选择　　116
7.4.1　用于其他数据存储的摄入框架　　116
7.4.2　基于图形界面的ETL 工具　　117
7.4.3　流式处理框架　　117
7.5　总结　　117
第8章　跨集群数据镜像　　118
8.1　跨集群镜像的使用场景　　118
8.2　多集群架构　　119
8.2.1　跨数据中心通信的一些现实情况　　119
8.2.2　Hub和Spoke架构　　120
8.2.3　双活架构　　121
8.2.4　主备架构　　123
8.2.5　延展集群　　127
8.3　Kafka的MirrorMaker　　128
8.3.1　如何配置　　129
8.3.2　在生产环境部署MirrorMaker　　130
8.3.3　MirrorMaker调优　　132
8.4　其他跨集群镜像方案　　134
8.4.1　优步的uReplicator　　134
8.4.2　Confluent的Replicator　　135
第9章　管理Kafka　　136
9.1　主题操作　　136
9.1.1　创建主题　　137
9.1.2　增加分区　　138
9.1.3　删除主题　　138
9.1.4　列出集群里的所有主题　　139
9.1.5　列出主题详细信息　　139
9.2　消费者群组　　140
9.2.1　列出并描述群组　　140
9.2.2　删除群组　　142
9.2.3　偏移量管理　　142
9.3　动态配置变更　　143
9.3.1　覆盖主题的默认配置　　143
9.3.2　覆盖客户端的默认配置　　145
9.3.3　列出被覆盖的配置　　145
9.3.4　移除被覆盖的配置　　146
9.4　分区管理　　146
9.4.1　首选的首领选举　　146
9.4.2　修改分区副本　　147
9.4.3　修改复制系数　　150
9.4.4　转储日志片段　　151
9.4.5　副本验证　　152
9.5　消费和生产　　153
9.5.1　控制台消费者　　153
9.5.2　控制台生产者　　155
9.6　客户端ACL　　157
9.7　不安全的操作　　157
9.7.1　移动集群控制器　　157
9.7.2　取消分区重分配　　157
9.7.3　移除待删除的主题　　158
9.7.4　手动删除主题　　158
第10章　监控Kafka　　160
10.1　度量指标基础　　160
10.1.1　度量指标在哪里　　160
10.1.2　内部或外部度量　　161
10.1.3　应用程序健康检测　　161
10.1.4　度量指标的覆盖面　　161
10.2　broker的度量指标　　162
10.2.1　非同步分区　　162
10.2.2　broker度量指标　　166
10.2.3　主题和分区的度量指标　　173
10.2.4　Java虚拟机监控　　174
10.2.5　操作系统监控　　175
10.2.6　日志　　176
10.3　客户端监控　　177
10.3.1　生产者度量指标　　177
10.3.2　消费者度量指标　　179
10.3.3　配额　　181
10.4　延时监控　　182
10.5　端到端监控　　183
第11章　流式处理　　184
11.1　什么是流式处理　　185
11.2　流式处理的一些概念　　186
11.2.1　时间　　187
11.2.2　状态　　188
11.2.3　流和表的二元性　　188
11.2.4　时间窗口　　189
11.3　流式处理的设计模式　　190
11.3.1　单个事件处理　　191
11.3.2　使用本地状态　　191
11.3.3　多阶段处理和重分区　　193
11.3.4　使用外部查找——流和表的连接　　193
11.3.5　流与流的连接　　195
11.3.6　乱序的事件　　195
11.3.7　重新处理　　196
11.4　Streams示例　　197
11.4.1　字数统计　　197
11.4.2　股票市场统计　　199
11.4.3　填充点击事件流　　201
11.5　Kafka Streams的架构概览　　202
11.5.1　构建拓扑　　202
11.5.2　对拓扑进行伸缩　　203
11.5.3　从故障中存活下来　　205
11.6　流式处理使用场景　　205
11.7　如何选择流式处理框架　　206
附录A　在其他操作系统上安装Kafka　　209

1．1　Kafka流式数据平台 1
1．2　Kafka的基本概念 3
1．2．1　分区模型 3
1．2．2　消费模型 4
1．2．3　分布式模型 5
1．3　Kafka的设计与实现 6
1．3．1　文件系统的持久化与数据传输效率 6
1．3．2　生产者与消费者 8
1．3．3　副本机制和容错处理 10
1．4　快速开始 11
1．4．1　单机模式 12
1．4．2　分布式模式 14
1．4．3　消费组示例 16
1．5　环境准备 18
第2章　生产者 22
2．1　新生产者客户端 22
2．1．1　同步和异步发送消息 23
2．1．2　客户端消息发送线程 29
2．1．3　客户端网络连接对象 31
2．1．4　选择器处理网络请求 35
2．2　旧生产者客户端 43
2．2．1　事件处理器处理客户端发送的消息 44
2．2．2　对消息集按照节点和分区进行整理 46
2．2．3　生产者使用阻塞通道发送请求 48
2．3　服务端网络连接 49
2．3．1　服务端使用接收器接受客户端的连接 50
2．3．2　处理器使用选择器的轮询处理网络请求 53
2．3．3　请求通道的请求队列和响应队列 56
2．3．4　Kafka请求处理线程 58
2．3．5　服务端的请求处理入口 58

第3章　消费者：高级API和低级API 61
3．1　消费者启动和初始化 67
3．1．1　创建并初始化消费者连接器 69
3．1．2　消费者客户端的线程模型 70
3．1．3　重新初始化消费者 72
3．2　消费者再平衡操作 73
3．2．1　分区的所有权 74
3．2．2　为消费者分配分区 75
3．2．3　创建分区信息对象 78
3．2．4　关闭和更新拉取线程管理器 80
3．2．5　分区信息对象的偏移量 80
3．3　消费者拉取数据 82
3．3．1　拉取线程管理器 82
3．3．2　抽象拉取线程 87
3．3．3　消费者拉取线程 90
3．4　消费者消费消息 94
3．4．1　Kafka消息流 94
3．4．2　消费者迭代消费消息 95
3．5　消费者提交分区偏移量 97
3．5．1　提交偏移量到ZK 98
3．5．2　提交偏移量到内部主题 99
3．5．3　连接偏移量管理器 101
3．5．4　服务端处理提交偏移量的请求 103
3．5．5　缓存分区的偏移量 106
3．6　消费者低级API示例 108
3．6．1　消息消费主流程 109
3．6．2　找出分区的主副本 112
3．6．3　获取分区的读取偏移量 113
3．6．4　发送拉取请求并消费消息 116
3．7．1　消费者线程模型 117
3．7．2　再平衡和分区分配 119

第4章　新消费者 121
4．1　新消费者客户端 125
4．1．1　消费者的订阅状态 125
4．1．2　消费者轮询的准备工作 134
4．1．3　消费者轮询的流程 138
4．1．4　消费者拉取消息 146
4．1．5　消费者获取记录 149
4．1．6　消费消息 160
4．2　消费者的网络客户端轮询 161
4．2．1　异步请求 162
4．2．2　异步请求高级模式 169
4．2．3　网络客户端轮询 184
4．3　心跳任务 188
4．3．1　发送心跳请求 188
4．3．2　心跳状态 189
4．3．3　运行心跳任务 191
4．3．4　处理心跳结果的示例 192
4．3．5　心跳和协调者的关系 193
4．4　消费者提交偏移量 195
4．4．1　自动提交任务 195
4．4．2　将拉取偏移量作为提交偏移量 197
4．4．3　同步提交偏移量 201
4．4．4　消费者的消息处理语义 202
4．5　小结 206
第5章　协调者 210
5．1　消费者加入消费组 211
5．1．1　元数据与分区分配器 212
5．1．2　消费者的加入组和同步组 213
5．1．3　主消费者执行分配任务 220
5．1．4　加入组的准备、完成和监听器 224
5．2　协调者处理请求 229
5．2．1　服务端定义发送响应结果的回调方法 229
5．2．2　消费者和消费组元数据 232
5．2．3　协调者处理请求前的条件检查 236
5．2．4　协调者调用回调方法发送响应给客户端 237
5．3　延迟的加入组操作 242
5．3．1 “准备再平衡” 242
5．3．2　延迟操作和延迟缓存 244
5．3．3　尝试完成延迟的加入操作 246
5．3．4　消费组稳定后，原有消费者重新加入消费组 250
5．3．5　消费组未稳定，原有消费者重新加入消费组 251
5．4　消费组状态机 254
5．4．1　再平衡操作与监听器 254
5．4．2　消费组的状态转换 262
5．4．3　协调者处理“加入组请求” 264
5．4．4　协调者处理“同步组请求” 274
5．4．5　协调者处理“离开组请求” 276
5．4．6　再平衡超时与会话超时 278
5．4．7　延迟的心跳 282
5．5　小结 290
第6章　存储层 293
6．1　日志的读写 293
6．1．1　分区、副本、日志、日志
分段 294
6．1．2　写入日志 297
6．1．3　日志分段 305
6．1．4　读取日志 315
6．1．5　日志管理 329
6．1．6　日志压缩 336
6．2　服务端处理读写请求 348
6．2．1　副本管理器 351
6．2．2　分区与副本 362
6．3　延迟操作 373
6．3．1　延迟操作接口 374
6．3．2　延迟操作与延迟缓存 383
6．3．3　延迟缓存 391
6．4　小结 400
第7章　控制器 402
7．1　Kafka控制器 402
7．1．1　控制器选举 403
7．1．2　控制器上下文 406
7．1．3　ZK监听器 408
7．1．4　分区状态机和副本状态机 410
7．1．5　删除主题 430
7．1．6　重新分配分区 436
7．1．7　控制器的网络通道管理器 445
7．2　服务端处理LeaderAndIsr请求 448
7．2．1　创建分区 449
7．2．2　创建主副本、备份副本 451
7．2．3　消费组元数据迁移 463
7．3　元数据缓存 468
7．3．1　服务端的元数据缓存 472
7．3．2　客户端更新元数据 473
7．4　Kafka服务关闭 483
7．5　小结 487
第8章　基于Kafka构建数据流管道 490
8．1　Kafka集群同步工具：MirrorMaker 490
8．1．1　单机模拟数据同步 491
8．1．2　数据同步的流程 493
8．2　Uber集群同步工具：uReplicator 498
8．2．1　Apache Helix介绍 498
8．2．2　Helix控制器 501
8．2．3　Helix工作节点 504
8．3　Kafka连接器 505
8．3．1　连接器的使用示例 507
8．3．2　开发一个简单的连接器 510
8．3．3　连接器的架构模型 515
8．3．4　Herder的实现 520
8．3．5　Worker的实现 524
8．3．6　配置存储与状态存储 530
8．3．7　连接器与任务的实现 550
8．4　小结 565
第9章　Kafka流处理 569
9．1　低级Processor API 569
9．1．1　流处理应用程序示例 569
9．1．2　流处理的拓扑 575
9．1．3　流处理的线程模型 580
9．1．4　状态存储 613
9．2　高级流式DSL 636
9．2．1　DSL应用程序示例 636
9．2．2　KStream和KTable 638
9．2．3　连接操作 665
9．2．4　窗口操作 672
9．3　小结 684
第10章　高级特性介绍 686
10．1　客户端配额 686
10．2　消息与时间戳 692
10．3　事务处理 699
10．4　小结 703

1.1　Kafka快速入门 1
1.1.1　下载并解压缩Kafka二进制代码压缩包文件 2
1.1.2　启动服务器 3
1.1.3　创建topic 3
1.1.4　发送消息 4
1.1.5　消费消息 4
1.2　消息引擎系统 5
1.2.1　消息设计 6
1.2.2　传输协议设计 6
1.2.3　消息引擎范型 6
1.2.4　Java消息服务 8

1.3　Kafka概要设计 8
1.3.1　吞吐量/延时 8
1.3.2　消息持久化 11
1.3.3　负载均衡和故障转移 12
1.3.4　伸缩性 13

1.4　Kafka基本概念与术语 13
1.4.1　消息 14
1.4.2　topic和partition 16
1.4.3　offset 17
1.4.4　replica 18
1.4.5　leader和follower 18
1.4.6　ISR 19

1.5　Kafka使用场景 20

1.5.1　消息传输 20

1.5.2　网站行为日志追踪 20

1.5.3　审计数据收集 20

1.5.4　日志收集 20

1.5.5　Event Sourcing 21

1.5.6　流式处理 21

2.1　Kafka的历史 22

2.1.1　背景 22

2.1.2　Kafka横空出世 23

2.1.3　Kafka开源 24

2.2　Kafka版本变迁 25

2.2.1　Kafka的版本演进 25

2.2.2　Kafka的版本格式 26

2.2.3　新版本功能简介 26

2.2.4　旧版本功能简介 31

2.3　如何选择Kafka版本 35

2.3.1　根据功能场景 35

2.3.2　根据客户端使用场景 35

2.4　Kafka与Confluent 36

2.5　本章小结 37

第3章　Kafka线上环境部署 38

3.1　集群环境规划 38

3.1.1　操作系统的选型 38

3.1.2　磁盘规划 40

3.1.3　磁盘容量规划 42

3.1.4　内存规划 43

3.1.5　CPU规划 43

3.1.6　带宽规划 44

3.1.7　典型线上环境配置 45

3.2　伪分布式环境安装 45

3.2.1　安装Java 46

3.2.2　安装ZooKeeper 47

3.2.3　安装单节点Kafka集群 48

3.3　多节点环境安装 49

3.3.1　安装多节点ZooKeeper集群 50

3.3.2　安装多节点Kafka 54

3.4　验证部署 55

3.4.1　测试topic创建与删除 55

3.4.2　测试消息发送与消费 57

3.4.3　生产者吞吐量测试 58

3.4.4　消费者吞吐量测试 58

3.5　参数设置 59

3.5.1　broker端参数 59

3.5.2　topic级别参数 62

3.5.3　GC参数 63

3.5.4　JVM参数 64

3.5.5　OS参数 64

3.6　本章小结 65

第4章　producer开发 66

4.1　producer概览 66

4.2　构造producer 69

4.2.1　producer程序实例 69

4.2.2　producer主要参数 75

4.3　消息分区机制 80

4.3.1　分区策略 80

4.3.2　自定义分区机制 80

4.4　消息序列化 83

4.4.1　默认序列化 83

4.4.2　自定义序列化 84

4.5　producer拦截器 87

4.6　无消息丢失配置 90

4.6.1　producer端配置 91

4.6.2　broker端配置 92

4.7　消息压缩 92

4.7.1　Kafka支持的压缩算法 93

4.7.2　算法性能比较与调优 93

4.8　多线程处理 95

4.9　旧版本producer 96

4.10　本章小结 98

第5章　consumer开发 99

5.1　consumer概览 99

5.1.1　消费者（consumer） 99

5.1.2　消费者组（consumer group） 101

5.1.3　位移（offset） 102

5.1.4　位移提交 103

5.1.5　__consumer_offsets 104

5.1.6　消费者组重平衡（consumer group rebalance） 106

5.2　构建consumer 106

5.2.1　consumer程序实例 106

5.2.2　consumer脚本命令 111

5.2.3　consumer主要参数 112

5.3　订阅topic 115

5.3.1　订阅topic列表 115

5.3.2　基于正则表达式订阅topic 115

5.4　消息轮询 115

5.4.1　poll内部原理 115

5.4.2　poll使用方法 116

5.5　位移管理 118

5.5.1　consumer位移 119

5.5.2　新版本consumer位移管理 120

5.5.3　自动提交与手动提交 121

5.5.4　旧版本consumer位移管理 123

5.6　重平衡（rebalance） 123

5.6.1　rebalance概览 123

5.6.2　rebalance触发条件 124

5.6.3　rebalance分区分配 124

5.6.4　rebalance generation 126

5.6.5　rebalance协议 126

5.6.6　rebalance流程 127

5.6.7　rebalance监听器 128

5.7　解序列化 130

5.7.1　默认解序列化器 130

5.7.2　自定义解序列化器 131

5.8　多线程消费实例 132

5.8.1　每个线程维护一个KafkaConsumer 133

5.8.2　单KafkaConsumer实例+多worker线程 135

5.8.3　两种方法对比 140

5.9　独立consumer 141

5.10　旧版本consumer 142

5.10.1　概览 142

5.10.2　high-level consumer 143

5.10.3　low-level consumer 147

5.11　本章小结 153

第6章　Kafka设计原理 154

6.1　broker端设计架构 154

6.1.1　消息设计 155

6.1.2　集群管理 166

6.1.3　副本与ISR设计 169

6.1.4　水印（watermark）和leader epoch 174

6.1.5　日志存储设计 185

6.1.6　通信协议（wire protocol） 194

6.1.7　controller设计 205

6.1.8　broker请求处理 216

6.2　producer端设计 219

6.2.1　producer端基本数据结构 219

6.2.2　工作流程 220

6.3　consumer端设计 223

6.3.1　consumer group状态机 223

6.3.2　group管理协议 226

6.3.3　rebalance场景剖析 227

6.4　实现精确一次处理语义 230

6.4.1　消息交付语义 230

6.4.2　幂等性producer（idempotent producer） 231

6.4.3　事务（transaction） 232

6.5　本章小结 234

第7章　管理Kafka集群 235

7.1　集群管理 235

7.1.1　启动broker 235

7.1.2　关闭broker 236

7.1.3　设置JMX端口 237

7.1.4　增加broker 238

7.1.5　升级broker版本 238

7.2　topic管理 241

7.2.1　创建topic 241

7.2.2　删除topic 243

7.2.3　查询topic列表 244

7.2.4　查询topic详情 244

7.2.5　修改topic 245

7.3　topic动态配置管理 246

7.3.1　增加topic配置 246

7.3.2　查看topic配置 247

7.3.3　删除topic配置 248

7.4　consumer相关管理 248

7.4.1　查询消费者组 248

7.4.2　重设消费者组位移 251

7.4.3　删除消费者组 256

7.4.4　kafka-consumer-offset-checker 257

7.5　topic分区管理 258

7.5.1　preferred leader选举 258

7.5.2　分区重分配 260

7.5.3　增加副本因子 263

7.6　Kafka常见脚本工具 264

7.6.1　kafka-console-producer脚本 264

7.6.2　kafka-console-consumer脚本 265

7.6.3　kafka-run-class脚本 267

7.6.4　查看消息元数据 268

7.6.5　获取topic当前消息数 270

7.6.6　查询__consumer_offsets 271

7.7　API方式管理集群 273

7.7.1　服务器端API管理topic 273

7.7.2　服务器端API管理位移 275

7.7.3　客户端API管理topic 276

7.7.4　客户端API查看位移 280

7.7.5　0.11.0.0版本客户端API 281

7.8　MirrorMaker 285

7.8.1　概要介绍 285

7.8.2　主要参数 286

7.8.3　使用实例 287

7.9　Kafka安全 288

7.9.1　SASL+ACL 289

7.9.2　SSL加密 297

7.10　常见问题 301

7.11　本章小结 304

第8章　监控Kafka集群 305

8.1　集群健康度检查 305

8.2　MBean监控 306

8.2.1　监控指标 306

8.2.2　指标分类 308

8.2.3　定义和查询JMX端口 309

8.3　broker端JMX监控 310

8.3.1　消息入站/出站速率 310

8.3.2　controller存活JMX指标 311

8.3.3　备份不足的分区数 312

8.3.4　leader分区数 312

8.3.5　ISR变化速率 313

8.3.6　broker I/O工作处理线程空闲率 313

8.3.7　broker网络处理线程空闲率 314

8.3.8　单个topic总字节数 314

8.4　clients端JMX监控 314

8.4.1　producer端JMX监控 314

8.4.2　consumer端JMX监控 316

8.5　JVM监控 317

8.5.1　进程状态 318

8.5.2　GC性能 318

8.6　OS监控 318

8.7　主流监控框架 319

8.7.1　JmxTool 320

8.7.2　kafka-manager 320

8.7.3　Kafka Monitor 325

8.7.4　Kafka Offset Monitor 327

8.7.5　CruiseControl 329

8.8　本章小结 330

第9章　调优Kafka集群 331

9.1　引言 331

9.2　确定调优目标 333

9.3　集群基础调优 334

9.3.1　禁止atime更新 335

9.3.2　文件系统选择 335

9.3.3　设置swapiness 336

9.3.4　JVM设置 337

9.3.5　其他调优 337

9.4　调优吞吐量 338

9.5　调优延时 342

9.6　调优持久性 343

9.7　调优可用性 347

9.8　本章小结 349

第10章　Kafka Connect与Kafka Streams 350

10.2　Kafka Connect 351
10.2.1　概要介绍 351
10.2.2　standalone Connect 353
10.2.3　distributed Connect 356
10.2.4　开发connector 359
10.3　Kafka Streams 362
10.3.1　流处理 362

10.3.2　Kafka Streams核心概念 364
10.3.3　Kafka Streams与其他框架的异同 368
10.3.4　Word Count实例 369
10.3.5　Kafka Streams应用开发 372
10.3.6　Kafka Streams状态查询 382






152. kafka 可以脱离 zookeeper 单独使用吗？为什么？

kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。

153. kafka 有几种数据保留的策略？

kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。

154. kafka 同时设置了 7 天和 10G 清除数据，到第五天的时候消息达到了 10G，这个时候 kafka 将如何处理？

这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。

155. 什么情况会导致 kafka 运行变慢？

cpu 性能瓶颈

磁盘读写瓶颈

网络瓶颈


156. 使用 kafka 集群需要注意什么？

集群的数量不是越多越好，最好不要超过 7 个，因为节点越多，消息复制需要的时间就越长，整个群组的吞吐量就越低。

集群数量最好是单数，因为超过一半故障集群就不能用了，设置为单数容错率更高。

https://help.aliyun.com/document_detail/68165.html?spm=a2c4g.11174283.6.592.2f2e1eefpIsinD

发布者最佳实践
更新时间：2019-04-17 17:32:10


本页目录
Key 和 Value
失败重试
异步发送
线程安全
Acks
Batch
OOM
分区顺序
技术交流
本文主要介绍消息队列 Kafka 发布者的最佳实践，从而帮助您更好的使用该产品。

注意：以下最佳实践基于消息队列 Kafka 的 Java 客户端；对于其它语言的客户端，其基本概念与思想是通用的，但实现细节可能有差异，仅供参考。

Kafka 的发送非常简单，示例代码片段如下：

 Future<RecordMetadata> metadataFuture = producer.send(new ProducerRecord<String, String>(
            topic,   \\ topic
            null,    \\ 分区编号，这里最好为 null，交给 producer 去分配
            System.currentTimeMillis(), \\时间戳
            String.valueOf(message.hashCode()), \\ key，可以在控制台通过这个 Key 查找消息，这个 key 最好唯一；
            message)); \\ value，消息内容
详细 Demo 可参见 Demo 示例。

Key 和 Value
Kafka 0.10.0.0 的消息字段只有两个：Key 和 Value。Key 是消息的标识，Value 即消息内容。为了便于追踪，重要消息最好都设置一个唯一的 Key。通过 Key 追踪某消息，打印发送日志和消费日志，了解该消息的发送和消费情况；更重要的是，您可以在控制台可以根据 Key 查询消息的内容。

失败重试
在分布式环境下，由于网络等原因，偶尔的发送失败是常见的。导致这种失败的原因有可能是消息已经发送成功，但是 Ack 失败，也有可能是确实没发送成功。

消息队列 Kafka 是 VIP 网络架构，会主动掐掉空闲连接（30 秒没活动），也就是说，不是一直活跃的客户端会经常收到”connection rest by peer”这样的错误，因此建议都考虑重试消息发送。

异步发送
发送接口是异步的；如果你想得到发送的结果，可以调用metadataFuture.get(timeout, TimeUnit.MILLISECONDS)。

线程安全
Producer 是线程安全的，且可以往任何 Topic 发送消息。通常情况下，一个应用对应一个 Producer 就足够了。

Acks
acks=0，表示无需服务端的 Response，性能较高，丢数据风险较大；

acks=1，服务端主节点写成功即返回Response，性能中等，丢数据风险中等，主节点宕机可能导致数据丢失；

acks=all，服务端主节点写成功，且备节点同步成功，才返回Response，性能较差，数据较为安全，主节点和备节点都宕机才会导致数据丢失。

一般建议选择acks=1，重要的服务可以设置acks=all。

Batch
Batch 的基本思路是：把消息缓存在内存中，并进行打包发送。Kafka 通过 Batch 来提高吞吐，但同时也会增加延迟，生产时应该对两者予以权衡。
在构建 Producer 时，需要考虑以下两个参数：

batch.size : 发往每个分区（Partition）的消息缓存量（消息内容的字节数之和，不是条数）达到这个数值时，就会触发一次网络请求，然后客户端把消息真正发往服务器；
linger.ms : 每条消息待在缓存中的最长时间。若超过这个时间，就会忽略 batch.size 的限制，然后客户端立即把消息发往服务器。
由此可见，Kafka 客户端什么时候把消息真正发往服务器，是通过上面两个参数共同决定的：
batch.size 有助于提高吞吐，linger.ms有助于控制延迟。您可以根据具体业务需求进行调整。

OOM
结合 Kafka 的 Batch 设计思路，Kafka 会缓存消息并打包发送，如果缓存太多，则有可能造成 OOM（Out of Memory）。

buffer.memory : 所有缓存消息的总体大小超过这个数值后，就会触发把消息发往服务器。此时会忽略 batch.size 和 linger.ms 的限制。
buffer.memory 的默认数值是 32MB，对于单个 Producer 来说，可以保证足够的性能。需要注意的是，如果你在同一个 JVM 中启动多个 Producer，那么每个 Producer 都有可能占用 32MB 缓存空间，此时便有可能触发 OOM。
在生产时，一般没有必要启动多个 Producer；如果特殊情况需要，则需要考虑buffer.memory的大小，避免触发 OOM。
分区顺序
单个分区（Partition）内，消息是按照发送顺序储存的，是基本有序的。

默认情况下，消息队列 Kafka 为了提升可用性，并不保证单个分区内绝对有序，在升级或者宕机时，会发生少量消息乱序（某个分区挂掉后把消息 Failover 到其它分区）。

如果业务要求分区保证严格有序，请在创建Topic时指定保序。

技术交流
如果你有其它使用方面的困惑，可通过在github demo地址里提交 Issue 进行反馈。






订阅者最佳实践
更新时间：2019-04-28 12:01:16


本页目录
消费消息基本流程
负载均衡
多个订阅
消费位点
消费位点提交
消费位点重置
消息重复和消费幂等
消费失败
消费阻塞以及堆积
提高消费速度
消息过滤
消息广播
订阅关系
本文主要介绍消息队列 Kafka 订阅者的最佳实践，从而帮助您更好的使用该产品。

消费消息基本流程
Kafka 订阅者在订阅消息时的基本流程是：

Poll 数据
执行消费逻辑
再次 poll 数据
负载均衡
每个 Consumer Group 可以包含多个消费实例，即可以启动多个 Kafka Consumer，并把参数 group.id 设置成相同的值。属于同一个 Consumer Group 的消费实例会负载消费订阅的 Topic。

举例：Consumer Group A 订阅了 Topic A，并开启三个消费实例 C1、C2、C3，则发送到 Topic A 的每条消息最终只会传给 C1、C2、C3 的某一个。Kafka 默认会均匀地把消息传给各个消息实例，以做到消费负载均衡。

Kafka 负载消费的内部原理是，把订阅的 Topic 的分区，平均分配给各个消费实例。因此，消费实例的个数不要大于分区的数量，否则会有实例分配不到任何分区而处于空跑状态。这个负载均衡发生的时间，除了第一次启动上线之外，后续消费实例发生重启、增加、减少等变更时，都会触发一次负载均衡。

消息队列 Kafka 的每个 Topic 的分区数量默认是 16 个，已经足够满足大部分场景的需求，且云上服务会根据容量调整分区数。

多个订阅
一个 Consumer Group 可以订阅多个 Topic。一个 Topic 也可以被多个 Consumer Group 订阅，且各个 Consumer Group 独立消费 Topic 下的所有消息。

举例：Consumer Group A 订阅了 Topic A，Consumer Group B 也订阅了 Topic A，则发送到 Topic A 的每条消息，不仅会传一份给 Consumer Group A 的消费实例，也会传一份给 Consumer Group B 的消费实例，且这两个过程相互独立，相互没有任何影响。

消费位点
每个 Topic 会有多个分区，每个分区会统计当前消息的总条数，这个称为最大位点 MaxOffset。Kafka Consumer 会按顺序依次消费分区内的每条消息，记录已经消费了的消息条数，称为ConsumerOffset。

剩余的未消费的条数（也称为消息堆积量） = MaxOffset - ConsumerOffset

消费位点提交
Kafka 消费者有两个相关参数：

enable.auto.commit：默认值为 true。
auto.commit.interval.ms： 默认值为 1000，也即 1s。
这两个参数组合的结果就是，每次 poll 数据前会先检查上次提交位点的时间，如果距离当前时间已经超过参数auto.commit.interval.ms规定的时长，则客户端会启动位点提交动作。

因此，如果将enable.auto.commit设置为 true，则需要在每次 poll 数据时，确保前一次 poll 出来的数据已经消费完毕，否则可能导致位点跳跃。

如果想自己控制位点提交，请把 enable.auto.commit 设为 false，并调用 commit(offsets)函数自行控制位点提交。

消费位点重置
以下两种情况，会发生消费位点重置：

当服务端不存在曾经提交过的位点时（比如客户端第一次上线）
当从非法位点拉取消息时（比如某个分区最大位点是10，但客户端却从11开始拉取消息）
Java 客户端可以通过auto.offset.reset来配置重置策略，主要策略有：

“latest”，从最大位点开始消费
“earliest”，从最小位点开始消费
‘none’, 不做任何操作，也即不重置
建议：

强烈建议设置成“latest”，而不要设置成“earliest”，避免因位点非法时从头开始消费，从而造成大量重复
如果是客户自己管理位点，可以设置成”none”
消息重复和消费幂等
Kafka 消费的语义是 “at least once”， 也就是至少投递一次，保证消息不丢，但是不会保证消息不重复。在出现网络问题、客户端重启时均有可能出现少量重复消息，此时应用消费端如果对消息重复比较敏感（比如说订单交易类），则应该做到消息幂等。

以数据库类应用为例，常用做法是：

发送消息时，传入 key 作为唯一流水号ID；
消费消息时，判断 key 是否已经消费过，如果已经消费过了，则忽略，如果没消费过，则消费一次；
当然，如果应用本身对少量消息重复不敏感，则不需要做此类幂等检查。

消费失败
Kafka 是按分区一条一条消息顺序向前推进消费的，如果消费端拿到某条消息后执行消费逻辑失败，比如应用服务器出现了脏数据，导致某条消息处理失败，等待人工干预，那么有以下两种处理方式：

失败后一直尝试再次执行消费逻辑。这种方式有可能造成消费线程阻塞在当前消息，无法向前推进，造成消息堆积；
由于 Kafka 自身没有处理失败消息的设计，实践中通常会打印失败的消息、或者存储到某个服务（比如创建一个 Topic 专门用来放失败的消息），然后定时 check 失败消息的情况，分析失败原因，根据情况处理。
消费阻塞以及堆积
消费端最常见的问题就是消费堆积，最常造成堆积的原因是：

消费速度跟不上生产速度，此时应该提高消费速度，详见下一节《提高消费速度》；
消费端产生了阻塞。
消费端拿到消息后，执行消费逻辑，通常会执行一些远程调用，如果这个时候同步等待结果，则有可能造成一直等待，消费进程无法向前推进。

消费端应该竭力避免堵塞消费线程，如果存在等待调用结果的情况，建议设置等待的超时时间，超时后作消费失败处理。

提高消费速度
提高消费速度有以下两个办法：

增加 Consumer 实例个数
增加消费线程
增加 Consumer 实例
可以在进程内直接增加（需要保证每个实例对应一个线程，否则没有太大意义），也可以部署多个消费实例进程；需要注意的是，实例个数超过分区数量后就不再能提高速度，将会有消费实例不工作。

增加消费线程
增加 Consumer 实例本质上也是增加线程的方式来提升速度，因此更加重要的性能提升方式是增加消费线程，最基本的步骤如下：

定义一个线程池；
Poll 数据；
把数据提交到线程池进行并发处理；
等并发结果返回成功后，再次 poll 数据执行。
消息过滤
Kafka 自身没有消息过滤的语义。实践中可以采取以下两个办法：

如果过滤的种类不多，可以采取多个 Topic 的方式达到过滤的目的；
如果过滤的种类多，则最好在客户端业务层面自行过滤。
实践中请根据业务具体情况进行选择，也可以综合运用上面两种办法。

消息广播
Kafka 自身没有消息广播的语义，可以通过创建不同的 Consumer Group 来模拟实现。

订阅关系
同一个 Consumer Group 内，各个消费实例订阅的 Topic 最好保持一致，避免给排查问题带来干扰。



======================================================

Apache Kafka是一款流行的分布式数据流平台，它已经广泛地被诸如New Relic(数据智能平台)、Uber、Square(移动支付公司)等大型公司用来构建可扩展的、高吞吐量的、且高可靠的实时数据流系统。例如，在New Relic的生产环境中，Kafka群集每秒能够处理超过1500万条消息，而且其数据聚合率接近1 Tbps。

可见，Kafka大幅简化了对于数据流的处理，因此它也获得了众多应用开发人员和数据管理专家的青睐。然而，在大型系统中Kafka的应用会比较复杂。如果您的consumers无法跟上数据流的话，各种消息往往在未被查看之前就已经消失掉了。同时，它在自动化数据保留方面的限制，高流量的发布+订阅(publish-subscribe，pub/sub)模式等，可能都会影响到您系统的性能。可以毫不夸张地说，如果那些存放着数据流的系统无法按需扩容、或稳定性不可靠的话，估计您经常会寝食难安了。

针对Partitions的最佳实践
• 了解分区的数据速率，以确保提供合适的数据保存空间。此处所谓“分区的数据速率”是指数据的生成速率。换言之，它是由“平均消息大小”乘以“每秒消息数”得出的。数据速率决定了在给定时间内，所能保证的数据保存空间的大小(以字节为单位)。如果您不知道数据速率的话，则无法正确地计算出满足基于给定时间跨度的数据，所需要保存的空间大小。同时，数据速率也能够标识出单个consumer在不产生延时的情况下，所需要支持的最低性能值。

• 除非您有其他架构上的需要，否则在写topic时请使用随机分区。在您进行大型操作时，各个分区在数据速率上的参差不齐是非常难以管理的。其原因来自于如下三个方面：

首先，“热”(有较高吞吐量)分区上的consumer势必会比同组中的其他consumer处理更多的消息，因此很可能会导致出现在处理上和网络上的瓶颈。

其次，那些为具有最高数据速率的分区，所配置的最大保留空间，会导致topic中其他分区的磁盘使用量也做相应地增长。

第三，根据分区的leader关系所实施的最佳均衡方案，比简单地将leader关系分散到所有broker上，要更为复杂。在同一topic中，“热”分区会“承载”10倍于其他分区的权重。

针对Consumers的最佳实践

如果consumers运行的是比Kafka 0.10还要旧的版本，那么请马上升级。


在0.8.x 版中，consumer使用Apache ZooKeeper来协调consumer group，而许多已知的bug会导致其长期处于再均衡状态，或是直接导致再均衡算法的失败(我们称之为“再均衡风暴”)。因此在再均衡期间，一个或多个分区会被分配给同一组中的每个consumer。而在再均衡风暴中，分区的所有权会持续在各个consumers之间流转，这反而阻碍了任何一个consumer去真正获取分区的所有权。

调优consumer的套接字缓冲区(socket buffers)，以应对数据的高速流入。

在Kafka的0.10.x版本中，参数receive.buffer.bytes的默认值为64 kB。而在Kafka的0.8.x版本中，参数socket.receive.buffer.bytes的默认值为100 kB。这两个默认值对于高吞吐量的环境而言都太小了，特别是如果broker和consumer之间的网络带宽延迟积(bandwidth-delay product)大于局域网(local area network，LAN)时。对于延迟为1毫秒或更多的高带宽的网络(如10 Gbps或更高)，请考虑将套接字缓冲区设置为8或16 MB。

如果您的内存不足，也至少考虑设置为1 MB。当然，您也可以设置为-1，它会让底层操作系统根据网络的实际情况，去调整缓冲区的大小。

但是，对于需要启动“热”分区的consumers来说，自动调整可能不会那么快。


设计具有高吞吐量的consumers，以便按需实施背压(back-pressure)。通常，我们应该保证系统只去处理其能力范围内的数据，而不要超负荷“消费”，进而导致进程中断“挂起”，或出现consume group的溢出。如果是在Java虚拟机(JVM)中运行，consumers应当使用固定大小的缓冲区(请参见Disruptor模式：http://lmax-exchange.github.io/disruptor/files/Disruptor-1.0.pdf)，而且最好是使用堆外内存(off-heap)。固定大小的缓冲区能够阻止consumer将过多的数据拉到堆栈上，以至于JVM花费掉其所有的时间去执行垃圾回收，进而无法履行其处理消息的本质工作。

在JVM上运行各种consumers时，请警惕垃圾回收对它们可能产生的影响。例如，长时间垃圾回收的停滞，可能导致ZooKeeper的会话被丢弃、或consumer group处于再均衡状态。对于broker来说也如此，如果垃圾回收停滞的时间太长，则会产生集群掉线的风险。

针对Producers的最佳实践
• 配置producer，以等待各种确认。籍此producer能够获知消息是否真正被发送到了broker的分区上。在Kafka的0.10.x版本上，其设置是acks;而在0.8.x版本上，则为request.required.acks。Kafka通过复制，来提供容错功能，因此单个节点的故障、或分区leader关系的更改不会影响到系统的可用性。如果您没有用acks来配置producer(或称“fire and forget”)的话，则消息可能会悄然丢失。

• 为各个producer配置retries。其默认值为3，当然是非常低的。不过，正确的设定值取决于您的应用程序，即：就那些对于数据丢失零容忍的应用而言，请考虑设置为Integer.MAX_VALUE(有效且最大)。这样将能够应对broker的leader分区出现无法立刻响应produce请求的情况。

• 为高吞吐量的producer，调优缓冲区的大小，特别是buffer.memory和batch.size(以字节为单位)。由于batch.size是按照分区设定的，而producer的性能和内存的使用量，都可以与topic中的分区数量相关联。因此，此处的设定值将取决于如下几个因素：producer数据速率(消息的大小和数量)、要生成的分区数、以及可用的内存量。请记住，将缓冲区调大并不总是好事，如果producer由于某种原因而失效了(例如，某个leader的响应速度比确认还要慢)，那么在堆内内存(on-heap)中的缓冲的数据量越多，其需要回收的垃圾也就越多。

• 检测应用程序，以跟踪诸如生成的消息数、平均消息大小、以及已使用的消息数等指标。

针对Brokers的最佳实践

• 在各个brokers上，请压缩topics所需的内存和CPU资源。日志压缩需要各个broker上的堆栈(内存)和CPU周期都能成功地配合实现。而如果让那些失败的日志压缩数据持续增长的话，则会给brokers分区带来风险。您可以在broker上调整log.cleaner.dedupe.buffer.size和log.cleaner.threads这两个参数，但是请记住，这两个值都会影响到各个brokers上的堆栈使用。如果某个broker抛出OutOfMemoryError异常，那么它将会被关闭、并可能造成数据的丢失。而缓冲区的大小和线程的计数，则取决于需要被清除的topic partition数量、以及这些分区中消息的数据速率与密钥的大小。对于Kafka的0.10.2.1版本而言，通过ERROR条目来监控日志清理程序的日志文件，是检测其线程可能出现问题的最可靠方法。

• 通过网络吞吐量来监控brokers。请监控发向(transmit，TX)和收向(receive，RX)的流量，以及磁盘的I/O、磁盘的空间、以及CPU的使用率，而且容量规划是维护群集整体性能的关键步骤。

• 在群集的各个brokers之间分配分区的leader关系。Leader通常会需要大量的网络I/O资源。例如，当我们将复制因子(replication factor)配置为3、并运行起来时，leader必须首先获取分区的数据，然后将两套副本发送给另两个followers，进而再传输到多个需要该数据的consumers上。因此在该例子中，单个leader所使用的网络I/O，至少是follower的四倍。而且，leader还可能需要对磁盘进行读操作，而follower只需进行写操作。

• 不要忽略监控brokers的in-sync replica(ISR)shrinks、under-replicated partitions和unpreferred leaders。这些都是集群中潜在问题的迹象。例如，单个分区频繁出现ISR收缩，则暗示着该分区的数据速率超过了leader的能力，已无法为consumer和其他副本线程提供服务了。

• 按需修改Apache Log4j的各种属性。Kafka的broker日志记录会耗费大量的磁盘空间，但是我们却不能完全关闭它。因为有时在发生事故之后，需要重建事件序列，那么broker日志就会是我们最好的、甚至是唯一的方法。

• 禁用topic的自动创建，或针对那些未被使用的topics建立清除策略。例如，在设定的x天内，如果未出现新的消息，您应该考虑该topic是否已经失效，并将其从群集中予以删除。此举可避免您花时间去管理群集中被额外创建的元数据。

• 对于那些具有持续高吞吐量的brokers，请提供足够的内存，以避免它们从磁盘子系统中进行读操作。我们应尽可能地直接从操作系统的缓存中直接获取分区的数据。然而，这就意味着您必须确保自己的consumers能够跟得上“节奏”，而对于那些延迟的consumer就只能强制broker从磁盘中读取了。

• 对于具有高吞吐量服务级别目标(service level objectives，SLOs)的大型群集，请考虑为brokers的子集隔离出不同的topic。至于如何确定需要隔离的topics，则完全取决于您自己的业务需要。例如，您有一些使用相同群集的联机事务处理(multiple online transaction processing，OLTP)系统，那么将每个系统的topics隔离到不同brokers子集中，则能够有助于限制潜在事件的影响半径。

• 在旧的客户端上使用新的topic消息格式。应当代替客户端，在各个brokers上加载额外的格式转换服务。当然，最好还是要尽量避免这种情况的发生。

• 不要错误地认为在本地主机上测试好broker，就能代表生产环境中的真实性能了。要知道，如果使用复制因子为1，并在环回接口上对分区所做的测试，是与大多数生产环境截然不同的。在环回接口上网络延迟几乎可以被忽略的，而在不涉及到复制的情况下，接收leader确认所需的时间则同样会出现巨大的差异。


本文英文原文《20 Best Practices for Working With Apache Kafka at Scale》：https://blog.newrelic.com/engineering/kafka-best-practices/

--------


“ 这篇文章，同样给大家聊一个硬核的技术知识，我们通过Kafka内核源码中的一些设计思想，来看你设计Kafka架构的技术大牛，是怎么优化JVM的GC问题的？



1、Kafka的客户端缓冲机制
首先，先得给大家明确一个事情，那就是在客户端发送消息给kafka服务器的时候，一定是有一个内存缓冲机制的。



也就是说，消息会先写入一个内存缓冲中，然后直到多条消息组成了一个Batch，才会一次网络通信把Batch发送过去。



整个过程如下图所示：







2、内存缓冲造成的频繁GC问题
那么这种内存缓冲机制的本意，其实就是把多条消息组成一个Batch，一次网络请求就是一个Batch或者多个Batch。



这样每次网络请求都可以发送很多数据过去，避免了一条消息一次网络请求。从而提升了吞吐量，即单位时间内发送的数据量。



但是问题来了，大家可以思考一下，一个Batch中的数据，会取出来然后封装在底层的网络包里，通过网络发送出去到达Kafka服务器。



那么然后呢？这个Batch里的数据都发送过去了，现在Batch里的数据应该怎么处理？



你要知道，这些Batch里的数据此时可还在客户端的JVM的内存里啊！那么此时从代码实现层面，一定会尝试避免任何变量去引用这些Batch对应的数据，然后尝试触发JVM自动回收掉这些内存垃圾。



这样不断的让JVM回收垃圾，就可以不断的清理掉已经发送成功的Batch了，然后就可以不断的腾出来新的内存空间让后面新的数据来使用。



这种想法很好，但是实际线上运行的时候一定会有问题，最大的问题，就是JVM GC问题。



大家都知道一点，JVM GC在回收内存垃圾的时候，他会有一个“Stop the World”的过程，也就是垃圾回收线程运行的时候，会导致其他工作线程短暂的停顿，这样可以便于他自己安安静静的回收内存垃圾。



这个也很容易想明白，毕竟你要是在回收内存垃圾的时候，你的工作线程还在不断的往内存里写数据，制造更多的内存垃圾，那你让人家JVM怎么回收垃圾？



这就好比在大马路上，如果地上有很多垃圾，现在要把垃圾都扫干净，最好的办法是什么？大家都让开，把马路空出来，然后清洁工就是把垃圾清理干净。



但是如果清洁工在清扫垃圾的时候，结果一帮人在旁边不停的嗑瓜子扔瓜子壳，吃西瓜扔西瓜皮，不停的制造垃圾，你觉得清洁工内心啥感受？当然是很愤慨了，照这么搞，地上的垃圾永远的都搞不干净了！



通过了上面的语言描述，我们再来一张图，大家看看就更加清楚了







现在JVM GC是越来越先进，从CMS垃圾回收器到G1垃圾回收器，核心的目标之一就是不断的缩减垃圾回收的时候，导致其他工作线程停顿的时间。



所以现在越是新款的垃圾回收器导致工作线程停顿的时间越短，但是再怎么短，他也还是存在啊！



所以说，如何尽可能在自己的设计上避免JVM频繁的GC就是一个非常考验水平的事儿了。

3、Kafka设计者实现的缓冲池机制
在Kafka客户端内部，对这个问题实现了一个非常优秀的机制，就是缓冲池的机制

简单来说，就是每个Batch底层都对应一块内存空间，这个内存空间就是专门用来存放写入进去的消息的。

然后呢，当一个Batch被发送到了kafka服务器，这个Batch的数据不再需要了，就意味着这个Batch的内存空间不再使用了。



此时这个Batch底层的内存空间不要交给JVM去垃圾回收，而是把这块内存空间给放入一个缓冲池里。



这个缓冲池里放了很多块内存空间，下次如果你又有一个新的Batch了，那么不就可以直接从这个缓冲池里获取一块内存空间就ok了？



然后如果一个Batch发送出去了之后，再把内存空间给人家还回来不就好了？以此类推，循环往复。



同样，听完了上面的文字描述，再来一张图，看完这张图相信大伙儿就明白了：





一旦使用了这个缓冲池机制之后，就不涉及到频繁的大量内存的GC问题了。



为什么呢？因为他可以上来就占用固定的内存，比如32MB。然后把32MB划分为N多个内存块，比如说一个内存块是16KB，这样的话这个缓冲池里就会有很多的内存块。



然后你需要创建一个新的Batch，就从缓冲池里取一个16KB的内存块就可以了，然后这个Batch就不断的写入消息，但是最多就是写16KB，因为Batch底层的内存块就16KB。



接着如果Batch被发送到Kafka服务器了，此时Batch底层的内存块就直接还回缓冲池就可以了。



下次别人再要构建一个Batch的时候，再次使用缓冲池里的内存块就好了。这样就可以利用有限的内存，对他不停的反复重复的利用。因为如果你的Batch使用完了以后是把内存块还回到缓冲池中去，那么就不涉及到垃圾回收了。



如果没有频繁的垃圾回收，自然就避免了频繁导致的工作线程的停顿了，JVM GC问题是不是就得到了大幅度的优化？



没错，正是这个设计思想让Kafka客户端的性能和吞吐量都非常的高，这里蕴含了大量的优秀的机制。



那么此时有人说了，如果我现在把一个缓冲池里的内存资源都占满了，现在缓冲池里暂时没有内存块了，怎么办呢？



很简单，阻塞你的写入操作，不让你继续写入消息了。把你给阻塞住，不停的等待，直到有内存块释放出来，然后再继续让你写入消息。



4、总结一下
这篇文章我们从Kafka内存缓冲机制的设计思路开始，一直分析到了JVM GC问题的产生原因以及恶劣的影响。



接着谈到了Kafka优秀的缓冲池机制的设计思想以及他是如何解决这个问题的，分析了很多Kafka作者在设计的时候展现出的优秀的技术设计思想和能力。



希望大家多吸取这里的精华，在以后面试或者工作的时候，可以把这些优秀的思想纳为己用。



“ 请你简述一下Kafka中的分区分配 ！”



Duang！！！



当面试官问你这个问题的时候，你会怎么回答？



其实，这道题目里面就暗藏汹涌，因为Kafka中的分区分配在多处出现，而这个问题的表述方式是在潜意识里暗示你回答一种。



这样在你自认为很完美的回答完这个问题之后，面试官会冷不丁的来一句：还有呢？

当你回答完一个点的时候，面试官来一句还有呢，当你再补上一个的时候，他还是会来一句还有呢，就算你又补上第三个的时候，他还是会来一句还有呢？这个时候你会不会一脸懵逼？

今天就针对这个问题来告诉大家怎么样回答才能严丝合缝地抢得先机。

在Kafka中，分区分配是一个很重要的概念，却往往会被读者忽视，它会影响Kafka整体的性能均衡。

当遇到“分区分配”这个字眼的时候，一定要记住有三处地方，分别是生产者发送消息、消费者消费消息和创建主题。

虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。

在面对开篇的问题的时候，不如一下就进行总结性的陈词，说有三处，第一、第二、第三balabala。

当真的让你讲完三处的时候，时间也就差不多了。。聪明的面试官看到你一上来就做了一个规划总结，那他顶多也就让你说说你最熟悉的一种，其实说不定内心已经确认你是对的人。

下面针对这三处做个讲解。不过本文旨在罗列相关知识点，进行相关性的科普描述，让读者可以追根溯源，但并不陈述具体细节，因为细节很多，篇幅有限，如有需要请详参老朽的《深入理解Kafka》。

生产者的分区分配
对于用户而言，当调用send方法发送消息之后，消息就自然而然的发送到了broker中。

其实在这一过程中，有可能还要经过拦截器、序列化器和分区器（Partitioner）的一系列作用之后才能被真正地发往broker。

producer.send(record);
消息在发往broker之前是需要确定它所发往的分区的，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号。

如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。

Kafka中提供的默认分区器是DefaultPartitioner，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器），其中的partition方法就是用来实现具体的分区分配逻辑：

public int partition(String topic, Object key, byte[] keyBytes,
                     Object value, byte[] valueBytes, Cluster cluster);


默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率）

最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。

注意：如果key不为null，那么计算得到的分区号会是所有分区中的任意一个；



如果key为null并且有可用分区，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。


消费者的分区分配
在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。



如图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。

有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。

按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。

对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为RangeAssignor、RoundRobinAssignor以及StickyAssignor

其中RangeAssignor为默认的分区分配策略，至于这三种策略具体代表什么含义，可以去查阅相关资料，比如《深入理解Kafka》，嘿嘿。当然也可以通过实现ParitionAssignor接口来自定义分区分配策略。

在消费组中如果有多个消费者，那么这些消费者又可能会采用不同的分配策略，那么最后怎么“拍板”使用哪一种具体的分配策略呢？

对于这里，我想留一道思考题给大家：在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费，那么这个规则可以被打破么？

如果可以，怎么打破？打破的收益又是什么？



broker端的分区分配
生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区

而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。

分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。

在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；

如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。

使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。

如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。



大数据基础系列之kafka011生产者缓存超时，幂等性和事务实现
原创： 浪尖  Spark学习技巧  2017-07-30
一，demo及相关类

1，基本介绍

KafkaProducer是线程安全的，多线程间共享一个实例比共享多个实例更加高效。首先搞一个demo

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "all");
props.put("retries", 0);
props.put("batch.size", 16384);
props.put("linger.ms", 1);
props.put("buffer.memory", 33554432);
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

Producer<String, String> producer = new KafkaProducer<>(props);
for (int i = 0; i < 100; i++)
producer.send(new ProducerRecord<String, String>("my-topic", Integer.toString(i), Integer.toString(i)));

producer.close();

2，ProducerRecord

发往kafka的key/value对。由topic，分区id(可选)，key(可选)，timestamp(可选)，value组成。

如果一个有效的分区ID被指定，Record就会被发到指定的分区。如果，没指定分区id，只指定了key，就会按照key做hash后对分区数取余得到的数值作为分区的id。如果分区id，和key都没有指定，就会以轮训的形式发送Records。

Record还有一个timestamp属性。如果用户没有提供timestamp，生产者将会使用当前时间作为Record的timestamp。Kafka最终使用的时间戳取决于topic配置的时间类型。

1),如果topic配置使用了CreateTime，Broker就会使用生产者生产Record时带的时间戳。

2),如果topic配置使用了LogAppendTime，Record追加到log的时候，Broker会有本地时间代替Producer生产时带的时间戳。

无论是采用的上文中的哪种形式，timestamp都会被包含在RecordMetadata中返回。

ProducerRecord(String topic, Integer partition, K key, V value)
Creates a record to be sent to a specified topic and partition
  ProducerRecord(String topic, Integer partition, K key, V value, Iterable<Header> headers)
Creates a record to be sent to a specified topic and partition
  ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)
Creates a record with a specified timestamp to be sent to a specified topic and partition
  ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value, Iterable<Header> headers)
Creates a record with a specified timestamp to be sent to a specified topic and partition
  ProducerRecord(String topic, K key, V value)
Create a record to be sent to Kafka
  ProducerRecord(String topic, V value)
Create a record with no key

二，缓存和超时

生产者内部有一个buffer，用来缓存Record，同时内部有一个后台线程负责将Record转化为请求，然后将请求发给kafka集群。使用生产者后未关闭，会导致这些资源泄漏。

send方法是异步的。调用他实际上是将Record添加到Buffer中，然后立即返回。这使得生产者可以批量提交消息来提升性能。

acks配置控制发送请求完成的标准。如果设置成all，将会导致生产阻塞，等待所有副本提交日志成功后才算发送完成，超级低效但是可以最大限度的容错。

如果请求失败，生产者会自动尝试，前提是不要设置retries为零。当然，开启失败尝试也就意味着带来了数据重复发送的风险。

生产者为每个分区维护一个buffer，这个buffer的大小由batch.size指定，该值越大表示批量发送的消息数越多，也意味着需要更大的内存。内存数可以估计的。

默认情况下，即使buffer还有剩余的空间没有填充，消息也会被立即发送。如果你想减少请求的次数，可以设置linger.ms参数为大于0的某一值。使生产者发送消息前等待linger.ms指定的时间，这样就可以有更多的消息加入到该batch来。这很像TCP中的Nagle原理。例如，在上面的代码片段中，由于我们设置linger.ms为1ms，100条消息可能在一次请求中全部发送到了Server端。然而，这也意味着加入消息一直不能填充满buffer，我们要延迟一毫秒。

buffer.memory决定者生产者所能用于buffer的总内存大小。如果，消息发送的速度比传输到Server端的速度快，这个buffer空间就会耗尽。当buffer空间耗尽，send调用就会阻塞，超过max.block.ms设置的超时时间后会抛出TimeoutException。

三，序列化

Key.serializer和value.serialize决定者如何将key和value对象转化为字节数组。你可以使用包括bytearrayserializer或stringserializer简单的字符串或字节类型。也可以实现自定义的序列化方式。

四，幂等性

从kafka0.11版本开始，Kafka支持两种额外的模式：幂等性生产者和事务生产者。幂等性强化消息的传递语义，从至少一次到仅仅一次。特别是生产者重试将不再导致消息重复发送。事务生产者允许应用程序将消息原子的发送到多个分区（和主题！）。

设置enable.idempotence为true来开启幂等性，如果设置了这个参数retries配置将会被设置为默认值，也即Integer.MAX_VALUE，max.inflight.requests.per.connection会被设置为1，acks会被设置为all。幂等性生产者不需要修改API，所以现有的应用程序不需要修改就可以使用该特性。

为了利用幂等生产者，必须避免应用程序级重新发送，因为这些不能被去重。例如，如果应用程序运行幂等性，建议不要设置retries，因为他会被设置为默认值(Integer.MAX_VALUE).此外，如果send（producerrecord）返回一个错误甚至无限重试（例如,如果消息送前缓冲区满了），建议关闭生产和检查最后产生消息的内容以确保不重复。

五，事务

为了使用事务生产者和相关的APIs，必须要设置transactional.id属性.如果设置了transactional.id幂等性会自动被启用。支持事务的topic必须要进行容错配置。特别的replication.factor应该设置为3，topic的min.insync.replicas配置必须设置为2.最后，为了从端到端实现事务性保证，必须配置消费者只读取committed 的消息。

transactional.id目的是单生产者实例能从多会话中恢复。该特性就是分区的，状态的应用程序程序中的一个碎片标识符。transactional.id值在一个分区的应用中每个消费者实例必须是唯一的。

所有新的事务性API都会被阻塞，将在失败时抛出异常。举一个简单的例子，一次事务中提交100条消息。

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("transactional.id", "my-transactional-id");
Producer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());

producer.initTransactions();

try {
  producer.beginTransaction();
  for (int i = 0; i < 100; i++)
  producer.send(new ProducerRecord<>("my-topic", Integer.toString(i), Integer.toString(i)));
  producer.commitTransaction();
} catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {
  // We can't recover from these exceptions, so our only option is to close the producer and exit.
  producer.close();
} catch (KafkaException e) {
  // For all other exceptions, just abort the transaction and try again.
  producer.abortTransaction();
}
producer.close();

就如例子一样，每个消费者只能有一个事务开启。在beginTransaction() 和commitTransaction()中间发送的所有消息，都是一次事务的一部分。

事务生产者使用execeptions进行错误状态交流。特别之处，我们不需要为producer.send指定回调函数。任何在事务中不可恢复的错误发生都会抛出一个KafkaException异常(http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send(org.apache.kafka.clients.producer.ProducerRecord))。

在接受到一个kafkaexection异常之后，通过调用producer.abortTransaction()，可以保证所有的已经写入成功的消息会被标记为aborted，因此保证事务传输。
==========
简历写了会Kafka，面试官90%会让你讲讲acks参数对消息持久化的影响




面试大厂时，一旦简历上写了Kafka，几乎必然会被问到一个问题：说说acks参数对消息持久化的影响？
这个acks参数在kafka的使用中，是非常核心以及关键的一个参数，决定了很多东西。


（1）如何保证宕机的时候数据不丢失？


如果要想理解这个acks参数的含义，首先就得搞明白kafka的高可用架构原理。



比如下面的图里就是表明了对于每一个Topic，我们都可以设置他包含几个Partition，每个Partition负责存储这个Topic一部分的数据。



然后Kafka的Broker集群中，每台机器上都存储了一些Partition，也就存放了Topic的一部分数据，这样就实现了Topic的数据分布式存储在一个Broker集群上。



但是有一个问题，万一 一个Kafka Broker宕机了，此时上面存储的数据不就丢失了吗？



没错，这就是一个比较大的问题了，分布式系统的数据丢失问题，是他首先必须要解决的，一旦说任何一台机器宕机，此时就会导致数据的丢失。







（2）多副本冗余的高可用机制


所以如果大家去分析任何一个分布式系统的原理，比如说zookeeper、kafka、redis cluster、elasticsearch、hdfs，等等，其实他都有自己内部的一套多副本冗余的机制，多副本冗余几乎是现在任何一个优秀的分布式系统都一般要具备的功能。



在kafka集群中，每个Partition都有多个副本，其中一个副本叫做leader，其他的副本叫做follower，如下图。



如上图所示，假设一个Topic拆分为了3个Partition，分别是Partition0，Partiton1，Partition2，此时每个Partition都有2个副本。



比如Partition0有一个副本是Leader，另外一个副本是Follower，Leader和Follower两个副本是分布在不同机器上的。



这样的多副本冗余机制，可以保证任何一台机器挂掉，都不会导致数据彻底丢失，因为起码还是有副本在别的机器上的。







（3）多副本之间数据如何同步？


其实任何一个Partition，只有Leader是对外提供读写服务的

也就是说，如果有一个客户端往一个Partition写入数据，此时一般就是写入这个Partition的Leader副本。


然后Leader副本接收到数据之后，Follower副本会不停的给他发送请求尝试去拉取最新的数据，拉取到自己本地后，写入磁盘中。如下图所示：







（4）ISR到底指的是什么东西？


既然大家已经知道了Partiton的多副本同步数据的机制了，那么就可以来看看ISR是什么了。



ISR全称是“In-Sync Replicas”，也就是保持同步的副本，他的含义就是，跟Leader始终保持同步的Follower有哪些。



大家可以想一下 ，如果说某个Follower所在的Broker因为JVM FullGC之类的问题，导致自己卡顿了，无法及时从Leader拉取同步数据，那么是不是会导致Follower的数据比Leader要落后很多？



所以这个时候，就意味着Follower已经跟Leader不再处于同步的关系了。但是只要Follower一直及时从Leader同步数据，就可以保证他们是处于同步的关系的。



所以每个Partition都有一个ISR，这个ISR里一定会有Leader自己，因为Leader肯定数据是最新的，然后就是那些跟Leader保持同步的Follower，也会在ISR里。







（5）acks参数的含义


铺垫了那么多的东西，最后终于可以进入主题来聊一下acks参数的含义了。



如果大家没看明白前面的那些副本机制、同步机制、ISR机制，那么就无法充分的理解acks参数的含义，这个参数实际上决定了很多重要的东西。



首先这个acks参数，是在KafkaProducer，也就是生产者客户端里设置的



也就是说，你往kafka写数据的时候，就可以来设置这个acks参数。然后这个参数实际上有三种常见的值可以设置，分别是：0、1 和 all。



第一种选择是把acks参数设置为0，意思就是我的KafkaProducer在客户端，只要把消息发送出去，不管那条数据有没有在哪怕Partition Leader上落到磁盘，我就不管他了，直接就认为这个消息发送成功了。



如果你采用这种设置的话，那么你必须注意的一点是，可能你发送出去的消息还在半路。结果呢，Partition Leader所在Broker就直接挂了，然后结果你的客户端还认为消息发送成功了，此时就会导致这条消息就丢失了。





第二种选择是设置 acks = 1，意思就是说只要Partition Leader接收到消息而且写入本地磁盘了，就认为成功了，不管他其他的Follower有没有同步过去这条消息了。



这种设置其实是kafka默认的设置，大家请注意，划重点！这是默认的设置



也就是说，默认情况下，你要是不管acks这个参数，只要Partition Leader写成功就算成功。



但是这里有一个问题，万一Partition Leader刚刚接收到消息，Follower还没来得及同步过去，结果Leader所在的broker宕机了，此时也会导致这条消息丢失，因为人家客户端已经认为发送成功了。



最后一种情况，就是设置acks=all，这个意思就是说，Partition Leader接收到消息之后，还必须要求ISR列表里跟Leader保持同步的那些Follower都要把消息同步过去，才能认为这条消息是写入成功了。



如果说Partition Leader刚接收到了消息，但是结果Follower没有收到消息，此时Leader宕机了，那么客户端会感知到这个消息没发送成功，他会重试再次发送消息过去。



此时可能Partition 2的Follower变成Leader了，此时ISR列表里只有最新的这个Follower转变成的Leader了，那么只要这个新的Leader接收消息就算成功了。






（6）最后的思考


acks=all 就可以代表数据一定不会丢失了吗？
当然不是，如果你的Partition只有一个副本，也就是一个Leader，任何Follower都没有，你认为acks=all有用吗？
当然没用了，因为ISR里就一个Leader，他接收完消息后宕机，也会导致数据丢失。

所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。
这样才能保证说写一条数据过去，一定是2个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。


----------------------------

Kafka分区分配策略(Partition Assignment Strategy)
原创： 过往记忆  过往记忆大数据  2018-05-09
问题
用过 Kafka 的同学用过都知道，每个 Topic 一般会有很多个 partitions。为了使得我们能够及时消费消息，我们也可能会启动多个 Consumer 去消费，而每个 Consumer 又会启动一个或多个streams去分别消费 Topic 里面的数据。我们又知道，Kafka 存在 Consumer Group 的概念，也就是 group.id 一样的 Consumer，这些 Consumer 属于同一个Consumer Group，组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费。那么问题来了，同一个 Consumer Group 里面的 Consumer 是如何知道该消费哪些分区里面的数据呢？



如果想及时了解Spark、Hadoop或者Hbase相关的文章，欢迎关注微信公共帐号：iteblog_hadoop

如上图，Consumer1 为啥消费的是 Partition0 和 Partition2，而不是 Partition0 和 Partition3？这就涉及到 Kafka 内部分区分配策略（Partition Assignment Strategy）了。

在 Kafka 内部存在两种默认的分区分配策略：Range 和 RoundRobin。当以下事件发生时，Kafka 将会进行一次分区分配：

同一个 Consumer Group 内新增消费者

消费者离开当前所属的Consumer Group，包括shuts down 或 crashes

订阅的主题新增分区

将分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance），如何rebalance就涉及到本文提到的分区分配策略。下面我们将详细介绍 Kafka 内置的两种分区分配策略。本文假设我们有个名为 T1 的主题，其包含了10个分区，然后我们有两个消费者（C1，C2）来消费这10个分区里面的数据，而且 C1 的 num.streams = 1，C2 的 num.streams = 2。

Range strategy
Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。在我们的例子里面，排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是C1-0, C2-0, C2-1。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程， 10 / 3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：

C1-0 将消费 0, 1, 2, 3 分区
C2-0 将消费 4, 5, 6 分区
C2-1 将消费 7, 8, 9 分区
假如我们有11个分区，那么最后分区分配的结果看起来是这样的：

C1-0 将消费 0, 1, 2, 3 分区
C2-0 将消费 4, 5, 6, 7 分区
C2-1 将消费 8, 9, 10 分区
假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的：

C1-0 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区
C2-0 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区
C2-1 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区
可以看出，C1-0 消费者线程比其他消费者线程多消费了2个分区，这就是Range strategy的一个很明显的弊端。

RoundRobin strategy
使用RoundRobin策略有两个前提条件必须满足：

同一个Consumer Group里面的所有消费者的num.streams必须相等；

每个消费者订阅的主题必须相同。

所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，这里文字可能说不清，看下面的代码应该会明白：

val allTopicPartitions = ctx.partitionsForTopic.flatMap { case(topic, partitions) =>
  info("Consumer %s rebalancing the following partitions for topic %s: %s"
       .format(ctx.consumerId, topic, partitions))
  partitions.map(partition => {
    TopicAndPartition(topic, partition)
  })
}.toSeq.sortWith((topicPartition1, topicPartition2) => {
  /*
   * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending
   * up on one consumer (if it has a high enough stream count).
   */
  topicPartition1.toString.hashCode < topicPartition2.toString.hashCode
})
最后按照round-robin风格将分区分别分配给不同的消费者线程。

在我们的例子里面，加入按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为：

C1-0 将消费 T1-5, T1-2, T1-6 分区；
C1-1 将消费 T1-3, T1-1, T1-9 分区；
C2-0 将消费 T1-0, T1-4 分区；
C2-1 将消费 T1-8, T1-7 分区；
多个主题的分区分配和单个主题类似，这里就不在介绍了。

根据上面的详细介绍相信大家已经对Kafka的分区分配策略原理很清楚了。不过遗憾的是，目前我们还不能自定义分区分配策略，只能通过partition.assignment.strategy参数选择 range 或 roundrobin。partition.assignment.strategy参数默认的值是range。



水位或水印（watermark）一词，也可称为高水位(high watermark)，通常被用在流式处理领域（比如Apache Flink、Apache Spark等），以表征元素或事件在基于时间层面上的进度。一个比较经典的表述为：流式系统保证在水位t时刻，创建时间（event time） = t'且t' ≤ t的所有事件都已经到达或被观测到。在Kafka中，水位的概念反而与时间无关，而是与位置信息相关。严格来说，它表示的就是位置信息，即位移（offset）。网上有一些关于Kafka watermark的介绍，本不应再赘述，但鉴于本文想要重点强调的leader epoch与watermark息息相关，故这里再费些篇幅阐述一下watermark。注意：由于Kafka源码中使用的名字是高水位，故本文将始终使用high watermaker或干脆简称为HW。

Kafka分区下有可能有很多个副本(replica)用于实现冗余，从而进一步实现高可用。副本根据角色的不同可分为3类：

leader副本：响应clients端读写请求的副本
follower副本：被动地备份leader副本中的数据，不能响应clients端读写请求。
ISR副本：包含了leader副本和所有与leader副本保持同步的follower副本——如何判定是否与leader同步后面会提到
每个Kafka副本对象都有两个重要的属性：LEO和HW。注意是所有的副本，而不只是leader副本。

LEO：即日志末端位移(log end offset)，记录了该副本底层日志(log)中下一条消息的位移值。注意是下一条消息！也就是说，如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0, 9]。另外，leader LEO和follower LEO的更新是有区别的。我们后面会详细说
HW：即上面提到的水位值。对于同一个副本对象而言，其HW值不会大于LEO值。小于等于HW值的所有消息都被认为是“已备份”的（replicated）。同理，leader副本和follower副本的HW更新是有区别的，我们后面详谈。
我们使用下图来形象化地说明两者的关系：

<ignore_js_op>

上图中，HW值是7，表示位移是0~7的所有消息都已经处于“已备份状态”（committed），而LEO值是15，那么8~14的消息就是尚未完全备份（fully replicated）——为什么没有15？因为刚才说过了，LEO指向的是下一条消息到来时的位移，故上图使用虚线框表示。我们总说consumer无法消费未提交消息。这句话如果用以上名词来解读的话，应该表述为：consumer无法消费分区下leader副本中位移值大于分区HW的任何消息。这里需要特别注意分区HW就是leader副本的HW值。

既然副本分为leader副本和follower副本，而每个副本又都有HW和LEO，那么它们是怎么被更新的呢？它们更新的机制又有什么区别呢？我们一一来分析下：

一、follower副本何时更新LEO？

如前所述，follower副本只是被动地向leader副本请求数据，具体表现为follower副本不停地向leader副本所在的broker发送FETCH请求，一旦获取消息后写入自己的日志中进行备份。那么follower副本的LEO是何时更新的呢？首先我必须言明，Kafka有两套follower副本LEO(明白这个是搞懂后面内容的关键，因此请多花一点时间来思考)：1. 一套LEO保存在follower副本所在broker的副本管理机中；2. 另一套LEO保存在leader副本所在broker的副本管理机中——换句话说，leader副本机器上保存了所有的follower副本的LEO。

为什么要保存两套？这是因为Kafka使用前者帮助follower副本更新其HW值；而利用后者帮助leader副本更新其HW使用。下面我们分别看下它们被更新的时机。

1 follower副本端的follower副本LEO何时更新？

follower副本端的LEO值就是其底层日志的LEO值，也就是说每当新写入一条消息，其LEO值就会被更新(类似于LEO += 1)。当follower发送FETCH请求后，leader将数据返回给follower，此时follower开始向底层log写数据，从而自动地更新LEO值

2 leader副本端的follower副本LEO何时更新？

leader副本端的follower副本LEO的更新发生在leader在处理follower FETCH请求时。一旦leader接收到follower发送的FETCH请求，它首先会从自己的log中读取相应的数据，但是在给follower返回数据之前它先去更新follower的LEO(即上面所说的第二套LEO)

二、follower副本何时更新HW？

follower更新HW发生在其更新LEO之后，一旦follower向log写完数据，它会尝试更新它自己的HW值。具体算法就是比较当前LEO值与FETCH响应中leader的HW值，取两者的小者作为新的HW值。这告诉我们一个事实：如果follower的LEO值超过了leader的HW值，那么follower HW值是不会越过leader HW值的。

三、leader副本何时更新LEO？

和follower更新LEO道理相同，leader写log时就会自动地更新它自己的LEO值。

四、leader副本何时更新HW值？

前面说过了，leader的HW值就是分区HW值，因此何时更新这个值是我们最关心的，因为它直接影响了分区数据对于consumer的可见性 。以下4种情况下leader会尝试去更新分区HW——切记是尝试，有可能因为不满足条件而不做任何更新：

副本成为leader副本时：当某个副本成为了分区的leader副本，Kafka会尝试去更新分区HW。这是显而易见的道理，毕竟分区leader发生了变更，这个副本的状态是一定要检查的！不过，本文讨论的是当系统稳定后且正常工作时备份机制可能出现的问题，故这个条件不在我们的讨论之列。
broker出现崩溃导致副本被踢出ISR时：若有broker崩溃则必须查看下是否会波及此分区，因此检查下分区HW值是否需要更新是有必要的。本文不对这种情况做深入讨论
producer向leader副本写入消息时：因为写入消息会更新leader的LEO，故有必要再查看下HW值是否也需要修改
leader处理follower FETCH请求时：当leader处理follower的FETCH请求时首先会从底层的log读取数据，之后会尝试更新分区HW值
特别注意上面4个条件中的最后两个。它揭示了一个事实——当Kafka broker都正常工作时，分区HW值的更新时机有两个：leader处理PRODUCE请求时和leader处理FETCH请求时。另外，leader是如何更新它的HW值的呢？前面说过了，leader broker上保存了一套follower副本的LEO以及它自己的LEO。当尝试确定分区HW时，它会选出所有满足条件的副本，比较它们的LEO(当然也包括leader自己的LEO)，并选择最小的LEO值作为HW值。这里的满足条件主要是指副本要满足以下两个条件之一：

处于ISR中
副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值(默认是10s)
乍看上去好像这两个条件说得是一回事，毕竟ISR的定义就是第二个条件描述的那样。但某些情况下Kafka的确可能出现副本已经“追上”了leader的进度，但却不在ISR中——比如某个从failure中恢复的副本。如果Kafka只判断第一个条件的话，确定分区HW值时就不会考虑这些未在ISR中的副本，但这些副本已经具备了“立刻进入ISR”的资格，因此就可能出现分区HW值越过ISR中副本LEO的情况——这肯定是不允许的，因为分区HW实际上就是ISR中所有副本LEO的最小值。

好了，理论部分我觉得说的差不多了，下面举个实际的例子。我们假设有一个topic，单分区，副本因子是2，即一个leader副本和一个follower副本。我们看下当producer发送一条消息时，broker端的副本到底会发生什么事情以及分区HW是如何被更新的。

下图是初始状态，我们稍微解释一下：初始时leader和follower的HW和LEO都是0(严格来说源代码会初始化LEO为-1，不过这不影响之后的讨论)。leader中的remote LEO指的就是leader端保存的follower LEO，也被初始化成0。此时，producer没有发送任何消息给leader，而follower已经开始不断地给leader发送FETCH请求了，但因为没有数据因此什么都不会发生。值得一提的是，follower发送过来的FETCH请求因为无数据而暂时会被寄存到leader端的purgatory中，待500ms(replica.fetch.wait.max.ms参数)超时后会强制完成。倘若在寄存期间producer端发送过来数据，那么会Kafka会自动唤醒该FETCH请求，让leader继续处理之。

虽然purgatory不是本文的重点，但FETCH请求发送和PRODUCE请求处理的时机会影响我们的讨论。因此后续我们也将分两种情况来讨论分区HW的更新。

<ignore_js_op>

第一种情况：follower发送FETCH请求在leader处理完PRODUCE请求之后

producer给该topic分区发送了一条消息。此时的状态如下图所示：

<ignore_js_op>

如图所示，leader接收到PRODUCE请求主要做两件事情：

把消息写入写底层log（同时也就自动地更新了leader的LEO）
尝试更新leader HW值（前面leader副本何时更新HW值一节中的第三个条件触发）。我们已经假设此时follower尚未发送FETCH请求，那么leader端保存的remote LEO依然是0，因此leader会比较它自己的LEO值和remote LEO值，发现最小值是0，与当前HW值相同，故不会更新分区HW值
所以，PRODUCE请求处理完成后leader端的HW值依然是0，而LEO是1，remote LEO是1。假设此时follower发送了FETCH请求(或者说follower早已发送了FETCH请求，只不过在broker的请求队列中排队)，那么状态变更如下图所示：

<ignore_js_op>

本例中当follower发送FETCH请求时，leader端的处理依次是：

读取底层log数据
更新remote LEO = 0（为什么是0？ 因为此时follower还没有写入这条消息。leader如何确认follower还未写入呢？这是通过follower发来的FETCH请求中的fetch offset来确定的）
尝试更新分区HW——此时leader LEO = 1，remote LEO = 0，故分区HW值= min(leader LEO, follower remote LEO) = 0
把数据和当前分区HW值（依然是0）发送给follower副本
而follower副本接收到FETCH response后依次执行下列操作：

写入本地log（同时更新follower LEO）
更新follower HW——比较本地LEO和当前leader LEO取小者，故follower HW = 0
此时，第一轮FETCH RPC结束，我们会发现虽然leader和follower都已经在log中保存了这条消息，但分区HW值尚未被更新。实际上，它是在第二轮FETCH RPC中被更新的，如下图所示：

<ignore_js_op>

上图中，follower发来了第二轮FETCH请求，leader端接收到后仍然会依次执行下列操作：

读取底层log数据
更新remote LEO = 1（这次为什么是1了？ 因为这轮FETCH RPC携带的fetch offset是1，那么为什么这轮携带的就是1了呢，因为上一轮结束后follower LEO被更新为1了）
尝试更新分区HW——此时leader LEO = 1，remote LEO = 1，故分区HW值= min(leader LEO, follower remote LEO) = 1。注意分区HW值此时被更新了！！！
把数据（实际上没有数据）和当前分区HW值（已更新为1）发送给follower副本
同样地，follower副本接收到FETCH response后依次执行下列操作：

写入本地log，当然没东西可写，故follower LEO也不会变化，依然是1
更新follower HW——比较本地LEO和当前leader LEO取小者。由于此时两者都是1，故更新follower HW = 1 （注意：我特意用了两种颜色来描述这两步，后续会谈到原因！）
Okay，producer端发送消息后broker端完整的处理流程就讲完了。此时消息已经成功地被复制到leader和follower的log中且分区HW是1，表明consumer能够消费offset = 0的这条消息。下面我们来分析下PRODUCE和FETCH请求交互的第二种情况。

第二种情况：FETCH请求保存在purgatory中PRODUCE请求到来

这种情况实际上和第一种情况差不多。前面说过了，当leader无法立即满足FECTH返回要求的时候(比如没有数据)，那么该FETCH请求会被暂存到leader端的purgatory中，待时机成熟时会尝试再次处理它。不过Kafka不会无限期地将其缓存着，默认有个超时时间（500ms），一旦超时时间已过，则这个请求会被强制完成。不过我们要讨论的场景是在寄存期间，producer发送PRODUCE请求从而使之满足了条件从而被唤醒。此时，leader端处理流程如下：

leader写入本地log（同时自动更新leader LEO）
尝试唤醒在purgatory中寄存的FETCH请求
尝试更新分区HW
至于唤醒后的FETCH请求的处理与第一种情况完全一致，故这里不做详细展开了。

以上所有的东西其实就想说明一件事情：Kafka使用HW值来决定副本备份的进度，而HW值的更新通常需要额外一轮FETCH RPC才能完成，故而这种设计是有问题的。它们可能引起的问题包括：

备份数据丢失
备份数据不一致
我们一一分析下：

一、数据丢失

如前所述，使用HW值来确定备份进度时其值的更新是在下一轮RPC中完成的。现在翻到上面使用两种不同颜色标记的步骤处思考下， 如果follower副本在蓝色标记的第一步与紫色标记的第二步之间发生崩溃，那么就有可能造成数据的丢失。我们举个例子来看下。

<ignore_js_op>

上图中有两个副本：A和B。开始状态是A是leader。我们假设producer端min.insync.replicas设置为1，那么当producer发送两条消息给A后，A写入到底层log，此时Kafka会通知producer说这两条消息写入成功。

但是在broker端，leader和follower底层的log虽都写入了2条消息且分区HW已经被更新到2，但follower HW尚未被更新（也就是上面紫色颜色标记的第二步尚未执行）。倘若此时副本B所在的broker宕机，那么重启回来后B会自动把LEO调整到之前的HW值，故副本B会做日志截断(log truncation)，将offset = 1的那条消息从log中删除，并调整LEO = 1，此时follower副本底层log中就只有一条消息，即offset = 0的消息。

B重启之后需要给A发FETCH请求，但若A所在broker机器在此时宕机，那么Kafka会令B成为新的leader，而当A重启回来后也会执行日志截断，将HW调整回1。这样，位移=1的消息就从两个副本的log中被删除，即永远地丢失了。

这个场景丢失数据的前提是在min.insync.replicas=1时，一旦消息被写入leader端log即被认为是“已提交”，而延迟一轮FETCH RPC更新HW值的设计使得follower HW值是异步延迟更新的，倘若在这个过程中leader发生变更，那么成为新leader的follower的HW值就有可能是过期的，使得clients端认为是成功提交的消息被删除。

二、leader/follower数据离散

除了可能造成的数据丢失以外，这种设计还有一个潜在的问题，即造成leader端log和follower端log的数据不一致。比如leader端保存的记录序列是r1,r2,r3,r4,r5,....；而follower端保存的序列可能是r1,r3,r4,r5,r6...。这也是非法的场景，因为顾名思义，follower必须追随leader，完整地备份leader端的数据。

我们依然使用一张图来说明这种场景是如何发生的：

<ignore_js_op>

这种情况的初始状态与情况1有一些不同的：A依然是leader，A的log写入了2条消息，但B的log只写入了1条消息。分区HW更新到2，但B的HW还是1，同时producer端的min.insync.replicas = 1。

这次我们让A和B所在机器同时挂掉，然后假设B先重启回来，因此成为leader，分区HW = 1。假设此时producer发送了第3条消息(绿色框表示)给B，于是B的log中offset = 1的消息变成了绿色框表示的消息，同时分区HW更新到2（A还没有回来，就B一个副本，故可以直接更新HW而不用理会A）之后A重启回来，需要执行日志截断，但发现此时分区HW=2而A之前的HW值也是2，故不做任何调整。此后A和B将以这种状态继续正常工作。

显然，这种场景下，A和Bdicenglog中保存在offset = 1的消息是不同的记录，从而引发不一致的情形出现。


Kafka 0.11.0.0.版本解决方案

造成上述两个问题的根本原因在于HW值被用于衡量副本备份的成功与否以及在出现failture时作为日志截断的依据，但HW值的更新是异步延迟的，特别是需要额外的FETCH请求处理流程才能更新，故这中间发生的任何崩溃都可能导致HW值的过期。鉴于这些原因，Kafka 0.11引入了leader epoch来取代HW值。Leader端多开辟一段内存区域专门保存leader的epoch信息，这样即使出现上面的两个场景也能很好地规避这些问题。

所谓leader epoch实际上是一对值：（epoch，offset）。epoch表示leader的版本号，从0开始，当leader变更过1次时epoch就会+1，而offset则对应于该epoch版本的leader写入第一条消息的位移。因此假设有两对值：

(0, 0)
(1, 120)

则表示第一个leader从位移0开始写入消息；共写了120条[0, 119]；而第二个leader版本号是1，从位移120处开始写入消息。

leader broker中会保存这样的一个缓存，并定期地写入到一个checkpoint文件中。

当leader写底层log时它会尝试更新整个缓存——如果这个leader首次写消息，则会在缓存中增加一个条目；否则就不做更新。而每次副本重新成为leader时会查询这部分缓存，获取出对应leader版本的位移，这就不会发生数据不一致和丢失的情况。


下面我们依然使用图的方式来说明下利用leader epoch如何规避上述两种情况

一、规避数据丢失

<ignore_js_op>

上图左半边已经给出了简要的流程描述，这里不详细展开具体的leader epoch实现细节（比如OffsetsForLeaderEpochRequest的实现），我们只需要知道每个副本都引入了新的状态来保存自己当leader时开始写入的第一条消息的offset以及leader版本。这样在恢复的时候完全使用这些信息而非水位来判断是否需要截断日志。

二、规避数据不一致

<ignore_js_op>


同样的道理，依靠leader epoch的信息可以有效地规避数据不一致的问题。

总结

0.11.0.0版本的Kafka通过引入leader epoch解决了原先依赖水位表示副本进度可能造成的数据丢失/数据不一致问题。有兴趣的读者可以阅读源代码进一步地了解其中的工作原理。

源代码位置：kafka.server.epoch.LeaderEpochCache.scala （leader epoch数据结构）、
kafka.server.checkpoints.LeaderEpochCheckpointFile（checkpoint检查点文件操作类）还有分布在Log中的CRUD操作。