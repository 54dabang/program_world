第13章Kafka设计理念与基本架构
13.1Kafka产生的背景
13.2消息队列系统
13.2.1概述
13.2.2常用的消息队列系统对比
13.2.3Kafka特点及特性
13.2.4Kafka系统应用场景
13.3Kafka设计理念
13.3.1专业术语解析
13.3.2消息存储与缓存设计
13.3.3消费者与生产者模型
13.3.4Push与Pull机制
13.3.5镜像机制
13.4Kafka整体架构
13.4.1Kafka基本组成结构
13.4.2Kafka工作流程
13.5Kafka性能分析及优化
13.6Kafka未来研究方向
13.7小结
第14章Kafka核心组件及核心特性剖析
14.1Kafka核心组件剖析
14.1.1Producers
14.1.2Consumers
14.1.3Low Level Consumer
14.1.4High Level Consumer
14.2Kafka核心特性剖析
14.2.1Topic、Partitions
14.2.2Replication和Leader Election
14.2.3Consumer Rebalance
14.2.4消息传送机制
14.2.5Kafka的可靠性
14.2.6Kafka的高效性
14.3Kafka即将发布版本核心组件及特性剖析
14.3.1重新设计的Consumer
14.3.2Coordinator Rebalance
14.4小结
第15章Kafka应用实践
15.1Kafka开发环境搭建及运行环境部署
15.1.1Kafka开发环境配置
15.1.2Kafka运行环境安装与部署
15.2基于Kafka客户端开发
15.2.1消息生产者（Producer）设计
15.2.2消息消费者（Consumer）设计
15.2.3Kafka消费者与生产者配置

．1　基本概念
1．2　安装与配置
1．3　生产与消费
1．4　服务端参数配置
1．5　总结
第2章　生产者
2．1　客户端开发
2．1．1　必要的参数配置
2．1．2　消息的发送
2．1．3　序列化
2．1．4　分区器
2．1．5　生产者拦截器
2．2　原理分析
2．2．1　整体架构
2．2．2　元数据的更新
2．3　重要的生产者参数
2．4　总结
第3章　消费者
3．1　消费者与消费组
3．2　客户端开发
3．2．1　必要的参数配置
3．2．2　订阅主题与分区
3．2．3　反序列化
3．2．4　消息消费
3．2．5　位移提交
3．2．6　控制或关闭消费
3．2．7　指定位移消费
3．2．8　再均衡
3．2．9　消费者拦截器
3．2．10　多线程实现
3．2．11　重要的消费者参数
3．3　总结
第4章　主题与分区
4．1　主题的管理
4．1．1　创建主题
4．1．2　分区副本的分配
4．1．3　查看主题
4．1．4　修改主题
4．1．5　配置管理
4．1．6　主题端参数
4．1．7　删除主题
4．2　初识KafkaAdminClient
4．2．1　基本使用
4．2．2　主题合法性验证
4．3　分区的管理
4．3．1　优先副本的选举
4．3．2　分区重分配
4．3．3　复制限流
4．3．4　修改副本因子
4．4　如何选择合适的分区数
4．4．1　性能测试工具
4．4．2　分区数越多吞吐量就越高吗
4．4．3　分区数的上限
4．4．4　考量因素
4．5　总结
第5章　日志存储
5．1　文件目录布局
5．2　日志格式的演变
5．2．1　v0版本
5．2．2　v1版本
5．2．3　消息压缩
5．2．4　变长字段
5．2．5　v2版本
5．3　日志索引
5．3．1　偏移量索引
5．3．2　时间戳索引
5．4　日志清理
5．4．1　日志删除
5．4．2　日志压缩
5．5　磁盘存储
5．5．1　页缓存
5．5．2　磁盘I/O流程
5．5．3　零拷贝
5．6　总结
第6章　深入服务端
6．1　协议设计
6．2　时间轮
6．3　延时操作
6．4　控制器
6．4．1　控制器的选举及异常恢复
6．4．2　优雅关闭
6．4．3　分区leader的选举
6．5　参数解密
6．5．1　broker．id
6．5．2　bootstrap．servers111111111111
6．5．3　服务端参数列表
6．6　总结
第7章　深入客户端
7．1　分区分配策略
7．1．1　RangeAssignor分配策略
7．1．2　RoundRobinAssignor分配策略
7．1．3　StickyAssignor分配策略
7．1．4　自定义分区分配策略
7．2　消费者协调器和组协调器
7．2．1　旧版消费者客户端的问题
7．2．2　再均衡的原理
7．3　__consumer_offsets剖析
7．4　事务
7．4．1　消息传输保障
7．4．2　幂等
7．4．3　事务
7．5　总结
第8章　可靠性探究
8．1　副本剖析
8．1．1　失效副本
8．1．2　ISR的伸缩
8．1．3　LEO与HW
8．1．4　Leader Epoch的介入
8．1．5　为什么不支持读写分离
8．2　日志同步机制
8．3　可靠性分析
8．4　总结
第9章　Kafka应用
9．1　命令行工具
9．1．1　消费组管理
9．1．2　消费位移管理
9．1．3　手动删除消息
9．2　Kafka Connect
9．2．1　独立模式
9．2．2　REST API
9．2．3　分布式模式
9．3　Kafka Mirror Maker
9．4　Kafka Streams
9．5　总结
第10章　Kafka监控
10．1　监控数据的来源
10．1．1　OneMinuteRate
10．1．2　获取监控指标
10．2　消费滞后
10．3　同步失效分区
10．4　监控指标说明
10．5　监控模块
10．6　总结
第11章　高级应用
11．1　过期时间（TTL）
11．2　延时队列
11．3　死信队列和重试队列
11．4　消息路由
11．5　消息轨迹
11．6　消息审计
11．7　消息代理
11．7．1　快速入门
11．7．2　REST API介绍及示例
11．7．3　服务端配置及部署
11．7．4　应用思考
11．8　消息中间件选型
11．8．1　各类消息中间件简述
11．8．2　选型要点概述
11．8．3　消息中间件选型误区探讨
11．9　总结
第12章　Kafka与Spark的集成
12．1　Spark的安装及简单应用
12．2　Spark编程模型
12．3　Spark的运行结构
12．4　Spark Streaming简介
12．5　Kafka与Spark Streaming的整合
12．6　Spark SQL
12．7　Structured Streaming
12．8　Kafka与Structured Streaming的整合

val df= spark                                                                                                                

.readStream

.format("kafka")

.option("kafka.bootstrap.servers","host1:port1,host2:port2")

.option("subscribe","topic1")

.load()

df.selectExpr("CAST(key AS STRING)","CAST(value AS STRING)")

.as[(String,String)]

// Subscribe to multiple topics

val df= spark

.readStream

.format("kafka")

.option("kafka.bootstrap.servers","host1:port1,host2:port2")

.option("subscribe","topic1,topic2")

.load()

df.selectExpr("CAST(key AS STRING)","CAST(value AS STRING)")

.as[(String,String)]

// Subscribe to a pattern

val df= spark

.readStream

.format("kafka")

.option("kafka.bootstrap.servers","host1:port1,host2:port2")

.option("subscribePattern","topic.*")

.load()

df.selectExpr("CAST(key AS STRING)","CAST(value AS STRING)")

.as[(String,String)]


valschema=StructType(Seq(

StructField("schema",StringType, true),

StructField("payload",StringType, true)

))

 
 
valdf= ds1.selectExpr("cast (value as string) as json")

.select(from_json($"json",schema=schema).as("data"))

.select("data.payload")


package com.test

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession

object CommonStructuedKafka {
  def main(args: Array[String]): Unit = {

    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    Logger.getLogger("org.apache.kafka").setLevel(Level.WARN)

    // 读取配置文件信息
    val masterUrl = Props.get("master", "local")
    val appName = Props.get("appName", "Test7")
    val className = Props.get("className", "")
    val kafkaBootstrapServers = Props.get("kafka.bootstrap.servers", "localhost:9092")
    val subscribe = Props.get("subscribe", "test")
    val tmpTable = Props.get("tmpTable", "tmp")
    val sparksql = Props.get("sparksql", "select * from tmp")


    val spark = SparkSession.builder()
      .master(masterUrl)
      .appName(appName)
      .getOrCreate()


    // 读取kafka数据
    val lines = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", kafkaBootstrapServers)
      .option("subscribe", subscribe)
      .load()

    //隐式转换
    import spark.implicits._

    val values = lines.selectExpr("cast(value as string)").as[String]

    val res = values.map { value =>
      // 将json数据解析成list集合
      val list = Tools.parseJson(value, className)
      // 将List转成元组
      Tools.list2Tuple7(list)
    }

    res.createOrReplaceTempView(tmpTable)

    val result = spark.sql(sparksql)

    val query = result.writeStream
      .format("console")
      .outputMode("append")
      .start()

    query.awaitTermination()
  }
}


/** 
  * Created by dongyunlong on 2018/1/5. 
  * {"a":"1","b":"2","c":"2018-01-08","d":[23.9,45]} 
  */  
object SparkStructuredStreaming {  
   
  /** 
    * 创建SparkSession 
    * @return 
    */  
  def getSparkSession={  
    SparkSession  
      .builder()  
      .config(new SparkConf().setMaster("local[2]"))  
      .appName(getClass.getName)  
      .getOrCreate()  
  }  
   
  /** 
    * 解析kafka json数据，并将其映射为spark临时表 
    * @param spark 
    * @param kafkaTopic 
    * @param sourceName 
    */  
  def createOrReplaceTempView(spark:SparkSession, kafkaTopic:String, sourceName:String): Unit ={  
    import spark.implicits._  
    val df = spark  
      .readStream  
      .format("kafka")  
      .option("kafka.bootstrap.servers", "XX.XX.XX.XX:9092")  
      .option("subscribe", kafkaTopic)  
      .option("startingOffsets", "earliest")  
    .load()  
//    val schema = SocSchemaCollection.getSchemaBySourceName(sourceName) //从数据库加载json schema  
    val schema = StructType(mutable.Seq(  
      StructField("a", DataTypes.StringType),  
      StructField("b", DataTypes.StringType),  
      StructField("c", DataTypes.StringType),  
      StructField("d", DataTypes.createArrayType(DataTypes.StringType))  
    ))  
    if(schema != null){  
      val jsonDf = df.selectExpr("CAST(key AS STRING)", "cast (value as string) as json")  
          .select(from_json($"json", schema=schema).as("data"))  
      jsonDf.select("data.*").createOrReplaceTempView(sourceName)  
    }else{  
      println("error,schema is null")  
    }  
  }  
   
  /** 
    * 输出spark sql的查询结果 
    * @param spark 
    * @param sql 
    * @return 
    */  
  def sqlWriteStream(spark:SparkSession, sql:String): StreamingQuery ={  
    val query = spark.sql(sql)  
      .writeStream  
      .outputMode("append")  
      .format("console")  
      .start()  
    query  
  }  
   
  /** 
    * 注册spark临时表，执行sql语句，注意这里每一个sql都是一个writeStream，最后使用spark.streams.awaitAnyTermination()等待所有查询 
    * @param spark 
    */  
  def sparkReadKafka(spark:SparkSession): Unit ={  
    createOrReplaceTempView(spark, "dyl_test01", "dyl_test")  
    val sqls = Array("select * from dyl_test","select *,'2' as e from dyl_test")  
    val querys = mutable.ListBuffer[StreamingQuery]()  
    for(sql <- sqls){  
      println(sql)  
      querys += sqlWriteStream(spark, sql)  
    }  
  }  
   
  /** 
    * 主函数 
    * @param args 
    */  
  def main(args: Array[String]) {  
    println("hello world")  
    val spark = getSparkSession  
    sparkReadKafka(spark)  
    spark.streams.awaitAnyTermination()  
  }  
}  


import java.io.{FileInputStream, InputStream}
import java.nio.file.{Files, Paths}
import java.util.Properties

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

object Props {
  private val prop = new Properties()

  prop.load(getPropertyFileInputStream)

  /**
    * 在spark-submit中加入--driver-java-options -DPropPath=/home/spark/prop.properties的参数后，
    * 使用System.getProperty("PropPath")就能获取路径：/home/spark/prop.properties如果spark-submit中指定了
    * prop.properties文件的路径，那么使用prop.properties中的属性，否则使用该类中定义的属性
    */
  private def getPropertyFileInputStream: InputStream = {
    var is: InputStream = null
    val filePath = System.getProperty("PropPath")
    if (filePath != null && filePath.length > 0) {
      if (Files.exists(Paths.get(filePath))) {
        is = new FileInputStream(filePath)
      } else {
        println(s"在本地未找到config文件$filePath，尝试在HDFS上获取文件")
        val fs = FileSystem.get(new Configuration())
        if (fs.exists(new Path(filePath))) {
          val fis = fs.open(new Path(filePath))
          is = fis.getWrappedStream
        } else {
          println(s"在HDFS上找不到config文件$filePath，加载失败...")
        }
      }
    } else {
      println(s"未设置配置文件PropPath")
    }
    is
  }


  def get(propertyName: String, defaultValue: String): String = {
    prop.getProperty(propertyName, defaultValue)
  }


  def get(): Properties = {
    println("prop:" + this.prop)
    this.prop
  }


  def reload(): Properties = {
    prop.load(getPropertyFileInputStream)
    prop
  }
}


import com.google.gson.Gson

import scala.collection.mutable


object Tools {

  def main(args: Array[String]): Unit = {
    val tools = new Tools()
    val res = tools.parse("{'name':'caocao','age':'32','sex':'male'}", "com.test.People")
    println(res)
  }

  def parseJson(json: String, className: String): List[String] = {
    val tools = new Tools()
    tools.parse(json, className)
  }

  // 将List转成Tuple7元组类，这里仅仅是定义7个字段，可以定义更多字段。（ps：这种处理方式很不雅，一时也没想到好办法）
  def list2Tuple7(list: List[String]): (String, String, String, String, String, String, String) = {
    val t = list match {
      case List(a) => (a, "", "", "", "", "", "")
      case List(a, b) => (a, b, "", "", "", "", "")
      case List(a, b, c) => (a, b, c, "", "", "", "")
      case List(a, b, c, d) => (a, b, c, d, "", "", "")
      case List(a, b, c, d, e) => (a, b, c, d, e, "", "")
      case List(a, b, c, d, e, f) => (a, b, c, d, e, f, "")
      case List(a, b, c, d, e, f, g) => (a, b, c, d, e, f, g)
      case _ => ("", "", "", "", "", "", "")
    }
    t
  }
}


class Tools {
  // 通过传进来的Bean的全类名，进行反射，解析json，返回一个List()
  def parse(json: String, className: String): List[String] = {
    val list = mutable.ListBuffer[String]()
    val gson = new Gson()
    val clazz = Class.forName(className)
    val obj = gson.fromJson(json, clazz)
    val aClass = obj.getClass
    val fields = aClass.getDeclaredFields
    fields.foreach { f =>
      val fName = f.getName
      val m = aClass.getDeclaredMethod(fName)
      val value = m.invoke(obj).toString
      list.append(value)
    }
    list.toList
  }
}






12．9　总结
附录A　Kafka源码环境搭建

1.1.1 下载并解压缩Kafka二进制代码压缩包文件
1.1.2 启动服务器
1.1.3 创建topic
1.1.4 发送消息
1.1.5 消费消息
1.2 消息引擎系统
1.2.1 消息设计
1.2.2 传输协议设计
1.2.3 消息引擎范型
1.2.4 Java消息服务
1.3 Kafka概要设计
1.3.1 吞吐量/延时
1.3.2 消息持久化
1.3.3 负载均衡和故障转移
1.3.4 伸缩性
1.4 Kafka基本概念与术语
1.4.1 消息
1.4.2 topic和partition
1.4.3 offset
1.4.4 replica
1.4.5 leader和follower
1.4.6 ISR
1.5 Kafka使用场景
1.5.1 消息传输
1.5.2 网站行为日志追踪
1.5.3 审计数据收集
1.5.4 日志收集
1.5.5 Event Sourcing
1.5.6 流式处理
1.6 本章小结
第2章 Kafka发展历史
2.1 Kafka的历史
2.1.1 背景
2.1.2 Kafka横空出世
2.1.3 Kafka开源
2.2 Kafka版本变迁
2.2.1 Kafka的版本演进
2.2.2 Kafka的版本格式
2.2.3 新版本功能简介
2.2.4 旧版本功能简介
2.3 如何选择Kafka版本
2.3.1 根据功能场景
2.3.2 根据客户端使用场景
2.4 Kafka与Confluent
2.5 本章小结
第3章 Kafka线上环境部署
3.1 集群环境规划
3.1.1 操作系统的选型
3.1.2 磁盘规划
3.1.3 磁盘容量规划
3.1.4 内存规划
3.1.5 CPU规划
3.1.6 带宽规划
3.1.7 典型线上环境配置
3.2 伪分布式环境安装
3.2.1 安装Java
3.2.2 安装ZooKeeper
3.2.3 安装单节点Kafka集群
3.3 多节点环境安装
3.3.1 安装多节点ZooKeeper集群
3.3.2 安装多节点Kafka
3.4 验证部署
3.4.1 测试topic创建与删除
3.4.2 测试消息发送与消费
3.4.3 生产者吞吐量测试
3.4.4 消费者吞吐量测试
3.5 参数设置
3.5.1 broker端参数
3.5.2 topic级别参数
3.5.3 GC参数
3.5.4 JVM参数
3.5.5 OS参数
3.6 本章小结
第4章 producer开发
4.1 producer概览
4.2 构造producer
4.2.1 producer程序实例
4.2.2 producer主要参数
4.3 消息分区机制
4.3.1 分区策略
4.3.2 自定义分区机制
4.4 消息序列化
4.4.1 默认序列化
4.4.2 自定义序列化
4.5 producer拦截器
4.6 无消息丢失配置
4.6.1 producer端配置
4.6.2 broker端配置
4.7 消息压缩
4.7.1 Kafka支持的压缩算法
4.7.2 算法性能比较与调优
4.8 多线程处理
4.9 旧版本producer
4.10 本章小结
第5章 consumer开发
5.1 consumer概览
5.1.1 消费者（consumer）
5.1.2 消费者组（consumer group）
5.1.3 位移（offset）
5.1.4 位移提交
5.1.5__consumer_offsets
5.1.6 消费者组重平衡（consumer group rebalance）
5.2 构建consumer
5.2.1 consumer程序实例
5.2.2 consumer脚本命令
5.2.3 consumer主要参数
5.3 订阅topic
5.3.1 订阅topic列表
5.3.2 基于正则表达式订阅topic
5.4 消息轮询
5.4.1 poll内部原理
5.4.2 poll使用方法
5.5 位移管理
5.5.1 consumer位移
5.5.2 新版本consumer位移管理
5.5.3 自动提交与手动提交
5.5.4 旧版本consumer位移管理
5.6 重平衡（rebalance）
5.6.1 rebalance概览
5.6.2 rebalance触发条件
5.6.3 rebalance分区分配
5.6.4 rebalance generation
5.6.5 rebalance协议
5.6.6 rebalance流程
5.6.7 rebalance监听器
5.7 解序列化
5.7.1 默认解序列化器
5.7.2 自定义解序列化器
5.8 多线程消费实例
5.8.1 每个线程维护一个KafkaConsumer
5.8.2 单KafkaConsumer实例+多worker线程
5.8.3 两种方法对比
5.9 独立consumer
5.10 旧版本consumer
5.10.1 概览
5.10.2 high-level consumer
5.10.3 low-level consumer
5.11 本章小结
第6章 Kafka设计原理
6.1 broker端设计架构
6.1.1 消息设计
6.1.2 集群管理
6.1.3 副本与ISR设计
6.1.4 水印（watermark）和leader epoch
6.1.5 日志存储设计
6.1.6 通信协议（wire protocol）
6.1.7 controller设计
6.1.8 broker请求处理
6.2 producer端设计
6.2.1 producer端基本数据结构
6.2.2 工作流程
6.3 consumer端设计
6.3.1 consumer group状态机
6.3.2 group管理协议
6.3.3 rebalance场景剖析
6.4 实现精确一次处理语义
6.4.1 消息交付语义
6.4.2 幂等性producer（idempotent producer）
6.4.3 事务（transaction）
6.5 本章小结
第7章 管理Kafka集群
7.1 集群管理
7.1.1 启动broker
7.1.2 关闭broker
7.1.3 设置JMX端口
7.1.4 增加broker
7.1.5 升级broker版本
7.2 topic管理
7.2.1 创建topic
7.2.2 删除topic
7.2.3 查询topic列表
7.2.4 查询topic详情
7.2.5 修改topic
7.3 topic动态配置管理
7.3.1 增加topic配置
7.3.2 查看topic配置
7.3.3 删除topic配置
7.4 consumer相关管理
7.4.1 查询消费者组
7.4.2 重设消费者组位移
7.4.3 删除消费者组
7.4.4 kafka-consumer-offset-checker
7.5 topic分区管理
7.5.1 preferred leader选举
7.5.2 分区重分配
7.5.3 增加副本因子
7.6 Kafka常见脚本工具
7.6.1 kafka-console-producer脚本
7.6.2 kafka-console-consumer脚本
7.6.3 kafka-run-class脚本
7.6.4 查看消息元数据
7.6.5 获取topic当前消息数
7.6.6 查询__consumer_offsets
7.7 API方式管理集群
7.7.1 服务器端API管理topic
7.7.2 服务器端API管理位移
7.7.3 客户端API管理topic
7.7.4 客户端API查看位移
7.7.5 0.11.0.0版本客户端API
7.8 MirrorMaker
7.8.1 概要介绍
7.8.2 主要参数
7.8.3 使用实例
7.9 Kafka安全
7.9.1 SASL+ACL
7.9.2 SSL加密
7.10 常见问题
7.11 本章小结
第8章 监控Kafka集群
8.1 集群健康度检查
8.2 MBean监控
8.2.1 监控指标
8.2.2 指标分类
8.2.3 定义和查询JMX端口
8.3 broker端JMX监控
8.3.1 消息入站/出站速率
8.3.2 controller存活JMX指标
8.3.3 备份不足的分区数
8.3.4 leader分区数
8.3.5 ISR变化速率
8.3.6 broker I/O工作处理线程空闲率
8.3.7 broker网络处理线程空闲率
8.3.8 单个topic总字节数
8.4 clients端JMX监控
8.4.1 producer端JMX监控
8.4.2 consumer端JMX监控
8.5 JVM监控
8.5.1 进程状态
8.5.2 GC性能
8.6 OS监控
8.7 主流监控框架
8.7.1 JmxTool
8.7.2 kafka-manager
8.7.3 Kafka Monitor
8.7.4 Kafka Offset Monitor
8.7.5 CruiseControl
8.8 本章小结
第9章 调优Kafka集群
9.1 引言
9.2 确定调优目标
9.3 集群基础调优
9.3.1 禁止atime更新
9.3.2 文件系统选择
9.3.3 设置swapiness
9.3.4 JVM设置
9.3.5 其他调优
9.4 调优吞吐量
9.5 调优延时
9.6 调优持久性
9.7 调优可用性
9.8 本章小结
第10章 Kafka Connect与Kafka Streams
10.1 引言
10.2 Kafka Connect
10.2.1 概要介绍
10.2.2 standalone Connect
10.2.3 distributed Connect
10.2.4 开发connector
10.3 Kafka Streams
10.3.1 流处理
10.3.2 Kafka Streams核心概念
10.3.3 Kafka Streams与其他框架的异同
10.3.4 Word Count实例
10.3.5 Kafka Streams应用开发
10.3.6 Kafka Streams状态查询
第1章　初识 Kafka
1.1　发布与订阅消息系统
1.2　Kafka登场
1.3　为什么选择Kafka
1.4　数据生态系统
1.5　起源故事
1.6　开始Kafka之旅
第2章　安装 Kafka
2.1　要事先行
2.2　安装Kafka Broker
2.3　broker配置
2.4　硬件的选择
2.5　云端的Kafka
2.6　Kafka集群
2.7　生产环境的注意事项
2.8　总结
第3章　Kafka 生产者——向 Kafka 写入数据
3.1　生产者概览
3.2　创建Kafka生产者
3.3　发送消息到Kafka
3.4　生产者的配置
3.5　序列化器
3.6　分区
3.7　旧版的生产者API
3.8　总结
第4章　Kafka 消费者——从 Kafka 读取数据
4.1　KafkaConsumer概念
4.2　创建Kafka消费者
4.3　订阅主题
4.4　轮询
4.5　消费者的配置
4.6　提交和偏移量
4.7　再均衡监听器
4.8　从特定偏移量处开始处理记录
4.9　如何退出
4.10　反序列化器
4.11　独立消费者——为什么以及怎样使用没有群组的消费者
4.12　旧版的消费者API
4.13　总结
第5章　深入 Kafka
5.1　集群成员关系
5.2　控制器
5.3　复制
5.4　处理请求
5.5　物理存储
5.6　总结
第6章　可靠的数据传递
6.1　可靠性保证
6.2　复制
6.3　broker配置
6.4　在可靠的系统里使用生产者
6.5　在可靠的系统里使用消费者
6.6　验证系统可靠性
6.7　总结
第7章　构建数据管道
7.1　构建数据管道时需要考虑的问题
7.2　如何在Connect API和客户端API之间作出选择
7.3　Kafka Connect
7.4　Connect之外的选择
7.5　总结
第8章　跨集群数据镜像
8.1　跨集群镜像的使用场景
8.2　多集群架构
8.3　Kafka的MirrorMaker
8.4　其他跨集群镜像方案
8.5　总结
第9章　管理 Kafka
9.1　主题操作
9.2　消费者群组
9.3　动态配置变更
9.4　分区管理
9.5　消费和生产
9.6　客户端ACL
9.7　不安全的操作
9.8　总结
第10章　监控 Kafka
10.1　度量指标基础
10.2　broker的度量指标
10.3　客户端监控
10.4　延时监控
10.5　端到端监控
10.6　总结
第11章　流式处理
11.1　什么是流式处理
11.2　流式处理的一些概念
11.3　流式处理的设计模式
11.4　Streams示例
11.5　Kafka Streams的架构概览
11.6　流式处理使用场景
11.7　如何选择流式处理框架
11.8　总结
附录A　在其他操作系统上安装 Kafka
A.1　在Windows上安装Kafka
A.2　在MacOS上安装Kafka

1．1　基本概念
1．2　安装与配置
1．3　生产与消费
1．4　服务端参数配置
1．5　总结
第2章　生产者
2．1　客户端开发
2．1．1　必要的参数配置
2．1．2　消息的发送
2．1．3　序列化
2．1．4　分区器
2．1．5　生产者拦截器
2．2　原理分析
2．2．1　整体架构
2．2．2　元数据的更新
2．3　重要的生产者参数
2．4　总结
第3章　消费者
3．1　消费者与消费组
3．2　客户端开发
3．2．1　必要的参数配置
3．2．2　订阅主题与分区
3．2．3　反序列化
3．2．4　消息消费
3．2．5　位移提交
3．2．6　控制或关闭消费
3．2．7　指定位移消费
3．2．8　再均衡
3．2．9　消费者拦截器
3．2．10　多线程实现
3．2．11　重要的消费者参数
3．3　总结
第4章　主题与分区
4．1　主题的管理
4．1．1　创建主题
4．1．2　分区副本的分配
4．1．3　查看主题
4．1．4　修改主题
4．1．5　配置管理
4．1．6　主题端参数
4．1．7　删除主题
4．2　初识KafkaAdminClient
4．2．1　基本使用
4．2．2　主题合法性验证
4．3　分区的管理
4．3．1　优先副本的选举
4．3．2　分区重分配
4．3．3　复制限流
4．3．4　修改副本因子
4．4　如何选择合适的分区数
4．4．1　性能测试工具
4．4．2　分区数越多吞吐量就越高吗
4．4．3　分区数的上限
4．4．4　考量因素
4．5　总结
第5章　日志存储
5．1　文件目录布局
5．2　日志格式的演变
5．2．1　v0版本
5．2．2　v1版本
5．2．3　消息压缩
5．2．4　变长字段
5．2．5　v2版本
5．3　日志索引
5．3．1　偏移量索引
5．3．2　时间戳索引
5．4　日志清理
5．4．1　日志删除
5．4．2　日志压缩
5．5　磁盘存储
5．5．1　页缓存
5．5．2　磁盘I/O流程
5．5．3　零拷贝
第6章　深入服务端
6．1　协议设计
6．2　时间轮
6．3　延时操作
6．4　控制器
6．4．1　控制器的选举及异常恢复
6．4．2　优雅关闭
6．4．3　分区leader的选举
6．5　参数解密
6．5．1　broker．id
6．5．2　bootstrap．servers
6．5．3　服务端参数列表
6．6　总结
第7章　深入客户端
7．1　分区分配策略
7．1．1　RangeAssignor分配策略
7．1．2　RoundRobinAssignor分配策略
7．1．3　StickyAssignor分配策略
7．1．4　自定义分区分配策略
7．2　消费者协调器和组协调器
7．2．1　旧版消费者客户端的问题
7．2．2　再均衡的原理
7．3　__consumer_offsets剖析
7．4　事务
7．4．1　消息传输保障
7．4．2　幂等
7．4．3　事务
第8章　可靠性探究
8．1　副本剖析
8．1．1　失效副本
8．1．2　ISR的伸缩
8．1．3　LEO与HW
8．1．4　Leader Epoch的介入
8．1．5　为什么不支持读写分离
8．2　日志同步机制
8．3　可靠性分析
第9章　Kafka应用
9．1　命令行工具
9．1．1　消费组管理
9．1．2　消费位移管理
9．1．3　手动删除消息
9．2　Kafka Connect
9．2．1　独立模式
9．2．2　REST API
9．2．3　分布式模式
9．3　Kafka Mirror Maker
9．4　Kafka Streams
第10章　Kafka监控
10．1　监控数据的来源
10．1．1　OneMinuteRate
10．1．2　获取监控指标
10．2　消费滞后
10．3　同步失效分区
10．4　监控指标说明
10．5　监控模块
第11章　高级应用
11．1　过期时间（TTL）
11．2　延时队列
11．3　死信队列和重试队列
11．4　消息路由
11．5　消息轨迹
11．6　消息审计
11．7　消息代理
11．7．1　快速入门
11．7．2　REST API介绍及示例
11．7．3　服务端配置及部署
11．7．4　应用思考
11．8　消息中间件选型
11．8．1　各类消息中间件简述
11．8．2　选型要点概述
11．8．3　消息中间件选型误区探讨
11．9　总结
第12章　Kafka与Spark的集成
12．1　Spark的安装及简单应用
12．2　Spark编程模型
12．3　Spark的运行结构
12．4　Spark Streaming简介
12．5　Kafka与Spark Streaming的整合
12．6　Spark SQL
12．7　Structured Streaming
12．8　Kafka与Structured Streaming的整合
附录A　Kafka源码环境搭建

1.1　发布与订阅消息系统　　1
1.1.1　如何开始　　2
1.1.2　独立的队列系统　　3
1.2　Kafka登场　　4
1.2.1　消息和批次　　4
1.2.2　模式　　4
1.2.3　主题和分区　　5
1.2.4　生产者和消费者　　5
1.2.5　broker和集群　　6
1.2.6　多集群　　7
1.3　为什么选择Kafka　　8
1.3.1　多个生产者　　8
1.3.2　多个消费者　　8
1.3.3　基于磁盘的数据存储　　9
1.3.4　伸缩性　　9
1.3.5　高性能　　9
1.4　数据生态系统　　9
1.5　起源故事　　11
1.5.1　LinkedIn的问题　　11
1.5.2　Kafka的诞生　　12
1.5.3　走向开源　　12
1.5.4　命名　　13
1.6　开始Kafka之旅　　13
第2章　安装Kafka　　14
2.1　要事先行　　14
2.1.1　选择操作系统　　14
2.1.2　安装Java　　14
2.1.3　安装Zookeeper　　15
2.2　安装Kafka Broker　　17
2.3　broker配置　　18
2.3.1　常规配置　　18
2.3.2　主题的默认配置　　19
2.4　硬件的选择　　23
2.4.1　磁盘吞吐量　　23
2.4.2　磁盘容量　　23
2.4.3　内存　　23
2.4.4　网络　　24
2.4.5　CPU　　24
2.5　云端的Kafka　　24
2.6　Kafka集群　　24
2.6.1　需要多少个broker　　25
2.6.2　broker配置　　25
2.6.3　操作系统调优　　26
2.7　生产环境的注意事项　　28
2.7.1　垃圾回收器选项　　28
2.7.2　数据中心布局　　29
2.7.3　共享Zookeeper　　29
2.8　总结　　30
第3章　Kafka生产者——向Kafka写入数据　　31
3.1　生产者概览　　32
3.2　创建Kafka生产者　　33
3.3　发送消息到Kafka　　34
3.3.1　同步发送消息　　35
3.3.2　异步发送消息　　35
3.4　生产者的配置　　36
3.5　序列化器　　39
3.5.1　自定义序列化器　　39
3.5.2　使用Avro序列化　　41
3.5.3　在Kafka里使用Avro　　42
3.6　分区　　45
3.7　旧版的生产者API　　46
3.8　总结　　47
第4章　Kafka消费者——从Kafka读取数据　　48
4.1　KafkaConsumer概念　　48
4.1.1　消费者和消费者群组　　48
4.1.2　消费者群组和分区再均衡　　51
4.2　创建Kafka 消费者　　52
4.3　订阅主题　　53
4.4　轮询　　53
4.5　消费者的配置　　55
4.6　提交和偏移量　　57
4.6.1　自动提交　　58
4.6.2　提交当前偏移量　　59
4.6.3　异步提交　　59
4.6.4　同步和异步组合提交　　61
4.6.5　提交特定的偏移量　　61
4.7　再均衡监听器　　62
4.8　从特定偏移量处开始处理记录　　64
4.9　如何退出　　66
4.10　反序列化器　　67
4.11　独立消费者——为什么以及怎样使用没有群组的消费者　　71
4.12　旧版的消费者API　　71
4.13　总结　　72
第5章　深入Kafka　　73
5.1　集群成员关系　　73
5.2　控制器　　74
5.3　复制　　74
5.4　处理请求　　76
5.4.1　生产请求　　78
5.4.2　获取请求　　78
5.4.3　其他请求　　80
5.5　物理存储　　81
5.5.1　分区分配　　81
5.5.2　文件管理　　82
5.5.3　文件格式　　83
5.5.4　索引　　84
5.5.5　清理　　84
5.5.6　清理的工作原理　　84
5.5.7　被删除的事件　　86
5.5.8　何时会清理主题　　86
5.6　总结　　86
第6章　可靠的数据传递　　87
6.1　可靠性保证　　87
6.2　复制　　88
6.3　broker配置　　89
6.3.1　复制系数　　89
6.3.2　不完全的首领选举　　90
6.3.3　最少同步副本　　91
6.4　在可靠的系统里使用生产者　　92
6.4.1　发送确认　　92
6.4.2　配置生产者的重试参数　　93
6.4.3　额外的错误处理　　94
6.5　在可靠的系统里使用消费者　　94
6.5.1　消费者的可靠性配置　　95
6.5.2　显式提交偏移量　　95
6.6　验证系统可靠性　　97
6.6.1　配置验证　　98
6.6.2　应用程序验证　　98
6.6.3　在生产环境监控可靠性　　99
6.7　总结　　100
第7章　构建数据管道　　101
7.1　构建数据管道时需要考虑的问题　　102
7.1.1　及时性　　102
7.1.2　可靠性　　102
7.1.3　高吞吐量和动态吞吐量　　103
7.1.4　数据格式　　103
7.1.5　转换　　104
7.1.6　安全性　　104
7.1.7　故障处理能力　　104
7.1.8　耦合性和灵活性　　105
7.2　如何在Connect API和客户端API之间作出选择　　105
7.3　Kafka Connect　　106
7.3.1　运行Connect　　106
7.3.2　连接器示例——文件数据源和文件数据池　　107
7.3.3　连接器示例——从MySQL到ElasticSearch　　109
7.3.4　深入理解Connect　　114
7.4　Connect之外的选择　　116
7.4.1　用于其他数据存储的摄入框架　　116
7.4.2　基于图形界面的ETL 工具　　117
7.4.3　流式处理框架　　117
7.5　总结　　117
第8章　跨集群数据镜像　　118
8.1　跨集群镜像的使用场景　　118
8.2　多集群架构　　119
8.2.1　跨数据中心通信的一些现实情况　　119
8.2.2　Hub和Spoke架构　　120
8.2.3　双活架构　　121
8.2.4　主备架构　　123
8.2.5　延展集群　　127
8.3　Kafka的MirrorMaker　　128
8.3.1　如何配置　　129
8.3.2　在生产环境部署MirrorMaker　　130
8.3.3　MirrorMaker调优　　132
8.4　其他跨集群镜像方案　　134
8.4.1　优步的uReplicator　　134
8.4.2　Confluent的Replicator　　135
第9章　管理Kafka　　136
9.1　主题操作　　136
9.1.1　创建主题　　137
9.1.2　增加分区　　138
9.1.3　删除主题　　138
9.1.4　列出集群里的所有主题　　139
9.1.5　列出主题详细信息　　139
9.2　消费者群组　　140
9.2.1　列出并描述群组　　140
9.2.2　删除群组　　142
9.2.3　偏移量管理　　142
9.3　动态配置变更　　143
9.3.1　覆盖主题的默认配置　　143
9.3.2　覆盖客户端的默认配置　　145
9.3.3　列出被覆盖的配置　　145
9.3.4　移除被覆盖的配置　　146
9.4　分区管理　　146
9.4.1　首选的首领选举　　146
9.4.2　修改分区副本　　147
9.4.3　修改复制系数　　150
9.4.4　转储日志片段　　151
9.4.5　副本验证　　152
9.5　消费和生产　　153
9.5.1　控制台消费者　　153
9.5.2　控制台生产者　　155
9.6　客户端ACL　　157
9.7　不安全的操作　　157
9.7.1　移动集群控制器　　157
9.7.2　取消分区重分配　　157
9.7.3　移除待删除的主题　　158
9.7.4　手动删除主题　　158
第10章　监控Kafka　　160
10.1　度量指标基础　　160
10.1.1　度量指标在哪里　　160
10.1.2　内部或外部度量　　161
10.1.3　应用程序健康检测　　161
10.1.4　度量指标的覆盖面　　161
10.2　broker的度量指标　　162
10.2.1　非同步分区　　162
10.2.2　broker度量指标　　166
10.2.3　主题和分区的度量指标　　173
10.2.4　Java虚拟机监控　　174
10.2.5　操作系统监控　　175
10.2.6　日志　　176
10.3　客户端监控　　177
10.3.1　生产者度量指标　　177
10.3.2　消费者度量指标　　179
10.3.3　配额　　181
10.4　延时监控　　182
10.5　端到端监控　　183
第11章　流式处理　　184
11.1　什么是流式处理　　185
11.2　流式处理的一些概念　　186
11.2.1　时间　　187
11.2.2　状态　　188
11.2.3　流和表的二元性　　188
11.2.4　时间窗口　　189
11.3　流式处理的设计模式　　190
11.3.1　单个事件处理　　191
11.3.2　使用本地状态　　191
11.3.3　多阶段处理和重分区　　193
11.3.4　使用外部查找——流和表的连接　　193
11.3.5　流与流的连接　　195
11.3.6　乱序的事件　　195
11.3.7　重新处理　　196
11.4　Streams示例　　197
11.4.1　字数统计　　197
11.4.2　股票市场统计　　199
11.4.3　填充点击事件流　　201
11.5　Kafka Streams的架构概览　　202
11.5.1　构建拓扑　　202
11.5.2　对拓扑进行伸缩　　203
11.5.3　从故障中存活下来　　205
11.6　流式处理使用场景　　205
11.7　如何选择流式处理框架　　206
附录A　在其他操作系统上安装Kafka　　209

1．1　Kafka流式数据平台 1
1．2　Kafka的基本概念 3
1．2．1　分区模型 3
1．2．2　消费模型 4
1．2．3　分布式模型 5
1．3　Kafka的设计与实现 6
1．3．1　文件系统的持久化与数据传输效率 6
1．3．2　生产者与消费者 8
1．3．3　副本机制和容错处理 10
1．4　快速开始 11
1．4．1　单机模式 12
1．4．2　分布式模式 14
1．4．3　消费组示例 16
1．5　环境准备 18
第2章　生产者 22
2．1　新生产者客户端 22
2．1．1　同步和异步发送消息 23
2．1．2　客户端消息发送线程 29
2．1．3　客户端网络连接对象 31
2．1．4　选择器处理网络请求 35
2．2　旧生产者客户端 43
2．2．1　事件处理器处理客户端发送的消息 44
2．2．2　对消息集按照节点和分区进行整理 46
2．2．3　生产者使用阻塞通道发送请求 48
2．3　服务端网络连接 49
2．3．1　服务端使用接收器接受客户端的连接 50
2．3．2　处理器使用选择器的轮询处理网络请求 53
2．3．3　请求通道的请求队列和响应队列 56
2．3．4　Kafka请求处理线程 58
2．3．5　服务端的请求处理入口 58

第3章　消费者：高级API和低级API 61
3．1　消费者启动和初始化 67
3．1．1　创建并初始化消费者连接器 69
3．1．2　消费者客户端的线程模型 70
3．1．3　重新初始化消费者 72
3．2　消费者再平衡操作 73
3．2．1　分区的所有权 74
3．2．2　为消费者分配分区 75
3．2．3　创建分区信息对象 78
3．2．4　关闭和更新拉取线程管理器 80
3．2．5　分区信息对象的偏移量 80
3．3　消费者拉取数据 82
3．3．1　拉取线程管理器 82
3．3．2　抽象拉取线程 87
3．3．3　消费者拉取线程 90
3．4　消费者消费消息 94
3．4．1　Kafka消息流 94
3．4．2　消费者迭代消费消息 95
3．5　消费者提交分区偏移量 97
3．5．1　提交偏移量到ZK 98
3．5．2　提交偏移量到内部主题 99
3．5．3　连接偏移量管理器 101
3．5．4　服务端处理提交偏移量的请求 103
3．5．5　缓存分区的偏移量 106
3．6　消费者低级API示例 108
3．6．1　消息消费主流程 109
3．6．2　找出分区的主副本 112
3．6．3　获取分区的读取偏移量 113
3．6．4　发送拉取请求并消费消息 116
3．7．1　消费者线程模型 117
3．7．2　再平衡和分区分配 119

第4章　新消费者 121
4．1　新消费者客户端 125
4．1．1　消费者的订阅状态 125
4．1．2　消费者轮询的准备工作 134
4．1．3　消费者轮询的流程 138
4．1．4　消费者拉取消息 146
4．1．5　消费者获取记录 149
4．1．6　消费消息 160
4．2　消费者的网络客户端轮询 161
4．2．1　异步请求 162
4．2．2　异步请求高级模式 169
4．2．3　网络客户端轮询 184
4．3　心跳任务 188
4．3．1　发送心跳请求 188
4．3．2　心跳状态 189
4．3．3　运行心跳任务 191
4．3．4　处理心跳结果的示例 192
4．3．5　心跳和协调者的关系 193
4．4　消费者提交偏移量 195
4．4．1　自动提交任务 195
4．4．2　将拉取偏移量作为提交偏移量 197
4．4．3　同步提交偏移量 201
4．4．4　消费者的消息处理语义 202
4．5　小结 206
第5章　协调者 210
5．1　消费者加入消费组 211
5．1．1　元数据与分区分配器 212
5．1．2　消费者的加入组和同步组 213
5．1．3　主消费者执行分配任务 220
5．1．4　加入组的准备、完成和监听器 224
5．2　协调者处理请求 229
5．2．1　服务端定义发送响应结果的回调方法 229
5．2．2　消费者和消费组元数据 232
5．2．3　协调者处理请求前的条件检查 236
5．2．4　协调者调用回调方法发送响应给客户端 237
5．3　延迟的加入组操作 242
5．3．1 “准备再平衡” 242
5．3．2　延迟操作和延迟缓存 244
5．3．3　尝试完成延迟的加入操作 246
5．3．4　消费组稳定后，原有消费者重新加入消费组 250
5．3．5　消费组未稳定，原有消费者重新加入消费组 251
5．4　消费组状态机 254
5．4．1　再平衡操作与监听器 254
5．4．2　消费组的状态转换 262
5．4．3　协调者处理“加入组请求” 264
5．4．4　协调者处理“同步组请求” 274
5．4．5　协调者处理“离开组请求” 276
5．4．6　再平衡超时与会话超时 278
5．4．7　延迟的心跳 282
5．5　小结 290
第6章　存储层 293
6．1　日志的读写 293
6．1．1　分区、副本、日志、日志
分段 294
6．1．2　写入日志 297
6．1．3　日志分段 305
6．1．4　读取日志 315
6．1．5　日志管理 329
6．1．6　日志压缩 336
6．2　服务端处理读写请求 348
6．2．1　副本管理器 351
6．2．2　分区与副本 362
6．3　延迟操作 373
6．3．1　延迟操作接口 374
6．3．2　延迟操作与延迟缓存 383
6．3．3　延迟缓存 391
6．4　小结 400
第7章　控制器 402
7．1　Kafka控制器 402
7．1．1　控制器选举 403
7．1．2　控制器上下文 406
7．1．3　ZK监听器 408
7．1．4　分区状态机和副本状态机 410
7．1．5　删除主题 430
7．1．6　重新分配分区 436
7．1．7　控制器的网络通道管理器 445
7．2　服务端处理LeaderAndIsr请求 448
7．2．1　创建分区 449
7．2．2　创建主副本、备份副本 451
7．2．3　消费组元数据迁移 463
7．3　元数据缓存 468
7．3．1　服务端的元数据缓存 472
7．3．2　客户端更新元数据 473
7．4　Kafka服务关闭 483
7．5　小结 487
第8章　基于Kafka构建数据流管道 490
8．1　Kafka集群同步工具：MirrorMaker 490
8．1．1　单机模拟数据同步 491
8．1．2　数据同步的流程 493
8．2　Uber集群同步工具：uReplicator 498
8．2．1　Apache Helix介绍 498
8．2．2　Helix控制器 501
8．2．3　Helix工作节点 504
8．3　Kafka连接器 505
8．3．1　连接器的使用示例 507
8．3．2　开发一个简单的连接器 510
8．3．3　连接器的架构模型 515
8．3．4　Herder的实现 520
8．3．5　Worker的实现 524
8．3．6　配置存储与状态存储 530
8．3．7　连接器与任务的实现 550
8．4　小结 565
第9章　Kafka流处理 569
9．1　低级Processor API 569
9．1．1　流处理应用程序示例 569
9．1．2　流处理的拓扑 575
9．1．3　流处理的线程模型 580
9．1．4　状态存储 613
9．2　高级流式DSL 636
9．2．1　DSL应用程序示例 636
9．2．2　KStream和KTable 638
9．2．3　连接操作 665
9．2．4　窗口操作 672
9．3　小结 684
第10章　高级特性介绍 686
10．1　客户端配额 686
10．2　消息与时间戳 692
10．3　事务处理 699
10．4　小结 703

1.1　Kafka快速入门 1
1.1.1　下载并解压缩Kafka二进制代码压缩包文件 2
1.1.2　启动服务器 3
1.1.3　创建topic 3
1.1.4　发送消息 4
1.1.5　消费消息 4
1.2　消息引擎系统 5
1.2.1　消息设计 6
1.2.2　传输协议设计 6
1.2.3　消息引擎范型 6
1.2.4　Java消息服务 8

1.3　Kafka概要设计 8
1.3.1　吞吐量/延时 8
1.3.2　消息持久化 11
1.3.3　负载均衡和故障转移 12
1.3.4　伸缩性 13

1.4　Kafka基本概念与术语 13
1.4.1　消息 14
1.4.2　topic和partition 16
1.4.3　offset 17
1.4.4　replica 18
1.4.5　leader和follower 18
1.4.6　ISR 19

1.5　Kafka使用场景 20

1.5.1　消息传输 20

1.5.2　网站行为日志追踪 20

1.5.3　审计数据收集 20

1.5.4　日志收集 20

1.5.5　Event Sourcing 21

1.5.6　流式处理 21

2.1　Kafka的历史 22

2.1.1　背景 22

2.1.2　Kafka横空出世 23

2.1.3　Kafka开源 24

2.2　Kafka版本变迁 25

2.2.1　Kafka的版本演进 25

2.2.2　Kafka的版本格式 26

2.2.3　新版本功能简介 26

2.2.4　旧版本功能简介 31

2.3　如何选择Kafka版本 35

2.3.1　根据功能场景 35

2.3.2　根据客户端使用场景 35

2.4　Kafka与Confluent 36

2.5　本章小结 37

第3章　Kafka线上环境部署 38

3.1　集群环境规划 38

3.1.1　操作系统的选型 38

3.1.2　磁盘规划 40

3.1.3　磁盘容量规划 42

3.1.4　内存规划 43

3.1.5　CPU规划 43

3.1.6　带宽规划 44

3.1.7　典型线上环境配置 45

3.2　伪分布式环境安装 45

3.2.1　安装Java 46

3.2.2　安装ZooKeeper 47

3.2.3　安装单节点Kafka集群 48

3.3　多节点环境安装 49

3.3.1　安装多节点ZooKeeper集群 50

3.3.2　安装多节点Kafka 54

3.4　验证部署 55

3.4.1　测试topic创建与删除 55

3.4.2　测试消息发送与消费 57

3.4.3　生产者吞吐量测试 58

3.4.4　消费者吞吐量测试 58

3.5　参数设置 59

3.5.1　broker端参数 59

3.5.2　topic级别参数 62

3.5.3　GC参数 63

3.5.4　JVM参数 64

3.5.5　OS参数 64

3.6　本章小结 65

第4章　producer开发 66

4.1　producer概览 66

4.2　构造producer 69

4.2.1　producer程序实例 69

4.2.2　producer主要参数 75

4.3　消息分区机制 80

4.3.1　分区策略 80

4.3.2　自定义分区机制 80

4.4　消息序列化 83

4.4.1　默认序列化 83

4.4.2　自定义序列化 84

4.5　producer拦截器 87

4.6　无消息丢失配置 90

4.6.1　producer端配置 91

4.6.2　broker端配置 92

4.7　消息压缩 92

4.7.1　Kafka支持的压缩算法 93

4.7.2　算法性能比较与调优 93

4.8　多线程处理 95

4.9　旧版本producer 96

4.10　本章小结 98

第5章　consumer开发 99

5.1　consumer概览 99

5.1.1　消费者（consumer） 99

5.1.2　消费者组（consumer group） 101

5.1.3　位移（offset） 102

5.1.4　位移提交 103

5.1.5　__consumer_offsets 104

5.1.6　消费者组重平衡（consumer group rebalance） 106

5.2　构建consumer 106

5.2.1　consumer程序实例 106

5.2.2　consumer脚本命令 111

5.2.3　consumer主要参数 112

5.3　订阅topic 115

5.3.1　订阅topic列表 115

5.3.2　基于正则表达式订阅topic 115

5.4　消息轮询 115

5.4.1　poll内部原理 115

5.4.2　poll使用方法 116

5.5　位移管理 118

5.5.1　consumer位移 119

5.5.2　新版本consumer位移管理 120

5.5.3　自动提交与手动提交 121

5.5.4　旧版本consumer位移管理 123

5.6　重平衡（rebalance） 123

5.6.1　rebalance概览 123

5.6.2　rebalance触发条件 124

5.6.3　rebalance分区分配 124

5.6.4　rebalance generation 126

5.6.5　rebalance协议 126

5.6.6　rebalance流程 127

5.6.7　rebalance监听器 128

5.7　解序列化 130

5.7.1　默认解序列化器 130

5.7.2　自定义解序列化器 131

5.8　多线程消费实例 132

5.8.1　每个线程维护一个KafkaConsumer 133

5.8.2　单KafkaConsumer实例+多worker线程 135

5.8.3　两种方法对比 140

5.9　独立consumer 141

5.10　旧版本consumer 142

5.10.1　概览 142

5.10.2　high-level consumer 143

5.10.3　low-level consumer 147

5.11　本章小结 153

第6章　Kafka设计原理 154

6.1　broker端设计架构 154

6.1.1　消息设计 155

6.1.2　集群管理 166

6.1.3　副本与ISR设计 169

6.1.4　水印（watermark）和leader epoch 174

6.1.5　日志存储设计 185

6.1.6　通信协议（wire protocol） 194

6.1.7　controller设计 205

6.1.8　broker请求处理 216

6.2　producer端设计 219

6.2.1　producer端基本数据结构 219

6.2.2　工作流程 220

6.3　consumer端设计 223

6.3.1　consumer group状态机 223

6.3.2　group管理协议 226

6.3.3　rebalance场景剖析 227

6.4　实现精确一次处理语义 230

6.4.1　消息交付语义 230

6.4.2　幂等性producer（idempotent producer） 231

6.4.3　事务（transaction） 232

6.5　本章小结 234

第7章　管理Kafka集群 235

7.1　集群管理 235

7.1.1　启动broker 235

7.1.2　关闭broker 236

7.1.3　设置JMX端口 237

7.1.4　增加broker 238

7.1.5　升级broker版本 238

7.2　topic管理 241

7.2.1　创建topic 241

7.2.2　删除topic 243

7.2.3　查询topic列表 244

7.2.4　查询topic详情 244

7.2.5　修改topic 245

7.3　topic动态配置管理 246

7.3.1　增加topic配置 246

7.3.2　查看topic配置 247

7.3.3　删除topic配置 248

7.4　consumer相关管理 248

7.4.1　查询消费者组 248

7.4.2　重设消费者组位移 251

7.4.3　删除消费者组 256

7.4.4　kafka-consumer-offset-checker 257

7.5　topic分区管理 258

7.5.1　preferred leader选举 258

7.5.2　分区重分配 260

7.5.3　增加副本因子 263

7.6　Kafka常见脚本工具 264

7.6.1　kafka-console-producer脚本 264

7.6.2　kafka-console-consumer脚本 265

7.6.3　kafka-run-class脚本 267

7.6.4　查看消息元数据 268

7.6.5　获取topic当前消息数 270

7.6.6　查询__consumer_offsets 271

7.7　API方式管理集群 273

7.7.1　服务器端API管理topic 273

7.7.2　服务器端API管理位移 275

7.7.3　客户端API管理topic 276

7.7.4　客户端API查看位移 280

7.7.5　0.11.0.0版本客户端API 281

7.8　MirrorMaker 285

7.8.1　概要介绍 285

7.8.2　主要参数 286

7.8.3　使用实例 287

7.9　Kafka安全 288

7.9.1　SASL+ACL 289

7.9.2　SSL加密 297

7.10　常见问题 301

7.11　本章小结 304

第8章　监控Kafka集群 305

8.1　集群健康度检查 305

8.2　MBean监控 306

8.2.1　监控指标 306

8.2.2　指标分类 308

8.2.3　定义和查询JMX端口 309

8.3　broker端JMX监控 310

8.3.1　消息入站/出站速率 310

8.3.2　controller存活JMX指标 311

8.3.3　备份不足的分区数 312

8.3.4　leader分区数 312

8.3.5　ISR变化速率 313

8.3.6　broker I/O工作处理线程空闲率 313

8.3.7　broker网络处理线程空闲率 314

8.3.8　单个topic总字节数 314

8.4　clients端JMX监控 314

8.4.1　producer端JMX监控 314

8.4.2　consumer端JMX监控 316

8.5　JVM监控 317

8.5.1　进程状态 318

8.5.2　GC性能 318

8.6　OS监控 318

8.7　主流监控框架 319

8.7.1　JmxTool 320

8.7.2　kafka-manager 320

8.7.3　Kafka Monitor 325

8.7.4　Kafka Offset Monitor 327

8.7.5　CruiseControl 329

8.8　本章小结 330

第9章　调优Kafka集群 331

9.1　引言 331

9.2　确定调优目标 333

9.3　集群基础调优 334

9.3.1　禁止atime更新 335

9.3.2　文件系统选择 335

9.3.3　设置swapiness 336

9.3.4　JVM设置 337

9.3.5　其他调优 337

9.4　调优吞吐量 338

9.5　调优延时 342

9.6　调优持久性 343

9.7　调优可用性 347

9.8　本章小结 349

第10章　Kafka Connect与Kafka Streams 350

10.2　Kafka Connect 351
10.2.1　概要介绍 351
10.2.2　standalone Connect 353
10.2.3　distributed Connect 356
10.2.4　开发connector 359
10.3　Kafka Streams 362
10.3.1　流处理 362

10.3.2　Kafka Streams核心概念 364
10.3.3　Kafka Streams与其他框架的异同 368
10.3.4　Word Count实例 369
10.3.5　Kafka Streams应用开发 372
10.3.6　Kafka Streams状态查询 382






152. kafka 可以脱离 zookeeper 单独使用吗？为什么？

kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。

153. kafka 有几种数据保留的策略？

kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。

154. kafka 同时设置了 7 天和 10G 清除数据，到第五天的时候消息达到了 10G，这个时候 kafka 将如何处理？

这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。

155. 什么情况会导致 kafka 运行变慢？

cpu 性能瓶颈

磁盘读写瓶颈

网络瓶颈


156. 使用 kafka 集群需要注意什么？

集群的数量不是越多越好，最好不要超过 7 个，因为节点越多，消息复制需要的时间就越长，整个群组的吞吐量就越低。

集群数量最好是单数，因为超过一半故障集群就不能用了，设置为单数容错率更高。

https://help.aliyun.com/document_detail/68165.html?spm=a2c4g.11174283.6.592.2f2e1eefpIsinD

发布者最佳实践
更新时间：2019-04-17 17:32:10


本页目录
Key 和 Value
失败重试
异步发送
线程安全
Acks
Batch
OOM
分区顺序
技术交流
本文主要介绍消息队列 Kafka 发布者的最佳实践，从而帮助您更好的使用该产品。

注意：以下最佳实践基于消息队列 Kafka 的 Java 客户端；对于其它语言的客户端，其基本概念与思想是通用的，但实现细节可能有差异，仅供参考。

Kafka 的发送非常简单，示例代码片段如下：

 Future<RecordMetadata> metadataFuture = producer.send(new ProducerRecord<String, String>(
            topic,   \\ topic
            null,    \\ 分区编号，这里最好为 null，交给 producer 去分配
            System.currentTimeMillis(), \\时间戳
            String.valueOf(message.hashCode()), \\ key，可以在控制台通过这个 Key 查找消息，这个 key 最好唯一；
            message)); \\ value，消息内容
详细 Demo 可参见 Demo 示例。

Key 和 Value
Kafka 0.10.0.0 的消息字段只有两个：Key 和 Value。Key 是消息的标识，Value 即消息内容。为了便于追踪，重要消息最好都设置一个唯一的 Key。通过 Key 追踪某消息，打印发送日志和消费日志，了解该消息的发送和消费情况；更重要的是，您可以在控制台可以根据 Key 查询消息的内容。

失败重试
在分布式环境下，由于网络等原因，偶尔的发送失败是常见的。导致这种失败的原因有可能是消息已经发送成功，但是 Ack 失败，也有可能是确实没发送成功。

消息队列 Kafka 是 VIP 网络架构，会主动掐掉空闲连接（30 秒没活动），也就是说，不是一直活跃的客户端会经常收到”connection rest by peer”这样的错误，因此建议都考虑重试消息发送。

异步发送
发送接口是异步的；如果你想得到发送的结果，可以调用metadataFuture.get(timeout, TimeUnit.MILLISECONDS)。

线程安全
Producer 是线程安全的，且可以往任何 Topic 发送消息。通常情况下，一个应用对应一个 Producer 就足够了。

Acks
acks=0，表示无需服务端的 Response，性能较高，丢数据风险较大；

acks=1，服务端主节点写成功即返回Response，性能中等，丢数据风险中等，主节点宕机可能导致数据丢失；

acks=all，服务端主节点写成功，且备节点同步成功，才返回Response，性能较差，数据较为安全，主节点和备节点都宕机才会导致数据丢失。

一般建议选择acks=1，重要的服务可以设置acks=all。

Batch
Batch 的基本思路是：把消息缓存在内存中，并进行打包发送。Kafka 通过 Batch 来提高吞吐，但同时也会增加延迟，生产时应该对两者予以权衡。
在构建 Producer 时，需要考虑以下两个参数：

batch.size : 发往每个分区（Partition）的消息缓存量（消息内容的字节数之和，不是条数）达到这个数值时，就会触发一次网络请求，然后客户端把消息真正发往服务器；
linger.ms : 每条消息待在缓存中的最长时间。若超过这个时间，就会忽略 batch.size 的限制，然后客户端立即把消息发往服务器。
由此可见，Kafka 客户端什么时候把消息真正发往服务器，是通过上面两个参数共同决定的：
batch.size 有助于提高吞吐，linger.ms有助于控制延迟。您可以根据具体业务需求进行调整。

OOM
结合 Kafka 的 Batch 设计思路，Kafka 会缓存消息并打包发送，如果缓存太多，则有可能造成 OOM（Out of Memory）。

buffer.memory : 所有缓存消息的总体大小超过这个数值后，就会触发把消息发往服务器。此时会忽略 batch.size 和 linger.ms 的限制。
buffer.memory 的默认数值是 32MB，对于单个 Producer 来说，可以保证足够的性能。需要注意的是，如果你在同一个 JVM 中启动多个 Producer，那么每个 Producer 都有可能占用 32MB 缓存空间，此时便有可能触发 OOM。
在生产时，一般没有必要启动多个 Producer；如果特殊情况需要，则需要考虑buffer.memory的大小，避免触发 OOM。
分区顺序
单个分区（Partition）内，消息是按照发送顺序储存的，是基本有序的。

默认情况下，消息队列 Kafka 为了提升可用性，并不保证单个分区内绝对有序，在升级或者宕机时，会发生少量消息乱序（某个分区挂掉后把消息 Failover 到其它分区）。

如果业务要求分区保证严格有序，请在创建Topic时指定保序。

技术交流
如果你有其它使用方面的困惑，可通过在github demo地址里提交 Issue 进行反馈。






订阅者最佳实践
更新时间：2019-04-28 12:01:16


本页目录
消费消息基本流程
负载均衡
多个订阅
消费位点
消费位点提交
消费位点重置
消息重复和消费幂等
消费失败
消费阻塞以及堆积
提高消费速度
消息过滤
消息广播
订阅关系
本文主要介绍消息队列 Kafka 订阅者的最佳实践，从而帮助您更好的使用该产品。

消费消息基本流程
Kafka 订阅者在订阅消息时的基本流程是：

Poll 数据
执行消费逻辑
再次 poll 数据
负载均衡
每个 Consumer Group 可以包含多个消费实例，即可以启动多个 Kafka Consumer，并把参数 group.id 设置成相同的值。属于同一个 Consumer Group 的消费实例会负载消费订阅的 Topic。

举例：Consumer Group A 订阅了 Topic A，并开启三个消费实例 C1、C2、C3，则发送到 Topic A 的每条消息最终只会传给 C1、C2、C3 的某一个。Kafka 默认会均匀地把消息传给各个消息实例，以做到消费负载均衡。

Kafka 负载消费的内部原理是，把订阅的 Topic 的分区，平均分配给各个消费实例。因此，消费实例的个数不要大于分区的数量，否则会有实例分配不到任何分区而处于空跑状态。这个负载均衡发生的时间，除了第一次启动上线之外，后续消费实例发生重启、增加、减少等变更时，都会触发一次负载均衡。

消息队列 Kafka 的每个 Topic 的分区数量默认是 16 个，已经足够满足大部分场景的需求，且云上服务会根据容量调整分区数。

多个订阅
一个 Consumer Group 可以订阅多个 Topic。一个 Topic 也可以被多个 Consumer Group 订阅，且各个 Consumer Group 独立消费 Topic 下的所有消息。

举例：Consumer Group A 订阅了 Topic A，Consumer Group B 也订阅了 Topic A，则发送到 Topic A 的每条消息，不仅会传一份给 Consumer Group A 的消费实例，也会传一份给 Consumer Group B 的消费实例，且这两个过程相互独立，相互没有任何影响。

消费位点
每个 Topic 会有多个分区，每个分区会统计当前消息的总条数，这个称为最大位点 MaxOffset。Kafka Consumer 会按顺序依次消费分区内的每条消息，记录已经消费了的消息条数，称为ConsumerOffset。

剩余的未消费的条数（也称为消息堆积量） = MaxOffset - ConsumerOffset

消费位点提交
Kafka 消费者有两个相关参数：

enable.auto.commit：默认值为 true。
auto.commit.interval.ms： 默认值为 1000，也即 1s。
这两个参数组合的结果就是，每次 poll 数据前会先检查上次提交位点的时间，如果距离当前时间已经超过参数auto.commit.interval.ms规定的时长，则客户端会启动位点提交动作。

因此，如果将enable.auto.commit设置为 true，则需要在每次 poll 数据时，确保前一次 poll 出来的数据已经消费完毕，否则可能导致位点跳跃。

如果想自己控制位点提交，请把 enable.auto.commit 设为 false，并调用 commit(offsets)函数自行控制位点提交。

消费位点重置
以下两种情况，会发生消费位点重置：

当服务端不存在曾经提交过的位点时（比如客户端第一次上线）
当从非法位点拉取消息时（比如某个分区最大位点是10，但客户端却从11开始拉取消息）
Java 客户端可以通过auto.offset.reset来配置重置策略，主要策略有：

“latest”，从最大位点开始消费
“earliest”，从最小位点开始消费
‘none’, 不做任何操作，也即不重置
建议：

强烈建议设置成“latest”，而不要设置成“earliest”，避免因位点非法时从头开始消费，从而造成大量重复
如果是客户自己管理位点，可以设置成”none”
消息重复和消费幂等
Kafka 消费的语义是 “at least once”， 也就是至少投递一次，保证消息不丢，但是不会保证消息不重复。在出现网络问题、客户端重启时均有可能出现少量重复消息，此时应用消费端如果对消息重复比较敏感（比如说订单交易类），则应该做到消息幂等。

以数据库类应用为例，常用做法是：

发送消息时，传入 key 作为唯一流水号ID；
消费消息时，判断 key 是否已经消费过，如果已经消费过了，则忽略，如果没消费过，则消费一次；
当然，如果应用本身对少量消息重复不敏感，则不需要做此类幂等检查。

消费失败
Kafka 是按分区一条一条消息顺序向前推进消费的，如果消费端拿到某条消息后执行消费逻辑失败，比如应用服务器出现了脏数据，导致某条消息处理失败，等待人工干预，那么有以下两种处理方式：

失败后一直尝试再次执行消费逻辑。这种方式有可能造成消费线程阻塞在当前消息，无法向前推进，造成消息堆积；
由于 Kafka 自身没有处理失败消息的设计，实践中通常会打印失败的消息、或者存储到某个服务（比如创建一个 Topic 专门用来放失败的消息），然后定时 check 失败消息的情况，分析失败原因，根据情况处理。
消费阻塞以及堆积
消费端最常见的问题就是消费堆积，最常造成堆积的原因是：

消费速度跟不上生产速度，此时应该提高消费速度，详见下一节《提高消费速度》；
消费端产生了阻塞。
消费端拿到消息后，执行消费逻辑，通常会执行一些远程调用，如果这个时候同步等待结果，则有可能造成一直等待，消费进程无法向前推进。

消费端应该竭力避免堵塞消费线程，如果存在等待调用结果的情况，建议设置等待的超时时间，超时后作消费失败处理。

提高消费速度
提高消费速度有以下两个办法：

增加 Consumer 实例个数
增加消费线程
增加 Consumer 实例
可以在进程内直接增加（需要保证每个实例对应一个线程，否则没有太大意义），也可以部署多个消费实例进程；需要注意的是，实例个数超过分区数量后就不再能提高速度，将会有消费实例不工作。

增加消费线程
增加 Consumer 实例本质上也是增加线程的方式来提升速度，因此更加重要的性能提升方式是增加消费线程，最基本的步骤如下：

定义一个线程池；
Poll 数据；
把数据提交到线程池进行并发处理；
等并发结果返回成功后，再次 poll 数据执行。
消息过滤
Kafka 自身没有消息过滤的语义。实践中可以采取以下两个办法：

如果过滤的种类不多，可以采取多个 Topic 的方式达到过滤的目的；
如果过滤的种类多，则最好在客户端业务层面自行过滤。
实践中请根据业务具体情况进行选择，也可以综合运用上面两种办法。

消息广播
Kafka 自身没有消息广播的语义，可以通过创建不同的 Consumer Group 来模拟实现。

订阅关系
同一个 Consumer Group 内，各个消费实例订阅的 Topic 最好保持一致，避免给排查问题带来干扰。



======================================================

Apache Kafka是一款流行的分布式数据流平台，它已经广泛地被诸如New Relic(数据智能平台)、Uber、Square(移动支付公司)等大型公司用来构建可扩展的、高吞吐量的、且高可靠的实时数据流系统。例如，在New Relic的生产环境中，Kafka群集每秒能够处理超过1500万条消息，而且其数据聚合率接近1 Tbps。

可见，Kafka大幅简化了对于数据流的处理，因此它也获得了众多应用开发人员和数据管理专家的青睐。然而，在大型系统中Kafka的应用会比较复杂。如果您的consumers无法跟上数据流的话，各种消息往往在未被查看之前就已经消失掉了。同时，它在自动化数据保留方面的限制，高流量的发布+订阅(publish-subscribe，pub/sub)模式等，可能都会影响到您系统的性能。可以毫不夸张地说，如果那些存放着数据流的系统无法按需扩容、或稳定性不可靠的话，估计您经常会寝食难安了。

针对Partitions的最佳实践
• 了解分区的数据速率，以确保提供合适的数据保存空间。此处所谓“分区的数据速率”是指数据的生成速率。换言之，它是由“平均消息大小”乘以“每秒消息数”得出的。数据速率决定了在给定时间内，所能保证的数据保存空间的大小(以字节为单位)。如果您不知道数据速率的话，则无法正确地计算出满足基于给定时间跨度的数据，所需要保存的空间大小。同时，数据速率也能够标识出单个consumer在不产生延时的情况下，所需要支持的最低性能值。

• 除非您有其他架构上的需要，否则在写topic时请使用随机分区。在您进行大型操作时，各个分区在数据速率上的参差不齐是非常难以管理的。其原因来自于如下三个方面：

首先，“热”(有较高吞吐量)分区上的consumer势必会比同组中的其他consumer处理更多的消息，因此很可能会导致出现在处理上和网络上的瓶颈。

其次，那些为具有最高数据速率的分区，所配置的最大保留空间，会导致topic中其他分区的磁盘使用量也做相应地增长。

第三，根据分区的leader关系所实施的最佳均衡方案，比简单地将leader关系分散到所有broker上，要更为复杂。在同一topic中，“热”分区会“承载”10倍于其他分区的权重。

针对Consumers的最佳实践

如果consumers运行的是比Kafka 0.10还要旧的版本，那么请马上升级。


在0.8.x 版中，consumer使用Apache ZooKeeper来协调consumer group，而许多已知的bug会导致其长期处于再均衡状态，或是直接导致再均衡算法的失败(我们称之为“再均衡风暴”)。因此在再均衡期间，一个或多个分区会被分配给同一组中的每个consumer。而在再均衡风暴中，分区的所有权会持续在各个consumers之间流转，这反而阻碍了任何一个consumer去真正获取分区的所有权。

调优consumer的套接字缓冲区(socket buffers)，以应对数据的高速流入。

在Kafka的0.10.x版本中，参数receive.buffer.bytes的默认值为64 kB。而在Kafka的0.8.x版本中，参数socket.receive.buffer.bytes的默认值为100 kB。这两个默认值对于高吞吐量的环境而言都太小了，特别是如果broker和consumer之间的网络带宽延迟积(bandwidth-delay product)大于局域网(local area network，LAN)时。对于延迟为1毫秒或更多的高带宽的网络(如10 Gbps或更高)，请考虑将套接字缓冲区设置为8或16 MB。

如果您的内存不足，也至少考虑设置为1 MB。当然，您也可以设置为-1，它会让底层操作系统根据网络的实际情况，去调整缓冲区的大小。

但是，对于需要启动“热”分区的consumers来说，自动调整可能不会那么快。


设计具有高吞吐量的consumers，以便按需实施背压(back-pressure)。通常，我们应该保证系统只去处理其能力范围内的数据，而不要超负荷“消费”，进而导致进程中断“挂起”，或出现consume group的溢出。如果是在Java虚拟机(JVM)中运行，consumers应当使用固定大小的缓冲区(请参见Disruptor模式：http://lmax-exchange.github.io/disruptor/files/Disruptor-1.0.pdf)，而且最好是使用堆外内存(off-heap)。固定大小的缓冲区能够阻止consumer将过多的数据拉到堆栈上，以至于JVM花费掉其所有的时间去执行垃圾回收，进而无法履行其处理消息的本质工作。

在JVM上运行各种consumers时，请警惕垃圾回收对它们可能产生的影响。例如，长时间垃圾回收的停滞，可能导致ZooKeeper的会话被丢弃、或consumer group处于再均衡状态。对于broker来说也如此，如果垃圾回收停滞的时间太长，则会产生集群掉线的风险。

针对Producers的最佳实践
• 配置producer，以等待各种确认。籍此producer能够获知消息是否真正被发送到了broker的分区上。在Kafka的0.10.x版本上，其设置是acks;而在0.8.x版本上，则为request.required.acks。Kafka通过复制，来提供容错功能，因此单个节点的故障、或分区leader关系的更改不会影响到系统的可用性。如果您没有用acks来配置producer(或称“fire and forget”)的话，则消息可能会悄然丢失。

• 为各个producer配置retries。其默认值为3，当然是非常低的。不过，正确的设定值取决于您的应用程序，即：就那些对于数据丢失零容忍的应用而言，请考虑设置为Integer.MAX_VALUE(有效且最大)。这样将能够应对broker的leader分区出现无法立刻响应produce请求的情况。

• 为高吞吐量的producer，调优缓冲区的大小，特别是buffer.memory和batch.size(以字节为单位)。由于batch.size是按照分区设定的，而producer的性能和内存的使用量，都可以与topic中的分区数量相关联。因此，此处的设定值将取决于如下几个因素：producer数据速率(消息的大小和数量)、要生成的分区数、以及可用的内存量。请记住，将缓冲区调大并不总是好事，如果producer由于某种原因而失效了(例如，某个leader的响应速度比确认还要慢)，那么在堆内内存(on-heap)中的缓冲的数据量越多，其需要回收的垃圾也就越多。

• 检测应用程序，以跟踪诸如生成的消息数、平均消息大小、以及已使用的消息数等指标。

针对Brokers的最佳实践

• 在各个brokers上，请压缩topics所需的内存和CPU资源。日志压缩需要各个broker上的堆栈(内存)和CPU周期都能成功地配合实现。而如果让那些失败的日志压缩数据持续增长的话，则会给brokers分区带来风险。您可以在broker上调整log.cleaner.dedupe.buffer.size和log.cleaner.threads这两个参数，但是请记住，这两个值都会影响到各个brokers上的堆栈使用。如果某个broker抛出OutOfMemoryError异常，那么它将会被关闭、并可能造成数据的丢失。而缓冲区的大小和线程的计数，则取决于需要被清除的topic partition数量、以及这些分区中消息的数据速率与密钥的大小。对于Kafka的0.10.2.1版本而言，通过ERROR条目来监控日志清理程序的日志文件，是检测其线程可能出现问题的最可靠方法。

• 通过网络吞吐量来监控brokers。请监控发向(transmit，TX)和收向(receive，RX)的流量，以及磁盘的I/O、磁盘的空间、以及CPU的使用率，而且容量规划是维护群集整体性能的关键步骤。

• 在群集的各个brokers之间分配分区的leader关系。Leader通常会需要大量的网络I/O资源。例如，当我们将复制因子(replication factor)配置为3、并运行起来时，leader必须首先获取分区的数据，然后将两套副本发送给另两个followers，进而再传输到多个需要该数据的consumers上。因此在该例子中，单个leader所使用的网络I/O，至少是follower的四倍。而且，leader还可能需要对磁盘进行读操作，而follower只需进行写操作。

• 不要忽略监控brokers的in-sync replica(ISR)shrinks、under-replicated partitions和unpreferred leaders。这些都是集群中潜在问题的迹象。例如，单个分区频繁出现ISR收缩，则暗示着该分区的数据速率超过了leader的能力，已无法为consumer和其他副本线程提供服务了。

• 按需修改Apache Log4j的各种属性。Kafka的broker日志记录会耗费大量的磁盘空间，但是我们却不能完全关闭它。因为有时在发生事故之后，需要重建事件序列，那么broker日志就会是我们最好的、甚至是唯一的方法。

• 禁用topic的自动创建，或针对那些未被使用的topics建立清除策略。例如，在设定的x天内，如果未出现新的消息，您应该考虑该topic是否已经失效，并将其从群集中予以删除。此举可避免您花时间去管理群集中被额外创建的元数据。

• 对于那些具有持续高吞吐量的brokers，请提供足够的内存，以避免它们从磁盘子系统中进行读操作。我们应尽可能地直接从操作系统的缓存中直接获取分区的数据。然而，这就意味着您必须确保自己的consumers能够跟得上“节奏”，而对于那些延迟的consumer就只能强制broker从磁盘中读取了。

• 对于具有高吞吐量服务级别目标(service level objectives，SLOs)的大型群集，请考虑为brokers的子集隔离出不同的topic。至于如何确定需要隔离的topics，则完全取决于您自己的业务需要。例如，您有一些使用相同群集的联机事务处理(multiple online transaction processing，OLTP)系统，那么将每个系统的topics隔离到不同brokers子集中，则能够有助于限制潜在事件的影响半径。

• 在旧的客户端上使用新的topic消息格式。应当代替客户端，在各个brokers上加载额外的格式转换服务。当然，最好还是要尽量避免这种情况的发生。

• 不要错误地认为在本地主机上测试好broker，就能代表生产环境中的真实性能了。要知道，如果使用复制因子为1，并在环回接口上对分区所做的测试，是与大多数生产环境截然不同的。在环回接口上网络延迟几乎可以被忽略的，而在不涉及到复制的情况下，接收leader确认所需的时间则同样会出现巨大的差异。


本文英文原文《20 Best Practices for Working With Apache Kafka at Scale》：https://blog.newrelic.com/engineering/kafka-best-practices/

--------


“ 这篇文章，同样给大家聊一个硬核的技术知识，我们通过Kafka内核源码中的一些设计思想，来看你设计Kafka架构的技术大牛，是怎么优化JVM的GC问题的？



1、Kafka的客户端缓冲机制
首先，先得给大家明确一个事情，那就是在客户端发送消息给kafka服务器的时候，一定是有一个内存缓冲机制的。



也就是说，消息会先写入一个内存缓冲中，然后直到多条消息组成了一个Batch，才会一次网络通信把Batch发送过去。



整个过程如下图所示：







2、内存缓冲造成的频繁GC问题
那么这种内存缓冲机制的本意，其实就是把多条消息组成一个Batch，一次网络请求就是一个Batch或者多个Batch。



这样每次网络请求都可以发送很多数据过去，避免了一条消息一次网络请求。从而提升了吞吐量，即单位时间内发送的数据量。



但是问题来了，大家可以思考一下，一个Batch中的数据，会取出来然后封装在底层的网络包里，通过网络发送出去到达Kafka服务器。



那么然后呢？这个Batch里的数据都发送过去了，现在Batch里的数据应该怎么处理？



你要知道，这些Batch里的数据此时可还在客户端的JVM的内存里啊！那么此时从代码实现层面，一定会尝试避免任何变量去引用这些Batch对应的数据，然后尝试触发JVM自动回收掉这些内存垃圾。



这样不断的让JVM回收垃圾，就可以不断的清理掉已经发送成功的Batch了，然后就可以不断的腾出来新的内存空间让后面新的数据来使用。



这种想法很好，但是实际线上运行的时候一定会有问题，最大的问题，就是JVM GC问题。



大家都知道一点，JVM GC在回收内存垃圾的时候，他会有一个“Stop the World”的过程，也就是垃圾回收线程运行的时候，会导致其他工作线程短暂的停顿，这样可以便于他自己安安静静的回收内存垃圾。



这个也很容易想明白，毕竟你要是在回收内存垃圾的时候，你的工作线程还在不断的往内存里写数据，制造更多的内存垃圾，那你让人家JVM怎么回收垃圾？



这就好比在大马路上，如果地上有很多垃圾，现在要把垃圾都扫干净，最好的办法是什么？大家都让开，把马路空出来，然后清洁工就是把垃圾清理干净。



但是如果清洁工在清扫垃圾的时候，结果一帮人在旁边不停的嗑瓜子扔瓜子壳，吃西瓜扔西瓜皮，不停的制造垃圾，你觉得清洁工内心啥感受？当然是很愤慨了，照这么搞，地上的垃圾永远的都搞不干净了！



通过了上面的语言描述，我们再来一张图，大家看看就更加清楚了







现在JVM GC是越来越先进，从CMS垃圾回收器到G1垃圾回收器，核心的目标之一就是不断的缩减垃圾回收的时候，导致其他工作线程停顿的时间。



所以现在越是新款的垃圾回收器导致工作线程停顿的时间越短，但是再怎么短，他也还是存在啊！



所以说，如何尽可能在自己的设计上避免JVM频繁的GC就是一个非常考验水平的事儿了。

3、Kafka设计者实现的缓冲池机制
在Kafka客户端内部，对这个问题实现了一个非常优秀的机制，就是缓冲池的机制

简单来说，就是每个Batch底层都对应一块内存空间，这个内存空间就是专门用来存放写入进去的消息的。

然后呢，当一个Batch被发送到了kafka服务器，这个Batch的数据不再需要了，就意味着这个Batch的内存空间不再使用了。



此时这个Batch底层的内存空间不要交给JVM去垃圾回收，而是把这块内存空间给放入一个缓冲池里。



这个缓冲池里放了很多块内存空间，下次如果你又有一个新的Batch了，那么不就可以直接从这个缓冲池里获取一块内存空间就ok了？



然后如果一个Batch发送出去了之后，再把内存空间给人家还回来不就好了？以此类推，循环往复。



同样，听完了上面的文字描述，再来一张图，看完这张图相信大伙儿就明白了：





一旦使用了这个缓冲池机制之后，就不涉及到频繁的大量内存的GC问题了。



为什么呢？因为他可以上来就占用固定的内存，比如32MB。然后把32MB划分为N多个内存块，比如说一个内存块是16KB，这样的话这个缓冲池里就会有很多的内存块。



然后你需要创建一个新的Batch，就从缓冲池里取一个16KB的内存块就可以了，然后这个Batch就不断的写入消息，但是最多就是写16KB，因为Batch底层的内存块就16KB。



接着如果Batch被发送到Kafka服务器了，此时Batch底层的内存块就直接还回缓冲池就可以了。



下次别人再要构建一个Batch的时候，再次使用缓冲池里的内存块就好了。这样就可以利用有限的内存，对他不停的反复重复的利用。因为如果你的Batch使用完了以后是把内存块还回到缓冲池中去，那么就不涉及到垃圾回收了。



如果没有频繁的垃圾回收，自然就避免了频繁导致的工作线程的停顿了，JVM GC问题是不是就得到了大幅度的优化？



没错，正是这个设计思想让Kafka客户端的性能和吞吐量都非常的高，这里蕴含了大量的优秀的机制。



那么此时有人说了，如果我现在把一个缓冲池里的内存资源都占满了，现在缓冲池里暂时没有内存块了，怎么办呢？



很简单，阻塞你的写入操作，不让你继续写入消息了。把你给阻塞住，不停的等待，直到有内存块释放出来，然后再继续让你写入消息。



4、总结一下
这篇文章我们从Kafka内存缓冲机制的设计思路开始，一直分析到了JVM GC问题的产生原因以及恶劣的影响。



接着谈到了Kafka优秀的缓冲池机制的设计思想以及他是如何解决这个问题的，分析了很多Kafka作者在设计的时候展现出的优秀的技术设计思想和能力。



希望大家多吸取这里的精华，在以后面试或者工作的时候，可以把这些优秀的思想纳为己用。



“ 请你简述一下Kafka中的分区分配 ！”



Duang！！！



当面试官问你这个问题的时候，你会怎么回答？



其实，这道题目里面就暗藏汹涌，因为Kafka中的分区分配在多处出现，而这个问题的表述方式是在潜意识里暗示你回答一种。



这样在你自认为很完美的回答完这个问题之后，面试官会冷不丁的来一句：还有呢？

当你回答完一个点的时候，面试官来一句还有呢，当你再补上一个的时候，他还是会来一句还有呢，就算你又补上第三个的时候，他还是会来一句还有呢？这个时候你会不会一脸懵逼？

今天就针对这个问题来告诉大家怎么样回答才能严丝合缝地抢得先机。

在Kafka中，分区分配是一个很重要的概念，却往往会被读者忽视，它会影响Kafka整体的性能均衡。

当遇到“分区分配”这个字眼的时候，一定要记住有三处地方，分别是生产者发送消息、消费者消费消息和创建主题。

虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。

在面对开篇的问题的时候，不如一下就进行总结性的陈词，说有三处，第一、第二、第三balabala。

当真的让你讲完三处的时候，时间也就差不多了。。聪明的面试官看到你一上来就做了一个规划总结，那他顶多也就让你说说你最熟悉的一种，其实说不定内心已经确认你是对的人。

下面针对这三处做个讲解。不过本文旨在罗列相关知识点，进行相关性的科普描述，让读者可以追根溯源，但并不陈述具体细节，因为细节很多，篇幅有限，如有需要请详参老朽的《深入理解Kafka》。

生产者的分区分配
对于用户而言，当调用send方法发送消息之后，消息就自然而然的发送到了broker中。

其实在这一过程中，有可能还要经过拦截器、序列化器和分区器（Partitioner）的一系列作用之后才能被真正地发往broker。

producer.send(record);
消息在发往broker之前是需要确定它所发往的分区的，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号。

如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。

Kafka中提供的默认分区器是DefaultPartitioner，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器），其中的partition方法就是用来实现具体的分区分配逻辑：

public int partition(String topic, Object key, byte[] keyBytes,
                     Object value, byte[] valueBytes, Cluster cluster);


默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率）

最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。

注意：如果key不为null，那么计算得到的分区号会是所有分区中的任意一个；



如果key为null并且有可用分区，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。


消费者的分区分配
在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。



如图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。

有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。

按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。

对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为RangeAssignor、RoundRobinAssignor以及StickyAssignor

其中RangeAssignor为默认的分区分配策略，至于这三种策略具体代表什么含义，可以去查阅相关资料，比如《深入理解Kafka》，嘿嘿。当然也可以通过实现ParitionAssignor接口来自定义分区分配策略。

在消费组中如果有多个消费者，那么这些消费者又可能会采用不同的分配策略，那么最后怎么“拍板”使用哪一种具体的分配策略呢？

对于这里，我想留一道思考题给大家：在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费，那么这个规则可以被打破么？

如果可以，怎么打破？打破的收益又是什么？



broker端的分区分配
生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区

而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。

分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。

在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；

如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。

使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。

如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。



大数据基础系列之kafka011生产者缓存超时，幂等性和事务实现
原创： 浪尖  Spark学习技巧  2017-07-30
一，demo及相关类

1，基本介绍

KafkaProducer是线程安全的，多线程间共享一个实例比共享多个实例更加高效。首先搞一个demo

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "all");
props.put("retries", 0);
props.put("batch.size", 16384);
props.put("linger.ms", 1);
props.put("buffer.memory", 33554432);
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

Producer<String, String> producer = new KafkaProducer<>(props);
for (int i = 0; i < 100; i++)
producer.send(new ProducerRecord<String, String>("my-topic", Integer.toString(i), Integer.toString(i)));

producer.close();

2，ProducerRecord

发往kafka的key/value对。由topic，分区id(可选)，key(可选)，timestamp(可选)，value组成。

如果一个有效的分区ID被指定，Record就会被发到指定的分区。如果，没指定分区id，只指定了key，就会按照key做hash后对分区数取余得到的数值作为分区的id。如果分区id，和key都没有指定，就会以轮训的形式发送Records。

Record还有一个timestamp属性。如果用户没有提供timestamp，生产者将会使用当前时间作为Record的timestamp。Kafka最终使用的时间戳取决于topic配置的时间类型。

1),如果topic配置使用了CreateTime，Broker就会使用生产者生产Record时带的时间戳。

2),如果topic配置使用了LogAppendTime，Record追加到log的时候，Broker会有本地时间代替Producer生产时带的时间戳。

无论是采用的上文中的哪种形式，timestamp都会被包含在RecordMetadata中返回。

ProducerRecord(String topic, Integer partition, K key, V value)
Creates a record to be sent to a specified topic and partition
  ProducerRecord(String topic, Integer partition, K key, V value, Iterable<Header> headers)
Creates a record to be sent to a specified topic and partition
  ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)
Creates a record with a specified timestamp to be sent to a specified topic and partition
  ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value, Iterable<Header> headers)
Creates a record with a specified timestamp to be sent to a specified topic and partition
  ProducerRecord(String topic, K key, V value)
Create a record to be sent to Kafka
  ProducerRecord(String topic, V value)
Create a record with no key

二，缓存和超时

生产者内部有一个buffer，用来缓存Record，同时内部有一个后台线程负责将Record转化为请求，然后将请求发给kafka集群。使用生产者后未关闭，会导致这些资源泄漏。

send方法是异步的。调用他实际上是将Record添加到Buffer中，然后立即返回。这使得生产者可以批量提交消息来提升性能。

acks配置控制发送请求完成的标准。如果设置成all，将会导致生产阻塞，等待所有副本提交日志成功后才算发送完成，超级低效但是可以最大限度的容错。

如果请求失败，生产者会自动尝试，前提是不要设置retries为零。当然，开启失败尝试也就意味着带来了数据重复发送的风险。

生产者为每个分区维护一个buffer，这个buffer的大小由batch.size指定，该值越大表示批量发送的消息数越多，也意味着需要更大的内存。内存数可以估计的。

默认情况下，即使buffer还有剩余的空间没有填充，消息也会被立即发送。如果你想减少请求的次数，可以设置linger.ms参数为大于0的某一值。使生产者发送消息前等待linger.ms指定的时间，这样就可以有更多的消息加入到该batch来。这很像TCP中的Nagle原理。例如，在上面的代码片段中，由于我们设置linger.ms为1ms，100条消息可能在一次请求中全部发送到了Server端。然而，这也意味着加入消息一直不能填充满buffer，我们要延迟一毫秒。

buffer.memory决定者生产者所能用于buffer的总内存大小。如果，消息发送的速度比传输到Server端的速度快，这个buffer空间就会耗尽。当buffer空间耗尽，send调用就会阻塞，超过max.block.ms设置的超时时间后会抛出TimeoutException。

三，序列化

Key.serializer和value.serialize决定者如何将key和value对象转化为字节数组。你可以使用包括bytearrayserializer或stringserializer简单的字符串或字节类型。也可以实现自定义的序列化方式。

四，幂等性

从kafka0.11版本开始，Kafka支持两种额外的模式：幂等性生产者和事务生产者。幂等性强化消息的传递语义，从至少一次到仅仅一次。特别是生产者重试将不再导致消息重复发送。事务生产者允许应用程序将消息原子的发送到多个分区（和主题！）。

设置enable.idempotence为true来开启幂等性，如果设置了这个参数retries配置将会被设置为默认值，也即Integer.MAX_VALUE，max.inflight.requests.per.connection会被设置为1，acks会被设置为all。幂等性生产者不需要修改API，所以现有的应用程序不需要修改就可以使用该特性。

为了利用幂等生产者，必须避免应用程序级重新发送，因为这些不能被去重。例如，如果应用程序运行幂等性，建议不要设置retries，因为他会被设置为默认值(Integer.MAX_VALUE).此外，如果send（producerrecord）返回一个错误甚至无限重试（例如,如果消息送前缓冲区满了），建议关闭生产和检查最后产生消息的内容以确保不重复。

五，事务

为了使用事务生产者和相关的APIs，必须要设置transactional.id属性.如果设置了transactional.id幂等性会自动被启用。支持事务的topic必须要进行容错配置。特别的replication.factor应该设置为3，topic的min.insync.replicas配置必须设置为2.最后，为了从端到端实现事务性保证，必须配置消费者只读取committed 的消息。

transactional.id目的是单生产者实例能从多会话中恢复。该特性就是分区的，状态的应用程序程序中的一个碎片标识符。transactional.id值在一个分区的应用中每个消费者实例必须是唯一的。

所有新的事务性API都会被阻塞，将在失败时抛出异常。举一个简单的例子，一次事务中提交100条消息。

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("transactional.id", "my-transactional-id");
Producer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());

producer.initTransactions();

try {
  producer.beginTransaction();
  for (int i = 0; i < 100; i++)
  producer.send(new ProducerRecord<>("my-topic", Integer.toString(i), Integer.toString(i)));
  producer.commitTransaction();
} catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {
  // We can't recover from these exceptions, so our only option is to close the producer and exit.
  producer.close();
} catch (KafkaException e) {
  // For all other exceptions, just abort the transaction and try again.
  producer.abortTransaction();
}
producer.close();

就如例子一样，每个消费者只能有一个事务开启。在beginTransaction() 和commitTransaction()中间发送的所有消息，都是一次事务的一部分。

事务生产者使用execeptions进行错误状态交流。特别之处，我们不需要为producer.send指定回调函数。任何在事务中不可恢复的错误发生都会抛出一个KafkaException异常(http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send(org.apache.kafka.clients.producer.ProducerRecord))。

在接受到一个kafkaexection异常之后，通过调用producer.abortTransaction()，可以保证所有的已经写入成功的消息会被标记为aborted，因此保证事务传输。
==========
简历写了会Kafka，面试官90%会让你讲讲acks参数对消息持久化的影响




面试大厂时，一旦简历上写了Kafka，几乎必然会被问到一个问题：说说acks参数对消息持久化的影响？
这个acks参数在kafka的使用中，是非常核心以及关键的一个参数，决定了很多东西。


（1）如何保证宕机的时候数据不丢失？


如果要想理解这个acks参数的含义，首先就得搞明白kafka的高可用架构原理。



比如下面的图里就是表明了对于每一个Topic，我们都可以设置他包含几个Partition，每个Partition负责存储这个Topic一部分的数据。



然后Kafka的Broker集群中，每台机器上都存储了一些Partition，也就存放了Topic的一部分数据，这样就实现了Topic的数据分布式存储在一个Broker集群上。



但是有一个问题，万一 一个Kafka Broker宕机了，此时上面存储的数据不就丢失了吗？



没错，这就是一个比较大的问题了，分布式系统的数据丢失问题，是他首先必须要解决的，一旦说任何一台机器宕机，此时就会导致数据的丢失。







（2）多副本冗余的高可用机制


所以如果大家去分析任何一个分布式系统的原理，比如说zookeeper、kafka、redis cluster、elasticsearch、hdfs，等等，其实他都有自己内部的一套多副本冗余的机制，多副本冗余几乎是现在任何一个优秀的分布式系统都一般要具备的功能。



在kafka集群中，每个Partition都有多个副本，其中一个副本叫做leader，其他的副本叫做follower，如下图。



如上图所示，假设一个Topic拆分为了3个Partition，分别是Partition0，Partiton1，Partition2，此时每个Partition都有2个副本。



比如Partition0有一个副本是Leader，另外一个副本是Follower，Leader和Follower两个副本是分布在不同机器上的。



这样的多副本冗余机制，可以保证任何一台机器挂掉，都不会导致数据彻底丢失，因为起码还是有副本在别的机器上的。







（3）多副本之间数据如何同步？


其实任何一个Partition，只有Leader是对外提供读写服务的

也就是说，如果有一个客户端往一个Partition写入数据，此时一般就是写入这个Partition的Leader副本。


然后Leader副本接收到数据之后，Follower副本会不停的给他发送请求尝试去拉取最新的数据，拉取到自己本地后，写入磁盘中。如下图所示：







（4）ISR到底指的是什么东西？


既然大家已经知道了Partiton的多副本同步数据的机制了，那么就可以来看看ISR是什么了。



ISR全称是“In-Sync Replicas”，也就是保持同步的副本，他的含义就是，跟Leader始终保持同步的Follower有哪些。



大家可以想一下 ，如果说某个Follower所在的Broker因为JVM FullGC之类的问题，导致自己卡顿了，无法及时从Leader拉取同步数据，那么是不是会导致Follower的数据比Leader要落后很多？



所以这个时候，就意味着Follower已经跟Leader不再处于同步的关系了。但是只要Follower一直及时从Leader同步数据，就可以保证他们是处于同步的关系的。



所以每个Partition都有一个ISR，这个ISR里一定会有Leader自己，因为Leader肯定数据是最新的，然后就是那些跟Leader保持同步的Follower，也会在ISR里。







（5）acks参数的含义


铺垫了那么多的东西，最后终于可以进入主题来聊一下acks参数的含义了。



如果大家没看明白前面的那些副本机制、同步机制、ISR机制，那么就无法充分的理解acks参数的含义，这个参数实际上决定了很多重要的东西。



首先这个acks参数，是在KafkaProducer，也就是生产者客户端里设置的



也就是说，你往kafka写数据的时候，就可以来设置这个acks参数。然后这个参数实际上有三种常见的值可以设置，分别是：0、1 和 all。



第一种选择是把acks参数设置为0，意思就是我的KafkaProducer在客户端，只要把消息发送出去，不管那条数据有没有在哪怕Partition Leader上落到磁盘，我就不管他了，直接就认为这个消息发送成功了。



如果你采用这种设置的话，那么你必须注意的一点是，可能你发送出去的消息还在半路。结果呢，Partition Leader所在Broker就直接挂了，然后结果你的客户端还认为消息发送成功了，此时就会导致这条消息就丢失了。





第二种选择是设置 acks = 1，意思就是说只要Partition Leader接收到消息而且写入本地磁盘了，就认为成功了，不管他其他的Follower有没有同步过去这条消息了。



这种设置其实是kafka默认的设置，大家请注意，划重点！这是默认的设置



也就是说，默认情况下，你要是不管acks这个参数，只要Partition Leader写成功就算成功。



但是这里有一个问题，万一Partition Leader刚刚接收到消息，Follower还没来得及同步过去，结果Leader所在的broker宕机了，此时也会导致这条消息丢失，因为人家客户端已经认为发送成功了。



最后一种情况，就是设置acks=all，这个意思就是说，Partition Leader接收到消息之后，还必须要求ISR列表里跟Leader保持同步的那些Follower都要把消息同步过去，才能认为这条消息是写入成功了。



如果说Partition Leader刚接收到了消息，但是结果Follower没有收到消息，此时Leader宕机了，那么客户端会感知到这个消息没发送成功，他会重试再次发送消息过去。



此时可能Partition 2的Follower变成Leader了，此时ISR列表里只有最新的这个Follower转变成的Leader了，那么只要这个新的Leader接收消息就算成功了。






（6）最后的思考


acks=all 就可以代表数据一定不会丢失了吗？
当然不是，如果你的Partition只有一个副本，也就是一个Leader，任何Follower都没有，你认为acks=all有用吗？
当然没用了，因为ISR里就一个Leader，他接收完消息后宕机，也会导致数据丢失。

所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。
这样才能保证说写一条数据过去，一定是2个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。


----------------------------

Kafka分区分配策略(Partition Assignment Strategy)
原创： 过往记忆  过往记忆大数据  2018-05-09
问题
用过 Kafka 的同学用过都知道，每个 Topic 一般会有很多个 partitions。为了使得我们能够及时消费消息，我们也可能会启动多个 Consumer 去消费，而每个 Consumer 又会启动一个或多个streams去分别消费 Topic 里面的数据。我们又知道，Kafka 存在 Consumer Group 的概念，也就是 group.id 一样的 Consumer，这些 Consumer 属于同一个Consumer Group，组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费。那么问题来了，同一个 Consumer Group 里面的 Consumer 是如何知道该消费哪些分区里面的数据呢？



如果想及时了解Spark、Hadoop或者Hbase相关的文章，欢迎关注微信公共帐号：iteblog_hadoop

如上图，Consumer1 为啥消费的是 Partition0 和 Partition2，而不是 Partition0 和 Partition3？这就涉及到 Kafka 内部分区分配策略（Partition Assignment Strategy）了。

在 Kafka 内部存在两种默认的分区分配策略：Range 和 RoundRobin。当以下事件发生时，Kafka 将会进行一次分区分配：

同一个 Consumer Group 内新增消费者

消费者离开当前所属的Consumer Group，包括shuts down 或 crashes

订阅的主题新增分区

将分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance），如何rebalance就涉及到本文提到的分区分配策略。下面我们将详细介绍 Kafka 内置的两种分区分配策略。本文假设我们有个名为 T1 的主题，其包含了10个分区，然后我们有两个消费者（C1，C2）来消费这10个分区里面的数据，而且 C1 的 num.streams = 1，C2 的 num.streams = 2。

Range strategy
Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。在我们的例子里面，排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是C1-0, C2-0, C2-1。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程， 10 / 3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：

C1-0 将消费 0, 1, 2, 3 分区
C2-0 将消费 4, 5, 6 分区
C2-1 将消费 7, 8, 9 分区
假如我们有11个分区，那么最后分区分配的结果看起来是这样的：

C1-0 将消费 0, 1, 2, 3 分区
C2-0 将消费 4, 5, 6, 7 分区
C2-1 将消费 8, 9, 10 分区
假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的：

C1-0 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区
C2-0 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区
C2-1 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区
可以看出，C1-0 消费者线程比其他消费者线程多消费了2个分区，这就是Range strategy的一个很明显的弊端。

RoundRobin strategy
使用RoundRobin策略有两个前提条件必须满足：

同一个Consumer Group里面的所有消费者的num.streams必须相等；

每个消费者订阅的主题必须相同。

所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，这里文字可能说不清，看下面的代码应该会明白：

val allTopicPartitions = ctx.partitionsForTopic.flatMap { case(topic, partitions) =>
  info("Consumer %s rebalancing the following partitions for topic %s: %s"
       .format(ctx.consumerId, topic, partitions))
  partitions.map(partition => {
    TopicAndPartition(topic, partition)
  })
}.toSeq.sortWith((topicPartition1, topicPartition2) => {
  /*
   * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending
   * up on one consumer (if it has a high enough stream count).
   */
  topicPartition1.toString.hashCode < topicPartition2.toString.hashCode
})
最后按照round-robin风格将分区分别分配给不同的消费者线程。

在我们的例子里面，加入按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为：

C1-0 将消费 T1-5, T1-2, T1-6 分区；
C1-1 将消费 T1-3, T1-1, T1-9 分区；
C2-0 将消费 T1-0, T1-4 分区；
C2-1 将消费 T1-8, T1-7 分区；
多个主题的分区分配和单个主题类似，这里就不在介绍了。

根据上面的详细介绍相信大家已经对Kafka的分区分配策略原理很清楚了。不过遗憾的是，目前我们还不能自定义分区分配策略，只能通过partition.assignment.strategy参数选择 range 或 roundrobin。partition.assignment.strategy参数默认的值是range。



水位或水印（watermark）一词，也可称为高水位(high watermark)，通常被用在流式处理领域（比如Apache Flink、Apache Spark等），以表征元素或事件在基于时间层面上的进度。一个比较经典的表述为：流式系统保证在水位t时刻，创建时间（event time） = t'且t' ≤ t的所有事件都已经到达或被观测到。在Kafka中，水位的概念反而与时间无关，而是与位置信息相关。严格来说，它表示的就是位置信息，即位移（offset）。网上有一些关于Kafka watermark的介绍，本不应再赘述，但鉴于本文想要重点强调的leader epoch与watermark息息相关，故这里再费些篇幅阐述一下watermark。注意：由于Kafka源码中使用的名字是高水位，故本文将始终使用high watermaker或干脆简称为HW。

Kafka分区下有可能有很多个副本(replica)用于实现冗余，从而进一步实现高可用。副本根据角色的不同可分为3类：

leader副本：响应clients端读写请求的副本
follower副本：被动地备份leader副本中的数据，不能响应clients端读写请求。
ISR副本：包含了leader副本和所有与leader副本保持同步的follower副本——如何判定是否与leader同步后面会提到
每个Kafka副本对象都有两个重要的属性：LEO和HW。注意是所有的副本，而不只是leader副本。

LEO：即日志末端位移(log end offset)，记录了该副本底层日志(log)中下一条消息的位移值。注意是下一条消息！也就是说，如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0, 9]。另外，leader LEO和follower LEO的更新是有区别的。我们后面会详细说
HW：即上面提到的水位值。对于同一个副本对象而言，其HW值不会大于LEO值。小于等于HW值的所有消息都被认为是“已备份”的（replicated）。同理，leader副本和follower副本的HW更新是有区别的，我们后面详谈。
我们使用下图来形象化地说明两者的关系：

<ignore_js_op>

上图中，HW值是7，表示位移是0~7的所有消息都已经处于“已备份状态”（committed），而LEO值是15，那么8~14的消息就是尚未完全备份（fully replicated）——为什么没有15？因为刚才说过了，LEO指向的是下一条消息到来时的位移，故上图使用虚线框表示。我们总说consumer无法消费未提交消息。这句话如果用以上名词来解读的话，应该表述为：consumer无法消费分区下leader副本中位移值大于分区HW的任何消息。这里需要特别注意分区HW就是leader副本的HW值。

既然副本分为leader副本和follower副本，而每个副本又都有HW和LEO，那么它们是怎么被更新的呢？它们更新的机制又有什么区别呢？我们一一来分析下：

一、follower副本何时更新LEO？

如前所述，follower副本只是被动地向leader副本请求数据，具体表现为follower副本不停地向leader副本所在的broker发送FETCH请求，一旦获取消息后写入自己的日志中进行备份。那么follower副本的LEO是何时更新的呢？首先我必须言明，Kafka有两套follower副本LEO(明白这个是搞懂后面内容的关键，因此请多花一点时间来思考)：1. 一套LEO保存在follower副本所在broker的副本管理机中；2. 另一套LEO保存在leader副本所在broker的副本管理机中——换句话说，leader副本机器上保存了所有的follower副本的LEO。

为什么要保存两套？这是因为Kafka使用前者帮助follower副本更新其HW值；而利用后者帮助leader副本更新其HW使用。下面我们分别看下它们被更新的时机。

1 follower副本端的follower副本LEO何时更新？

follower副本端的LEO值就是其底层日志的LEO值，也就是说每当新写入一条消息，其LEO值就会被更新(类似于LEO += 1)。当follower发送FETCH请求后，leader将数据返回给follower，此时follower开始向底层log写数据，从而自动地更新LEO值

2 leader副本端的follower副本LEO何时更新？

leader副本端的follower副本LEO的更新发生在leader在处理follower FETCH请求时。一旦leader接收到follower发送的FETCH请求，它首先会从自己的log中读取相应的数据，但是在给follower返回数据之前它先去更新follower的LEO(即上面所说的第二套LEO)

二、follower副本何时更新HW？

follower更新HW发生在其更新LEO之后，一旦follower向log写完数据，它会尝试更新它自己的HW值。具体算法就是比较当前LEO值与FETCH响应中leader的HW值，取两者的小者作为新的HW值。这告诉我们一个事实：如果follower的LEO值超过了leader的HW值，那么follower HW值是不会越过leader HW值的。

三、leader副本何时更新LEO？

和follower更新LEO道理相同，leader写log时就会自动地更新它自己的LEO值。

四、leader副本何时更新HW值？

前面说过了，leader的HW值就是分区HW值，因此何时更新这个值是我们最关心的，因为它直接影响了分区数据对于consumer的可见性 。以下4种情况下leader会尝试去更新分区HW——切记是尝试，有可能因为不满足条件而不做任何更新：

副本成为leader副本时：当某个副本成为了分区的leader副本，Kafka会尝试去更新分区HW。这是显而易见的道理，毕竟分区leader发生了变更，这个副本的状态是一定要检查的！不过，本文讨论的是当系统稳定后且正常工作时备份机制可能出现的问题，故这个条件不在我们的讨论之列。
broker出现崩溃导致副本被踢出ISR时：若有broker崩溃则必须查看下是否会波及此分区，因此检查下分区HW值是否需要更新是有必要的。本文不对这种情况做深入讨论
producer向leader副本写入消息时：因为写入消息会更新leader的LEO，故有必要再查看下HW值是否也需要修改
leader处理follower FETCH请求时：当leader处理follower的FETCH请求时首先会从底层的log读取数据，之后会尝试更新分区HW值
特别注意上面4个条件中的最后两个。它揭示了一个事实——当Kafka broker都正常工作时，分区HW值的更新时机有两个：leader处理PRODUCE请求时和leader处理FETCH请求时。另外，leader是如何更新它的HW值的呢？前面说过了，leader broker上保存了一套follower副本的LEO以及它自己的LEO。当尝试确定分区HW时，它会选出所有满足条件的副本，比较它们的LEO(当然也包括leader自己的LEO)，并选择最小的LEO值作为HW值。这里的满足条件主要是指副本要满足以下两个条件之一：

处于ISR中
副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值(默认是10s)
乍看上去好像这两个条件说得是一回事，毕竟ISR的定义就是第二个条件描述的那样。但某些情况下Kafka的确可能出现副本已经“追上”了leader的进度，但却不在ISR中——比如某个从failure中恢复的副本。如果Kafka只判断第一个条件的话，确定分区HW值时就不会考虑这些未在ISR中的副本，但这些副本已经具备了“立刻进入ISR”的资格，因此就可能出现分区HW值越过ISR中副本LEO的情况——这肯定是不允许的，因为分区HW实际上就是ISR中所有副本LEO的最小值。

好了，理论部分我觉得说的差不多了，下面举个实际的例子。我们假设有一个topic，单分区，副本因子是2，即一个leader副本和一个follower副本。我们看下当producer发送一条消息时，broker端的副本到底会发生什么事情以及分区HW是如何被更新的。

下图是初始状态，我们稍微解释一下：初始时leader和follower的HW和LEO都是0(严格来说源代码会初始化LEO为-1，不过这不影响之后的讨论)。leader中的remote LEO指的就是leader端保存的follower LEO，也被初始化成0。此时，producer没有发送任何消息给leader，而follower已经开始不断地给leader发送FETCH请求了，但因为没有数据因此什么都不会发生。值得一提的是，follower发送过来的FETCH请求因为无数据而暂时会被寄存到leader端的purgatory中，待500ms(replica.fetch.wait.max.ms参数)超时后会强制完成。倘若在寄存期间producer端发送过来数据，那么会Kafka会自动唤醒该FETCH请求，让leader继续处理之。

虽然purgatory不是本文的重点，但FETCH请求发送和PRODUCE请求处理的时机会影响我们的讨论。因此后续我们也将分两种情况来讨论分区HW的更新。

<ignore_js_op>

第一种情况：follower发送FETCH请求在leader处理完PRODUCE请求之后

producer给该topic分区发送了一条消息。此时的状态如下图所示：

<ignore_js_op>

如图所示，leader接收到PRODUCE请求主要做两件事情：

把消息写入写底层log（同时也就自动地更新了leader的LEO）
尝试更新leader HW值（前面leader副本何时更新HW值一节中的第三个条件触发）。我们已经假设此时follower尚未发送FETCH请求，那么leader端保存的remote LEO依然是0，因此leader会比较它自己的LEO值和remote LEO值，发现最小值是0，与当前HW值相同，故不会更新分区HW值
所以，PRODUCE请求处理完成后leader端的HW值依然是0，而LEO是1，remote LEO是1。假设此时follower发送了FETCH请求(或者说follower早已发送了FETCH请求，只不过在broker的请求队列中排队)，那么状态变更如下图所示：

<ignore_js_op>

本例中当follower发送FETCH请求时，leader端的处理依次是：

读取底层log数据
更新remote LEO = 0（为什么是0？ 因为此时follower还没有写入这条消息。leader如何确认follower还未写入呢？这是通过follower发来的FETCH请求中的fetch offset来确定的）
尝试更新分区HW——此时leader LEO = 1，remote LEO = 0，故分区HW值= min(leader LEO, follower remote LEO) = 0
把数据和当前分区HW值（依然是0）发送给follower副本
而follower副本接收到FETCH response后依次执行下列操作：

写入本地log（同时更新follower LEO）
更新follower HW——比较本地LEO和当前leader LEO取小者，故follower HW = 0
此时，第一轮FETCH RPC结束，我们会发现虽然leader和follower都已经在log中保存了这条消息，但分区HW值尚未被更新。实际上，它是在第二轮FETCH RPC中被更新的，如下图所示：

<ignore_js_op>

上图中，follower发来了第二轮FETCH请求，leader端接收到后仍然会依次执行下列操作：

读取底层log数据
更新remote LEO = 1（这次为什么是1了？ 因为这轮FETCH RPC携带的fetch offset是1，那么为什么这轮携带的就是1了呢，因为上一轮结束后follower LEO被更新为1了）
尝试更新分区HW——此时leader LEO = 1，remote LEO = 1，故分区HW值= min(leader LEO, follower remote LEO) = 1。注意分区HW值此时被更新了！！！
把数据（实际上没有数据）和当前分区HW值（已更新为1）发送给follower副本
同样地，follower副本接收到FETCH response后依次执行下列操作：

写入本地log，当然没东西可写，故follower LEO也不会变化，依然是1
更新follower HW——比较本地LEO和当前leader LEO取小者。由于此时两者都是1，故更新follower HW = 1 （注意：我特意用了两种颜色来描述这两步，后续会谈到原因！）
Okay，producer端发送消息后broker端完整的处理流程就讲完了。此时消息已经成功地被复制到leader和follower的log中且分区HW是1，表明consumer能够消费offset = 0的这条消息。下面我们来分析下PRODUCE和FETCH请求交互的第二种情况。

第二种情况：FETCH请求保存在purgatory中PRODUCE请求到来

这种情况实际上和第一种情况差不多。前面说过了，当leader无法立即满足FECTH返回要求的时候(比如没有数据)，那么该FETCH请求会被暂存到leader端的purgatory中，待时机成熟时会尝试再次处理它。不过Kafka不会无限期地将其缓存着，默认有个超时时间（500ms），一旦超时时间已过，则这个请求会被强制完成。不过我们要讨论的场景是在寄存期间，producer发送PRODUCE请求从而使之满足了条件从而被唤醒。此时，leader端处理流程如下：

leader写入本地log（同时自动更新leader LEO）
尝试唤醒在purgatory中寄存的FETCH请求
尝试更新分区HW
至于唤醒后的FETCH请求的处理与第一种情况完全一致，故这里不做详细展开了。

以上所有的东西其实就想说明一件事情：Kafka使用HW值来决定副本备份的进度，而HW值的更新通常需要额外一轮FETCH RPC才能完成，故而这种设计是有问题的。它们可能引起的问题包括：

备份数据丢失
备份数据不一致
我们一一分析下：

一、数据丢失

如前所述，使用HW值来确定备份进度时其值的更新是在下一轮RPC中完成的。现在翻到上面使用两种不同颜色标记的步骤处思考下， 如果follower副本在蓝色标记的第一步与紫色标记的第二步之间发生崩溃，那么就有可能造成数据的丢失。我们举个例子来看下。

<ignore_js_op>

上图中有两个副本：A和B。开始状态是A是leader。我们假设producer端min.insync.replicas设置为1，那么当producer发送两条消息给A后，A写入到底层log，此时Kafka会通知producer说这两条消息写入成功。

但是在broker端，leader和follower底层的log虽都写入了2条消息且分区HW已经被更新到2，但follower HW尚未被更新（也就是上面紫色颜色标记的第二步尚未执行）。倘若此时副本B所在的broker宕机，那么重启回来后B会自动把LEO调整到之前的HW值，故副本B会做日志截断(log truncation)，将offset = 1的那条消息从log中删除，并调整LEO = 1，此时follower副本底层log中就只有一条消息，即offset = 0的消息。

B重启之后需要给A发FETCH请求，但若A所在broker机器在此时宕机，那么Kafka会令B成为新的leader，而当A重启回来后也会执行日志截断，将HW调整回1。这样，位移=1的消息就从两个副本的log中被删除，即永远地丢失了。

这个场景丢失数据的前提是在min.insync.replicas=1时，一旦消息被写入leader端log即被认为是“已提交”，而延迟一轮FETCH RPC更新HW值的设计使得follower HW值是异步延迟更新的，倘若在这个过程中leader发生变更，那么成为新leader的follower的HW值就有可能是过期的，使得clients端认为是成功提交的消息被删除。

二、leader/follower数据离散

除了可能造成的数据丢失以外，这种设计还有一个潜在的问题，即造成leader端log和follower端log的数据不一致。比如leader端保存的记录序列是r1,r2,r3,r4,r5,....；而follower端保存的序列可能是r1,r3,r4,r5,r6...。这也是非法的场景，因为顾名思义，follower必须追随leader，完整地备份leader端的数据。

我们依然使用一张图来说明这种场景是如何发生的：

<ignore_js_op>

这种情况的初始状态与情况1有一些不同的：A依然是leader，A的log写入了2条消息，但B的log只写入了1条消息。分区HW更新到2，但B的HW还是1，同时producer端的min.insync.replicas = 1。

这次我们让A和B所在机器同时挂掉，然后假设B先重启回来，因此成为leader，分区HW = 1。假设此时producer发送了第3条消息(绿色框表示)给B，于是B的log中offset = 1的消息变成了绿色框表示的消息，同时分区HW更新到2（A还没有回来，就B一个副本，故可以直接更新HW而不用理会A）之后A重启回来，需要执行日志截断，但发现此时分区HW=2而A之前的HW值也是2，故不做任何调整。此后A和B将以这种状态继续正常工作。

显然，这种场景下，A和Bdicenglog中保存在offset = 1的消息是不同的记录，从而引发不一致的情形出现。


Kafka 0.11.0.0.版本解决方案

造成上述两个问题的根本原因在于HW值被用于衡量副本备份的成功与否以及在出现failture时作为日志截断的依据，但HW值的更新是异步延迟的，特别是需要额外的FETCH请求处理流程才能更新，故这中间发生的任何崩溃都可能导致HW值的过期。鉴于这些原因，Kafka 0.11引入了leader epoch来取代HW值。Leader端多开辟一段内存区域专门保存leader的epoch信息，这样即使出现上面的两个场景也能很好地规避这些问题。

所谓leader epoch实际上是一对值：（epoch，offset）。epoch表示leader的版本号，从0开始，当leader变更过1次时epoch就会+1，而offset则对应于该epoch版本的leader写入第一条消息的位移。因此假设有两对值：

(0, 0)
(1, 120)

则表示第一个leader从位移0开始写入消息；共写了120条[0, 119]；而第二个leader版本号是1，从位移120处开始写入消息。

leader broker中会保存这样的一个缓存，并定期地写入到一个checkpoint文件中。

当leader写底层log时它会尝试更新整个缓存——如果这个leader首次写消息，则会在缓存中增加一个条目；否则就不做更新。而每次副本重新成为leader时会查询这部分缓存，获取出对应leader版本的位移，这就不会发生数据不一致和丢失的情况。


下面我们依然使用图的方式来说明下利用leader epoch如何规避上述两种情况

一、规避数据丢失

<ignore_js_op>

上图左半边已经给出了简要的流程描述，这里不详细展开具体的leader epoch实现细节（比如OffsetsForLeaderEpochRequest的实现），我们只需要知道每个副本都引入了新的状态来保存自己当leader时开始写入的第一条消息的offset以及leader版本。这样在恢复的时候完全使用这些信息而非水位来判断是否需要截断日志。

二、规避数据不一致

<ignore_js_op>


同样的道理，依靠leader epoch的信息可以有效地规避数据不一致的问题。

总结

0.11.0.0版本的Kafka通过引入leader epoch解决了原先依赖水位表示副本进度可能造成的数据丢失/数据不一致问题。有兴趣的读者可以阅读源代码进一步地了解其中的工作原理。

源代码位置：kafka.server.epoch.LeaderEpochCache.scala （leader epoch数据结构）、
kafka.server.checkpoints.LeaderEpochCheckpointFile（checkpoint检查点文件操作类）还有分布在Log中的CRUD操作。

===================


Kafka面试题全套整理 | 划重点要考！
原创： 朱小厮  朱小厮的博客  3月14日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！







有很多人问过我要过Kafka相关的面试题，我一直懒得整理，这几天花了点时间，结合之前面试被问过的、别人咨询过的、我会问别人的进行了相关的整理，也就几十题，大家花个几分钟看看应该都会。面试题列表如下：



Kafka的用途有哪些？使用场景如何？

Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么

Kafka中的HW、LEO、LSO、LW等分别代表什么？

Kafka中是怎么体现消息顺序性的？

Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？

Kafka生产者客户端的整体结构是什么样子的？

Kafka生产者客户端中使用了几个线程来处理？分别是什么？

Kafka的旧版Scala的消费者客户端的设计有什么缺陷？

“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？

消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?

有哪些情形会造成重复消费？

那些情景下会造成消息漏消费？

KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？

简述消费者与消费组之间的关系

当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？

topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？

topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？

创建topic时如何选择合适的分区数？

Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？

优先副本是什么？它有什么特殊的作用？

Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理

简述Kafka的日志目录结构

Kafka中有那些索引文件？

如果我指定了一个offset，Kafka怎么查找到对应的消息？

如果我指定了一个timestamp，Kafka怎么查找到对应的消息？

聊一聊你对Kafka的Log Retention的理解

聊一聊你对Kafka的Log Compaction的理解

聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）

聊一聊Kafka的延时操作的原理

聊一聊Kafka控制器的作用

消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）

Kafka中的幂等是怎么实现的

Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话...只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ....”）

Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？

失效副本是指什么？有那些应对措施？

多副本下，各个副本中的HW和LEO的演变过程

为什么Kafka不支持读写分离？

Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）

Kafka中怎么实现死信队列和重试队列？

Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）

Kafka中怎么做消息审计？

Kafka中怎么做消息轨迹？

Kafka中有那些配置参数比较有意思？聊一聊你的看法

Kafka中有那些命名比较有意思？聊一聊你的看法

Kafka有哪些指标需要着重关注？

怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)

Kafka的那些设计让它有如此高的性能？

Kafka有什么优缺点？

还用过什么同质类的其它产品，与Kafka相比有什么优缺点？

为什么选择Kafka?

在使用Kafka的过程中遇到过什么困难？怎么解决的？

怎么样才能确保Kafka极大程度上的可靠性？

聊一聊你对Kafka生态的理解



如果上面的问题都能掌握，相信在面试Kafka的时候肯定能够完全应付。如果还应付不了，请告知公司及职位，我去投简历会会他。如果上面1/3题回答都有难度的话，是该好好学习一下Kafka啦。



===============

Kafka科普系列 | 什么是LSO？
原创： 朱小厮  朱小厮的博客  4月3日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！



很多同学对于Kafka的认知仅限于在LEO和HW之间，有可能认知还出现错误，对此记住一点，这两个都是指最后一条的下一条的位置而不是指最后一条的位置。不过本文的关注点不在于此，而在于LSO这个概念。如果需要了解LEO和HW的同学可在文末留言，后面也可以考虑出个科普LEO和HW的文章。

LSO特指LastStableOffset，在上一篇《Kafka科普系列 | 什么是LW和logStartOffset》中提及过这个概念，它具体的与Kafka的事务有关。

可能大家在使用Kafka的时候并没有太在意一个消费端的参数——isolation.level，这个参数用来配置消费者的事务隔离级别。字符串类型，有效值为“read_uncommitted”和 “read_committed”，表示消费者所消费到的位置，如果设置为“read_committed”，那么消费者就会忽略事务未提交的消息，即只能消费到 LSO(LastStableOffset)的位置，默认情况下为 “read_uncommitted”，即可以消费到 HW(High Watermark)处的位置。注意：follower副本的事务隔离级别也为“read_uncommitted”，并且不可修改。

对于Kafka中事务的讲解会在后面的系列文章《Kafka科普系列 | 什么是Kafka的事务？》中进行解答，在这里只需了解：在开启Kafka事务时，生产者发送了若干消息（比如msg1、msg2、msg3）到broker中，如果生产者没有提交事务（执行commitTransaction），那么对于isolation.level = read_committed的消费者而言是看不到这些消息的，而isolation.level = read_uncommitted则可以看到。事务中的第一条消息的位置可以标记为firstUnstableOffset（也就是msg1的位置）。

这个LSO还会影响Kafka消费滞后量（也就是Kafka Lag，很多时候也会被称之为消息堆积量）的计算。不妨我们先来看一下下面这幅图。



在图中，对每一个分区而言，它的 Lag 等于 HW – ConsumerOffset 的值，其中 ConsumerOffset 表示当前的消费位移。当然这只是针对普通的情况。如果为消息引入了事务，那么 Lag 的计算方式就会有所不同。

如果消费者客户端的 isolation.level 参数配置为“read_uncommitted”(默认)，那么 Lag的计算方式不受影响；如果这个参数配置为“read_committed”，那么就要引入 LSO 来进行计 算了。



对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset，如上图所示)，对已完成的事务而言，它的值同 HW 相同， 所以我们可以得出一个结论:LSO≤HW≤LEO。（如下图所示）



所以，对于分区中有未完成的事务，并且消费者客户端的 isolation.level 参数配置为“read_committed”的情况，它对应的 Lag 等于 LSO – ConsumerOffset 的值。

这个知识点的掌握也能够为你在面试环节中加分。大多数情况下，除非面试官是专门做Kafka相关的开发工作，一般很少有人会关注LSO这个东西，也就是说大概率情况下面试官也不知道这是什么。“什么是Kafka Lag，怎么计算Kafka Lag”类似这种问题反而会被经常问及，那么你在回答完HW - ConsumedOffset=Lag之后，如果再接着补上LSO的内容，相信面试官会为之眼前一亮。

来源：本文修改自《深入理解Kafka》一书中若干篇幅。

当当6折优惠

   记得扫描下面的二维码关注我


=================

Kafka科普系列 | 什么是LW和logStartOffset?
原创： 朱小厮  朱小厮的博客  4月1日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！

很多读者对 Kafka 中的 HW 的概念并不陌生，但是却并不知道还有一个 LW 的 概念。HW 是 High Watermark 的缩写，俗称高水位，它标识 了一个特定的消息偏移量(offset)，消费者只能拉取到这个 offset 之前的消息。



如上图所示，它代表一个日志文件，这个日志文件中有 9 条消息，第一条消息的 offset( logStartOffset)为 0，最后一条消息的 offset 为 8，offset 为 9 的消息用虚线框表示，代表下一条待写入的消息。日志文件的 HW 为 6，表示消费者只能拉取到 offset 在 0 至 5 之间的消息， 而 offset 为 6 的消息对消费者而言是不可见的。

而LW 是 Low Watermark 的缩写，俗称“低水位”，代表 AR 集合中最小的 logStartOffset 值。副本的拉取请求(FetchRequest，它有可能触发新建日志分段而旧的被清理，进而导致 logStartOffset 的增加)和删除消息请求(DeleteRecordRequest)都有可能促使 LW 的增长。

在 Kafka 的日志管理器中会有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过 broker 端参数 log.retention.check.interval.ms来配置，默认值为 300000，即 5 分钟。当前日志分段的保留策略有 3 种：基于时间的保留策略、基于日志大小的保留策略和基于日志起始偏移量的保留策略。而“基于日志起始偏移量的保留策略”正是基于 logStartOffset来实现的。

一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求(比如使用 KafkaAdminClient 的 deleteRecords()方法、使用 kafka-delete-records.sh 脚本、日志的清理和截断等操作进行修改。

基于日志起始偏移量的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量 baseOffset 是否小于等于 logStartOffset，若是，则可以删除此日志分段。如下图所示。



假设 logStartOffset 等于 25，日志分段 1 的起始偏移量为 0，日志分段 2 的起始偏移量为 11， 日志分段 3 的起始偏移量为 23，那么通过如下动作收集可删除的日志分段的文件集合 deletableSegments:

从头开始遍历每个日志分段，日志分段 1 的下一个日志分段的起始偏移量为11，小于logStartOffset 的大小，将日志分段1加入 deletableSegments。

日志分段 2 的下一个日志偏移量的起始偏移量为 23，也小于 logStartOffset 的大小， 将日志分段 2 页加入 deletableSegments。

日志分段 3 的下一个日志偏移量在 logStartOffset 的右侧，故从日志分段 3 开始的所有日志分段都不会加入 deletableSegments。

收集完可删除的日志分段的文件集合之后的删除操作同基于日志大小的保留策略和基于时间的保留策略相同，具体的可以参看之前的文章《Kafka 日志清理之Log Deletion》，这里不再赘述。

注意LogStartOffset不可以缩写为LSO，因为在Kafka中，LSO特指LogStableOffset，这个会在后面的《Kafka科普系列 | 什么是LSO?》中一一指出。

来源：本文摘录自《深入理解Kafka》一书中若干篇幅。




《为什么Kafka不支持读写分离》

《Kafka面试题全套整理 》

《深入理解Kafka》随书代码下载


==========

Kafka科普系列 | Kafka中的事务是什么样子的？
原创： 朱小厮  朱小厮的博客  4月9日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！



事务，对于大家来说可能并不陌生，比如数据库事务、分布式事务，那么Kafka中的事务是什么样子的呢？

在说Kafka的事务之前，先要说一下Kafka中幂等的实现。幂等和事务是Kafka 0.11.0.0版本引入的两个特性，以此来实现EOS（exactly once semantics，精确一次处理语义）。

幂等，简单地说就是对接口的多次调用所产生的结果和调用一次是一致的。生产者在进行重试的时候有可能会重复写入消息，而使用Kafka的幂等性功能之后就可以避免这种情况。

开启幂等性功能的方式很简单，只需要显式地将生产者客户端参数enable.idempotence设置为true即可（这个参数的默认值为false）。

Kafka是如何具体实现幂等的呢？Kafka为此引入了producer id（以下简称PID）和序列号（sequence number）这两个概念。每个新的生产者实例在初始化的时候都会被分配一个PID，这个PID对用户而言是完全透明的。

对于每个PID，消息发送到的每一个分区都有对应的序列号，这些序列号从0开始单调递增。生产者每发送一条消息就会将对应的序列号的值加1。

broker端会在内存中为每一对维护一个序列号。对于收到的每一条消息，只有当它的序列号的值（SN_new）比broker端中维护的对应的序列号的值（SN_old）大1（即SN_new = SN_old + 1）时，broker才会接收它。

如果SN_new< SN_old + 1，那么说明消息被重复写入，broker可以直接将其丢弃。如果SN_new> SN_old + 1，那么说明中间有数据尚未写入，出现了乱序，暗示可能有消息丢失，这个异常是一个严重的异常。

引入序列号来实现幂等也只是针对每一对而言的，也就是说，Kafka的幂等只能保证单个生产者会话（session）中单分区的幂等。幂等性不能跨多个分区运作，而事务可以弥补这个缺陷。

事务可以保证对多个分区写入操作的原子性。操作的原子性是指多个操作要么全部成功，要么全部失败，不存在部分成功、部分失败的可能。

为了使用事务，应用程序必须提供唯一的transactionalId，这个transactionalId通过客户端参数transactional.id来显式设置。事务要求生产者开启幂等特性，因此通过将transactional.id参数设置为非空从而开启事务特性的同时需要将enable.idempotence设置为true（如果未显式设置，则KafkaProducer默认会将它的值设置为true），如果用户显式地将enable.idempotence设置为false，则会报出ConfigException的异常。

transactionalId与PID一一对应，两者之间所不同的是transactionalId由用户显式设置，而PID是由Kafka内部分配的。

另外，为了保证新的生产者启动后具有相同transactionalId的旧生产者能够立即失效，每个生产者通过transactionalId获取PID的同时，还会获取一个单调递增的producer epoch。如果使用同一个transactionalId开启两个生产者，那么前一个开启的生产者会报错。

从生产者的角度分析，通过事务，Kafka可以保证跨生产者会话的消息幂等发送，以及跨生产者会话的事务恢复。

前者表示具有相同transactionalId的新生产者实例被创建且工作的时候，旧的且拥有相同transactionalId的生产者实例将不再工作。

后者指当某个生产者实例宕机后，新的生产者实例可以保证任何未完成的旧事务要么被提交（Commit），要么被中止（Abort），如此可以使新的生产者实例从一个正常的状态开始工作。

KafkaProducer提供了5个与事务相关的方法，详细如下：





initTransactions()方法用来初始化事务；beginTransaction()方法用来开启事务；sendOffsetsToTransaction()方法为消费者提供在事务内的位移提交的操作；commitTransaction()方法用来提交事务；abortTransaction()方法用来中止事务，类似于事务回滚。

在消费端有一个参数isolation.level，与事务有着莫大的关联，这个参数的默认值为“read_uncommitted”，意思是说消费端应用可以看到（消费到）未提交的事务，当然对于已提交的事务也是可见的。

这个参数还可以设置为“read_committed”，表示消费端应用不可以看到尚未提交的事务内的消息。

举个例子，如果生产者开启事务并向某个分区值发送3条消息msg1、msg2和msg3，在执行commitTransaction()或abortTransaction()方法前，设置为“read_committed”的消费端应用是消费不到这些消息的，不过在KafkaConsumer内部会缓存这些消息，直到生产者执行commitTransaction()方法之后它才能将这些消息推送给消费端应用。反之，如果生产者执行了abortTransaction()方法，那么KafkaConsumer会将这些缓存的消息丢弃而不推送给消费端应用。

日志文件中除了普通的消息，还有一种消息专门用来标志一个事务的结束，它就是控制消息（ControlBatch）。控制消息一共有两种类型：COMMIT和ABORT，分别用来表征事务已经成功提交或已经被成功中止。



RecordBatch中attributes字段的第6位用来标识当前消息是否是控制消息。如果是控制消息，那么这一位会置为1，否则会置为0，如上图所示。

attributes字段中的第5位用来标识当前消息是否处于事务中，如果是事务中的消息，那么这一位置为1，否则置为0。由于控制消息也处于事务中，所以attributes字段的第5位和第6位都被置为1。



KafkaConsumer可以通过这个控制消息来判断对应的事务是被提交了还是被中止了，然后结合参数isolation.level配置的隔离级别来决定是否将相应的消息返回给消费端应用，如上图所示。注意ControlBatch对消费端应用不可见。

我们在上一篇Kafka科普系列中还讲过LSO——《Kafka科普系列 | 什么是LSO》，它与Kafka的事务有着密切的联系，看着下图，你回忆起来了嘛。







有需要的话，请加私人微信



==============

Kafka科普系列 | 轻松理解Kafka中的延时操作
朱小厮  朱小厮的博客  4月16日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！

本文起源于之前去面试的一道面试题，面试题大致上是这样的：消费者去Kafka里拉去消息，但是目前Kafka中又没有新的消息可以提供，那么Kafka会如何处理？

如下图所示，两个follower副本都已经拉取到了leader副本的最新位置，此时又向leader副本发送拉取请求，而leader副本并没有新的消息写入，那么此时leader副本该如何处理呢？可以直接返回空的拉取结果给follower副本，不过在leader副本一直没有新消息写入的情况下，follower副本会一直发送拉取请求，并且总收到空的拉取结果，这样徒耗资源，显然不太合理。



这里就涉及到了Kafka延迟操作的概念。Kafka在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数fetch.min.bytes配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给follower副本。

延迟操作不只是拉取消息时的特有操作，在Kafka中有多种延时操作，比如延时数据删除、延时生产等。

对于延时生产（消息）而言，如果在使用生产者客户端发送消息的时候将acks参数设置为-1，那么就意味着需要等待ISR集合中的所有副本都确认收到消息之后才能正确地收到响应的结果，或者捕获超时异常。



假设某个分区有3个副本：leader、follower1和follower2，它们都在分区的ISR集合中。为了简化说明，这里我们不考虑ISR集合伸缩的情况。Kafka在收到客户端的生产请求后，将消息3和消息4写入leader副本的本地日志文件，如上图所示。

由于客户端设置了acks为-1，那么需要等到follower1和follower2两个副本都收到消息3和消息4后才能告知客户端正确地接收了所发送的消息。如果在一定的时间内，follower1副本或follower2副本没能够完全拉取到消息3和消息4，那么就需要返回超时异常给客户端。生产请求的超时时间由参数request.timeout.ms配置，默认值为30000，即30s。





那么这里等待消息3和消息4写入follower1副本和follower2副本，并返回相应的响应结果给客户端的动作是由谁来执行的呢？在将消息写入leader副本的本地日志文件之后，Kafka会创建一个延时的生产操作（DelayedProduce），用来处理消息正常写入所有副本或超时的情况，以返回相应的响应结果给客户端。

延时操作需要延时返回响应的结果，首先它必须有一个超时时间（delayMs），如果在这个超时时间内没有完成既定的任务，那么就需要强制完成以返回响应结果给客户端。其次，延时操作不同于定时操作，定时操作是指在特定时间之后执行的操作，而延时操作可以在所设定的超时时间之前完成，所以延时操作能够支持外部事件的触发。

就延时生产操作而言，它的外部事件是所要写入消息的某个分区的HW（高水位）发生增长。也就是说，随着follower副本不断地与leader副本进行消息同步，进而促使HW进一步增长，HW每增长一次都会检测是否能够完成此次延时生产操作，如果可以就执行以此返回响应结果给客户端；如果在超时时间内始终无法完成，则强制执行。

回顾一下文中开头的延时拉取操作，它也同样如此，也是由超时触发或外部事件触发而被执行的。超时触发很好理解，就是等到超时时间之后触发第二次读取日志文件的操作。外部事件触发就稍复杂了一些，因为拉取请求不单单由follower副本发起，也可以由消费者客户端发起，两种情况所对应的外部事件也是不同的。如果是follower副本的延时拉取，它的外部事件就是消息追加到了leader副本的本地日志文件中；如果是消费者客户端的延时拉取，它的外部事件可以简单地理解为HW的增长。

延迟操作背后还有一些更深层次的内容，比如“炼狱”、“收割机”，至于.... 嘿嘿~~

  想知道更多？扫描下面的二维码关注我



《Kafka面试题全套整理》

《Kafka科普系列|什么是LSO》

《Kafka科普系列|什么是LW和logStartOffset》

《Kafka科普系列|Kafka中的事务是什么样子的》



>>>加微信<<

===============

Kafka科普系列 | 原来Kafka中的选举有这么多？
原创： 朱小厮  朱小厮的博客  4月18日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！

面试官在考查你Kafka知识的时候很可能会故弄玄虚的问你一下：Kafka中的选举时怎么回事？除非问你具体的哪种选举，否则问这种问题的面试官多半也是对Kafka一知半解，这个时候就是“弄死”他的时候。当然如果你没有一定的知识储备，那么就是你被“弄死”的时候。

一般问这个问题，那么他肯定知道其中的一种，比如分区leader的选举。所谓分区leader的选举就是当ISR中的leader副本歇菜了，再重新选举一个的过程。对于这个问题就是你反客为主的机会，因为Kafka中的选举有多处，可不止分区leader的选举这一处，就算指明问分区leader的选举，那么也需要分4种情况具体分析。而这里面的细节是大多数人不甚明了的。

Kafka中的选举大致可以分为三大类：控制器的选举、分区leader的选举以及消费者相关的选举，这里还可以具体细分为7个小类。我们一一来过一下，本文只是简单罗列下大致的内容，至于内部具体的细节逻辑就需要靠读者自己去探索啦。虐人还是被虐就靠你的自驱力了。

控制器的选举
在Kafka集群中会有一个或多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态等工作。比如当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。再比如当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。

Kafka Controller的选举是依赖Zookeeper来实现的，在Kafka集群中哪个broker能够成功创建/controller这个临时（EPHEMERAL）节点他就可以成为Kafka Controller。

这里需要说明一下的是Kafka Controller的实现还是相当复杂的，涉及到各个方面的内容，如果你掌握了Kafka Controller，你就掌握了Kafka的“半壁江山”。篇幅所限，这里就不一一展开了，有兴趣的读者可以查阅一下《深入理解Kafka》中第6章的相关内容。

分区leader的选举
这里不说什么一致性协议（PacificA）相关的内容，只讲述具体的选举内容。

分区leader副本的选举由Kafka Controller 负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的leader副本下线，此时分区需要选举一个新的leader上线来对外提供服务）的时候都需要执行leader的选举动作。

基本思路是按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。注意这里是根据AR的顺序而不是ISR的顺序进行选举的。这个说起来比较抽象，有兴趣的读者可以手动关闭/开启某个集群中的broker来观察一下具体的变化。

还有一些情况也会发生分区leader的选举，比如当分区进行重分配（reassign）的时候也需要执行leader的选举动作。这个思路比较简单：从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。

再比如当发生优先副本（preferred replica partition leader election）的选举时，直接将优先副本设置为leader即可，AR集合中的第一个副本即为优先副本。

Kafka中有很多XX副本的称呼，如果不是很了解，可以关注本系列的下一篇《Kafka科普系列 | Kafka中到底有多少种副本？》

还有一种情况就是当某节点被优雅地关闭（也就是执行ControlledShutdown）时，位于这个节点上的leader副本都会下线，所以与此对应的分区需要执行leader的选举。这里的具体思路为：从AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中，与此同时还要确保这个副本不处于正在被关闭的节点上。

消费者相关的选举
对于这部分内容的理解，额。。如果你对消费者、消费组、消费者协调器以及组协调器不甚理解的话，那么。。。职能毛遂自荐《深入理解Kafka》一书了，嘿嘿。

组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader。如果某一时刻leader消费者由于某些原因退出了消费组，那么会重新选举一个新的leader，这个重新选举leader的过程又更“随意”了，相关代码如下：

//scala code.
private val members = new mutable.HashMap[String, MemberMetadata]
var leaderId = members.keys.head
解释一下这2行代码：在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的member_id，而value是消费者相关的元数据信息。leaderId表示leader消费者的member_id，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机无异。总体上来说，消费组的leader选举过程是很随意的。

插播：近日发现文章被盗的厉害，发文几个小时文章就在各大门户网站上出现，全都是标的原创。虽然没有能力制止，但是我发现大多不仔细看直接抄的（有连我下面的公众号二维码也抄了去的，当然也有用PS来P掉我图片水印的鸡贼操作），所以机智的我。。不如在文章中插播一下我的书，让他们盗了去也好替我宣传宣传。我是皮的很~~

到这里就结束了吗？还有分区分配策略的选举呢。

许你对此有点陌生，但是用过Kafka的同学或许对partition.assignment.strategy（取值为RangeAssignor、RoundRobinAssignor、StickyAssignor等）这个参数并不陌生。每个消费者都可以设置自己的分区分配策略，对消费组而言需要从各个消费者呈报上来的各个分配策略中选举一个彼此都“信服”的策略来进行整体上的分区分配。这个分区分配的选举并非由leader消费者决定，而是根据消费组内的各个消费者投票来决定的。至于具体的细节么。。。嘿嘿。

 想知道更多？扫描下面的二维码关注我



《Kafka面试题全套整理》

《Kafka科普系列|什么是LSO》

《Kafka科普系列|什么是LW和logStartOffset》

《Kafka科普系列|Kafka中的事务是什么样子的》

《Kafka科普系列|轻松理解Kafka中的延时操作》



===============
干趴面试官系列 | 请你简述一下Kafka中的分区分配
原创： 朱小厮  朱小厮的博客  4月26日
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人！

“请你简述一下Kafka中的分区分配”，当面试官问你这个问题的时候，你会怎么回答？其实，这道题目里面就暗藏汹涌，因为Kafka中的分区分配在多处出现，而这个问题的表述方式是在潜意识里暗示你回答一种，这样在你自认为很完美的回答完这个问题之后，面试官会冷不丁的来一句：还有呢？

当你回答完一个点的时候，面试官来一句还有呢，当你再补上一个的时候，他还是会来一句还有呢，就算你又补上第三个的时候，他还是会来一句还有呢？这个时候你会不会一脸懵逼？

今天就针对这个问题来告诉大家怎么样回答才能严丝合缝地抢得先机。

在Kafka中，分区分配是一个很重要的概念，却往往会被读者忽视，它会影响Kafka整体的性能均衡。当遇到“分区分配”这个字眼的时候，一定要记住有三处地方，分别是生产者发送消息、消费者消费消息和创建主题。虽然这三处的对应操作都可以被称之为“分区分配”，但是其实质上所包含的内容却并不相同。

在面对开篇的问题的时候，不如一下就进行总结性的陈词，说有三处，第一、第二、第三balabala，当真的让你讲完三处的时候，时间也就差不多了。。聪明的面试官看到你一上来就做了一个规划总结，那他顶多也就让你说说你最熟悉的一种，其实说不定内心已经确认你是对的人。

下面针对这三处做个讲解。不过本文旨在罗列相关知识点，进行相关性的科普描述，让读者可以追根溯源，但并不陈述具体细节，因为细节很多，篇幅有限，如有需要请详参老朽的《深入理解Kafka》。

生产者的分区分配
对于用户而言，当调用send方法发送消息之后，消息就自然而然的发送到了broker中。其实在这一过程中，有可能还要经过拦截器、序列化器和分区器（Partitioner）的一系列作用之后才能被真正地发往broker。

producer.send(record);
消息在发往broker之前是需要确定它所发往的分区的，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号。如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。

Kafka中提供的默认分区器是DefaultPartitioner，它实现了Partitioner接口（用户可以实现这个接口来自定义分区器），其中的partition方法就是用来实现具体的分区分配逻辑：

public int partition(String topic, Object key, byte[] keyBytes,
                     Object value, byte[] valueBytes, Cluster cluster);
默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。

注意：如果key不为null，那么计算得到的分区号会是所有分区中的任意一个；如果key为null并且有可用分区，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。

消费者的分区分配
在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。



如图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。

对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为RangeAssignor、RoundRobinAssignor以及StickyAssignor，其中RangeAssignor为默认的分区分配策略，至于这三种策略具体代表什么含义，可以去查阅相关资料，比如《深入理解Kafka》，嘿嘿。当然也可以通过实现ParitionAssignor接口来自定义分区分配策略。

在消费组中如果有多个消费者，那么这些消费者又可能会采用不同的分配策略，那么最后怎么“拍板”使用哪一种具体的分配策略呢？

对于这里，我想留一道思考题给大家：在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费，那么这个规则可以被打破么？如果可以，怎么打破？打破的收益又是什么？

broker端的分区分配
生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区，而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。

在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。

想知道更多？扫描下面的二维码关注我



《Kafka面试题全套整理》

《Kafka科普系列|什么是LSO》

《Kafka科普系列|什么是LW和logStartOffset》

《Kafka科普系列|Kafka中的事务是什么样子的》

《Kafka科普系列|轻松理解Kafka中的延时操作》

《Kafka科普系列|原来Kafka中的选举有这么多》

==================

聊一聊Kafka分区的隐藏属性——二次归类
原创： 朱小厮  朱小厮的博客  5天前
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人



在使用Kafka的过程中，分区是一个不可忽视的概念。很多时候你会带着这样的疑问：Kafka的分区该怎么划分？按什么划分？分多少个？撰稿之时，我在《深入理解Kafka》一书中搜索了一下“分区”这个词，结果发现出现的频率至少有4位数之多。Kafka中分区的概念涉猎很多，比如：分区分配、分区重分配、失效分区等。在本公众号里也发表过几篇文章来讲述Kafka分区中的某些知识点：

如何为Kafka挑选合适的分区数？

Kafka主题中的分区数越多吞吐量就越高？

为什么Kafka中的分区数只能增加不能减少？

本文并不延续讲解这些内容，而是剑走偏锋，聊一聊Kafka分区被大家所忽视的一个隐藏属性——消息的归类。

在Kafka中主题和分区是两个非常重要的概念。Kafka中的消息向来以“主题”来进行归类（划分），一个主题可以包含若干个分区，一个分区只属于单个主题。同一主题下的不同分区所包含的消息不同的（如果需要数据冗余，那么涉及到的是Kafka的多副本机制，这里不展开论述）。对于单个分区而言，消息是顺序追加的，也因此为Kafka提供了分区内的顺序特性，这一点在RabbitMQ不是太好保证（其实RabbitMQ也不是不可以，就是有点麻烦）。

使用分区最主要的作用还是提高可扩展性。如果一个主题只有一个分区，那么所有的数据读写都经过这一个分区的话势必会造成性能瓶颈。当然，如果规定主题只能有一个分区的话，那么分区这个概念就可以去除掉了。如果主题有多个分区，那么可以将这种读写的压力分散开来。主题中的多个分区不一定要在同一个broker上，完全可以分布在多个broker上，特殊情况下还可以一个分区一个broker。

“消息的归类”并不是不是主题所独有的特性，其实分区也可以，分区可以看做是消息的二次归类，让分区变得有意义。

举个例子，我们创建了一个主题用来存储人员信息，这个人员信息就是对数据的一种归类。由于人员众多，我们想着是要用多个分区来分摊一下读写压力，那么我们可以依据什么来划分呢？

不妨试试第一种，按照生肖，这样就可以创建12个分区，分区编号从0开始分别代表鼠、牛、虎、兔、牛等等，这样的分区规则就是对人员信息的二次归类。第二种，我们估算12个分区的压力也有点顶不住，那么我们可以按照姓名拼音的首字母来划分，这也是二次归类。

在实际实践时可以将消息（ProducerRecord<生肖，人员信息>）对应的key设置为与其对应分区的“归类类别”即可。比如，按生肖分，那么“属牛的肥朝”可以这一条消息（人员信息）可以表示为：key=“牛”，value=“姓名：肥朝，特长：帅”。

聪明的同学还会接着想到用姓氏来归类。。也不是不可行，中国百家姓（其实何止）。但是要考虑到一种情况就是数据倾斜，“赵钱孙李”所对应的分区肯定比“令狐”、“独孤”、“上官”之类的分区所承担的数据负载要多很多，这样严重的数据倾斜会导致服务整体效能的降低。“赵钱孙李”可以视为热点数据，而“令狐”、“独孤”、“上官”可以视为冷数据，我们可以这样划分，参考下图：



对于大姓来说可以独占一个分区，而对于中等姓氏来说可以合用一个分区，而对于生僻姓氏来说可以共用一个分区。

在某些时候，按照字面的类别划分也很难避免数据倾斜。沿用上面的例子，宋朝时期，姓赵的人实在太多，划分到一个分区还是会有严重的数据倾斜，那么就需要对这一姓氏做多一些处理，多化几个分区给它。

举这些例子是为了说明：如果要让分区获得“二次归类”的特性，就需要做好“二次归类”的准备，也就是需要应对数据热点问题。为了尽可能的数据分布均匀而弱化热点问题应该根据归类的特性做好合理的规划。

在Kafka中创建消息的时候，如果不指定分区，那么就会根据消息的key来进行计算。

如果key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。前者根据key进行计算，潜在的使用了“二次归类”这个特性，后者采用轮询的方式，潜在的丢弃了“二次归类”的特性。

当然，我们还可以自定义Kafka中的分区器来实现随机的分类，这个实现很简单，从[0,分区数）中挑选一个整数而已，如此也同样会丢弃“二次归类”的特性。

关于分区的这个属性，你还有什么需要补充的吗？不妨在留言去留言来一起探讨。

想知道更多？扫描下面的二维码关注我


==============

聊一聊Kafka分区的隐藏属性——二次归类
原创： 朱小厮  朱小厮的博客  5天前
点击上方“朱小厮的博客”，选择“设为星标”

做积极的人，而不是积极废人



在使用Kafka的过程中，分区是一个不可忽视的概念。很多时候你会带着这样的疑问：Kafka的分区该怎么划分？按什么划分？分多少个？撰稿之时，我在《深入理解Kafka》一书中搜索了一下“分区”这个词，结果发现出现的频率至少有4位数之多。Kafka中分区的概念涉猎很多，比如：分区分配、分区重分配、失效分区等。在本公众号里也发表过几篇文章来讲述Kafka分区中的某些知识点：

如何为Kafka挑选合适的分区数？

Kafka主题中的分区数越多吞吐量就越高？

为什么Kafka中的分区数只能增加不能减少？

本文并不延续讲解这些内容，而是剑走偏锋，聊一聊Kafka分区被大家所忽视的一个隐藏属性——消息的归类。

在Kafka中主题和分区是两个非常重要的概念。Kafka中的消息向来以“主题”来进行归类（划分），一个主题可以包含若干个分区，一个分区只属于单个主题。同一主题下的不同分区所包含的消息不同的（如果需要数据冗余，那么涉及到的是Kafka的多副本机制，这里不展开论述）。对于单个分区而言，消息是顺序追加的，也因此为Kafka提供了分区内的顺序特性，这一点在RabbitMQ不是太好保证（其实RabbitMQ也不是不可以，就是有点麻烦）。

使用分区最主要的作用还是提高可扩展性。如果一个主题只有一个分区，那么所有的数据读写都经过这一个分区的话势必会造成性能瓶颈。当然，如果规定主题只能有一个分区的话，那么分区这个概念就可以去除掉了。如果主题有多个分区，那么可以将这种读写的压力分散开来。主题中的多个分区不一定要在同一个broker上，完全可以分布在多个broker上，特殊情况下还可以一个分区一个broker。

“消息的归类”并不是不是主题所独有的特性，其实分区也可以，分区可以看做是消息的二次归类，让分区变得有意义。

举个例子，我们创建了一个主题用来存储人员信息，这个人员信息就是对数据的一种归类。由于人员众多，我们想着是要用多个分区来分摊一下读写压力，那么我们可以依据什么来划分呢？

不妨试试第一种，按照生肖，这样就可以创建12个分区，分区编号从0开始分别代表鼠、牛、虎、兔、牛等等，这样的分区规则就是对人员信息的二次归类。第二种，我们估算12个分区的压力也有点顶不住，那么我们可以按照姓名拼音的首字母来划分，这也是二次归类。

在实际实践时可以将消息（ProducerRecord<生肖，人员信息>）对应的key设置为与其对应分区的“归类类别”即可。比如，按生肖分，那么“属牛的肥朝”可以这一条消息（人员信息）可以表示为：key=“牛”，value=“姓名：肥朝，特长：帅”。

聪明的同学还会接着想到用姓氏来归类。。也不是不可行，中国百家姓（其实何止）。但是要考虑到一种情况就是数据倾斜，“赵钱孙李”所对应的分区肯定比“令狐”、“独孤”、“上官”之类的分区所承担的数据负载要多很多，这样严重的数据倾斜会导致服务整体效能的降低。“赵钱孙李”可以视为热点数据，而“令狐”、“独孤”、“上官”可以视为冷数据，我们可以这样划分，参考下图：



对于大姓来说可以独占一个分区，而对于中等姓氏来说可以合用一个分区，而对于生僻姓氏来说可以共用一个分区。

在某些时候，按照字面的类别划分也很难避免数据倾斜。沿用上面的例子，宋朝时期，姓赵的人实在太多，划分到一个分区还是会有严重的数据倾斜，那么就需要对这一姓氏做多一些处理，多化几个分区给它。

举这些例子是为了说明：如果要让分区获得“二次归类”的特性，就需要做好“二次归类”的准备，也就是需要应对数据热点问题。为了尽可能的数据分布均匀而弱化热点问题应该根据归类的特性做好合理的规划。

在Kafka中创建消息的时候，如果不指定分区，那么就会根据消息的key来进行计算。

如果key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。前者根据key进行计算，潜在的使用了“二次归类”这个特性，后者采用轮询的方式，潜在的丢弃了“二次归类”的特性。

当然，我们还可以自定义Kafka中的分区器来实现随机的分类，这个实现很简单，从[0,分区数）中挑选一个整数而已，如此也同样会丢弃“二次归类”的特性。

关于分区的这个属性，你还有什么需要补充的吗？不妨在留言去留言来一起探讨。

想知道更多？扫描下面的二维码关注我



============
历史性难题——如何为Kafka挑选合适的分区数？
原创： 朱小厮  朱小厮的博客  2018-10-09
点击上方“朱小厮的博客”，选择“置顶公众号”

技术文章第一时间送达！

如何为Kafka挑选合适的分区数？很多人都为这个问题伤过脑筋。

从吞吐量方面考虑，增加合适的分区数可以很大程度上提升整体吞吐量，但是超过对应的阈值之后吞吐量不升反降，详细可以参考《Kafka主题中的分区数越多吞吐量就越高？》。如果应用对吞吐量有着一定程度上的要求，建议在投入生产环境之前对同款硬件资源做一个完备的吞吐量相关的测试，以找到合适的分区数阈值期间。

在创建完主题之后，虽然我们还是能够增加分区的个数，但是基于key计算的主题需要严谨对待。当生产者向Kafka中写入基于key的消息时，Kafka通过消息的key来计算出消息将要写入到哪个具体的分区中，这样具有相同key的数据可以写入到同一个分区中。Kafka的这一功能对于一部分应用是即为重要的，比如日志压缩，详细可以参考《Kafka日志清理之Log Compaction》；再比如对于同一个key的所有消息，消费者需要按消息的顺序进行有序的消费，如果分区的数量发生变化，那么有序性就得不到保证。在创建主题时，最好能够确定好分区数，这样也可以省去后期增加所带来的多余操作。尤其对于与key高关联的应用，在创建主题时可以适当地多创建一些分区，以满足未来的需求。通常情况下，可以根据未来2年内的目标吞吐量来设定分区数。当然如果应用与key弱关联，并且也具备便捷的增加分区数的操作接口，那么也可以不用考虑那么长远的目标。

有些应用场景会要求主题中的消息都能保证顺序性，这种情况下在创建主题时可以设定分区数为1，这样通过分区有序性的这一特性来达到主题有序性的目的。

当然分区数也不能一昧地增加，参考《如何把一个运行完好的Kafka搞崩？》的内容，分区数会占用文件描述符，而一个进程所能支配的文件描述符是有限的，这个也是我们通常意义上所说的文件句柄的开销。虽然我们可以通过修改配置来增加可用文件描述符的个数，但是凡事总有一个上限，在选择合适的分区数之前，最好再考量一下当前Kafka进程中已经使用的文件描述符的个数。

分区数的多少还会影响系统的可用性。Kafka通过多副本机制来实现集群的高可用和高可靠，每个分区都会有一至多个副本，每个副本分别存在于不同的broker节点上，并且只有leader副本对外提供服务。在Kafka集群的内部，所有的副本都采用自动化的方式进行管理，并确保所有的副本中的数据都能保持一定程度上的同步。当broker发生故障时，对于leader副本所宿主的broker节点上的所有分区将会暂时处于不可用的状态，此时Kafka会自动的在其他的follower副本中选举出新的leader用于接收外部客户端的请求，整个过程由Kafka控制器负责完成。分区进行leader角色切换的过程中会变得不可用，不过对于单个分区来说这个过程非常的短暂，对于用户而言可以忽略不计。但是如果集群中的某个broker节点宕机，那么就会有大量的分区需要同时进行leader角色切换，这个切换的过程将会耗费一笔可观的时间，并且在这个时间窗口内这些分区也会变得不可用。

假如，一个3节点的Kafka集群中存在3000个分区，每个分区拥有3个数据副本。当其中一个broker节点宕机时，所有1000个分区同时变得不可用。假设每一个分区恢复时间是5ms，那么1000个分区的恢复时间将会花费5秒钟。因此，在这种情况下，用户将会观察到系统存在5秒钟的不可用时间窗口。可以适当地增加一些broker节点来减少单broker节点所负荷的分区，进而降低单broker节点故障引起的短期服务不可用的影响。

如果宕机的broker节点恰好又是Kafka集群的控制器时，在控制器被重新选举到新的broker节点之前这些分区leader角色切换的过程是不会开始进行的。虽说控制器的恢复（重新选举新的控制器）也是自动进行的，整体上不会有太大的问题，但是新的控制器需要加载集群中所有的元数据信息，其中就包括了所有的分区信息，分区数越多加载的耗时就会越长，进而拖慢了控制器的恢复进度，最终也就拖慢了分区服务的恢复进度。

分区数越多也会让Kafka的正常启动和关闭的耗时变得越长，与此同时，主题的分区数越多不仅会增加日志清理的耗时，而且在被删除时也会耗费更多的时间。对于旧版的生产者和消费者客户端而言，分区数越多也会增加它们的开销，不过这一点在新版的生产者和消费者客户端中有效地得到了抑制。

如何选择合适的分区数？从某种意思来说，考验的是决策者的实战经验，更透彻地来说，是对Kafka本身、业务应用、硬件资源、环境配置等多方面的考量而做出的抉择。在设定完分区数，或者更确切的说是创建完主题之后，还要对其追踪、监控、调优以求更改更好的利用它。读者看到本文的内容之前或许没有对分区数有太大的困扰，可能看完之后反而困惑了起来，其实大可不必太过惊慌，一般情况下，根据预估的吞吐量以及是否与key相关的规则来设定分区数即可，后期可以通过增加分区数、增加broker或者分区重分配等手段来进行改进。如果一定要给一个准则的话，笔者给的一个建议是分区数设定为集群中broker的倍数，即假定集群中有3个broker节点，可以设定分区数为3、6、9等，至于倍数的选定可以参考预估的吞吐量。不过，如果集群中的broker节点数有很多，比如大几十或者上百、上千，这种准则也不太适用，在选定分区数时进一步的可以引入基架等参考因素。

对于这个问题，网上也有很多的资料，笔者也看过，由于版本更迭，现在只能赞同其中的部分内容。本文也是笔者对分区数抉择的小小认知，如果你对此问题有相同或者相反的意见，欢迎在留言区探讨。





往
期
精
彩
《Kafka日志清理之Log Compaction》

《如何把一个运行完好的Kafka搞崩》

《为什么Kafka中的分区数只能增加不能减少？》

《Kafka主题中的分区数越多吞吐量就越高？》


===========
Kafka主题中的分区数越多吞吐量就越高？BULLSHIT!!!
原创： 朱小厮  朱小厮的博客  2018-09-28
点击上方“朱小厮的博客”，选择“置顶公众号”

技术文章第一时间送达！

分区是Kafka中最小的并行操作单元，对于生产者而言，对于每一个分区的数据写入是完全可以并行化的；对于消费者而言，Kafka只允许单个分区中的消息被一个消费者线程所消费，一个消费组的消费并行度完全依赖于所消费的分区数。如此看来，如果一个主题中的分区数越多，理论上所能达到的吞吐量就越大，那么事实真的如预想的一样么？

不妨我们使用kafka-producer-perf-test.sh脚本和kafka-consumer-perf-test.sh脚本这两个性能测试工具来实际地测试一下。首先我们分别创建分区数为1、20、50、100、200、500、1000的主题，对应的主题名称分别为topic-1、topic-20、topic-50、topic-100、topic-200、topic-500、topic-1000，所有主题的副本因子都设置为1。

消息中间件的性能一般是指吞吐量。抛开硬件资源的影响，消息写入的吞吐量还会受到消息大小、消息压缩方式、消息发送方式（同步/异步）、消息确认类型（acks）、副本因子等参数的影响，消息消费的吞吐量还会受到应用逻辑处理速度的影响。本次案例中暂不考虑这些因素的影响，所有的测试除了主题的分区数不同之外，其余的因素都保持相同。

本次案例中所使用的测试环境为一个由3台普通云主机组成的3节点的Kafka集群，每台云主机内存8G、磁盘40GB、4核CPU主频为2600MHz。JVM版本为1.8.0_112，Linux系统版本为2.6.32-504.23.4.el6.x86_64。

使用kafka-producer-perf-test.sh脚本分别往这些主题中发送100万条消息体大小为1KB的消息，相对应的测试命令如下：

bin/kafka-producer-perf-test.sh --topic topic-xxx
--num-records 1000000 --record-size 1024
--throughput 100000000 --producer-props
bootstrap.servers=localhost:9092 acks=1
相对应的测试结果如下图所示。对于不同的硬件环境，甚至不同批次的测试得到的测试结果也不会完全相同，但是总体趋势还是会保持和图中的一样。



在上图中，我们可以看到分区数为1时吞吐量最低，随着分区数的增长，相应的吞吐量也跟着上涨。一旦分区数超过了某个阈值之后整体的吞吐量是不升反降的，也就是说并不是分区数越多吞吐量也就越大。这里的分区数临界阈值针对不同的测试环境也会表现出不同的结果，实际应用中可以通过类似的测试案例来找到一个合理的临界值区间。

上面针对的是消息生产者的测试，对于消息消费者而言同样也有吞吐量方面的考量。使用kafka-consumer-perf-test.sh脚本分别消费这些主题中的100万条消息，相对应的测试命令如下：

bin/kafka-consumer-perf-test.sh --topic topic-xxx
--messages 1000000 --broker-list localhost:9092
消费者性能测试的结果如下图所示。与生产者性能测试相同的是，对于不同的测试环境或者不同的测试批次所得到的测试结果也不尽相同，但总体趋势还是会保持和图中的一样。



在上图中，开始随着分区数的增加相应的吞吐量也会有多增长。一旦分区数超过了某个阈值之后整体的吞吐量也同样是不升反降的，同样说明了分区数越多并不会使得吞吐量一直增长。

分区数越多吞吐量也就越高？很多资料都认可这一观点，但实际上很多事情都会有一个临界值，当超过这个临界值之后，很多原本符合既定逻辑的走向又会变得不同。读者需要对此有个清晰的认知，懂得去伪求真，而实地测试验证不失为一座通向真知的桥梁。

>>>加微信，进入消息生态圈群聊<<



友情推荐阿里大牛的公众号——【服务端思维】：带你一起聊聊服务端开发技术，项目架构与实战经验。同时，拥有众多一线互联网大牛的「后端圈」组织，期待你的加入，一群同频者，一起成长，一起精进，打破认知的局限性。


==============

为什么Kafka中的分区数只能增加不能减少？
原创： 朱小厮  朱小厮的博客  2018-09-21
当一个主题被创建之后，依然允许我们对其做一定的修改，比如修改分区个数、修改配置等，这个修改的功能就是由kafka-topics.sh脚本中的alter指令所提供。我们首先来看如何增加主题的分区数。以前面的主题topic-config为例，当前分区数为1，修改为3，示例如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper
localhost:2181/kafka --alter --topic topic-config --partitions 3
WARNING: If partitions are increased for a topic that has a key,
the partition logic or ordering of the messages will be affected
Adding partitions succeeded!

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper
localhost:2181/kafka --describe --topic topic-config
Topic:topic-config    PartitionCount:3  ReplicationFactor:1   Configs:
Topic: topic-config    Partition: 0 Leader: 2    Replicas: 2  Isr: 2
Topic: topic-config    Partition: 1 Leader: 0    Replicas: 0  Isr: 0
Topic: topic-config    Partition: 2 Leader: 1    Replicas: 1  Isr: 1
注意上面提示的告警信息：当主题中的消息包含有key时（即key不为null），根据key来计算分区的行为就会有所影响。当topic-config的分区数为1时，不管消息的key为何值，消息都会发往这一个分区中；当分区数增加到3时，那么就会根据消息的key来计算分区号，原本发往分区0的消息现在有可能会发往分区1或者分区2中。如此还会影响既定消息的顺序，所以在增加分区数时一定要三思而后行。对于基于key计算的主题而言，建议在一开始就设置好分区数量，避免以后对其进行调整。

目前Kafka只支持增加分区数而不支持减少分区数。比如我们再将主题topic-config的分区数修改为1，就会报出InvalidPartitionException的异常，示例如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper
localhost:2181/kafka --alter --topic topic-config --partitions 1
WARNING: If partitions are increased for a topic that has a key,
the partition logic or ordering of the messages will be affected
Error while executing topic command : The number of partitions
for a topic can only be increased. Topic topic-config currently
has 3 partitions, 1 would not be an increase.
[2018-09-10 19:28:40,031] ERROR
org.apache.kafka.common.errors.InvalidPartitionsException:
The number of partitions for a topic can only be increased.
Topic topic-config currently has 3 partitions, 1 would not
be an increase. (kafka.admin.TopicCommand$)
为什么不支持减少分区？按照Kafka现有的代码逻辑而言，此功能完全可以实现，不过也会使得代码的复杂度急剧增大。实现此功能需要考虑的因素很多，比如删除掉的分区中的消息该作何处理？如果随着分区一起消失则消息的可靠性得不到保障；如果需要保留则又需要考虑如何保留。直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于Spark、Flink这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入到现有的分区中，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时，顺序性问题、事务性问题、以及分区和副本的状态机切换问题都是不得不面对的。反观这个功能的收益点却是很低，如果真的需要实现此类的功能，完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去即可。

虽然分区数不可以减少，但是分区对应的副本数是可以减少的，这个其实很好理解，你关闭一个副本时就相当于副本数减少了。不过正规的做法是使用kafka-reassign-partition.sh脚本来实现，具体用法可以自行搜索。

=========================






