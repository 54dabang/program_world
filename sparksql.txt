第 1 章 Spark SQL 背景
1．1 大数据与 Spark 系统
1．2 关系模型与 SQL 语言
1．3 Spark SQL 发展历程
1．4 本章小结
第 2 章 Spark 基础知识介绍
2．1 RDD 编程模型
2．2 DataFrame 与 Dataset
2．3 本章小结
第 3 章 Spark SQL 执行全过程概述
3．1 从 SQL 到 RDD：一个简单的案例
3．2 重要概念
3．2．1 InternalRow 体系
3．2．2 TreeNode 体系
3．2．3 Expression 体系
3．3 内部数据类型系统
3．4 本章小结
第 4 章 Spark SQL 编译器 Parser
4．1 DSL 工具之 ANTLR 简介
4．1．1 基于 ANTLR 4 的计算器
4．1．2 访问者模式
4．2 SparkSqlParser 之 AstBuilder
4．3 常见 SQL 生成的抽象语法树概览
4．4 本章小结
第 5 章 Spark SQL 逻辑计划（LogicalPlan）
5．1 Spark SQL 逻辑计划概述
5．2 LogicalPlan 简介
5．2．1 QueryPlan 概述
5．2．2 LogicalPlan 基本操作与分类
5．2．3 LeafNode 类型的 LogicalPlan
5．2．4 UnaryNode 类型的 LogicalPlan
5．2．5 BinaryNode 类型的 LogicalPlan
5．2．6 其他类型的 LogicalPlan
5．3 AstBuilder 机制：Unresolved LogicalPlan 生成
5．4 Analyzer 机制：Analyzed LogicalPlan 生成
5．4．1 Catalog 体系分析
5．4．2 Rule 体系
5．4．3 Analyzed LogicalPlan 生成过程
5．5 Spark SQL 优化器 Optimizer
5．5．1 Optimizer 概述
5．5．2 Optimizer 规则体系
5．5．3 Optimized LogicalPlan 的生成过程
5．6 本章小结
第 6 章 Spark SQL 物理计划（PhysicalPlan）
6．1 Spark SQL 物理计划概述
6．2 SparkPlan 简介
6．2．1 LeafExecNode 类型
6．2．2 UnaryExecNode 类型
6．2．3 BinaryExecNode 类型
6．2．4 其他类型的 SparkPlan
6．3 Metadata 与 Metrics 体系
6．4 Partitioning 与 Ordering 体系
6．4．1 Distribution 与 Partitioning 的概念
6．4．2 SparkPlan 的常用分区排序操作
6．5 SparkPlan 生成
6．5．1 物理计划 Strategy 体系
6．5．2 常见 Strategy 分析
6．6 执行前的准备
6．6．1 PlanSubqueries 规则
6．6．2 EnsureRequirements 规则
6．7 本章小结
第 7 章 Spark SQL 之 Aggregation 实现
7．1 Aggregation 执行概述
7．1．1 文法定义
7．1．2 聚合语句 Unresolved LogicalPlan 生成
7．1．3 从逻辑算子树到物理算子树
7．2 聚合函数（AggregateFunction）
7．2．1 聚合缓冲区与聚合模式（AggregateMode）
7．2．2 DeclarativeAggregate 聚合函数
7．2．3 ImperativeAggregate 聚合函数
7．2．4 TypedImperativeAggregate 聚合函数
7．3 聚合执行
7．3．1 执行框架 AggregationIterator
7．3．2 基于排序的聚合算子 SortAggregateExec
7．3．3 基于 Hash 的聚合算子 HashAggregateExec
7．4 窗口（Window）函数
7．4．1 窗口函数定义与简介
7．4．2 窗口函数相关表达式
7．4．3 窗口函数的逻辑计划阶段与物理计划阶段
7．4．4 窗口函数的执行
7．5 多维分析
7．5．1 OLAP 多维分析背景
7．5．2 Spark SQL 多维查询
7．5．3 多维分析 LogicalPlan 阶段
7．5．4 多维分析 PhysicalPlan 与执行
7．6 本章小结
第 8 章 Spark SQL 之 Join 实现
8．1 Join 查询概述
8．2 文法定义与抽象语法树
8．3 Join 查询逻辑计划
8．3．1 从 AST 到 Unresolved LogicalPlan
8．3．2 从 Unresolve LogicalPlan 到 Analyzed LogicalPlan
8．3．3 从 Analyzed LogicalPlan 到 Optimized LogicalPlan
8．4 Join 查询物理计划
8．4．1 Join 物理计划的生成
8．4．2 Join 物理计划的选取
8．5 Join 查询执行
8．5．1 Join 执行基本框架
8．5．2 BroadcastJoinExec 执行机制
8．5．3 ShuffledHashJoinExec 执行机制
8．5．4 SortMergeJoinExec 执行机制
8．6 本章小结
第 9 章 Tungsten 技术实现
9．1 内存管理与二进制处理
9．1．1 Spark 内存管理基础
9．1．2 Tungsten 内存管理优化基础
9．1．3 Tungsten 内存优化应用
9．2 缓存敏感计算（Cache-aware computation）
9．3 动态代码生成（Code generation）
9．3．1 漫谈代码生成
9．3．2 Janino 编译器实践
9．3．3 基本（表达式）代码生成
9．3．4 全阶段代码生成（WholeStageCodegen）
9．4 本章小结
第 10 章 Spark SQL 连接 Hive
10．1 Spark SQL 连接 Hive 概述
10．2 Hive 相关的规则和策略
10．2．1 HiveSessionCatalog 体系
10．2．2 Analyzer 之 Hive-Specific 分析规则
10．2．3 SparkPlanner 之 Hive-Specific 转换策略
10．2．4 Hive 相关的任务执行
10．3 Spark SQL 与 Hive 数据类型
10．3．1 Hive 数据类型与 SerDe 框架
10．3．2 DataTypeToInspector 与 Data Wrapping
10．3．3 InspectorToDataType 与 Data Unwrapping
10．4 Hive UDF 管理机制
10．5 Spark Thrift Server 实现
10．5．1 Service 体系
10．5．2 Operation 与 OperationManager
10．5．3 Session 与 SessionManager
10．5．4 Authentication 安全认证管理
10．5．5 Spark Thrift Server 执行流程
10．6 本章小结
第 11 章 Spark SQL 开发与实践
11．1 腾讯大数据平台（TDW）简介
11．2 腾讯大数据平台 SQL 引擎（TDW-SQL-Engine）
11．2．1 SQL-Engine 背景与演化历程
11．2．2 SQL-Engine 整体架构
11．3 TDW-Spark SQL 开发与优化
11．3．1 业务运行支撑框架
11．3．2 新功能开发案例
11．3．3 性能优化开发案例
11．4 业务实践经验与教训
11．4．1 Spark SQL 集群管理的经验
11．4．2 Spark SQL 业务层面调优
11．4．3 SQL 写法的“陷阱”

第1章 初识Spark SQL
1.1 Spark SQL的前世今生
1.2 Spark SQL能做什么
第2章 Spark安装、编程环境搭建以及打包提交
2.1 Spark的简易安装
2.2 准备编写Spark应用程序的IDEA环境
2.3 将编写好的Spark应用程序打包成jar提交到Spark上
第二部分 基础篇
第3章 Spark上的RDD编程
3.1 RDD基础
3.2 RDD简单实例—wordcount
3.3 创建RDD
3.4 RDD操作
3.5 向Spark传递函数
3.6 常见的转化操作和行动操作
3.7 深入理解RDD
3.8 RDD缓存、持久化
3.9 RDD checkpoint容错机制
第4章 Spark SQL编程入门
4.1 Spark SQL概述
4.2 Spark SQL编程入门示例
第5章 Spark SQL的DataFrame操作大全
5.1 由JSON文件生成所需的DataFrame对象
5.2 DataFrame上的行动操作
5.3 DataFrame上的转化操作
第6章 Spark SQL支持的多种数据源
6.1 概述
6.2 典型结构化数据源
第三部分 实战篇
第7章 Spark SQL工程实战之基于WiFi探针的商业大数据分析技术
7.1 功能需求
7.2 系统架构
7.3 功能设计
7.4 数据库结构
7.5 本章小结
第8章 第一个Spark SQL应用程序
8.1 完全分布式环境搭建
8.2 数据清洗
8.3 数据处理流程
8.4 Spark程序远程调试
8.5 Spark的Web界面
8.6 本章小结
第四部分 优化篇
第9章 让Spark程序再快一点
9.1 Spark执行流程
9.2 Spark内存简介
9.3 Spark的一些概念
9.4 Spark编程四大守则
9.5 Spark调优七式
9.6 解决数据倾斜问题
9.7 Spark执行引擎Tungsten简介
9.8 Spark SQL解析引擎Catalyst简介

1.1Spark SQL概述
1.1.1Spark SQL与DataFrame
1.1.2DataFrame与RDD的差异
1.1.3Spark SQL的发展历程
1.2从零起步掌握Hive
1.2.1Hive的本质是什么
1.2.2Hive安装和配置
1.2.3使用Hive分析搜索数据
1.3Spark SQL on Hive安装与配置
1.3.1安装Spark SQL
1.3.2安装MySQL
1.3.3启动Hive Metastore
1.4Spark SQL初试
1.4.1通过spark-shell来使用Spark SQL
1.4.2Spark SQL的命令终端
1.4.3Spark的Web UI
1.5本章小结
第2章DataFrame原理与常用操作
2.1DataFrame编程模型
2.2DataFrame基本操作实战
2.2.1数据准备
2.2.2启动交互式界面
2.2.3数据处理与分析
2.3通过RDD来构建DataFrame
2.4缓存表（列式存储）
2.5DataFrame API应用示例
2.6本章小结
第3章Spark SQL 操作多种数据源
3.1通用的加载/保存功能
3.1.1Spark SQL加载数据
3.1.2Spark SQL保存数据
3.1.3综合案例——电商热销商品排名
3.2Spark SQL操作Hive示例
3.3Spark SQL操作JSON数据集示例
3.4Spark SQL操作HBase示例
3.5Spark SQL操作MySQL示例
3.5.1安装并启动MySQL
3.5.2准备数据表
3.5.3操作MySQL表
3.6Spark SQL操作MongoDB示例
3.6.1安装配置MongoDB
3.6.2启动MongoDB
3.6.3准备数据
3.6.4Spark SQL操作MongoDB
3.7本章小结
第4章Parquet列式存储
4.1Parquet概述
4.1.1Parquet的基本概念
4.1.2Parquet数据列式存储格式应用举例
4.2Parquet的Block配置及数据分片
4.2.1Parquet的Block的配置
4.2.2Parquet 内部的数据分片
4.3Parquet序列化
4.3.1Spark实施序列化的目的
4.3.2Parquet两种序列化方式
4.4本章小结
第5章Spark SQL内置函数与窗口函数
5.1Spark SQL内置函数
5.1.1Spark SQL内置函数概述
5.1.2Spark SQL内置函数应用实例
5.2Spark SQL窗口函数
5.2.1Spark SQL窗口函数概述
5.2.2Spark SQL窗口函数分数查询统计案例
5.2.3Spark SQL窗口函数NBA常规赛数据统计案例
5.3本章小结
第6章Spark SQL UDF与UDAF
6.1UDF概述
6.2UDF示例
6.2.1Hobby_count函数
6.2.2Combine函数
6.2.3Str2Int函数
6.2.4Wsternstate函数
6.2.5ManyCustomers函数
6.2.6StateRegion函数
6.2.7DiscountRatio函数
6.2.8MakeStruct函数
6.2.9MyDateFilter函数
6.2.10MakeDT函数
6.3UDAF概述
6.4UDAF示例
6.4.1ScalaAggregateFunction函数
6.4.2GeometricMean函数
6.4.3CustomMean函数
6.4.4BelowThreshold函数
6.4.5YearCompare函数
6.4.6WordCount函数
6.5本章小结
第7章Thrift Server
7.1Thrift概述
7.1.1Thrift的基本概念
7.1.2Thrift的工作机制
7.1.3Thrift的运行机制
7.1.4一个简单的Thrift 实例
7.2Thrift Server的启动过程
7.2.1Thrift Sever启动详解
7.2.2HiveThriftServer2类的解析
7.3Beeline操作
7.3.1Beeline连接方式
7.3.2在Beeline中进行SQL查询操作
7.3.3通过Web控制台查看用户进行的操作
7.4Thrift Server应用示例
7.4.1示例源代码
7.4.2关键代码行解析
7.4.3测试运行
7.4.4运行结果解析
7.4.5Spark Web控制台查看运行日志
7.5本章小结
第8章Spark SQL综合应用案例
8.1综合案例实战——电商网站日志多维度数据分析
8.1.1数据准备
8.1.2数据说明
8.1.3数据创建
8.1.4数据导入
8.1.5数据测试和处理
8.2综合案例实战——电商网站搜索排名统计
8.2.1案例概述
8.2.2数据准备
8.2.3实现用户每天搜索前3名的商品排名统计
8.3本章小结

1.1 Spark SQL的前世今生 3
1.2 Spark SQL能做什么 4
第2章 Spark安装、编程环境搭建以及打包提交 6
2.1 Spark的简易安装 6
2.2 准备编写Spark应用程序的IDEA环境 10
2.3 将编写好的Spark应用程序打包成jar提交到Spark上 18
第二部分 基础篇
第3章 Spark上的RDD编程 23
3.1 RDD基础 24
3.1.1 创建RDD 24
3.1.2 RDD转化操作、行动操作 24
3.1.3 惰性求值 25
3.1.4 RDD缓存概述 26
3.1.5 RDD基本编程步骤 26
3.2 RDD简单实例—wordcount 27
3.3 创建RDD 28
3.3.1 程序内部数据作为数据源 28
3.3.2 外部数据源 29
3.4 RDD操作 33
3.4.1 转化操作 34
3.4.2 行动操作 37
3.4.3 惰性求值 38
3.5 向Spark传递函数 39
3.5.1 传入匿名函数 39
3.5.2 传入静态方法和传入方法的引用 40
3.5.3 闭包的理解 41
3.5.4 关于向Spark传递函数与闭包的总结 42
3.6 常见的转化操作和行动操作 42
3.6.1 基本RDD转化操作 43
3.6.2 基本RDD行动操作 48
3.6.3 键值对RDD 52
3.6.4 不同类型RDD之间的转换 56
3.7 深入理解RDD 57
3.8 RDD 缓存、持久化 59
3.8.1 RDD缓存 59
3.8.2 RDD持久化 61
3.8.3 持久化存储等级选取策略 63
3.9 RDD checkpoint容错机制 64
第4章 Spark SQL编程入门 66
4.1 Spark SQL概述 66
4.1.1 Spark SQL是什么 66
4.1.2 Spark SQL通过什么来实现 66
4.1.3 Spark SQL 处理数据的优势 67
4.1.4 Spark SQL数据核心抽象——DataFrame 67
4.2 Spark SQL编程入门示例 69
4.2.1 程序主入口：SparkSession 69
4.2.2 创建 DataFrame 70
4.2.3 DataFrame基本操作 70
4.2.4 执行SQL查询 72
4.2.5 全局临时表 73
4.2.6 Dataset 73
4.2.7 将RDDs转化为DataFrame 75
4.2.8 用户自定义函数 78
第5章 Spark SQL的DataFrame操作大全 82
5.1 由JSON文件生成所需的DataFrame对象 82
5.2 DataFrame上的行动操作 84
5.3 DataFrame上的转化操作 91
5.3.1 where条件相关 92
5.3.2 查询指定列 94
5.3.3 思维开拓：Column的巧妙应用 99
5.3.4 limit操作 102
5.3.5 排序操作：order by和sort 103
5.3.6 group by操作 106
5.3.7 distinct、dropDuplicates去重操作 107
5.3.8 聚合操作 109
5.3.9 union合并操作 110
5.3.10 join操作 111
5.3.11 获取指定字段统计信息 114
5.3.12 获取两个DataFrame中共有的记录 116
5.3.13 获取一个DataFrame中有另一个DataFrame中没有的记录 116
5.3.14 操作字段名 117
5.3.15 处理空值列 118
第6章 Spark SQL支持的多种数据源 121
6.1 概述 121
6.1.1 通用load/save 函数 121
6.1.2 手动指定选项 123
6.1.3 在文件上直接进行SQL查询 123
6.1.4 存储模式 123
6.1.5 持久化到表 124
6.1.6 bucket、排序、分区操作 124
6.2 典型结构化数据源 125
6.2.1 Parquet 文件 125
6.2.2 JSON 数据集 129
6.2.3 Hive表 130
6.2.4 其他数据库中的数据表 133
第三部分 实践篇
第7章 Spark SQL 工程实战之基于WiFi探针的商业大数据分析技术 139
7.1 功能需求 139
7.1.1 数据收集 139
7.1.2 数据清洗 140
7.1.3 客流数据分析 141
7.1.4 数据导出 142
7.2 系统架构 142
7.3 功能设计 143
7.4 数据库结构 144

Title Page
Copyright
Learning Spark SQL
Credits
About the Author
About the Reviewer
www.PacktPub.com
Why subscribe?
Customer Feedback
Preface
What this book covers
What you need for this book
Who this book is for
Conventions
Reader feedback
Customer support
Downloading the example code
Downloading the color images of this book
Errata
Piracy
Questions
Getting Started with Spark SQL
What is Spark SQL?
Introducing SparkSession
Understanding Spark SQL concepts
Understanding Resilient Distributed Datasets (RDDs)
Understanding DataFrames and Datasets
Understanding the Catalyst optimizer
Understanding Catalyst optimizations
Understanding Catalyst transformations
Introducing Project Tungsten
Using Spark SQL in streaming applications
Understanding Structured Streaming internals
Summary
Using Spark SQL for Processing Structured and Semistructured Data
Understanding data sources in Spark applications
Selecting Spark data sources
Using Spark with relational databases
Using Spark with MongoDB (NoSQL database)
Using Spark with JSON data
Using Spark with Avro files
Using Spark with Parquet files
Defining and using custom data sources in Spark
Summary
Using Spark SQL for Data Exploration
Introducing Exploratory Data Analysis (EDA)
Using Spark SQL for basic data analysis
Identifying missing data
Computing basic statistics
Identifying data outliers
Visualizing data with Apache Zeppelin
Sampling data with Spark SQL APIs
Sampling with the DataFrame/Dataset API
Sampling with the RDD API
Using Spark SQL for creating pivot tables
Summary
Using Spark SQL for Data Munging
Introducing data munging
Exploring data munging techniques
Pre-processing of the?household electric consumption Dataset
Computing basic statistics and aggregations
Augmenting the Dataset
Executing other miscellaneous processing steps
Pre-processing of?the weather Dataset
Analyzing missing data
Combining data using a JOIN operation
Munging textual data
Processing multiple input data files
Removing stop words
Munging time series data
Pre-processing of the?time-series Dataset
Processing date fields
Persisting and loading data
Defining a date-time index
Using the??TimeSeriesRDD?object
Handling missing time-series data
Computing basic statistics
Dealing with variable length records
Converting variable-length records to fixed-length records
Extracting data from "messy" columns
Preparing data for machine learning
Pre-processing data for machine learning
Creating and running a machine learning pipeline
Summary
Using Spark SQL in Streaming Applications
Introducing streaming data applications
Building Spark streaming applications
Implementing sliding window-based functionality
Joining a streaming Dataset with a static Dataset
Using the Dataset API in Structured Streaming
Using output sinks
Using the Foreach Sink for arbitrary computations on output
Using the Memory Sink to save output to a table
Using the File Sink to save output to a partitioned table
Monitoring streaming queries
Using Kafka with Spark Structured Streaming
Introducing Kafka concepts
Introducing ZooKeeper concepts
Introducing Kafka-Spark integration
Introducing Kafka-Spark Structured Streaming
Writing a receiver for a custom data source
Summary
Using Spark SQL in Machine Learning Applications
Introducing machine learning applications
Understanding Spark ML pipelines and their components
Understanding the steps in a pipeline application development process
Introducing feature engineering
Creating new features from raw data
Estimating the importance of a feature
Understanding dimensionality reduction
Deriving good features
Implementing a Spark ML classification model
Exploring the diabetes Dataset
Pre-processing the data
Building the Spark ML pipeline
Using StringIndexer for indexing categorical features and labels
Using VectorAssembler for assembling features into one column
Using a Spark ML classifier
Creating a Spark ML pipeline
Creating the training and test Datasets
Making predictions using the PipelineModel
Selecting the best model
Changing the ML algorithm in the pipeline
Introducing Spark ML tools and utilities
Using Principal Component Analysis to select features
Using encoders
Using Bucketizer
Using VectorSlicer
Using Chi-squared selector
Using a Normalizer
Retrieving our original labels
Implementing a Spark ML clustering model
Summary
Using Spark SQL in Graph Applications
Introducing large-scale graph applications
Exploring graphs using GraphFrames
Constructing a GraphFrame
Basic graph queries and operations
Motif analysis using GraphFrames
Processing subgraphs
Applying graph algorithms
Saving and loading GraphFrames
Analyzing JSON input modeled as a graph?
Processing graphs containing multiple types of relationships
Understanding GraphFrame internals
Viewing GraphFrame physical execution plan
Understanding partitioning in GraphFrames
Summary
Using Spark SQL with SparkR
Introducing SparkR
Understanding the SparkR architecture
Understanding SparkR DataFrames
Using SparkR for EDA and data munging tasks
Reading and writing Spark DataFrames
Exploring structure and contents of Spark DataFrames
Running basic operations on Spark DataFrames
Executing SQL statements on Spark DataFrames
Merging SparkR DataFrames
Using User Defined Functions (UDFs)
Using SparkR for computing summary statistics
Using SparkR for data visualization
Visualizing data on a map
Visualizing graph nodes and edges
Using SparkR for machine learning
Summary
Developing Applications with Spark SQL
Introducing Spark SQL applications
Understanding text analysis applications
Using Spark SQL for textual analysis
Preprocessing textual data
Computing readability
Using word lists
Creating data preprocessing pipelines
Understanding themes in document corpuses
Using Naive Bayes classifiers
Developing a machine learning application
Summary
Using Spark SQL in Deep Learning Applications
Introducing neural networks
Understanding deep learning
Understanding representation learning
Understanding stochastic gradient descent
Introducing deep learning in Spark
Introducing CaffeOnSpark
Introducing DL4J
Introducing TensorFrames
Working with BigDL
Tuning hyperparameters of deep learning models
Introducing deep learning pipelines
Understanding Supervised learning
Understanding convolutional neural networks
Using neural networks for text classification
Using deep neural networks for language processing
Understanding Recurrent Neural Networks
Introducing autoencoders
Summary
Tuning Spark SQL Components for Performance
Introducing performance tuning in Spark SQL
Understanding DataFrame/Dataset APIs
Optimizing data serialization
Understanding Catalyst optimizations
Understanding the Dataset/DataFrame API
Understanding Catalyst transformations
Visualizing Spark application execution
Exploring Spark application execution metrics
Using external tools for performance tuning
Cost-based optimizer in Apache Spark 2.2
Understanding the?CBO statistics collection
Statistics collection functions
Filter operator
Join operator
Build side selection
Understanding multi-way JOIN ordering optimization
Understanding performance improvements using whole-stage code generation
Summary
Spark SQL in Large-Scale Application Architectures
Understanding Spark-based application architectures
Using Apache Spark for batch processing
Using Apache Spark for stream processing
Understanding the Lambda architecture
Understanding the Kappa Architecture
Design considerations for building scalable stream processing applications
Building robust ETL pipelines using Spark SQL
Choosing appropriate data formats
Transforming data in ETL pipelines
Addressing errors in ETL pipelines
Implementing a scalable monitoring solution
Deploying Spark machine learning pipelines
Understanding the challenges in typical ML deployment environments
Understanding types of model scoring architectures
Using cluster managers
Summary