第 1 章 Spark SQL 背景
1．1 大数据与 Spark 系统
1．2 关系模型与 SQL 语言
1．3 Spark SQL 发展历程
1．4 本章小结
第 2 章 Spark 基础知识介绍
2．1 RDD 编程模型
2．2 DataFrame 与 Dataset
2．3 本章小结
第 3 章 Spark SQL 执行全过程概述
3．1 从 SQL 到 RDD：一个简单的案例
3．2 重要概念
3．2．1 InternalRow 体系
3．2．2 TreeNode 体系
3．2．3 Expression 体系
3．3 内部数据类型系统
3．4 本章小结
第 4 章 Spark SQL 编译器 Parser
4．1 DSL 工具之 ANTLR 简介
4．1．1 基于 ANTLR 4 的计算器
4．1．2 访问者模式
4．2 SparkSqlParser 之 AstBuilder
4．3 常见 SQL 生成的抽象语法树概览
4．4 本章小结
第 5 章 Spark SQL 逻辑计划（LogicalPlan）
5．1 Spark SQL 逻辑计划概述
5．2 LogicalPlan 简介
5．2．1 QueryPlan 概述
5．2．2 LogicalPlan 基本操作与分类
5．2．3 LeafNode 类型的 LogicalPlan
5．2．4 UnaryNode 类型的 LogicalPlan
5．2．5 BinaryNode 类型的 LogicalPlan
5．2．6 其他类型的 LogicalPlan
5．3 AstBuilder 机制：Unresolved LogicalPlan 生成
5．4 Analyzer 机制：Analyzed LogicalPlan 生成
5．4．1 Catalog 体系分析
5．4．2 Rule 体系
5．4．3 Analyzed LogicalPlan 生成过程
5．5 Spark SQL 优化器 Optimizer
5．5．1 Optimizer 概述
5．5．2 Optimizer 规则体系
5．5．3 Optimized LogicalPlan 的生成过程
5．6 本章小结
第 6 章 Spark SQL 物理计划（PhysicalPlan）
6．1 Spark SQL 物理计划概述
6．2 SparkPlan 简介
6．2．1 LeafExecNode 类型
6．2．2 UnaryExecNode 类型
6．2．3 BinaryExecNode 类型
6．2．4 其他类型的 SparkPlan
6．3 Metadata 与 Metrics 体系
6．4 Partitioning 与 Ordering 体系
6．4．1 Distribution 与 Partitioning 的概念
6．4．2 SparkPlan 的常用分区排序操作
6．5 SparkPlan 生成
6．5．1 物理计划 Strategy 体系
6．5．2 常见 Strategy 分析
6．6 执行前的准备
6．6．1 PlanSubqueries 规则
6．6．2 EnsureRequirements 规则
6．7 本章小结
第 7 章 Spark SQL 之 Aggregation 实现
7．1 Aggregation 执行概述
7．1．1 文法定义
7．1．2 聚合语句 Unresolved LogicalPlan 生成
7．1．3 从逻辑算子树到物理算子树
7．2 聚合函数（AggregateFunction）
7．2．1 聚合缓冲区与聚合模式（AggregateMode）
7．2．2 DeclarativeAggregate 聚合函数
7．2．3 ImperativeAggregate 聚合函数
7．2．4 TypedImperativeAggregate 聚合函数
7．3 聚合执行
7．3．1 执行框架 AggregationIterator
7．3．2 基于排序的聚合算子 SortAggregateExec
7．3．3 基于 Hash 的聚合算子 HashAggregateExec
7．4 窗口（Window）函数
7．4．1 窗口函数定义与简介
7．4．2 窗口函数相关表达式
7．4．3 窗口函数的逻辑计划阶段与物理计划阶段
7．4．4 窗口函数的执行
7．5 多维分析
7．5．1 OLAP 多维分析背景
7．5．2 Spark SQL 多维查询
7．5．3 多维分析 LogicalPlan 阶段
7．5．4 多维分析 PhysicalPlan 与执行
7．6 本章小结
第 8 章 Spark SQL 之 Join 实现
8．1 Join 查询概述
8．2 文法定义与抽象语法树
8．3 Join 查询逻辑计划
8．3．1 从 AST 到 Unresolved LogicalPlan
8．3．2 从 Unresolve LogicalPlan 到 Analyzed LogicalPlan
8．3．3 从 Analyzed LogicalPlan 到 Optimized LogicalPlan
8．4 Join 查询物理计划
8．4．1 Join 物理计划的生成
8．4．2 Join 物理计划的选取
8．5 Join 查询执行
8．5．1 Join 执行基本框架
8．5．2 BroadcastJoinExec 执行机制
8．5．3 ShuffledHashJoinExec 执行机制
8．5．4 SortMergeJoinExec 执行机制
8．6 本章小结
第 9 章 Tungsten 技术实现
9．1 内存管理与二进制处理
9．1．1 Spark 内存管理基础
9．1．2 Tungsten 内存管理优化基础
9．1．3 Tungsten 内存优化应用
9．2 缓存敏感计算（Cache-aware computation）
9．3 动态代码生成（Code generation）
9．3．1 漫谈代码生成
9．3．2 Janino 编译器实践
9．3．3 基本（表达式）代码生成
9．3．4 全阶段代码生成（WholeStageCodegen）
9．4 本章小结
第 10 章 Spark SQL 连接 Hive
10．1 Spark SQL 连接 Hive 概述
10．2 Hive 相关的规则和策略
10．2．1 HiveSessionCatalog 体系
10．2．2 Analyzer 之 Hive-Specific 分析规则
10．2．3 SparkPlanner 之 Hive-Specific 转换策略
10．2．4 Hive 相关的任务执行
10．3 Spark SQL 与 Hive 数据类型
10．3．1 Hive 数据类型与 SerDe 框架
10．3．2 DataTypeToInspector 与 Data Wrapping
10．3．3 InspectorToDataType 与 Data Unwrapping
10．4 Hive UDF 管理机制
10．5 Spark Thrift Server 实现
10．5．1 Service 体系
10．5．2 Operation 与 OperationManager
10．5．3 Session 与 SessionManager
10．5．4 Authentication 安全认证管理
10．5．5 Spark Thrift Server 执行流程
10．6 本章小结
第 11 章 Spark SQL 开发与实践
11．1 腾讯大数据平台（TDW）简介
11．2 腾讯大数据平台 SQL 引擎（TDW-SQL-Engine）
11．2．1 SQL-Engine 背景与演化历程
11．2．2 SQL-Engine 整体架构
11．3 TDW-Spark SQL 开发与优化
11．3．1 业务运行支撑框架
11．3．2 新功能开发案例
11．3．3 性能优化开发案例
11．4 业务实践经验与教训
11．4．1 Spark SQL 集群管理的经验
11．4．2 Spark SQL 业务层面调优
11．4．3 SQL 写法的“陷阱”

第1章 初识Spark SQL
1.1 Spark SQL的前世今生
1.2 Spark SQL能做什么
第2章 Spark安装、编程环境搭建以及打包提交
2.1 Spark的简易安装
2.2 准备编写Spark应用程序的IDEA环境
2.3 将编写好的Spark应用程序打包成jar提交到Spark上
第二部分 基础篇
第3章 Spark上的RDD编程
3.1 RDD基础
3.2 RDD简单实例—wordcount
3.3 创建RDD
3.4 RDD操作
3.5 向Spark传递函数
3.6 常见的转化操作和行动操作
3.7 深入理解RDD
3.8 RDD缓存、持久化
3.9 RDD checkpoint容错机制
第4章 Spark SQL编程入门
4.1 Spark SQL概述
4.2 Spark SQL编程入门示例
第5章 Spark SQL的DataFrame操作大全
5.1 由JSON文件生成所需的DataFrame对象
5.2 DataFrame上的行动操作
5.3 DataFrame上的转化操作
第6章 Spark SQL支持的多种数据源
6.1 概述
6.2 典型结构化数据源
第三部分 实战篇
第7章 Spark SQL工程实战之基于WiFi探针的商业大数据分析技术
7.1 功能需求
7.2 系统架构
7.3 功能设计
7.4 数据库结构
7.5 本章小结
第8章 第一个Spark SQL应用程序
8.1 完全分布式环境搭建
8.2 数据清洗
8.3 数据处理流程
8.4 Spark程序远程调试
8.5 Spark的Web界面
8.6 本章小结
第四部分 优化篇
第9章 让Spark程序再快一点
9.1 Spark执行流程
9.2 Spark内存简介
9.3 Spark的一些概念
9.4 Spark编程四大守则
9.5 Spark调优七式
9.6 解决数据倾斜问题
9.7 Spark执行引擎Tungsten简介
9.8 Spark SQL解析引擎Catalyst简介

1.1Spark SQL概述
1.1.1Spark SQL与DataFrame
1.1.2DataFrame与RDD的差异
1.1.3Spark SQL的发展历程
1.2从零起步掌握Hive
1.2.1Hive的本质是什么
1.2.2Hive安装和配置
1.2.3使用Hive分析搜索数据
1.3Spark SQL on Hive安装与配置
1.3.1安装Spark SQL
1.3.2安装MySQL
1.3.3启动Hive Metastore
1.4Spark SQL初试
1.4.1通过spark-shell来使用Spark SQL
1.4.2Spark SQL的命令终端
1.4.3Spark的Web UI
1.5本章小结
第2章DataFrame原理与常用操作
2.1DataFrame编程模型
2.2DataFrame基本操作实战
2.2.1数据准备
2.2.2启动交互式界面
2.2.3数据处理与分析
2.3通过RDD来构建DataFrame
2.4缓存表（列式存储）
2.5DataFrame API应用示例
2.6本章小结
第3章Spark SQL 操作多种数据源
3.1通用的加载/保存功能
3.1.1Spark SQL加载数据
3.1.2Spark SQL保存数据
3.1.3综合案例——电商热销商品排名
3.2Spark SQL操作Hive示例
3.3Spark SQL操作JSON数据集示例
3.4Spark SQL操作HBase示例
3.5Spark SQL操作MySQL示例
3.5.1安装并启动MySQL
3.5.2准备数据表
3.5.3操作MySQL表
3.6Spark SQL操作MongoDB示例
3.6.1安装配置MongoDB
3.6.2启动MongoDB
3.6.3准备数据
3.6.4Spark SQL操作MongoDB
3.7本章小结
第4章Parquet列式存储
4.1Parquet概述
4.1.1Parquet的基本概念
4.1.2Parquet数据列式存储格式应用举例
4.2Parquet的Block配置及数据分片
4.2.1Parquet的Block的配置
4.2.2Parquet 内部的数据分片
4.3Parquet序列化
4.3.1Spark实施序列化的目的
4.3.2Parquet两种序列化方式
4.4本章小结
第5章Spark SQL内置函数与窗口函数
5.1Spark SQL内置函数
5.1.1Spark SQL内置函数概述
5.1.2Spark SQL内置函数应用实例
5.2Spark SQL窗口函数
5.2.1Spark SQL窗口函数概述
5.2.2Spark SQL窗口函数分数查询统计案例
5.2.3Spark SQL窗口函数NBA常规赛数据统计案例
5.3本章小结
第6章Spark SQL UDF与UDAF
6.1UDF概述
6.2UDF示例
6.2.1Hobby_count函数
6.2.2Combine函数
6.2.3Str2Int函数
6.2.4Wsternstate函数
6.2.5ManyCustomers函数
6.2.6StateRegion函数
6.2.7DiscountRatio函数
6.2.8MakeStruct函数
6.2.9MyDateFilter函数
6.2.10MakeDT函数
6.3UDAF概述
6.4UDAF示例
6.4.1ScalaAggregateFunction函数
6.4.2GeometricMean函数
6.4.3CustomMean函数
6.4.4BelowThreshold函数
6.4.5YearCompare函数
6.4.6WordCount函数
6.5本章小结
第7章Thrift Server
7.1Thrift概述
7.1.1Thrift的基本概念
7.1.2Thrift的工作机制
7.1.3Thrift的运行机制
7.1.4一个简单的Thrift 实例
7.2Thrift Server的启动过程
7.2.1Thrift Sever启动详解
7.2.2HiveThriftServer2类的解析
7.3Beeline操作
7.3.1Beeline连接方式
7.3.2在Beeline中进行SQL查询操作
7.3.3通过Web控制台查看用户进行的操作
7.4Thrift Server应用示例
7.4.1示例源代码
7.4.2关键代码行解析
7.4.3测试运行
7.4.4运行结果解析
7.4.5Spark Web控制台查看运行日志
7.5本章小结
第8章Spark SQL综合应用案例
8.1综合案例实战——电商网站日志多维度数据分析
8.1.1数据准备
8.1.2数据说明
8.1.3数据创建
8.1.4数据导入
8.1.5数据测试和处理
8.2综合案例实战——电商网站搜索排名统计
8.2.1案例概述
8.2.2数据准备
8.2.3实现用户每天搜索前3名的商品排名统计
8.3本章小结

1.1 Spark SQL的前世今生 3
1.2 Spark SQL能做什么 4
第2章 Spark安装、编程环境搭建以及打包提交 6
2.1 Spark的简易安装 6
2.2 准备编写Spark应用程序的IDEA环境 10
2.3 将编写好的Spark应用程序打包成jar提交到Spark上 18
第二部分 基础篇
第3章 Spark上的RDD编程 23
3.1 RDD基础 24
3.1.1 创建RDD 24
3.1.2 RDD转化操作、行动操作 24
3.1.3 惰性求值 25
3.1.4 RDD缓存概述 26
3.1.5 RDD基本编程步骤 26
3.2 RDD简单实例—wordcount 27
3.3 创建RDD 28
3.3.1 程序内部数据作为数据源 28
3.3.2 外部数据源 29
3.4 RDD操作 33
3.4.1 转化操作 34
3.4.2 行动操作 37
3.4.3 惰性求值 38
3.5 向Spark传递函数 39
3.5.1 传入匿名函数 39
3.5.2 传入静态方法和传入方法的引用 40
3.5.3 闭包的理解 41
3.5.4 关于向Spark传递函数与闭包的总结 42
3.6 常见的转化操作和行动操作 42
3.6.1 基本RDD转化操作 43
3.6.2 基本RDD行动操作 48
3.6.3 键值对RDD 52
3.6.4 不同类型RDD之间的转换 56
3.7 深入理解RDD 57
3.8 RDD 缓存、持久化 59
3.8.1 RDD缓存 59
3.8.2 RDD持久化 61
3.8.3 持久化存储等级选取策略 63
3.9 RDD checkpoint容错机制 64
第4章 Spark SQL编程入门 66
4.1 Spark SQL概述 66
4.1.1 Spark SQL是什么 66
4.1.2 Spark SQL通过什么来实现 66
4.1.3 Spark SQL 处理数据的优势 67
4.1.4 Spark SQL数据核心抽象——DataFrame 67
4.2 Spark SQL编程入门示例 69
4.2.1 程序主入口：SparkSession 69
4.2.2 创建 DataFrame 70
4.2.3 DataFrame基本操作 70
4.2.4 执行SQL查询 72
4.2.5 全局临时表 73
4.2.6 Dataset 73
4.2.7 将RDDs转化为DataFrame 75
4.2.8 用户自定义函数 78
第5章 Spark SQL的DataFrame操作大全 82
5.1 由JSON文件生成所需的DataFrame对象 82
5.2 DataFrame上的行动操作 84
5.3 DataFrame上的转化操作 91
5.3.1 where条件相关 92
5.3.2 查询指定列 94
5.3.3 思维开拓：Column的巧妙应用 99
5.3.4 limit操作 102
5.3.5 排序操作：order by和sort 103
5.3.6 group by操作 106
5.3.7 distinct、dropDuplicates去重操作 107
5.3.8 聚合操作 109
5.3.9 union合并操作 110
5.3.10 join操作 111
5.3.11 获取指定字段统计信息 114
5.3.12 获取两个DataFrame中共有的记录 116
5.3.13 获取一个DataFrame中有另一个DataFrame中没有的记录 116
5.3.14 操作字段名 117
5.3.15 处理空值列 118
第6章 Spark SQL支持的多种数据源 121
6.1 概述 121
6.1.1 通用load/save 函数 121
6.1.2 手动指定选项 123
6.1.3 在文件上直接进行SQL查询 123
6.1.4 存储模式 123
6.1.5 持久化到表 124
6.1.6 bucket、排序、分区操作 124
6.2 典型结构化数据源 125
6.2.1 Parquet 文件 125
6.2.2 JSON 数据集 129
6.2.3 Hive表 130
6.2.4 其他数据库中的数据表 133
第三部分 实践篇
第7章 Spark SQL 工程实战之基于WiFi探针的商业大数据分析技术 139
7.1 功能需求 139
7.1.1 数据收集 139
7.1.2 数据清洗 140
7.1.3 客流数据分析 141
7.1.4 数据导出 142
7.2 系统架构 142
7.3 功能设计 143
7.4 数据库结构 144

Title Page
Copyright
Learning Spark SQL
Credits
About the Author
About the Reviewer
www.PacktPub.com
Why subscribe?
Customer Feedback
Preface
What this book covers
What you need for this book
Who this book is for
Conventions
Reader feedback
Customer support
Downloading the example code
Downloading the color images of this book
Errata
Piracy
Questions
Getting Started with Spark SQL
What is Spark SQL?
Introducing SparkSession
Understanding Spark SQL concepts
Understanding Resilient Distributed Datasets (RDDs)
Understanding DataFrames and Datasets
Understanding the Catalyst optimizer
Understanding Catalyst optimizations
Understanding Catalyst transformations
Introducing Project Tungsten
Using Spark SQL in streaming applications
Understanding Structured Streaming internals
Summary
Using Spark SQL for Processing Structured and Semistructured Data
Understanding data sources in Spark applications
Selecting Spark data sources
Using Spark with relational databases
Using Spark with MongoDB (NoSQL database)
Using Spark with JSON data
Using Spark with Avro files
Using Spark with Parquet files
Defining and using custom data sources in Spark
Summary
Using Spark SQL for Data Exploration
Introducing Exploratory Data Analysis (EDA)
Using Spark SQL for basic data analysis
Identifying missing data
Computing basic statistics
Identifying data outliers
Visualizing data with Apache Zeppelin
Sampling data with Spark SQL APIs
Sampling with the DataFrame/Dataset API
Sampling with the RDD API
Using Spark SQL for creating pivot tables
Summary
Using Spark SQL for Data Munging
Introducing data munging
Exploring data munging techniques
Pre-processing of the?household electric consumption Dataset
Computing basic statistics and aggregations
Augmenting the Dataset
Executing other miscellaneous processing steps
Pre-processing of?the weather Dataset
Analyzing missing data
Combining data using a JOIN operation
Munging textual data
Processing multiple input data files
Removing stop words
Munging time series data
Pre-processing of the?time-series Dataset
Processing date fields
Persisting and loading data
Defining a date-time index
Using the??TimeSeriesRDD?object
Handling missing time-series data
Computing basic statistics
Dealing with variable length records
Converting variable-length records to fixed-length records
Extracting data from "messy" columns
Preparing data for machine learning
Pre-processing data for machine learning
Creating and running a machine learning pipeline
Summary
Using Spark SQL in Streaming Applications
Introducing streaming data applications
Building Spark streaming applications
Implementing sliding window-based functionality
Joining a streaming Dataset with a static Dataset
Using the Dataset API in Structured Streaming
Using output sinks
Using the Foreach Sink for arbitrary computations on output
Using the Memory Sink to save output to a table
Using the File Sink to save output to a partitioned table
Monitoring streaming queries
Using Kafka with Spark Structured Streaming
Introducing Kafka concepts
Introducing ZooKeeper concepts
Introducing Kafka-Spark integration
Introducing Kafka-Spark Structured Streaming
Writing a receiver for a custom data source
Summary
Using Spark SQL in Machine Learning Applications
Introducing machine learning applications
Understanding Spark ML pipelines and their components
Understanding the steps in a pipeline application development process
Introducing feature engineering
Creating new features from raw data
Estimating the importance of a feature
Understanding dimensionality reduction
Deriving good features
Implementing a Spark ML classification model
Exploring the diabetes Dataset
Pre-processing the data
Building the Spark ML pipeline
Using StringIndexer for indexing categorical features and labels
Using VectorAssembler for assembling features into one column
Using a Spark ML classifier
Creating a Spark ML pipeline
Creating the training and test Datasets
Making predictions using the PipelineModel
Selecting the best model
Changing the ML algorithm in the pipeline
Introducing Spark ML tools and utilities
Using Principal Component Analysis to select features
Using encoders
Using Bucketizer
Using VectorSlicer
Using Chi-squared selector
Using a Normalizer
Retrieving our original labels
Implementing a Spark ML clustering model
Summary
Using Spark SQL in Graph Applications
Introducing large-scale graph applications
Exploring graphs using GraphFrames
Constructing a GraphFrame
Basic graph queries and operations
Motif analysis using GraphFrames
Processing subgraphs
Applying graph algorithms
Saving and loading GraphFrames
Analyzing JSON input modeled as a graph?
Processing graphs containing multiple types of relationships
Understanding GraphFrame internals
Viewing GraphFrame physical execution plan
Understanding partitioning in GraphFrames
Summary
Using Spark SQL with SparkR
Introducing SparkR
Understanding the SparkR architecture
Understanding SparkR DataFrames
Using SparkR for EDA and data munging tasks
Reading and writing Spark DataFrames
Exploring structure and contents of Spark DataFrames
Running basic operations on Spark DataFrames
Executing SQL statements on Spark DataFrames
Merging SparkR DataFrames
Using User Defined Functions (UDFs)
Using SparkR for computing summary statistics
Using SparkR for data visualization
Visualizing data on a map
Visualizing graph nodes and edges
Using SparkR for machine learning
Summary
Developing Applications with Spark SQL
Introducing Spark SQL applications
Understanding text analysis applications
Using Spark SQL for textual analysis
Preprocessing textual data
Computing readability
Using word lists
Creating data preprocessing pipelines
Understanding themes in document corpuses
Using Naive Bayes classifiers
Developing a machine learning application
Summary
Using Spark SQL in Deep Learning Applications
Introducing neural networks
Understanding deep learning
Understanding representation learning
Understanding stochastic gradient descent
Introducing deep learning in Spark
Introducing CaffeOnSpark
Introducing DL4J
Introducing TensorFrames
Working with BigDL
Tuning hyperparameters of deep learning models
Introducing deep learning pipelines
Understanding Supervised learning
Understanding convolutional neural networks
Using neural networks for text classification
Using deep neural networks for language processing
Understanding Recurrent Neural Networks
Introducing autoencoders
Summary
Tuning Spark SQL Components for Performance
Introducing performance tuning in Spark SQL
Understanding DataFrame/Dataset APIs
Optimizing data serialization
Understanding Catalyst optimizations
Understanding the Dataset/DataFrame API
Understanding Catalyst transformations
Visualizing Spark application execution
Exploring Spark application execution metrics
Using external tools for performance tuning
Cost-based optimizer in Apache Spark 2.2
Understanding the?CBO statistics collection
Statistics collection functions
Filter operator
Join operator
Build side selection
Understanding multi-way JOIN ordering optimization
Understanding performance improvements using whole-stage code generation
Summary
Spark SQL in Large-Scale Application Architectures
Understanding Spark-based application architectures
Using Apache Spark for batch processing
Using Apache Spark for stream processing
Understanding the Lambda architecture
Understanding the Kappa Architecture
Design considerations for building scalable stream processing applications
Building robust ETL pipelines using Spark SQL
Choosing appropriate data formats
Transforming data in ETL pipelines
Addressing errors in ETL pipelines
Implementing a scalable monitoring solution
Deploying Spark machine learning pipelines
Understanding the challenges in typical ML deployment environments
Understanding types of model scoring architectures
Using cluster managers
Summary

=============

导读：本文所述内容均基于 2018 年 9 月 17 日 Spark 最新 Spark Release 2.3.1 版本，以及截止到 2018 年 10 月 21 日 Adaptive Execution 最新开发代码。自动设置 Shuffle Partition 个数已进入 Spark Release 2.3.1 版本，动态调整执行计划与处理数据倾斜尚未进入 Spark Release 2.3.1
1 背  景
Spark SQL / Catalyst 和 CBO 的优化，从查询本身与目标数据的特点的角度尽可能保证了最终生成的执行计划的高效性。但是

执行计划一旦生成，便不可更改，即使执行过程中发现后续执行计划可以进一步优化，也只能按原计划执行；

CBO 基于统计信息生成最优执行计划，需要提前生成统计信息，成本较大，且不适合数据更新频繁的场景；

CBO 基于基础表的统计信息与操作对数据的影响推测中间结果的信息，只是估算，不够精确。

本文介绍的 Adaptive Execution 将可以根据执行过程中的中间数据优化后续执行，从而提高整体执行效率。核心在于两点：

执行计划可动态调整

调整的依据是中间结果的精确统计信息

2 动态设置 Shuffle Partition
2.1 Spark Shuffle 原理
Spark Shuffle 一般用于将上游 Stage 中的数据按 Key 分区，保证来自不同 Mapper （表示上游 Stage 的 Task）的相同的 Key 进入相同的 Reducer （表示下游 Stage 的 Task）。一般用于 group by 或者 Join 操作。



如上图所示，该 Shuffle 总共有 2 个 Mapper 与 5 个 Reducer。每个 Mapper 会按相同的规则（由 Partitioner 定义）将自己的数据分为五份。每个 Reducer 从这两个 Mapper 中拉取属于自己的那一份数据。

2.2 原有 Shuffle 的问题
使用 Spark SQL 时，可通过spark.sql.shuffle.partitions指定 Shuffle 时 Partition 个数，也即 Reducer 个数。

该参数决定了一个 Spark SQL Job 中包含的所有 Shuffle 的 Partition 个数。如下图所示，当该参数值为 3 时，所有 Shuffle 中 Reducer 个数都为 3。



这种方法有如下问题：

Partition 个数不宜设置过大；

Reducer（代指 Spark Shuffle 过程中执行 Shuffle Read 的 Task） 个数过多，每个 Reducer 处理的数据量过小。大量小 Task 造成不必要的 Task 调度开销与可能的资源调度开销（如果开启了 Dynamic Allocation）；

Reducer 个数过大，如果 Reducer 直接写 HDFS 会生成大量小文件，从而造成大量 addBlock RPC，Name node 可能成为瓶颈，并影响其它使用 HDFS 的应用；

过多 Reducer 写小文件，会造成后面读取这些小文件时产生大量 getBlock RPC，对 Name node 产生冲击；

Partition 个数不宜设置过小

每个 Reducer 处理的数据量太大，Spill 到磁盘开销增大；

Reducer GC 时间增长；

Reducer 如果写 HDFS，每个 Reducer 写入数据量较大，无法充分发挥并行处理优势；

很难保证所有 Shuffle 都最优

不同的 Shuffle 对应的数据量不一样，因此最优的 Partition 个数也不一样。使用统一的 Partition 个数很难保证所有 Shuffle 都最优；

定时任务不同时段数据量不一样，相同的 Partition 数设置无法保证所有时间段执行时都最优；

2.3 自动设置 Shuffle Partition 原理
如 Spark Shuffle 原理 一节图中所示，Stage 1 的 5 个 Partition 数据量分别为 60MB，40MB，1MB，2MB，50MB。其中 1MB 与 2MB 的 Partition 明显过小（实际场景中，部分小 Partition 只有几十 KB 及至几十字节）。

开启 Adaptive Execution 后：

Spark 在 Stage 0 的 Shuffle Write 结束后，根据各 Mapper 输出，统计得到各 Partition 的数据量，即 60MB，40MB，1MB，2MB，50MB；

通过 ExchangeCoordinator 计算出合适的 post-shuffle Partition 个数（即 Reducer）个数（本例中 Reducer 个数设置为 3）；

启动相应个数的 Reducer 任务；

每个 Reducer 读取一个或多个 Shuffle Write Partition 数据（如下图所示，Reducer 0 读取 Partition 0，Reducer 1 读取 Partition 1、2、3，Reducer 2 读取 Partition 4）。



三个 Reducer 这样分配是因为：

targetPostShuffleInputSize 默认为 64MB，每个 Reducer 读取数据量不超过 64MB；

如果 Partition 0 与 Partition 2 结合，Partition 1 与 Partition 3 结合，虽然也都不超过 64 MB。但读完 Partition 0 再读 Partition 2，对于同一个 Mapper 而言，如果每个 Partition 数据比较少，跳着读多个 Partition 相当于随机读，在HDD 上性能不高；

目前的做法是只结合相临的 Partition，从而保证顺序读，提高磁盘 IO 性能；

该方案只会合并多个小的 Partition，不会将大的 Partition 拆分，因为拆分过程需要引入一轮新的 Shuffle；

基于上面的原因，默认 Partition 个数（本例中为 5）可以大一点，然后由 ExchangeCoordinator 合并。如果设置的 Partition 个数太小，Adaptive Execution 在此场景下无法发挥作用。

由上图可见，Reducer 1 从每个 Mapper 读取 Partition 1、2、3 都有三根线，是因为原来的 Shuffle 设计中，每个 Reducer 每次通过 Fetch 请求从一个特定 Mapper 读数据时，只能读一个 Partition 的数据。也即在上图中，Reducer 1 读取 Mapper 0 的数据，需要 3 轮 Fetch 请求。对于 Mapper 而言，需要读三次磁盘，相当于随机 IO。

为了解决这个问题，Spark 新增接口，一次 Shuffle Read 可以读多个 Partition 的数据。如下图所示，Task 1 通过一轮请求即可同时读取 Task 0 内 Partition 0、1 和 2 的数据，减少了网络请求数量。同时 Mapper 0 一次性读取并返回三个 Partition 的数据，相当于顺序 IO，从而提升了性能。



由于 Adaptive Execution 的自动设置 Reducer 是由 ExchangeCoordinator 根据 Shuffle Write 统计信息决定的，因此即使在同一个 Job 中不同 Shuffle 的 Reducer 个数都可以不一样，从而使得每次 Shuffle 都尽可能最优。

上文 原有 Shuffle 的问题 一节中的例子，在启用 Adaptive Execution 后，三次 Shuffle 的 Reducer 个数从原来的全部为 3 变为 2、4、3。



2.4 使用与优化方法
可通过spark.sql.adaptive.enabled=true启用 Adaptive Execution 从而启用自动设置 Shuffle Reducer 这一特性。

通过spark.sql.adaptive.shuffle.targetPostShuffleInputSize可设置每个 Reducer 读取的目标数据量，其单位是字节，默认值为 64 MB。上文例子中，如果将该值设置为 50 MB，最终效果仍然如上文所示，而不会将 Partition 0 的 60MB 拆分。具体原因上文已说明。

3 动态调整执行计划
3.1 固定执行计划的不足
在不开启 Adaptive Execution 之前，执行计划一旦确定，即使发现后续执行计划可以优化，也不可更改。如下图所示，SortMergJoin 的 Shuffle Write 结束后，发现 Join 一方的 Shuffle 输出只有 46.9KB，仍然继续执行 SortMergeJoin。



此时完全可将 SortMergeJoin 变更为 BroadcastJoin 从而提高整体执行效率。

3.2 SortMergeJoin 原理
SortMergeJoin 是常用的分布式 Join 方式，它几乎可使用于所有需要 Join 的场景。但有些场景下，它的性能并不是最好的。

SortMergeJoin 的原理如下图所示：

将 Join 双方以 Join Key 为 Key 按照 HashPartitioner 分区，且保证分区数一致；

Stage 0 与 Stage 1 的所有 Task 在 Shuffle Write 时，都将数据分为 5 个 Partition，并且每个 Partition 内按 Join Key 排序；

Stage 2 启动 5 个 Task 分别去 Stage 0 与 Stage 1 中所有包含 Partition 分区数据的 Task 中取对应 Partition 的数据。（如果某个 Mapper 不包含该 Partition 的数据，则 Redcuer 无须向其发起读取请求）；

Stage 2 的 Task 2 分别从 Stage 0 的 Task 0、1、2 中读取 Partition 2 的数据，并且通过 MergeSort 对其进行排序；

Stage 2 的 Task 2 分别从 Stage 1 的 Task 0、1 中读取 Partition 2 的数据，且通过 MergeSort 对其进行排序；

Stage 2 的 Task 2 在上述两步 MergeSort 的同时，使用 SortMergeJoin 对二者进行 Join。



3.3 BroadcastJoin 原理
当参与 Join 的一方足够小，可全部置于 Executor 内存中时，可使用 Broadcast 机制将整个 RDD 数据广播到每一个 Executor 中，该 Executor 上运行的所有 Task 皆可直接读取其数据。（本文中，后续配图，为了方便展示，会将整个 RDD 的数据置于 Task 框内，而隐藏 Executor）。

对于大 RDD，按正常方式，每个 Task 读取并处理一个 Partition 的数据，同时读取 Executor 内的广播数据，该广播数据包含了小 RDD 的全量数据，因此可直接与每个 Task 处理的大 RDD 的部分数据直接 Join。



根据 Task 内具体的 Join 实现的不同，又可分为 BroadcastHashJoin 与 BroadcastNestedLoopJoin。后文不区分这两种实现，统称为 BroadcastJoin。

与 SortMergeJoin 相比，BroadcastJoin 不需要 Shuffle，减少了 Shuffle 带来的开销，同时也避免了 Shuffle 带来的数据倾斜，从而极大地提升了 Job 执行效率。

同时，BroadcastJoin 带来了广播小 RDD 的开销。另外，如果小 RDD 过大，无法存于 Executor 内存中，则无法使用 BroadcastJoin。

对于基础表的 Join，可在生成执行计划前，直接通过 HDFS 获取各表的大小，从而判断是否适合使用 BroadcastJoin。但对于中间表的 Join，无法提前准确判断中间表大小从而精确判断是否适合使用 BroadcastJoin。

《Spark SQL 性能优化再进一步 CBO 基于代价的优化》一文介绍的 CBO 可通过表的统计信息与各操作对数据统计信息的影响，推测出中间表的统计信息，但是该方法得到的统计信息不够准确。同时该方法要求提前分析表，具有较大开销。

而开启 Adaptive Execution 后，可直接根据 Shuffle Write 数据判断是否适用 BroadcastJoin。

3.4 动态调整执行计划原理
如上文 SortMergeJoin 原理 中配图所示，SortMergeJoin 需要先对 Stage 0 与 Stage 1 按同样的 Partitioner 进行 Shuffle Write。

Shuffle Write 结束后，可从每个 ShuffleMapTask 的 MapStatus 中统计得到按原计划执行时 Stage 2 各 Partition 的数据量以及 Stage 2 需要读取的总数据量。（一般来说，Partition 是 RDD 的属性而非 Stage 的属性，本文为了方便，不区分 Stage 与 RDD。可以简单认为一个 Stage 只有一个 RDD，此时 Stage 与 RDD 在本文讨论范围内等价）。

如果其中一个 Stage 的数据量较小，适合使用 BroadcastJoin，无须继续执行 Stage 2 的 Shuffle Read。相反，可利用 Stage 0 与 Stage 1 的数据进行 BroadcastJoin，如下图所示。



具体做法是：

将 Stage 1 全部 Shuffle Write 结果广播出去

启动 Stage 2，Partition 个数与 Stage 0 一样，都为 3

每个 Stage 2 每个 Task 读取 Stage 0 每个 Task 的 Shuffle Write 数据，同时与广播得到的 Stage 1 的全量数据进行 Join

注：广播数据存于每个 Executor 中，其上所有 Task 共享，无须为每个 Task 广播一份数据。上图中，为了更清晰展示为什么能够直接 Join 而将 Stage 2 每个 Task 方框内都放置了一份 Stage 1 的全量数据。

虽然 Shuffle Write 已完成，将后续的 SortMergeJoin 改为 Broadcast 仍然能提升执行效率：

SortMergeJoin 需要在 Shuffle Read 时对来自 Stage 0 与 Stage 1 的数据进行 Merge Sort，并且可能需要 Spill 到磁盘，开销较大；

SortMergeJoin 时，Stage 2 的所有 Task 需要取 Stage 0 与 Stage 1 的所有 Task 的输出数据（如果有它要的数据 ），会造成大量的网络连接。且当 Stage 2 的 Task 较多时，会造成大量的磁盘随机读操作，效率不高，且影响相同机器上其它 Job 的执行效率；

SortMergeJoin 时，Stage 2 每个 Task 需要从几乎所有 Stage 0 与 Stage 1 的 Task 取数据，无法很好利用 Locality；

Stage 2 改用 Broadcast，每个 Task 直接读取 Stage 0 的每个 Task 的数据（一对一），可很好利用 Locality 特性。最好在 Stage 0 使用的 Executor 上直接启动 Stage 2 的 Task。如果 Stage 0 的 Shuffle Write 数据并未 Spill 而是在内存中，则 Stage 2 的 Task 可直接读取内存中的数据，效率非常高。如果有 Spill，那可直接从本地文件中读取数据，且是顺序读取，效率远比通过网络随机读数据效率高。

3.5 使用与优化方法
该特性的使用方式如下：

当spark.sql.adaptive.enabled与spark.sql.adaptive.join.enabled都设置为true时，开启 Adaptive Execution 的动态调整 Join 功能；

spark.sql.adaptiveBroadcastJoinThreshold设置了 SortMergeJoin 转 BroadcastJoin 的阈值。如果不设置该参数，该阈值与spark.sql.autoBroadcastJoinThreshold的值相等；

除了本文所述 SortMergeJoin 转 BroadcastJoin，Adaptive Execution 还可提供其它 Join 优化策略。部分优化策略可能会需要增加 Shuffle。spark.sql.adaptive.allowAdditionalShuffle参数决定了是否允许为了优化 Join 而增加 Shuffle。其默认值为 false。

4 自动处理数据倾斜
4.1 解决数据倾斜典型方案
《Spark 性能优化之道——解决 Spark 数据倾斜（Data Skew）的 N 种姿势》一文讲述了数据倾斜的危害，产生原因，以及典型解决方法。

保证文件可 Split 从而避免读 HDFS 时数据倾斜；

保证 Kafka 各 Partition 数据均衡从而避免读 Kafka 引起的数据倾斜；

调整并行度或自定义 Partitioner 从而分散分配给同一 Task 的大量不同 Key；

使用 BroadcastJoin 代替 ReduceJoin 消除 Shuffle 从而避免 Shuffle 引起的数据倾斜；

对倾斜 Key 使用随机前缀或后缀从而分散大量倾斜 Key，同时将参与 Join 的小表扩容，从而保证 Join 结果的正确性。

4.2 自动解决数据倾斜
目前 Adaptive Execution 可解决 Join 时数据倾斜问题。其思路可理解为将部分倾斜的 Partition (倾斜的判断标准为该 Partition 数据是所有 Partition Shuffle Write 中位数的 N 倍) 进行单独处理，类似于 BroadcastJoin，如下图所示。



在上图中，左右两边分别是参与 Join 的 Stage 0 与 Stage 1 (实际应该是两个 RDD 进行 Join，但如同上文所述，这里不区分 RDD 与 Stage)，中间是获取 Join 结果的 Stage 2。

明显 Partition 0 的数据量较大，这里假设 Partition 0 符合“倾斜”的条件，其它 4 个 Partition 未倾斜。

以 Partition 对应的 Task 2 为例，它需获取 Stage 0 的三个 Task 中所有属于 Partition 2 的数据，并使用 MergeSort 排序。同时获取 Stage 1 的两个 Task 中所有属于 Partition 2 的数据并使用 MergeSort 排序。然后对二者进行 SortMergeJoin。

对于 Partition 0，可启动多个 Task：

在上图中，启动了两个 Task 处理 Partition 0 的数据，分别名为 Task 0-0 与 Task 0-1

Task 0-0 读取 Stage 0 Task 0 中属于 Partition 0 的数据

Task 0-1 读取 Stage 0 Task 1 与 Task 2 中属于 Partition 0 的数据，并进行 MergeSort

Task 0-0 与 Task 0-1 都从 Stage 1 的两个 Task 中所有属于 Partition 0 的数据

Task 0-0 与 Task 0-1 使用 Stage 0 中属于 Partition 0 的部分数据与 Stage 1中属于 Partition 0 的全量数据进行 Join

通过该方法，原本由一个 Task 处理的 Partition 0 的数据由多个 Task 共同处理，每个 Task 需处理的数据量减少，从而避免了 Partition 0 的倾斜。

对于 Partition 0 的处理，有点类似于 BroadcastJoin 的做法。但区别在于，Stage 2 的 Task 0-0 与 Task 0-1 同时获取 Stage 1 中属于 Partition 0 的全量数据，是通过正常的 Shuffle Read 机制实现，而非 BroadcastJoin 中的变量广播实现。

4.3 使用与优化方法
开启与调优该特性的方法如下：

将spark.sql.adaptive.skewedJoin.enabled设置为 true 即可自动处理 Join 时数据倾斜；

spark.sql.adaptive.skewedPartitionMaxSplits控制处理一个倾斜 Partition 的 Task 个数上限，默认值为 5；

spark.sql.adaptive.skewedPartitionRowCountThreshold设置了一个 Partition 被视为倾斜 Partition 的行数下限，也即行数低于该值的 Partition 不会被当作倾斜 Partition 处理。其默认值为 10L * 1000 * 1000 即一千万；

spark.sql.adaptive.skewedPartitionSizeThreshold设置了一个 Partition 被视为倾斜 Partition 的大小下限，也即大小小于该值的 Partition 不会被视作倾斜 Partition。其默认值为 64 * 1024 * 1024 也即 64MB；

spark.sql.adaptive.skewedPartitionFactor该参数设置了倾斜因子。如果一个 Partition 的大小大于spark.sql.adaptive.skewedPartitionSizeThreshold的同时大于各 Partition 大小中位数与该因子的乘积，或者行数大于spark.sql.adaptive.skewedPartitionRowCountThreshold的同时大于各 Partition 行数中位数与该因子的乘积，则它会被视为倾斜的 Partition。
