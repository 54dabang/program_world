第 1 章 Spark SQL 背景
1．1 大数据与 Spark 系统
1．2 关系模型与 SQL 语言
1．3 Spark SQL 发展历程
1．4 本章小结
第 2 章 Spark 基础知识介绍
2．1 RDD 编程模型
2．2 DataFrame 与 Dataset
2．3 本章小结
第 3 章 Spark SQL 执行全过程概述
3．1 从 SQL 到 RDD：一个简单的案例
3．2 重要概念
3．2．1 InternalRow 体系
3．2．2 TreeNode 体系
3．2．3 Expression 体系
3．3 内部数据类型系统
3．4 本章小结
第 4 章 Spark SQL 编译器 Parser
4．1 DSL 工具之 ANTLR 简介
4．1．1 基于 ANTLR 4 的计算器
4．1．2 访问者模式
4．2 SparkSqlParser 之 AstBuilder
4．3 常见 SQL 生成的抽象语法树概览
4．4 本章小结
第 5 章 Spark SQL 逻辑计划（LogicalPlan）
5．1 Spark SQL 逻辑计划概述
5．2 LogicalPlan 简介
5．2．1 QueryPlan 概述
5．2．2 LogicalPlan 基本操作与分类
5．2．3 LeafNode 类型的 LogicalPlan
5．2．4 UnaryNode 类型的 LogicalPlan
5．2．5 BinaryNode 类型的 LogicalPlan
5．2．6 其他类型的 LogicalPlan
5．3 AstBuilder 机制：Unresolved LogicalPlan 生成
5．4 Analyzer 机制：Analyzed LogicalPlan 生成
5．4．1 Catalog 体系分析
5．4．2 Rule 体系
5．4．3 Analyzed LogicalPlan 生成过程
5．5 Spark SQL 优化器 Optimizer
5．5．1 Optimizer 概述
5．5．2 Optimizer 规则体系
5．5．3 Optimized LogicalPlan 的生成过程
5．6 本章小结
第 6 章 Spark SQL 物理计划（PhysicalPlan）
6．1 Spark SQL 物理计划概述
6．2 SparkPlan 简介
6．2．1 LeafExecNode 类型
6．2．2 UnaryExecNode 类型
6．2．3 BinaryExecNode 类型
6．2．4 其他类型的 SparkPlan
6．3 Metadata 与 Metrics 体系
6．4 Partitioning 与 Ordering 体系
6．4．1 Distribution 与 Partitioning 的概念
6．4．2 SparkPlan 的常用分区排序操作
6．5 SparkPlan 生成
6．5．1 物理计划 Strategy 体系
6．5．2 常见 Strategy 分析
6．6 执行前的准备
6．6．1 PlanSubqueries 规则
6．6．2 EnsureRequirements 规则
6．7 本章小结
第 7 章 Spark SQL 之 Aggregation 实现
7．1 Aggregation 执行概述
7．1．1 文法定义
7．1．2 聚合语句 Unresolved LogicalPlan 生成
7．1．3 从逻辑算子树到物理算子树
7．2 聚合函数（AggregateFunction）
7．2．1 聚合缓冲区与聚合模式（AggregateMode）
7．2．2 DeclarativeAggregate 聚合函数
7．2．3 ImperativeAggregate 聚合函数
7．2．4 TypedImperativeAggregate 聚合函数
7．3 聚合执行
7．3．1 执行框架 AggregationIterator
7．3．2 基于排序的聚合算子 SortAggregateExec
7．3．3 基于 Hash 的聚合算子 HashAggregateExec
7．4 窗口（Window）函数
7．4．1 窗口函数定义与简介
7．4．2 窗口函数相关表达式
7．4．3 窗口函数的逻辑计划阶段与物理计划阶段
7．4．4 窗口函数的执行
7．5 多维分析
7．5．1 OLAP 多维分析背景
7．5．2 Spark SQL 多维查询
7．5．3 多维分析 LogicalPlan 阶段
7．5．4 多维分析 PhysicalPlan 与执行
7．6 本章小结
第 8 章 Spark SQL 之 Join 实现
8．1 Join 查询概述
8．2 文法定义与抽象语法树
8．3 Join 查询逻辑计划
8．3．1 从 AST 到 Unresolved LogicalPlan
8．3．2 从 Unresolve LogicalPlan 到 Analyzed LogicalPlan
8．3．3 从 Analyzed LogicalPlan 到 Optimized LogicalPlan
8．4 Join 查询物理计划
8．4．1 Join 物理计划的生成
8．4．2 Join 物理计划的选取
8．5 Join 查询执行
8．5．1 Join 执行基本框架
8．5．2 BroadcastJoinExec 执行机制
8．5．3 ShuffledHashJoinExec 执行机制
8．5．4 SortMergeJoinExec 执行机制
8．6 本章小结
第 9 章 Tungsten 技术实现
9．1 内存管理与二进制处理
9．1．1 Spark 内存管理基础
9．1．2 Tungsten 内存管理优化基础
9．1．3 Tungsten 内存优化应用
9．2 缓存敏感计算（Cache-aware computation）
9．3 动态代码生成（Code generation）
9．3．1 漫谈代码生成
9．3．2 Janino 编译器实践
9．3．3 基本（表达式）代码生成
9．3．4 全阶段代码生成（WholeStageCodegen）
9．4 本章小结
第 10 章 Spark SQL 连接 Hive
10．1 Spark SQL 连接 Hive 概述
10．2 Hive 相关的规则和策略
10．2．1 HiveSessionCatalog 体系
10．2．2 Analyzer 之 Hive-Specific 分析规则
10．2．3 SparkPlanner 之 Hive-Specific 转换策略
10．2．4 Hive 相关的任务执行
10．3 Spark SQL 与 Hive 数据类型
10．3．1 Hive 数据类型与 SerDe 框架
10．3．2 DataTypeToInspector 与 Data Wrapping
10．3．3 InspectorToDataType 与 Data Unwrapping
10．4 Hive UDF 管理机制
10．5 Spark Thrift Server 实现
10．5．1 Service 体系
10．5．2 Operation 与 OperationManager
10．5．3 Session 与 SessionManager
10．5．4 Authentication 安全认证管理
10．5．5 Spark Thrift Server 执行流程
10．6 本章小结
第 11 章 Spark SQL 开发与实践
11．1 腾讯大数据平台（TDW）简介
11．2 腾讯大数据平台 SQL 引擎（TDW-SQL-Engine）
11．2．1 SQL-Engine 背景与演化历程
11．2．2 SQL-Engine 整体架构
11．3 TDW-Spark SQL 开发与优化
11．3．1 业务运行支撑框架
11．3．2 新功能开发案例
11．3．3 性能优化开发案例
11．4 业务实践经验与教训
11．4．1 Spark SQL 集群管理的经验
11．4．2 Spark SQL 业务层面调优
11．4．3 SQL 写法的“陷阱”

第1章 初识Spark SQL
1.1 Spark SQL的前世今生
1.2 Spark SQL能做什么
第2章 Spark安装、编程环境搭建以及打包提交
2.1 Spark的简易安装
2.2 准备编写Spark应用程序的IDEA环境
2.3 将编写好的Spark应用程序打包成jar提交到Spark上
第二部分 基础篇
第3章 Spark上的RDD编程
3.1 RDD基础
3.2 RDD简单实例—wordcount
3.3 创建RDD
3.4 RDD操作
3.5 向Spark传递函数
3.6 常见的转化操作和行动操作
3.7 深入理解RDD
3.8 RDD缓存、持久化
3.9 RDD checkpoint容错机制
第4章 Spark SQL编程入门
4.1 Spark SQL概述
4.2 Spark SQL编程入门示例
第5章 Spark SQL的DataFrame操作大全
5.1 由JSON文件生成所需的DataFrame对象
5.2 DataFrame上的行动操作
5.3 DataFrame上的转化操作
第6章 Spark SQL支持的多种数据源
6.1 概述
6.2 典型结构化数据源
第三部分 实战篇
第7章 Spark SQL工程实战之基于WiFi探针的商业大数据分析技术
7.1 功能需求
7.2 系统架构
7.3 功能设计
7.4 数据库结构
7.5 本章小结
第8章 第一个Spark SQL应用程序
8.1 完全分布式环境搭建
8.2 数据清洗
8.3 数据处理流程
8.4 Spark程序远程调试
8.5 Spark的Web界面
8.6 本章小结
第四部分 优化篇
第9章 让Spark程序再快一点
9.1 Spark执行流程
9.2 Spark内存简介
9.3 Spark的一些概念
9.4 Spark编程四大守则
9.5 Spark调优七式
9.6 解决数据倾斜问题
9.7 Spark执行引擎Tungsten简介
9.8 Spark SQL解析引擎Catalyst简介

1.1Spark SQL概述
1.1.1Spark SQL与DataFrame
1.1.2DataFrame与RDD的差异
1.1.3Spark SQL的发展历程
1.2从零起步掌握Hive
1.2.1Hive的本质是什么
1.2.2Hive安装和配置
1.2.3使用Hive分析搜索数据
1.3Spark SQL on Hive安装与配置
1.3.1安装Spark SQL
1.3.2安装MySQL
1.3.3启动Hive Metastore
1.4Spark SQL初试
1.4.1通过spark-shell来使用Spark SQL
1.4.2Spark SQL的命令终端
1.4.3Spark的Web UI
1.5本章小结
第2章DataFrame原理与常用操作
2.1DataFrame编程模型
2.2DataFrame基本操作实战
2.2.1数据准备
2.2.2启动交互式界面
2.2.3数据处理与分析
2.3通过RDD来构建DataFrame
2.4缓存表（列式存储）
2.5DataFrame API应用示例
2.6本章小结
第3章Spark SQL 操作多种数据源
3.1通用的加载/保存功能
3.1.1Spark SQL加载数据
3.1.2Spark SQL保存数据
3.1.3综合案例——电商热销商品排名
3.2Spark SQL操作Hive示例
3.3Spark SQL操作JSON数据集示例
3.4Spark SQL操作HBase示例
3.5Spark SQL操作MySQL示例
3.5.1安装并启动MySQL
3.5.2准备数据表
3.5.3操作MySQL表
3.6Spark SQL操作MongoDB示例
3.6.1安装配置MongoDB
3.6.2启动MongoDB
3.6.3准备数据
3.6.4Spark SQL操作MongoDB
3.7本章小结
第4章Parquet列式存储
4.1Parquet概述
4.1.1Parquet的基本概念
4.1.2Parquet数据列式存储格式应用举例
4.2Parquet的Block配置及数据分片
4.2.1Parquet的Block的配置
4.2.2Parquet 内部的数据分片
4.3Parquet序列化
4.3.1Spark实施序列化的目的
4.3.2Parquet两种序列化方式
4.4本章小结
第5章Spark SQL内置函数与窗口函数
5.1Spark SQL内置函数
5.1.1Spark SQL内置函数概述
5.1.2Spark SQL内置函数应用实例
5.2Spark SQL窗口函数
5.2.1Spark SQL窗口函数概述
5.2.2Spark SQL窗口函数分数查询统计案例
5.2.3Spark SQL窗口函数NBA常规赛数据统计案例
5.3本章小结
第6章Spark SQL UDF与UDAF
6.1UDF概述
6.2UDF示例
6.2.1Hobby_count函数
6.2.2Combine函数
6.2.3Str2Int函数
6.2.4Wsternstate函数
6.2.5ManyCustomers函数
6.2.6StateRegion函数
6.2.7DiscountRatio函数
6.2.8MakeStruct函数
6.2.9MyDateFilter函数
6.2.10MakeDT函数
6.3UDAF概述
6.4UDAF示例
6.4.1ScalaAggregateFunction函数
6.4.2GeometricMean函数
6.4.3CustomMean函数
6.4.4BelowThreshold函数
6.4.5YearCompare函数
6.4.6WordCount函数
6.5本章小结
第7章Thrift Server
7.1Thrift概述
7.1.1Thrift的基本概念
7.1.2Thrift的工作机制
7.1.3Thrift的运行机制
7.1.4一个简单的Thrift 实例
7.2Thrift Server的启动过程
7.2.1Thrift Sever启动详解
7.2.2HiveThriftServer2类的解析
7.3Beeline操作
7.3.1Beeline连接方式
7.3.2在Beeline中进行SQL查询操作
7.3.3通过Web控制台查看用户进行的操作
7.4Thrift Server应用示例
7.4.1示例源代码
7.4.2关键代码行解析
7.4.3测试运行
7.4.4运行结果解析
7.4.5Spark Web控制台查看运行日志
7.5本章小结
第8章Spark SQL综合应用案例
8.1综合案例实战——电商网站日志多维度数据分析
8.1.1数据准备
8.1.2数据说明
8.1.3数据创建
8.1.4数据导入
8.1.5数据测试和处理
8.2综合案例实战——电商网站搜索排名统计
8.2.1案例概述
8.2.2数据准备
8.2.3实现用户每天搜索前3名的商品排名统计
8.3本章小结

1.1 Spark SQL的前世今生 3
1.2 Spark SQL能做什么 4
第2章 Spark安装、编程环境搭建以及打包提交 6
2.1 Spark的简易安装 6
2.2 准备编写Spark应用程序的IDEA环境 10
2.3 将编写好的Spark应用程序打包成jar提交到Spark上 18
第二部分 基础篇
第3章 Spark上的RDD编程 23
3.1 RDD基础 24
3.1.1 创建RDD 24
3.1.2 RDD转化操作、行动操作 24
3.1.3 惰性求值 25
3.1.4 RDD缓存概述 26
3.1.5 RDD基本编程步骤 26
3.2 RDD简单实例—wordcount 27
3.3 创建RDD 28
3.3.1 程序内部数据作为数据源 28
3.3.2 外部数据源 29
3.4 RDD操作 33
3.4.1 转化操作 34
3.4.2 行动操作 37
3.4.3 惰性求值 38
3.5 向Spark传递函数 39
3.5.1 传入匿名函数 39
3.5.2 传入静态方法和传入方法的引用 40
3.5.3 闭包的理解 41
3.5.4 关于向Spark传递函数与闭包的总结 42
3.6 常见的转化操作和行动操作 42
3.6.1 基本RDD转化操作 43
3.6.2 基本RDD行动操作 48
3.6.3 键值对RDD 52
3.6.4 不同类型RDD之间的转换 56
3.7 深入理解RDD 57
3.8 RDD 缓存、持久化 59
3.8.1 RDD缓存 59
3.8.2 RDD持久化 61
3.8.3 持久化存储等级选取策略 63
3.9 RDD checkpoint容错机制 64
第4章 Spark SQL编程入门 66
4.1 Spark SQL概述 66
4.1.1 Spark SQL是什么 66
4.1.2 Spark SQL通过什么来实现 66
4.1.3 Spark SQL 处理数据的优势 67
4.1.4 Spark SQL数据核心抽象——DataFrame 67
4.2 Spark SQL编程入门示例 69
4.2.1 程序主入口：SparkSession 69
4.2.2 创建 DataFrame 70
4.2.3 DataFrame基本操作 70
4.2.4 执行SQL查询 72
4.2.5 全局临时表 73
4.2.6 Dataset 73
4.2.7 将RDDs转化为DataFrame 75
4.2.8 用户自定义函数 78
第5章 Spark SQL的DataFrame操作大全 82
5.1 由JSON文件生成所需的DataFrame对象 82
5.2 DataFrame上的行动操作 84
5.3 DataFrame上的转化操作 91
5.3.1 where条件相关 92
5.3.2 查询指定列 94
5.3.3 思维开拓：Column的巧妙应用 99
5.3.4 limit操作 102
5.3.5 排序操作：order by和sort 103
5.3.6 group by操作 106
5.3.7 distinct、dropDuplicates去重操作 107
5.3.8 聚合操作 109
5.3.9 union合并操作 110
5.3.10 join操作 111
5.3.11 获取指定字段统计信息 114
5.3.12 获取两个DataFrame中共有的记录 116
5.3.13 获取一个DataFrame中有另一个DataFrame中没有的记录 116
5.3.14 操作字段名 117
5.3.15 处理空值列 118
第6章 Spark SQL支持的多种数据源 121
6.1 概述 121
6.1.1 通用load/save 函数 121
6.1.2 手动指定选项 123
6.1.3 在文件上直接进行SQL查询 123
6.1.4 存储模式 123
6.1.5 持久化到表 124
6.1.6 bucket、排序、分区操作 124
6.2 典型结构化数据源 125
6.2.1 Parquet 文件 125
6.2.2 JSON 数据集 129
6.2.3 Hive表 130
6.2.4 其他数据库中的数据表 133
第三部分 实践篇
第7章 Spark SQL 工程实战之基于WiFi探针的商业大数据分析技术 139
7.1 功能需求 139
7.1.1 数据收集 139
7.1.2 数据清洗 140
7.1.3 客流数据分析 141
7.1.4 数据导出 142
7.2 系统架构 142
7.3 功能设计 143
7.4 数据库结构 144

Title Page
Copyright
Learning Spark SQL
Credits
About the Author
About the Reviewer
www.PacktPub.com
Why subscribe?
Customer Feedback
Preface
What this book covers
What you need for this book
Who this book is for
Conventions
Reader feedback
Customer support
Downloading the example code
Downloading the color images of this book
Errata
Piracy
Questions
Getting Started with Spark SQL
What is Spark SQL?
Introducing SparkSession
Understanding Spark SQL concepts
Understanding Resilient Distributed Datasets (RDDs)
Understanding DataFrames and Datasets
Understanding the Catalyst optimizer
Understanding Catalyst optimizations
Understanding Catalyst transformations
Introducing Project Tungsten
Using Spark SQL in streaming applications
Understanding Structured Streaming internals
Summary
Using Spark SQL for Processing Structured and Semistructured Data
Understanding data sources in Spark applications
Selecting Spark data sources
Using Spark with relational databases
Using Spark with MongoDB (NoSQL database)
Using Spark with JSON data
Using Spark with Avro files
Using Spark with Parquet files
Defining and using custom data sources in Spark
Summary
Using Spark SQL for Data Exploration
Introducing Exploratory Data Analysis (EDA)
Using Spark SQL for basic data analysis
Identifying missing data
Computing basic statistics
Identifying data outliers
Visualizing data with Apache Zeppelin
Sampling data with Spark SQL APIs
Sampling with the DataFrame/Dataset API
Sampling with the RDD API
Using Spark SQL for creating pivot tables
Summary
Using Spark SQL for Data Munging
Introducing data munging
Exploring data munging techniques
Pre-processing of the?household electric consumption Dataset
Computing basic statistics and aggregations
Augmenting the Dataset
Executing other miscellaneous processing steps
Pre-processing of?the weather Dataset
Analyzing missing data
Combining data using a JOIN operation
Munging textual data
Processing multiple input data files
Removing stop words
Munging time series data
Pre-processing of the?time-series Dataset
Processing date fields
Persisting and loading data
Defining a date-time index
Using the??TimeSeriesRDD?object
Handling missing time-series data
Computing basic statistics
Dealing with variable length records
Converting variable-length records to fixed-length records
Extracting data from "messy" columns
Preparing data for machine learning
Pre-processing data for machine learning
Creating and running a machine learning pipeline
Summary
Using Spark SQL in Streaming Applications
Introducing streaming data applications
Building Spark streaming applications
Implementing sliding window-based functionality
Joining a streaming Dataset with a static Dataset
Using the Dataset API in Structured Streaming
Using output sinks
Using the Foreach Sink for arbitrary computations on output
Using the Memory Sink to save output to a table
Using the File Sink to save output to a partitioned table
Monitoring streaming queries
Using Kafka with Spark Structured Streaming
Introducing Kafka concepts
Introducing ZooKeeper concepts
Introducing Kafka-Spark integration
Introducing Kafka-Spark Structured Streaming
Writing a receiver for a custom data source
Summary
Using Spark SQL in Machine Learning Applications
Introducing machine learning applications
Understanding Spark ML pipelines and their components
Understanding the steps in a pipeline application development process
Introducing feature engineering
Creating new features from raw data
Estimating the importance of a feature
Understanding dimensionality reduction
Deriving good features
Implementing a Spark ML classification model
Exploring the diabetes Dataset
Pre-processing the data
Building the Spark ML pipeline
Using StringIndexer for indexing categorical features and labels
Using VectorAssembler for assembling features into one column
Using a Spark ML classifier
Creating a Spark ML pipeline
Creating the training and test Datasets
Making predictions using the PipelineModel
Selecting the best model
Changing the ML algorithm in the pipeline
Introducing Spark ML tools and utilities
Using Principal Component Analysis to select features
Using encoders
Using Bucketizer
Using VectorSlicer
Using Chi-squared selector
Using a Normalizer
Retrieving our original labels
Implementing a Spark ML clustering model
Summary
Using Spark SQL in Graph Applications
Introducing large-scale graph applications
Exploring graphs using GraphFrames
Constructing a GraphFrame
Basic graph queries and operations
Motif analysis using GraphFrames
Processing subgraphs
Applying graph algorithms
Saving and loading GraphFrames
Analyzing JSON input modeled as a graph?
Processing graphs containing multiple types of relationships
Understanding GraphFrame internals
Viewing GraphFrame physical execution plan
Understanding partitioning in GraphFrames
Summary
Using Spark SQL with SparkR
Introducing SparkR
Understanding the SparkR architecture
Understanding SparkR DataFrames
Using SparkR for EDA and data munging tasks
Reading and writing Spark DataFrames
Exploring structure and contents of Spark DataFrames
Running basic operations on Spark DataFrames
Executing SQL statements on Spark DataFrames
Merging SparkR DataFrames
Using User Defined Functions (UDFs)
Using SparkR for computing summary statistics
Using SparkR for data visualization
Visualizing data on a map
Visualizing graph nodes and edges
Using SparkR for machine learning
Summary
Developing Applications with Spark SQL
Introducing Spark SQL applications
Understanding text analysis applications
Using Spark SQL for textual analysis
Preprocessing textual data
Computing readability
Using word lists
Creating data preprocessing pipelines
Understanding themes in document corpuses
Using Naive Bayes classifiers
Developing a machine learning application
Summary
Using Spark SQL in Deep Learning Applications
Introducing neural networks
Understanding deep learning
Understanding representation learning
Understanding stochastic gradient descent
Introducing deep learning in Spark
Introducing CaffeOnSpark
Introducing DL4J
Introducing TensorFrames
Working with BigDL
Tuning hyperparameters of deep learning models
Introducing deep learning pipelines
Understanding Supervised learning
Understanding convolutional neural networks
Using neural networks for text classification
Using deep neural networks for language processing
Understanding Recurrent Neural Networks
Introducing autoencoders
Summary
Tuning Spark SQL Components for Performance
Introducing performance tuning in Spark SQL
Understanding DataFrame/Dataset APIs
Optimizing data serialization
Understanding Catalyst optimizations
Understanding the Dataset/DataFrame API
Understanding Catalyst transformations
Visualizing Spark application execution
Exploring Spark application execution metrics
Using external tools for performance tuning
Cost-based optimizer in Apache Spark 2.2
Understanding the?CBO statistics collection
Statistics collection functions
Filter operator
Join operator
Build side selection
Understanding multi-way JOIN ordering optimization
Understanding performance improvements using whole-stage code generation
Summary
Spark SQL in Large-Scale Application Architectures
Understanding Spark-based application architectures
Using Apache Spark for batch processing
Using Apache Spark for stream processing
Understanding the Lambda architecture
Understanding the Kappa Architecture
Design considerations for building scalable stream processing applications
Building robust ETL pipelines using Spark SQL
Choosing appropriate data formats
Transforming data in ETL pipelines
Addressing errors in ETL pipelines
Implementing a scalable monitoring solution
Deploying Spark machine learning pipelines
Understanding the challenges in typical ML deployment environments
Understanding types of model scoring architectures
Using cluster managers
Summary

=============

第1章 Spark SQL 概述
1.1 什么是 Spark SQL
1.2 RDD vs DataFrames vs DataSet
第2章 执行 Spark SQL 查询
2.1 命令行查询流程
2.2 IDEA 创建 Spark SQL 程序
第3章 Spark SQL 解析
3.1 新的起始点 SparkSession
3.2 创建 DataFrames
3.3 DataFrame 常用操作
3.4 创建 DataSet
3.5 DataFrame 和 RDD 互操作
3.6 类型之间的转换总结
3.7 用户自定义函数
第4章 Spark SQL 数据源
4.1 通用加载/保存方法
4.2 Parquet 文件
4.3 Hive 数据库
4.4 JSON 数据集
4.5 JDBC
第5章 JDBC/ODBC 服务器
第6章 运行 Spark SQL CLI
第7章 Spark SQL 实战
7.1 数据说明
7.2 加载数据
7.3 计算所有订单中每年的销售单数、销售总额
7.4 计算所有订单每年最大金额订单的销售额
7.5 计算所有订单中每年最畅销货品


第1章 Spark SQL 概述
1.1 什么是 Spark SQL
1.2 RDD vs DataFrames vs DataSet
1.2.1 RDD
1.2.2 DataFrame
1.2.3 DataSet
1.2.4 三者的共性
1.2.5 三者的区别
第2章 执行 Spark SQL 查询
2.1 命令行查询流程
2.2 IDEA 创建 Spark SQL 程序
第3章 Spark SQL 解析
3.1 新的起始点 SparkSession
3.2 创建 DataFrames
3.3 DataFrame 常用操作
3.3.1 DSL 风格语法
3.3.2 SQL 风格语法
3.4 创建 DataSet
3.5 DataFrame 和 RDD 互操作
3.5.1 通过反射的方式获取 Scheam
3.5.2 通过编程的方式设置 Schema(StructType)
3.6 类型之间的转换总结
3.7 用户自定义函数
3.7.1 用户自定义 UDF 函数
3.7.2 用户自定义 UDAF 函数(即聚合函数)
第4章 Spark SQL 数据源
4.1 通用加载/保存方法
4.1.1 手动指定选项
4.1.2 文件保存选项
4.2 Parquet 文件
4.2.1 Parquet 读写
4.2.2 解析分区信息
4.2.3 Schema 合并
4.3 Hive 数据库
4.3.1 内嵌 Hive 应用
4.3.2 外部 Hive 应用
4.4 JSON 数据集
4.5 JDBC
第5章 JDBC/ODBC 服务器
第6章 运行 Spark SQL CLI
第7章 Spark SQL 实战
7.1 数据说明
7.2 加载数据
7.3 计算所有订单中每年的销售单数、销售总额
7.4 计算所有订单每年最大金额订单的销售额
7.5 计算所有订单中每年最畅销货品

回到顶部
第1章 Spark SQL 概述
1.1 什么是 Spark SQL
Spark SQL：http://spark.apache.org/sql/


Spark SQL 是 Spark 用来处理结构化数据的一个模块，它提供了一个编程抽象叫做 DataFrame，并且作为分布式 SQL 查询引擎的作用。
我们已经学习了 Hive，它是将 Hive SQL 转换成 MapReduce 然后提交到集群上执行，大大简化了编写 MapReduce 的程序的复杂性，由于 MapReduce 这种计算模型执行效率比较慢。所以 Spark SQL 的应运而生，它是将 Spark SQL 转换成 RDD，然后提交到集群执行，执行效率非常快！

Spark SQL 的特点：
  1、易整合(易集成)
  2、统一的数据访问方式
  3、兼容 Hive
  4、标准的数据连接


Spark SQL我们要学什么？SparkSQL 可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。


1.2 RDD vs DataFrames vs DataSet
Spark SQL 的数据抽象


在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看：RDD(Spark1.0) —> DataFrame(Spark1.3) —> DataSet(Spark1.6)
如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。
在后期的 Spark 版本中，DataSet 会逐步取代 RDD 和 DataFrame 成为唯一的 API 接口。

1.2.1 RDD
RDD 弹性分布式数据集，Spark 计算的基石，为用户屏蔽了底层对数据的复杂抽象和处理，为用户提供了一组方便的数据转换与求值方法。
RDD 是一个懒执行的不可变的可以支持 Lambda 表达式的并行数据集合。
RDD 的最大好处就是简单，API 的人性化程度很高。
RDD 的劣势是性能限制，它是一个 JVM 驻内存对象，这也就决定了存在 GC 的限制和数据增加时 Java 序列化成本的升高。
RDD 例子如下：


1.2.2 DataFrame
  与 RDD 类似，DataFrame 也是一个分布式数据容器。然而 DataFrame 更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即 schema。同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。由于与 R 和 Pandas 的 DataFrame 类似，Spark DataFrame 很好地继承了传统单机数据分析的开发体验。


  上图直观地体现了 DataFrame 和 RDD 的区别。左侧的 RDD[Person] 虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame 多了数据的结构信息，即 schema。RDD 是分布式的 Java对象 的集合。DataFrame 是分布式的 Row对象 的集合。DataFrame 除了提供了比 RDD 更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如 filter 下推、裁剪等。


DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待。
DataFrame 也是懒执行的。
性能上比 RDD 要高，主要有两方面原因：
（1）定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了 GC 的限制。


（2）优化的执行计划：查询计划通过 Spark catalyst optimiser 进行优化。

比如下面一个例子：

人口数据分析的示例：

  为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个 DataFrame，将它们 join 之后又做了一次 filter 操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 join 是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将 filter 下推到 join 下方，先对 DataFrame 进行过滤，再 join 过滤后的较小的结果集，便可以有效缩短执行时间。而 Spark SQL 的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。
  得到的优化执行计划在转换成物理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中 filter 之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。
  对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。
  Dataframe 的劣势在于在编译期缺少类型安全检查，导致运行时出错。


1.2.3 DataSet
1）是 DataFrame API 的一个扩展，是 Spark 最新的数据抽象。
2）用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性。
3）DataSet 支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。
4）样例类被用来在 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到 DataSet 中的字段名称。
5）DataFrame 是 DataSet 的特列，type DataFrame = Dataset[Row] ，所以可以通过 as 方法将 DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。
6）DataSet 是强类型的。比如可以有 Dataset[Car]，Dataset[Person]，DataFrame 只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个 String 进行减法操作，在执行的时候才报错，而 DataSet 不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟 JSON 对象和类对象之间的类比。


RDD 让我们能够决定怎么做，而 DataFrame 和 DataSet 让我们决定做什么，控制的粒度不一样。


1.2.4 三者的共性
1、RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利。
2、三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到 action，如 foreach 时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在 action 中使用对应的结果，在执行时会被直接跳过。

val sparkconf = new SparkConf().setMaster("local").setAppName("test").set("spark.port.maxRetries","1000")
val spark = SparkSession.builder().config(sparkconf).getOrCreate()
val rdd=spark.sparkContext.parallelize(Seq(("a", 1), ("b", 1), ("a", 1)))
// map 不运行
rdd.map { line =>
  println("运行")
  line._1
}
3、三者都会根据 spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。
4、三者都有 partition 的概念。
5、三者有许多共同的函数，如 filter，排序等。
6、在对 DataFrame 和 DataSet 进行许多操作都需要这个包进行支持

import spark.implicits._
7、DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型
DataFrame：

testDF.map {
      case Row(col1: String, col2: Int) =>
        println(col1)
        println(col2)
        col1
      case _=>
        ""
    }
DataSet：

case class Coltest(col1: String, col2: Int)extends Serializable // 定义字段名和类型
    testDS.map {
      case Coltest(col1: String, col2: Int) =>
        println(col1)
        println(col2)
        col1
      case _=>
        ""
    }
1.2.5 三者的区别
RDD：
1、RDD 一般和 spark mlib 同时使用
2、RDD 不支持 sparksql 操作

DataFrame：
1、与 RDD 和 DataSet 不同，DataFrame 每一行的类型固定为 Row，只有通过解析才能获取各个字段的值，如

testDF.foreach{
  line =>
    val col1=line.getAs[String]("col1")
    val col2=line.getAs[String]("col2")
}
每一列的值没法直接访问
2、DataFrame 与 DataSet 一般与 spark ml 同时使用
3、DataFrame 与 DataSet 均支持 sparksql 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作，如

dataDF.createOrReplaceTempView("tmp")
spark.sql("select ROW, DATE from tmp where DATE is not null order by DATE").show(100, false)
4、DataFrame 与 DataSet 支持一些特别方便的保存方式，比如 保存成 csv，可以带上表头，这样每一列的字段名一目了然

// 保存
val saveoptions = Map("header" -> "true", "delimiter" -> "\t", "path" -> "hdfs://hadoop102:9000/test")
datawDF.write.format("com.atguigu.spark.csv").mode(SaveMode.Overwrite).options(saveoptions).save()

// 读取
val options = Map("header" -> "true", "delimiter" -> "\t", "path" -> "hdfs://hadoop102:9000/test")
val datarDF= spark.read.options(options).format("com.atguigu.spark.csv").load()
利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。

DataSet：
DataSet 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。
DataFrame 也可以叫 Dataset[Row]，即每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。
而 DataSet 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息。

case class Coltest(col1: String, col2: Int) extends Serializable // 定义字段名和类型
/**
 rdd
 ("a", 1)
 ("b", 1)
 ("a", 1)
**/
val test: Dataset[Coltest] = rdd.map { line =>
      Coltest(line._1, line._2)
    }.toDS
    test.map{
      line =>
        println(line.col1)
        println(line.col2)
    }
可以看出，DataSet 在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用 DataSet，行的类型又不确定，可能是各种 case class，无法实现适配，这时候用 DataFrame，即 Dataset[Row] 就能比较好的解决问题。

回到顶部
第2章 执行 Spark SQL 查询
2.1 命令行查询流程
打开 spark-shell
例子：查询大于 30 岁的用户
创建如下 JSON 文件，注意 JSON 的格式：

{"name":"Michael", "age":30}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
操作步骤如下：


2.2 IDEA 创建 Spark SQL 程序
Spark SQL 在 IDEA 中程序的打包和运行方式都和 Spark Core 类似，Maven 依赖中需要添加新的依赖项：

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>${spark.version}</version>
            <!-- provided 表示编译期可用，运行期不可用 -->
            <!--<scope>provided</scope>-->
        </dependency>
程序如下：

package com.atguigu.sparksql

import org.apache.spark.sql.SparkSession
import org.slf4j.LoggerFactory

object HelloWorld {

  val logger = LoggerFactory.getLogger(HelloWorld.getClass)

  def main(args: Array[String]) {
    // 创建 SparkSession 并设置 App 名称
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    // 通过隐式转换将 RDD 操作添加到 DataFrame 上
    import spark.implicits._

    // 通过 spark.read 操作读取 JSON 数据
    val df = spark.read.json("examples/src/main/resources/people.json")

    // show 操作类似于 Action，将 DataFrame 直接打印到 Console 上
    df.show()

    // DSL 风格的使用方式：属性的获取方法 $
    df.filter($"age" > 21).show()

    //将 DataFrame 注册为表
    df.createOrReplaceTempView("persons")

    // 执行 Spark SQL 查询操作
    spark.sql("select * from perosns where age > 21").show()

    // 关闭资源
    spark.stop()
  }
}
回到顶部
第3章 Spark SQL 解析
3.1 新的起始点 SparkSession
在老的版本中，SparkSQL 提供两种 SQL 查询起始点，一个叫 SQLContext，用于 Spark 自己提供的 SQL 查询，一个叫 HiveContext，用于连接 Hive 的查询，SparkSession 是 Spar k最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext 的组合，所以在 SQLContext 和HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。SparkSession 内部封装了 SparkContext，所以计算实际上是由 SparkContext 完成的。

    import org.apache.spark.sql.SparkSession

    // 创建 SparkSession 并设置 App 名称
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    // 通过隐式转换将 RDD 操作添加到 DataFrame 上
    import spark.implicits._
SparkSession.builder 用于创建一个 SparkSession。
import spark.implicits._ 的引入是用于将 DataFrames 隐式转换成 RDD，使 df 能够使用 RDD 中的方法。
如果需要 Hive 支持，则需要以下创建语句：

    import org.apache.spark.sql.SparkSession

    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .enableHiveSupport()
      .getOrCreate()

    // For implicit conversions like converting RDDs to DataFrames
    import spark.implicits._
3.2 创建 DataFrames
在 Spark SQL 中 SparkSession 是创建 DataFrames 和执行 SQL 的入口，创建 DataFrames 有三种方式，一种是可以从一个存在的 RDD 进行转换，还可以从 Hive Table 进行查询返回，或者通过 Spark 的数据源进行创建。

1、从 Spark 数据源进行创建：

val df = spark.read.json("examples/src/main/resources/people.json")
// Displays the content of the DataFrame to stdout
df.show()

+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+
2、从 RDD 进行转换：

/**
Michael, 29
Andy, 30
Justin, 19
**/
scala> val personRdd = sc.textFile("examples/src/main/resources/people.txt")
personRdd: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at <console>:24

// 把每一行的数据用 "," 隔开，然后通过第二个 map 转换成一个 Array 再通过 toDF 映射给 name 和 age
scala> val personDF3 = personRdd.map(_.split(",")).map(paras => (paras(0).trim(), paras(1).trim().toInt)).toDF("name", "age")
personDF3: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> personDF3.collect
res0: Array[org.apache.spark.sql.Row] = Array([Michael,29], [Andy,30], [Justin,19])

scala> personDF.show()

+-------+---+
|   name|age|
+-------+---+
|Michael| 29|
|   Andy| 30|
| Justin| 19|
+-------+---+
3、从 Hive Table 进行查询返回，我们在数据源章节介绍。

3.3 DataFrame 常用操作
3.3.1 DSL 风格语法
// This import is needed to use the $-notation
import spark.implicits._

// Print the schema in a tree format
df.printSchema()

root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show()

+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+

// Select everybody, but increment the age by 1
df.select($"name", $"age" + 1).show()

+-------+---------+
|   name|(age + 1)|
+-------+---------+
|Michael|       31|
|   Andy|       31|
| Justin|       20|
+-------+---------+

// Select person older than 21
df.filter($"age" > 21).show()

+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
+---+-------+

// Count person by age
df.groupBy("age").count().show()

+---+-----+
|age|count|
+---+-----+
| 19|    1|
| 30|    2|
+---+-----+
3.3.2 SQL 风格语法
// Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("persons")

val sqlDF = spark.sql("SELECT * FROM persons")
sqlDF.show()

+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+

// Register the DataFrame as a global temporary view
df.createGlobalTempView("persons")

// Global temporary view is tied to a system preserved database `global_temp`
spark.sql("SELECT * FROM global_temp.persons").show()

+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+

// Global temporary view is cross-session
spark.newSession().sql("SELECT * FROM global_temp.persons").show()

+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+
临时表是 Session 范围内的，Session 退出后，表就失效了。如果想应用范围内有效，可以使用全局表。
注意：使用全局表时需要全路径访问，如：global_temp.persons

3.4 创建 DataSet
DataSet 是具有强类型的数据集合，需要提供对应的类型信息。

scala> case class Person(name: String, age: Long)
defined class Person

scala> val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]

scala> caseClassDS.show()

+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+

scala> val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> primitiveDS.map(_ + 1).collect()
res1: Array[Int] = Array(2, 3, 4)

scala> val path = "examples/src/main/resources/people.json"
path: String = examples/src/main/resources/people.json

scala> val peopleDS = spark.read.json(path).as[Person]
peopleDS: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]

scala> peopleDS.show()

+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+
3.5 DataFrame 和 RDD 互操作
Spark SQL 支持通过两种方式将存在的 RDD 转换为 DataSet，转换的过程中需要让 DataSet 获取 RDD 中的 Schema 信息。
主要有两种方式：
  第一种：是通过反射来获取 RDD 中的 Schema 信息，这种方式适合于列名已知的情况下。
  第二种：是通过编程接口的方式将 Schema 信息应用于 RDD，这种方式可以处理那种在运行时才能知道列的情况下。

3.5.1 通过反射的方式获取 Scheam
Spark SQL 能够自动将包含有 case 类的 RDD 转换成 DataFrame，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。case 类可以包含诸如 Seqs 或者 Array 等复杂的结构。

// For implicit conversions from RDDs to DataFrames
import spark.implicits._

// Create an RDD of Person objects from a text file, convert it to a Dataframe
val peopleDF = spark.sparkContext
  .textFile("examples/src/main/resources/people.txt")
  .map(_.split(","))
  .map(attributes => Person(attributes(0).trim(), attributes(1).trim().toInt))
  .toDF()

peopleDF.show

+-------+---+
|   name|age|
+-------+---+
|Michael| 29|
|   Andy| 30|
| Justin| 19|
+-------+---+

// Register the DataFrame as a temporary view
peopleDF.createOrReplaceTempView("people")

// SQL statements can be run by using the sql methods provided by Spark
val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 25")

// The columns of a row in the result can be accessed by field index
teenagersDF.map(teenager => "Name: " + teenager(0)).show() // 通过 row 对象的索引进行访问

+------------+
|       value|
+------------+
|Name: Justin|
+------------+

// or by field name 通过 row 对象的 getAs 方法访问
teenagersDF.map(teenager => "Name: " + teenager.getAs[Int](0)).show() //  以索引访问
teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show() // 以列名访问

// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// No pre-defined encoders for Dataset[Map[K,V]], define explicitly (没有为数据集 [Map [K，V]] 预定义的编码器明确定义)
implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]

// Primitive types and case classes can be also defined as (原始类型和样例类也可以定义为)
// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T] (row.getValuesMap [T] 一次检索多个列到 Map [String，T])
teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
// Array[Map[String,Any]] = Array(Map(name -> Justin, age -> 19))
3.5.2 通过编程的方式设置 Schema(StructType)
如果 case 类不能够提前定义，可以通过下面三个步骤定义一个 DataFrame，步骤如下：
  1、创建一个多行结构的 RDD。
  2、创建用 StructType 来表示的行结构信息。
  3、通过 SparkSession 提供的 createDataFrame 方法来应用 Schema。

import org.apache.spark.sql.types._

// Create an RDD
val peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")

// The schema is encoded in a string (schema 以字符串形式编码)
val schemaString = "name age" // 实际开发中 schemaString 是动态生成的

// Generate the schema based on the string of schema (根据 schema 字符串生成 schema)
// 把 name 和 age 都设置成 StringType 类型
val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true)) // Array[org.apache.spark.sql.types.StructField]
val schema = StructType(fields)

// 把 name 设置成 StringType 类型，把 age 设置成 IntegerType 类型
// val fields = schemaString.split(" ").map(fieldName => fieldName match {
//  case "name" => StructField(fieldName, StringType, nullable = true);
//  case "age" => StructField(fieldName, IntegerType, nullable = true)
//  }) // Array[org.apache.spark.sql.types.StructField]

// Convert records of the RDD (people) to Rows (将 RDD (people) 的记录转换为很多行)
import org.apache.spark.sql._
val rowRDD = peopleRDD.map(_.split(",")).map(attributes => Row(attributes(0).trim, attributes(1).trim)) // org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]

// Apply the schema to the RDD
val peopleDF = spark.createDataFrame(rowRDD, schema)
peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: string]  注意：此时的 name 和 age 都是 StringType 类型

// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("people")

// SQL can be run over a temporary view created using DataFrames
val results = spark.sql("SELECT name FROM people")

// The results of SQL queries are DataFrames and support all the normal RDD operations
// The columns of a row in the result can be accessed by field index or by field name (可以通过字段索引或字段名称访问结果中行的列)
results.map(attributes => "Name: " + attributes(0)).show()

+-------------+
|        value|
+-------------+
|Name: Michael|
|   Name: Andy|
| Name: Justin|
+-------------+

results.show

+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+
3.6 类型之间的转换总结
RDD、DataFrame、Dataset 三者有许多共性，有各自适用的场景常常需要在三者之间转换。
  1、RDD -> DataFrame ： rdd.map(para => (para(0).trim(), para(1).trim().toInt)).toDF("name", "age")
  2、DataFrame -> RDD ： df.rdd
  注意输出类型：res2: Array[org.apache.spark.sql.Row] = Array([Michael,29], [Andy,30], [Justin,19])
  1、RDD -> DataSet ： rdd.map(para => Person(para(0).trim(), para(1).trim().toInt)).toDS
  2、DataSet -> RDD ： ds.rdd
  注意输出类型：res5: Array[Person] = Array(Person(Michael,29), Person(Andy,30), Person(Justin,19))
  1、DataFrame -> DataSet ： df.as[Person]
  2、DataSet -> DataFrame ： ds.toDF

小结：
DataFrame/Dataset 转 RDD：

val rdd1 = testDF.rdd
val rdd2 = testDS.rdd
RDD 转 DataFrame：

import spark.implicits._
val testDF = rdd.map { line =>
      (line._1, line._2)
    }.toDF("col1", "col2")
一般用元组把一行的数据写在一起，然后在 toDF 中指定字段名。

RDD 转 DataSet：

import spark.implicits._
case class Coltest(col1:String, col2:Int) extends Serializable // 定义字段名和类型
val testDS = rdd.map { line =>
      Coltest(line._1, line._2)
    }.toDS
可以注意到，定义每一行的类型 case class 时，已经给出了字段名和类型，后面只要往 case class 里面添加值即可。

Dataset 转 DataFrame：
这个也很简单，因为只是把 case class 封装成 Row。

import spark.implicits._
val testDF = testDS.toDF
DataFrame 转 DataSet：

import spark.implicits._
case class Coltest(col1:String, col2:Int) extends Serializable // 定义字段名和类型
val testDS = testDF.as[Coltest]
这种方法就是在给出每一列的类型后，使用 as 方法，转成 DataSet，这在数据类型是 DataFrame 又需要针对各个字段处理时极为方便。
在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然 toDF、toDS 无法使用。

3.7 用户自定义函数
通过 spark.udf 功能用户可以自定义函数。

3.7.1 用户自定义 UDF 函数
scala> val df = spark.read.json("examples/src/main/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.show()
+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+

scala> spark.udf.register("addName", (x: String) => "Name:" + x)
res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))

scala> df.createOrReplaceTempView("people")

scala> spark.sql("select addName(name), age from people").show()
+-----------------+---+
|UDF:addName(name)|age|
+-----------------+---+
|     Name:Michael| 30|
|        Name:Andy| 30|
|      Name:Justin| 19|
+-----------------+---+

scala> spark.sql("select addName(name) as newName, age from people").show()
+------------+---+
|     newName|age|
+------------+---+
|Name:Michael| 30|
|   Name:Andy| 30|
| Name:Justin| 19|
+------------+---+
3.7.2 用户自定义 UDAF 函数(即聚合函数)
强类型的 Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数，如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。

弱类型用户自定义聚合函数
通过继承 UserDefinedAggregateFunction 来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数：

package com.atguigu.spark

import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._

object MyAverage extends UserDefinedAggregateFunction {

  // 聚合函数输入参数的数据类型
  def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil) // :: 用于的是向队列的头部追加数据，产生新的列表

  // 聚合缓冲区中值的数据类型
  def bufferSchema: StructType = {
    StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil) // Nil 是一个空的 List，定义为 List[Nothing]
  }

  // 返回值的数据类型
  def dataType: DataType = DoubleType

  // 对于相同的输入是否一直返回相同的输出
  def deterministic: Boolean = true

  // 初始化
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    // 存工资的总额
    buffer(0) = 0L
    // 存工资的个数
    buffer(1) = 0L
  }

  // 相同 Execute 间的数据合并（同一分区）
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0)) {
      buffer(0) = buffer.getLong(0) + input.getLong(0)
      buffer(1) = buffer.getLong(1) + 1
    }
  }

  // 不同 Execute 间的数据合并（不同分区）
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }

  // 计算最终结果
  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)

  def main(args: Array[String]) {
    // 创建 SparkSession 并设置 App 名称
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      // .config("spark.some.config.option", "some-value")
        .master("local[*]") // 本地测试
      .getOrCreate()

    // For implicit conversions like converting RDDs to DataFrames
    spark.udf.register("myAverage", MyAverage)

    // val df = spark.read.json("examples/src/main/resources/employees.json")
    val df = spark.read.json("D:\\learn\\JetBrains\\workspace_idea\\spark\\doc\\employees.json")
    df.createOrReplaceTempView("employees")
    df.show()
    // +-------+------+
    // |   name|salary|
    // +-------+------+
    // |Michael|  3000|
    // |   Andy|  4500|
    // | Justin|  3500|
    // |  Berta|  4000|
    // +-------+------+

    val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
    result.show()
    // +--------------+
    // |average_salary|
    // +--------------+
    // |        3750.0|
    // +--------------+

    spark.stop()
  }
}
强类型用户自定义聚合函数
通过继承 Aggregator 来实现强类型自定义聚合函数，同样是求平均工资：

package com.atguigu.spark

import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.{Encoder, Encoders, SparkSession}

// 既然是强类型，可能有 case 类
case class Employee(name: String, salary: Long)

case class Average(var sum: Long, var count: Long)

// 其中 Employee 是在应用聚合函数的时候传入的对象，Average 是聚合函数在运行的时候内部需要的数据结构，Double 是聚合函数最终需要输出的类型
object MyAverage extends Aggregator[Employee, Average, Double] {

  // 定义一个数据结构，保存工资总数和工资总个数，初始都为0
  def zero: Average = Average(0L, 0L)

  // 相同 Execute 间的数据合并（同一分区）
  def reduce(buffer: Average, employee: Employee): Average = {
    buffer.sum += employee.salary
    buffer.count += 1
    buffer
  }

  // 聚合不同 Execute 的结果
  def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }

  // 计算最终结果
  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count

  // 设定之间值类型的编码器，要转换成 case 类
  // Encoders.product 是进行 scala 元组和 case 类转换的编码器
  def bufferEncoder: Encoder[Average] = Encoders.product

  // 设定最终输出值的编码器
  def outputEncoder: Encoder[Double] = Encoders.scalaDouble

  def main(args: Array[String]) {
    // 创建 SparkSession 并设置 App 名称
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      // .config("spark.some.config.option", "some-value")
      .master("local[*]") // 本地测试
      .getOrCreate()

    import spark.implicits._
    // For implicit conversions like converting RDDs to DataFrames
    // val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]
    val ds = spark.read.json("D:\\learn\\JetBrains\\workspace_idea\\spark\\doc\\employees.json").as[Employee]
    ds.show()
    // +-------+------+
    // |   name|salary|
    // +-------+------+
    // |Michael|  3000|
    // |   Andy|  4500|
    // | Justin|  3500|
    // |  Berta|  4000|
    // +-------+------+

    val averageSalary = MyAverage.toColumn.name("average_salary")
    val result = ds.select(averageSalary)
    result.show()
    // +--------------+
    // |average_salary|
    // +--------------+
    // |        3750.0|
    // +--------------+

    spark.stop()
  }
}
回到顶部
第4章 Spark SQL 数据源
4.1 通用加载/保存方法
4.1.1 手动指定选项
  Spark SQL 的 DataFrame 接口支持多种数据源的操作。一个 DataFrame 可以进行 RDDs 方式的操作，也可以被注册为临时表。把 DataFrame 注册为临时表之后，就可以对该 DataFrame 执行 SQL 查询。
  Spark SQL 的默认数据源为 Parquet 格式。数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作。修改配置项 spark.sql.sources.default，可修改默认数据源格式。示例代码如下：

val df = sqlContext.read.load("examples/src/main/resources/users.parquet")
df.select("name", "favorite_color").write.save("namesAndFavColors.parquet")
  当数据源格式不是 parquet 格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定 json, parquet, jdbc, orc, libsvm, csv, text 来指定数据的格式。
  可以通过 SparkSession 提供的 read.load 方法用于通用加载数据，使用 write 和 save 保存数据。 示例代码如下：

val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
peopleDF.write.format("parquet").save("hdfs://hadoop102:9000/namesAndAges.parquet")
  除此之外，还可以直接运行 SQL 在文件上。示例代码如下：

val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://hadoop102:9000/namesAndAges.parquet`")
sqlDF.show()
spark-shell 下的演示代码：

scala> val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json") // Spark SQL 的通用输入模式
peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> peopleDF.write.format("parquet").save("hdfs://hadoop102:9000/namesAndAges.parquet")  // Spark SQL 的通用输出模式

scala> peopleDF.show()
+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+

scala> val peopleDF = spark.read.json("examples/src/main/resources/people.json") // Spark SQL 的专业输入模式
peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> peopleDF.write.parquet("hdfs://hadoop102:9000/namesAndAges.parquet") // Spark SQL 的专业输出模式

scala> peopleDF.show()
+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+

scala> val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://hadoop102:9000/namesAndAges.parquet`")
19/04/27 21:32:55 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException
sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> sqlDF.show()
+---+-------+
|age|   name|
+---+-------+
| 30|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+
4.1.2 文件保存选项
  可以采用 SaveMode 执行存储操作，SaveMode 定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用 Overwrite 方式执行时，在输出新数据之前原数据就已经被删除。
  SaveMode 详细介绍如下表：


4.2 Parquet 文件
  Parquet 是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。


4.2.1 Parquet 读写
  Parquet 格式经常在 Hadoop 生态圈中被使用，它也支持 Spark SQL 的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。示例代码如下：

// Encoders for most common types are automatically provided by importing spark.implicits._
import spark.implicits._

val peopleDF = spark.read.json("examples/src/main/resources/people.json")

// DataFrames can be saved as Parquet files, maintaining the schema information
peopleDF.write.parquet("hdfs://hadoop102:9000/people.parquet")

// Read in the parquet file created above
// Parquet files are self-describing so the schema is preserved
// The result of loading a Parquet file is also a DataFrame
val parquetFileDF = spark.read.parquet("hdfs://hadoop102:9000/people.parquet")

// Parquet files can also be used to create a temporary view and then used in SQL statements
parquetFileDF.createOrReplaceTempView("parquetFile")
val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 25")
namesDF.map(attributes => "Name: " + attributes(0)).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+
4.2.2 解析分区信息
  对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet 数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为 gender 和 country，使用下面的目录结构：

path
└── to
    └── table
        ├── gender=male
        │   ├── ...
        │   │
        │   ├── country=US
        │   │   └── data.parquet
        │   ├── country=CN
        │   │   └── data.parquet
        │   └── ...
        └── gender=female
            ├── ...
            │
            ├── country=US
            │   └── data.parquet
            ├── country=CN
            │   └── data.parquet
            └── ...
  通过传递 path/to/table 给 SQLContext.read.parquet 或 SQLContext.read.load，Spark SQL 将自动解析分区信息。返回的 DataFrame 的 Schema 如下：

root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
  需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为 true。如果想关闭该功能，直接将该参数设置为 disabled。此时，分区列数据格式将被默认设置为 String 类型，不再进行类型解析。

4.2.3 Schema 合并
  像 ProtocolBuffer、Avro 和 Thrift 那样，Parquet 也支持 Schema evolution（Schema 演变）。用户可以先定义一个简单的 Schema，然后逐渐的向 Schema 中增加列描述。通过这种方式，用户可以获取多个有不同 Schema 但相互兼容的 Parquet 文件。现在 Parquet 数据源能自动检测这种情况，并合并这些文件的 schemas。
  因为 Schema 合并是一个高消耗的操作，在大多数情况下并不需要，所以 Spark SQL 从 1.5.0 开始默认关闭了该功能。可以通过下面两种方式开启该功能：
  当数据源为 Parquet 文件时，将数据源选项 mergeSchema 设置为 true。
  设置全局 SQL 选项 spark.sql.parquet.mergeSchema 为 true。

示例代码如下：

// sqlContext from the previous example is used in this example.
// This is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Create a simple DataFrame, stored into a partition directory
val df1 = sc.makeRDD(1 to 5).map(i => (i, i * 2)).toDF("single", "double")
df1.write.parquet("hdfs://hadoop102:9000/data/test_table/key=1")

// Create another DataFrame in a new partition directory,
// adding a new column and dropping an existing column
val df2 = sc.makeRDD(6 to 10).map(i => (i, i * 3)).toDF("single", "triple")
df2.write.parquet("hdfs://hadoop102:9000/data/test_table/key=2")

// Read the partitioned table
val df3 = sqlContext.read.option("mergeSchema", "true").parquet("hdfs://hadoop102:9000/data/test_table")
df3.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths.
// root
// |-- single: int (nullable = true)
// |-- double: int (nullable = true)
// |-- triple: int (nullable = true)
// |-- key : int (nullable = true)
4.3 Hive 数据库
  Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF(用户自定义函数) 以及 Hive 查询语言 (HiveQL/HQL) 等。需要强调的一点是，如果要在 Spark SQL 中包含 Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译 Spark SQL 时引入 Hive 支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了对 Hive 支持。
  若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark 的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE) 语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中 (如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。示例代码如下：

import java.io.File

import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

case class Record(key: Int, value: String)

// warehouseLocation points to the default location for managed databases and tables
val warehouseLocation = new File("spark-warehouse").getAbsolutePath

val spark = SparkSession
  .builder()
  .appName("Spark Hive Example")
  .config("spark.sql.warehouse.dir", warehouseLocation)
  .enableHiveSupport()
  .getOrCreate()

import spark.implicits._
import spark.sql

sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

// Queries are expressed in HiveQL
sql("SELECT * FROM src").show()
// +---+-------+
// |key|  value|
// +---+-------+
// |238|val_238|
// | 86| val_86|
// |311|val_311|
// ......

// Aggregation queries are also supported.
sql("SELECT COUNT(*) FROM src").show()
// +--------+
// |count(1)|
// +--------+
// |    500 |
// +--------+

// The results of SQL queries are themselves DataFrames and support all normal functions.
val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")

// The items in DataFrames are of type Row, which allows you to access each column by ordinal.
val stringsDS = sqlDF.map {
  case Row(key: Int, value: String) => s"Key: $key, Value: $value"
}
stringsDS.show()
// +--------------------+
// |               value|
// +--------------------+
// |Key: 0, Value: val_0|
// |Key: 0, Value: val_0|
// |Key: 0, Value: val_0|
// ......

// You can also use DataFrames to create temporary views within a SparkSession.
val recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
recordsDF.createOrReplaceTempView("records")

// Queries can then join DataFrame data with data stored in Hive.
sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()
// +---+------+---+------+
// |key| value|key| value|
// +---+------+---+------+
// |  2| val_2|  2| val_2|
// |  4| val_4|  4| val_4|
// |  5| val_5|  5| val_5|
// ......
4.3.1 内嵌 Hive 应用
先做两个准备工作：
（1）为了方便以后的操作，我们先将 /opt/module/hive/conf 目录下的 hive-site.xml 和 /opt/module/hadoop-2.7.2/etc/hadoop 目录下的 core-site.xml、hdfs-site.xml 拷贝至 /opt/module/spark-2.1.1-bin-hadoop2.7 目录下，然后分发至其他机器节点。以后我们就操作 /opt/module/spark-2.1.1-bin-hadoop2.7 目录下的文件就好了！
（2）由于我们使用 /opt/module/spark-2.1.1-bin-hadoop2.7/bin/spark-sql 和 /opt/module/spark-2.1.1-bin-hadoop2.7/bin/spark-shell 时打出的日志很多，影响观看，所以我们修改下日志的输出级别 INFO 为 WARN，然后分发至其他机器节点。

[atguigu@hadoop102 conf]$ pwd
/opt/module/spark-2.1.1-bin-hadoop2.7/conf
[atguigu@hadoop102 conf]$ cp log4j.properties.template log4j.properties
[atguigu@hadoop102 conf]$ vim log4j.properties
[atguigu@hadoop102 conf]$ xsync log4j.properties

将 log4j.rootCategory=INFO, console 修改为 log4j.rootCategory=WARN, console
1、Spark 内置有 Hive，Spark 2.1.1 内置的 Hive 是 1.2.1。
2、如果要使用内嵌的 Hive，什么都不用做，直接用就可以了。但是呢，此时的我们只能创建表，且表放在本地的 spark-warehouse 目录中，如果查询表的话会报错，原因是：本地有 spark-warehouse 目录，而其他机器节点没有 spark-warehouse 目录，自然无法访问表了。
解决办法如下：需要将 core-site.xml 和 hdfs-site.xml 拷贝到 spark 的 conf 目录下，然后分发至其他机器节点。如果 spark 路径下发现有 metastore_db 和 spark-warehouse，删除掉。然后重启集群。
3、在你第一次启动创建 metastore 的时候，你需要指定 spark.sql.warehouse.dir 这个参数（Spark 2.x 版本的新内容）：
比如：bin/spark-shell --conf spark.sql.warehouse.dir=hdfs://hadoop102:9000/spark_warehouse，之后我们再使用 bin/spark-sql 就可以了。此时我们创建的表放在 HDFS 集群上，那么就可以查询表了。
4、注意：如果在 load 数据的时候，需要将数据放到 HDFS 上。

4.3.2 外部 Hive 应用
如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：
  1) 将 Hive 中的 hive-site.xml 拷贝或者软连接到 Spark 安装目录下的 conf 目录下。
  2) 打开 spark-shell，注意带上访问 Hive 元数据库的 JDBC 客户端 或者 如果 hive 的 metestore 使用的是 mysql 数据库，那么需要将 mysql 的 jdbc 驱动包放到 spark 的 jars 目录下。

$ bin/spark-shell --master spark://hadoop102:7077 --jars mysql-connector-java-5.1.27-bin.jar
4.4 JSON 数据集
  Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json() 去加载一个 Dataset[String] 或者一个 JSON 文件。
  注意：这个 JSON 文件不是一个传统的 JSON 文件，每一行都得是一个 JSON 串。

示例 JSON 文件如下：

{"name":"Michael", "age":30}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
示例代码如下：

// Primitive types (Int, String, etc) and Product types (case classes) encoders are
// supported by importing this when creating a Dataset.
import spark.implicits._

// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files
val path = "examples/src/main/resources/people.json"
val peopleDF = spark.read.json(path)

// The inferred schema can be visualized using the printSchema() method
peopleDF.printSchema()
// root
//  |-- age: long (nullable = true)
//  |-- name: string (nullable = true)

// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("people")

// SQL statements can be run by using the sql methods provided by spark
val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")
teenagerNamesDF.show()
// +------+
// |  name|
// +------+
// |Justin|
// +------+

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// a Dataset[String] storing one JSON object per string
val otherPeopleDataset = spark.createDataset(
"""{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)
val otherPeople = spark.read.json(otherPeopleDataset)
otherPeople.show()
// +---------------+----+
// |        address|name|
// +---------------+----+
// |[Columbus,Ohio]| Yin|
// +---------------+----+
4.5 JDBC
  Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对 DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。
  注意：需要将相关的数据库驱动放到 spark 的类路径下。

$ bin/spark-shell --master spark://hadoop102:7077 --jars mysql-connector-java-5.1.27-bin.jar
示例代码：

// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
// Loading data from a JDBC source
val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://hadoop102:3306/mysql").option("dbtable", "db").option("user", "root").option("password", "123456").load()

val connectionProperties = new Properties()
connectionProperties.put("user", "root")
connectionProperties.put("password", "hive")
val jdbcDF2 = spark.read.jdbc("jdbc:mysql://hadoop102:3306/mysql", "db", connectionProperties)

// Saving data to a JDBC source
jdbcDF.write
  .format("jdbc")
  .option("url", "jdbc:mysql://hadoop102:3306/mysql")
  .option("dbtable", "db")
  .option("user", "root")
  .option("password", "123456")
  .save()

jdbcDF2.write.jdbc("jdbc:mysql://hadoop102:3306/mysql", "db", connectionProperties)

// Specifying create table column data types on write
jdbcDF.write
  .option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
  .jdbc("jdbc:mysql://hadoop102:3306/mysql", "db", connectionProperties)
回到顶部
第5章 JDBC/ODBC 服务器
  Spark SQL 也提供 JDBC 连接支持，这对于让商业智能(BI)工具连接到 Spark 集群上以及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表进行查询。集群的资源以及缓存数据都在所有用户之间共享。
  Spark SQL 的 JDBC 服务器与 Hive 中的 HiveServer2 相一致。由于使用了 Thrift 通信协议，它也被称为 “Thrift server”。
  服务器可以通过 Spark 目录中的 sbin/start-thriftserver.sh 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive 配置选项( hive.server2.thrift.port 和 hive.server2.thrift.bind.host )来修改。你也可以通过命令行参数 --hiveconf property=value 来设置 Hive 选项。

./sbin/start-thriftserver.sh \
--hiveconf hive.server2.thrift.port=<listening-port> \
--hiveconf hive.server2.thrift.bind.host=<listening-host> \
--master <master-uri>
......
./bin/beeline
beeline> !connect jdbc:hive2://hadoop102:10000
  在 Beeline 客户端中，你可以使用标准的 HiveQL 命令来创建、列举以及查询数据表。参考文档链接：https://www.cnblogs.com/chenmingjun/p/10428809.html#_label1_5

Hive 的 JDBC 访问：

[atguigu@hadoop102 spark-2.1.1-bin-hadoop2.7]$ pwd
/opt/module/spark-2.1.1-bin-hadoop2.7
[atguigu@hadoop102 spark-2.1.1-bin-hadoop2.7]$ ./sbin/start-thriftserver.sh
starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/module/spark-2.1.1-bin-hadoop2.7/logs/spark-atguigu-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop102.out
[atguigu@hadoop102 spark-2.1.1-bin-hadoop2.7]$ ./bin/beeline
Beeline version 1.2.1.spark2 by Apache Hive
beeline> !connect jdbc:hive2://hadoop102:10000
Connecting to jdbc:hive2://hadoop102:10000（回车）
Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车）
Enter password for jdbc:hive2://hadoop102:10000: （直接回车）
Connected to: Spark SQL (version 2.1.1)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://hadoop102:10000> show tables;
+-----------+---------------------+--------------+--+
| database  |      tableName      | isTemporary  |
+-----------+---------------------+--------------+--+
| default   | event_logs20151220  | false        |
| default   | student22           | false        |
+-----------+---------------------+--------------+--+
2 rows selected (0.519 seconds)
0: jdbc:hive2://hadoop102:10000>
回到顶部
第6章 运行 Spark SQL CLI
  Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI 不能与 Thrift JDBC 服务交互。
  在 Spark 目录下执行如下命令启动 Spark SQL CLI：

[atguigu@hadoop102 spark-2.1.1-bin-hadoop2.7]$ pwd
/opt/module/spark-2.1.1-bin-hadoop2.7
[atguigu@hadoop102 spark-2.1.1-bin-hadoop2.7]$ ./bin/spark-sql
如下图所示：


配置外部 Hive 需要替换 conf/ 下的 hive-site.xml 。
第7章 Spark SQL 实战
7.1 数据说明
数据集是货品交易数据集。


每个订单可能包含多个货品，每个订单可以产生多次交易，不同的货品有不同的单价。
7.2 加载数据
tbStock：

scala> case class tbStock(ordernumber: String, locationid: String, dateid: String) extends Serializable
defined class tbStock

scala> val tbStockRdd = spark.sparkContext.textFile("tbStock.txt")
tbStockRdd: org.apache.spark.rdd.RDD[String] = tbStock.txt MapPartitionsRDD[1] at textFile at <console>:23

scala> val tbStockDS = tbStockRdd.map(_.split(",")).map(attr => tbStock(attr(0), attr(1), attr(2))).toDS
tbStockDS: org.apache.spark.sql.Dataset[tbStock] = [ordernumber: string, locationid: string ... 1 more field]

scala> tbStockDS.show()
+------------+----------+---------+
| ordernumber|locationid|   dataid|
+------------+----------+---------+
|BYSL00000893|      ZHAO|2007-8-23|
|BYSL00000897|      ZHAO|2007-8-24|
|BYSL00000898|      ZHAO|2007-8-25|
|BYSL00000899|      ZHAO|2007-8-26|
|BYSL00000900|      ZHAO|2007-8-26|
|BYSL00000901|      ZHAO|2007-8-27|
|BYSL00000902|      ZHAO|2007-8-27|
|BYSL00000904|      ZHAO|2007-8-28|
|BYSL00000905|      ZHAO|2007-8-28|
|BYSL00000906|      ZHAO|2007-8-28|
|BYSL00000907|      ZHAO|2007-8-29|
|BYSL00000908|      ZHAO|2007-8-30|
|BYSL00000909|      ZHAO| 2007-9-1|
|BYSL00000910|      ZHAO| 2007-9-1|
|BYSL00000911|      ZHAO|2007-8-31|
|BYSL00000912|      ZHAO| 2007-9-2|
|BYSL00000913|      ZHAO| 2007-9-3|
|BYSL00000914|      ZHAO| 2007-9-3|
|BYSL00000915|      ZHAO| 2007-9-4|
|BYSL00000916|      ZHAO| 2007-9-4|
+------------+----------+---------+
only showing top 20 rows
tbStockDetail:

scala> case class tbStockDetail(ordernumber: String, rownum: Int, itemid: String, number: Int, price: Double, amount: Double) extends Serializable
defined class tbStockDetail

scala> val tbStockDetailRdd = spark.sparkContext.textFile("tbStockDetail.txt")
tbStockDetailRdd: org.apache.spark.rdd.RDD[String] = tbStockDetail.txt MapPartitionsRDD[13] at textFile at <console>:23

scala> val tbStockDetailDS = tbStockDetailRdd.map(_.split(",")).map(attr => tbStockDetail(attr(0), attr(1).trim().toInt, attr(2), attr(3).trim().toInt, attr(4).trim().toDouble, attr(5).trim().toDouble)).toDS
tbStockDetailDS: org.apache.spark.sql.Dataset[tbStockDetail] = [ordernumber: string, rownum: int ... 4 more fields]

scala> tbStockDetailDS.show()
+------------+------+--------------+------+-----+------+
| ordernumber|rownum|        itemid|number|price|amount|
+------------+------+--------------+------+-----+------+
|BYSL00000893|     0|FS527258160501|    -1|268.0|-268.0|
|BYSL00000893|     1|FS527258169701|     1|268.0| 268.0|
|BYSL00000893|     2|FS527230163001|     1|198.0| 198.0|
|BYSL00000893|     3|24627209125406|     1|298.0| 298.0|
|BYSL00000893|     4|K9527220210202|     1|120.0| 120.0|
|BYSL00000893|     5|01527291670102|     1|268.0| 268.0|
|BYSL00000893|     6|QY527271800242|     1|158.0| 158.0|
|BYSL00000893|     7|ST040000010000|     8|  0.0|   0.0|
|BYSL00000897|     0|04527200711305|     1|198.0| 198.0|
|BYSL00000897|     1|MY627234650201|     1|120.0| 120.0|
|BYSL00000897|     2|01227111791001|     1|249.0| 249.0|
|BYSL00000897|     3|MY627234610402|     1|120.0| 120.0|
|BYSL00000897|     4|01527282681202|     1|268.0| 268.0|
|BYSL00000897|     5|84126182820102|     1|158.0| 158.0|
|BYSL00000897|     6|K9127105010402|     1|239.0| 239.0|
|BYSL00000897|     7|QY127175210405|     1|199.0| 199.0|
|BYSL00000897|     8|24127151630206|     1|299.0| 299.0|
|BYSL00000897|     9|G1126101350002|     1|158.0| 158.0|
|BYSL00000897|    10|FS527258160501|     1|198.0| 198.0|
|BYSL00000897|    11|ST040000010000|    13|  0.0|   0.0|
+------------+------+--------------+------+-----+------+
only showing top 20 rows
tbDate:

scala> case class tbDate(dateid: String, years: Int, theyear: Int, month: Int, day: Int, weekday: Int, week: Int, quarter: Int, period: Int, halfmonth: Int) extends Serializable
defined class tbDate

scala> val tbDateRdd = spark.sparkContext.textFile("tbDate.txt")
tbDateRdd: org.apache.spark.rdd.RDD[String] = tbDate.txt MapPartitionsRDD[20] at textFile at <console>:23

scala> val tbDateDS = tbDateRdd.map(_.split(",")).map(attr => tbDate(attr(0), attr(1).trim().toInt, attr(2).trim().toInt, attr(3).trim().toInt, attr(4).trim().toInt, attr(5).trim().toInt, attr(6).trim().toInt, attr(7).trim().toInt, attr(8).trim().toInt, attr(9).trim().toInt)).toDS
tbDateDS: org.apache.spark.sql.Dataset[tbDate] = [dateid: string, years: int ... 8 more fields]

scala> tbDateDS.show()
+---------+------+-------+-----+---+-------+----+-------+------+---------+
|   dateid| years|theyear|month|day|weekday|week|quarter|period|halfmonth|
+---------+------+-------+-----+---+-------+----+-------+------+---------+
| 2003-1-1|200301|   2003|    1|  1|      3|   1|      1|     1|        1|
| 2003-1-2|200301|   2003|    1|  2|      4|   1|      1|     1|        1|
| 2003-1-3|200301|   2003|    1|  3|      5|   1|      1|     1|        1|
| 2003-1-4|200301|   2003|    1|  4|      6|   1|      1|     1|        1|
| 2003-1-5|200301|   2003|    1|  5|      7|   1|      1|     1|        1|
| 2003-1-6|200301|   2003|    1|  6|      1|   2|      1|     1|        1|
| 2003-1-7|200301|   2003|    1|  7|      2|   2|      1|     1|        1|
| 2003-1-8|200301|   2003|    1|  8|      3|   2|      1|     1|        1|
| 2003-1-9|200301|   2003|    1|  9|      4|   2|      1|     1|        1|
|2003-1-10|200301|   2003|    1| 10|      5|   2|      1|     1|        1|
|2003-1-11|200301|   2003|    1| 11|      6|   2|      1|     2|        1|
|2003-1-12|200301|   2003|    1| 12|      7|   2|      1|     2|        1|
|2003-1-13|200301|   2003|    1| 13|      1|   3|      1|     2|        1|
|2003-1-14|200301|   2003|    1| 14|      2|   3|      1|     2|        1|
|2003-1-15|200301|   2003|    1| 15|      3|   3|      1|     2|        1|
|2003-1-16|200301|   2003|    1| 16|      4|   3|      1|     2|        2|
|2003-1-17|200301|   2003|    1| 17|      5|   3|      1|     2|        2|
|2003-1-18|200301|   2003|    1| 18|      6|   3|      1|     2|        2|
|2003-1-19|200301|   2003|    1| 19|      7|   3|      1|     2|        2|
|2003-1-20|200301|   2003|    1| 20|      1|   4|      1|     2|        2|
+---------+------+-------+-----+---+-------+----+-------+------+---------+
only showing top 20 rows
注册为临时表：

scala> tbStockDS.createOrReplaceTempView("tbStock")

scala> tbDateDS.createOrReplaceTempView("tbDate")

scala> tbStockDetailDS.createOrReplaceTempView("tbStockDetail")
7.3 计算所有订单中每年的销售单数、销售总额
统计所有订单中每年的销售单数、销售总额
三个表连接后以 count(distinct a.ordernumber) 计销售单数，以 sum(b.amount) 计销售总额

示例代码：

SELECT c.theyear, COUNT(DISTINCT a.ordernumber), SUM(b.amount)
FROM tbStock a
    JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
    JOIN tbDate c ON a.dateid = c.dateid
GROUP BY c.theyear
ORDER BY c.theyear

spark.sql("SELECT c.theyear, COUNT(DISTINCT a.ordernumber), SUM(b.amount) FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber JOIN tbDate c ON a.dateid = c.dateid GROUP BY c.theyear ORDER BY c.theyear").show
输出结果如下：

+-------+---------------------------+--------------------+
|theyear|count(DISTINCT ordernumber)|         sum(amount)|
+-------+---------------------------+--------------------+
|   2004|                       1094|   3268115.499199999|
|   2005|                       3828|1.3257564149999991E7|
|   2006|                       3772|1.3680982900000006E7|
|   2007|                       4885|1.6719354559999993E7|
|   2008|                       4861| 1.467429530000001E7|
|   2009|                       2619|   6323697.189999999|
|   2010|                         94|  210949.65999999997|
+-------+---------------------------+--------------------+
7.4 计算所有订单每年最大金额订单的销售额
目标：统计每年最大金额订单的销售额。
第一步、统计每年，每个订单一共有多少销售额
示例代码：

SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount
FROM tbStock a
    JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
GROUP BY a.dateid, a.ordernumber

spark.sql("SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber GROUP BY a.dateid, a.ordernumber").show
输出结果如下：

+----------+------------+------------------+
|    dateid| ordernumber|       SumOfAmount|
+----------+------------+------------------+
|  2008-4-9|BYSL00001175|             350.0|
| 2008-5-12|BYSL00001214|             592.0|
| 2008-7-29|BYSL00011545|            2064.0|
|  2008-9-5|DGSL00012056|            1782.0|
| 2008-12-1|DGSL00013189|             318.0|
|2008-12-18|DGSL00013374|             963.0|
|  2009-8-9|DGSL00015223|            4655.0|
| 2009-10-5|DGSL00015585|            3445.0|
| 2010-1-14|DGSL00016374|            2934.0|
| 2006-9-24|GCSL00000673|3556.1000000000004|
| 2007-1-26|GCSL00000826| 9375.199999999999|
| 2007-5-24|GCSL00001020| 6171.300000000002|
|  2008-1-8|GCSL00001217|            7601.6|
| 2008-9-16|GCSL00012204|            2018.0|
| 2006-7-27|GHSL00000603|            2835.6|
|2006-11-15|GHSL00000741|           3951.94|
|  2007-6-6|GHSL00001149|               0.0|
| 2008-4-18|GHSL00001631|              12.0|
| 2008-7-15|GHSL00011367|             578.0|
|  2009-5-8|GHSL00014637|            1797.6|
+----------+------------+------------------+
第二步、以上一步查询结果为基础表，和表 tbDate 使用 dateid join，求出每年最大金额订单的销售额
示例代码：

SELECT theyear, MAX(c.SumOfAmount) AS SumOfAmount
FROM (SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount
    FROM tbStock a
        JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
    GROUP BY a.dateid, a.ordernumber
    ) c
    JOIN tbDate d ON c.dateid = d.dateid
GROUP BY theyear
ORDER BY theyear DESC

spark.sql("SELECT theyear, MAX(c.SumOfAmount) AS SumOfAmount FROM (SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber GROUP BY a.dateid, a.ordernumber ) c JOIN tbDate d ON c.dateid = d.dateid GROUP BY theyear ORDER BY theyear DESC").show
输出结果如下：

+-------+------------------+
|theyear|       SumOfAmount|
+-------+------------------+
|   2010|13065.280000000002|
|   2009|25813.200000000008|
|   2008|           55828.0|
|   2007|          159126.0|
|   2006|           36124.0|
|   2005|38186.399999999994|
|   2004| 23656.79999999997|
+-------+------------------+
7.5 计算所有订单中每年最畅销货品
目标：统计每年最畅销货品（哪个货品销售额 amount 在当年最高，哪个就是最畅销货品）。
第一步、求出每年每个货品的销售额
示例代码：

SELECT c.theyear, b.itemid, SUM(b.amount) AS SumOfAmount
FROM tbStock a
    JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
    JOIN tbDate c ON a.dateid = c.dateid
GROUP BY c.theyear, b.itemid

spark.sql("SELECT c.theyear, b.itemid, SUM(b.amount) AS SumOfAmount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber JOIN tbDate c ON a.dateid = c.dateid GROUP BY c.theyear, b.itemid").show
输出结果如下：

+-------+--------------+------------------+
|theyear|        itemid|       SumOfAmount|
+-------+--------------+------------------+
|   2004|43824480810202|           4474.72|
|   2006|YA214325360101|             556.0|
|   2006|BT624202120102|             360.0|
|   2007|AK215371910101|24603.639999999992|
|   2008|AK216169120201|29144.199999999997|
|   2008|YL526228310106|16073.099999999999|
|   2009|KM529221590106| 5124.800000000001|
|   2004|HT224181030201|2898.6000000000004|
|   2004|SG224308320206|           7307.06|
|   2007|04426485470201|14468.800000000001|
|   2007|84326389100102|           9134.11|
|   2007|B4426438020201|           19884.2|
|   2008|YL427437320101|12331.799999999997|
|   2008|MH215303070101|            8827.0|
|   2009|YL629228280106|           12698.4|
|   2009|BL529298020602|            2415.8|
|   2009|F5127363019006|             614.0|
|   2005|24425428180101|          34890.74|
|   2007|YA214127270101|             240.0|
|   2007|MY127134830105|          11099.92|
+-------+--------------+------------------+
第二步、在第一步的基础上，统计每年单个货品中的最大金额
示例代码：

SELECT d.theyear, MAX(d.SumOfAmount) AS MaxOfAmount
FROM (SELECT c.theyear, b.itemid, SUM(b.amount) AS SumOfAmount
    FROM tbStock a
        JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
        JOIN tbDate c ON a.dateid = c.dateid
    GROUP BY c.theyear, b.itemid
    ) d
GROUP BY d.theyear

spark.sql("SELECT d.theyear, MAX(d.SumOfAmount) AS MaxOfAmount FROM (SELECT c.theyear, b.itemid, SUM(b.amount) AS SumOfAmount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber JOIN tbDate c ON a.dateid = c.dateid GROUP BY c.theyear, b.itemid ) d GROUP BY d.theyear").show
输出结果如下：

+-------+------------------+
|theyear|       MaxOfAmount|
+-------+------------------+
|   2007|           70225.1|
|   2006|          113720.6|
|   2004|53401.759999999995|
|   2009|           30029.2|
|   2005|56627.329999999994|
|   2010|            4494.0|
|   2008| 98003.60000000003|
+-------+------------------+
第三步、用最大销售额和统计好的每个货品的销售额 join，以及用年 join，得到最畅销货品那一行信息
示例代码：

SELECT DISTINCT e.theyear, e.itemid, f.MaxOfAmount
FROM (SELECT c.theyear, b.itemid, SUM(b.amount) AS SumOfAmount
    FROM tbStock a
        JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
        JOIN tbDate c ON a.dateid = c.dateid
    GROUP BY c.theyear, b.itemid
    ) e
    JOIN (SELECT d.theyear, MAX(d.SumOfAmount) AS MaxOfAmount
        FROM (SELECT c.theyear, b.itemid, SUM(b.amount) AS SumOfAmount
            FROM tbStock a
                JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
                JOIN tbDate c ON a.dateid = c.dateid
            GROUP BY c.theyear, b.itemid
            ) d
        GROUP BY d.theyear
        ) f ON e.theyear = f.theyear
        AND e.SumOfAmount = f.MaxOfAmount
ORDER BY e.theyear

spark.sql("SELECT DISTINCT e.theyear, e.itemid, f.maxofamount FROM (SELECT c.theyear, b.itemid, SUM(b.amount) AS sumofamount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber JOIN tbDate c ON a.dateid = c.dateid GROUP BY c.theyear, b.itemid ) e JOIN (SELECT d.theyear, MAX(d.sumofamount) AS maxofamount FROM (SELECT c.theyear, b.itemid, SUM(b.amount) AS sumofamount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber JOIN tbDate c ON a.dateid = c.dateid GROUP BY c.theyear, b.itemid ) d GROUP BY d.theyear ) f ON e.theyear = f.theyear AND e.sumofamount = f.maxofamount ORDER BY e.theyear").show
输出结果如下：

+-------+--------------+------------------+
|theyear|        itemid|       maxofamount|
+-------+--------------+------------------+
|   2004|JY424420810101|53401.759999999995|
|   2005|24124118880102|56627.329999999994|
|   2006|JY425468460101|          113720.6|
|   2007|JY425468460101|           70225.1|
|   2008|E2628204040101| 98003.60000000003|
|   2009|YL327439080102|           30029.2|
|   2010|SQ429425090101|            4494.0|
+-------+--------------+------------------+











=============


导读：本文所述内容均基于 2018 年 9 月 17 日 Spark 最新 Spark Release 2.3.1 版本，以及截止到 2018 年 10 月 21 日 Adaptive Execution 最新开发代码。自动设置 Shuffle Partition 个数已进入 Spark Release 2.3.1 版本，动态调整执行计划与处理数据倾斜尚未进入 Spark Release 2.3.1
1 背  景
Spark SQL / Catalyst 和 CBO 的优化，从查询本身与目标数据的特点的角度尽可能保证了最终生成的执行计划的高效性。但是

执行计划一旦生成，便不可更改，即使执行过程中发现后续执行计划可以进一步优化，也只能按原计划执行；

CBO 基于统计信息生成最优执行计划，需要提前生成统计信息，成本较大，且不适合数据更新频繁的场景；

CBO 基于基础表的统计信息与操作对数据的影响推测中间结果的信息，只是估算，不够精确。

本文介绍的 Adaptive Execution 将可以根据执行过程中的中间数据优化后续执行，从而提高整体执行效率。核心在于两点：

执行计划可动态调整

调整的依据是中间结果的精确统计信息

2 动态设置 Shuffle Partition
2.1 Spark Shuffle 原理
Spark Shuffle 一般用于将上游 Stage 中的数据按 Key 分区，保证来自不同 Mapper （表示上游 Stage 的 Task）的相同的 Key 进入相同的 Reducer （表示下游 Stage 的 Task）。一般用于 group by 或者 Join 操作。



如上图所示，该 Shuffle 总共有 2 个 Mapper 与 5 个 Reducer。每个 Mapper 会按相同的规则（由 Partitioner 定义）将自己的数据分为五份。每个 Reducer 从这两个 Mapper 中拉取属于自己的那一份数据。

2.2 原有 Shuffle 的问题
使用 Spark SQL 时，可通过spark.sql.shuffle.partitions指定 Shuffle 时 Partition 个数，也即 Reducer 个数。

该参数决定了一个 Spark SQL Job 中包含的所有 Shuffle 的 Partition 个数。如下图所示，当该参数值为 3 时，所有 Shuffle 中 Reducer 个数都为 3。



这种方法有如下问题：

Partition 个数不宜设置过大；

Reducer（代指 Spark Shuffle 过程中执行 Shuffle Read 的 Task） 个数过多，每个 Reducer 处理的数据量过小。大量小 Task 造成不必要的 Task 调度开销与可能的资源调度开销（如果开启了 Dynamic Allocation）；

Reducer 个数过大，如果 Reducer 直接写 HDFS 会生成大量小文件，从而造成大量 addBlock RPC，Name node 可能成为瓶颈，并影响其它使用 HDFS 的应用；

过多 Reducer 写小文件，会造成后面读取这些小文件时产生大量 getBlock RPC，对 Name node 产生冲击；

Partition 个数不宜设置过小

每个 Reducer 处理的数据量太大，Spill 到磁盘开销增大；

Reducer GC 时间增长；

Reducer 如果写 HDFS，每个 Reducer 写入数据量较大，无法充分发挥并行处理优势；

很难保证所有 Shuffle 都最优

不同的 Shuffle 对应的数据量不一样，因此最优的 Partition 个数也不一样。使用统一的 Partition 个数很难保证所有 Shuffle 都最优；

定时任务不同时段数据量不一样，相同的 Partition 数设置无法保证所有时间段执行时都最优；

2.3 自动设置 Shuffle Partition 原理
如 Spark Shuffle 原理 一节图中所示，Stage 1 的 5 个 Partition 数据量分别为 60MB，40MB，1MB，2MB，50MB。其中 1MB 与 2MB 的 Partition 明显过小（实际场景中，部分小 Partition 只有几十 KB 及至几十字节）。

开启 Adaptive Execution 后：

Spark 在 Stage 0 的 Shuffle Write 结束后，根据各 Mapper 输出，统计得到各 Partition 的数据量，即 60MB，40MB，1MB，2MB，50MB；

通过 ExchangeCoordinator 计算出合适的 post-shuffle Partition 个数（即 Reducer）个数（本例中 Reducer 个数设置为 3）；

启动相应个数的 Reducer 任务；

每个 Reducer 读取一个或多个 Shuffle Write Partition 数据（如下图所示，Reducer 0 读取 Partition 0，Reducer 1 读取 Partition 1、2、3，Reducer 2 读取 Partition 4）。



三个 Reducer 这样分配是因为：

targetPostShuffleInputSize 默认为 64MB，每个 Reducer 读取数据量不超过 64MB；

如果 Partition 0 与 Partition 2 结合，Partition 1 与 Partition 3 结合，虽然也都不超过 64 MB。但读完 Partition 0 再读 Partition 2，对于同一个 Mapper 而言，如果每个 Partition 数据比较少，跳着读多个 Partition 相当于随机读，在HDD 上性能不高；

目前的做法是只结合相临的 Partition，从而保证顺序读，提高磁盘 IO 性能；

该方案只会合并多个小的 Partition，不会将大的 Partition 拆分，因为拆分过程需要引入一轮新的 Shuffle；

基于上面的原因，默认 Partition 个数（本例中为 5）可以大一点，然后由 ExchangeCoordinator 合并。如果设置的 Partition 个数太小，Adaptive Execution 在此场景下无法发挥作用。

由上图可见，Reducer 1 从每个 Mapper 读取 Partition 1、2、3 都有三根线，是因为原来的 Shuffle 设计中，每个 Reducer 每次通过 Fetch 请求从一个特定 Mapper 读数据时，只能读一个 Partition 的数据。也即在上图中，Reducer 1 读取 Mapper 0 的数据，需要 3 轮 Fetch 请求。对于 Mapper 而言，需要读三次磁盘，相当于随机 IO。

为了解决这个问题，Spark 新增接口，一次 Shuffle Read 可以读多个 Partition 的数据。如下图所示，Task 1 通过一轮请求即可同时读取 Task 0 内 Partition 0、1 和 2 的数据，减少了网络请求数量。同时 Mapper 0 一次性读取并返回三个 Partition 的数据，相当于顺序 IO，从而提升了性能。



由于 Adaptive Execution 的自动设置 Reducer 是由 ExchangeCoordinator 根据 Shuffle Write 统计信息决定的，因此即使在同一个 Job 中不同 Shuffle 的 Reducer 个数都可以不一样，从而使得每次 Shuffle 都尽可能最优。

上文 原有 Shuffle 的问题 一节中的例子，在启用 Adaptive Execution 后，三次 Shuffle 的 Reducer 个数从原来的全部为 3 变为 2、4、3。



2.4 使用与优化方法
可通过spark.sql.adaptive.enabled=true启用 Adaptive Execution 从而启用自动设置 Shuffle Reducer 这一特性。

通过spark.sql.adaptive.shuffle.targetPostShuffleInputSize可设置每个 Reducer 读取的目标数据量，其单位是字节，默认值为 64 MB。上文例子中，如果将该值设置为 50 MB，最终效果仍然如上文所示，而不会将 Partition 0 的 60MB 拆分。具体原因上文已说明。

3 动态调整执行计划
3.1 固定执行计划的不足
在不开启 Adaptive Execution 之前，执行计划一旦确定，即使发现后续执行计划可以优化，也不可更改。如下图所示，SortMergJoin 的 Shuffle Write 结束后，发现 Join 一方的 Shuffle 输出只有 46.9KB，仍然继续执行 SortMergeJoin。



此时完全可将 SortMergeJoin 变更为 BroadcastJoin 从而提高整体执行效率。

3.2 SortMergeJoin 原理
SortMergeJoin 是常用的分布式 Join 方式，它几乎可使用于所有需要 Join 的场景。但有些场景下，它的性能并不是最好的。

SortMergeJoin 的原理如下图所示：

将 Join 双方以 Join Key 为 Key 按照 HashPartitioner 分区，且保证分区数一致；

Stage 0 与 Stage 1 的所有 Task 在 Shuffle Write 时，都将数据分为 5 个 Partition，并且每个 Partition 内按 Join Key 排序；

Stage 2 启动 5 个 Task 分别去 Stage 0 与 Stage 1 中所有包含 Partition 分区数据的 Task 中取对应 Partition 的数据。（如果某个 Mapper 不包含该 Partition 的数据，则 Redcuer 无须向其发起读取请求）；

Stage 2 的 Task 2 分别从 Stage 0 的 Task 0、1、2 中读取 Partition 2 的数据，并且通过 MergeSort 对其进行排序；

Stage 2 的 Task 2 分别从 Stage 1 的 Task 0、1 中读取 Partition 2 的数据，且通过 MergeSort 对其进行排序；

Stage 2 的 Task 2 在上述两步 MergeSort 的同时，使用 SortMergeJoin 对二者进行 Join。



3.3 BroadcastJoin 原理
当参与 Join 的一方足够小，可全部置于 Executor 内存中时，可使用 Broadcast 机制将整个 RDD 数据广播到每一个 Executor 中，该 Executor 上运行的所有 Task 皆可直接读取其数据。（本文中，后续配图，为了方便展示，会将整个 RDD 的数据置于 Task 框内，而隐藏 Executor）。

对于大 RDD，按正常方式，每个 Task 读取并处理一个 Partition 的数据，同时读取 Executor 内的广播数据，该广播数据包含了小 RDD 的全量数据，因此可直接与每个 Task 处理的大 RDD 的部分数据直接 Join。



根据 Task 内具体的 Join 实现的不同，又可分为 BroadcastHashJoin 与 BroadcastNestedLoopJoin。后文不区分这两种实现，统称为 BroadcastJoin。

与 SortMergeJoin 相比，BroadcastJoin 不需要 Shuffle，减少了 Shuffle 带来的开销，同时也避免了 Shuffle 带来的数据倾斜，从而极大地提升了 Job 执行效率。

同时，BroadcastJoin 带来了广播小 RDD 的开销。另外，如果小 RDD 过大，无法存于 Executor 内存中，则无法使用 BroadcastJoin。

对于基础表的 Join，可在生成执行计划前，直接通过 HDFS 获取各表的大小，从而判断是否适合使用 BroadcastJoin。但对于中间表的 Join，无法提前准确判断中间表大小从而精确判断是否适合使用 BroadcastJoin。

《Spark SQL 性能优化再进一步 CBO 基于代价的优化》一文介绍的 CBO 可通过表的统计信息与各操作对数据统计信息的影响，推测出中间表的统计信息，但是该方法得到的统计信息不够准确。同时该方法要求提前分析表，具有较大开销。

而开启 Adaptive Execution 后，可直接根据 Shuffle Write 数据判断是否适用 BroadcastJoin。

3.4 动态调整执行计划原理
如上文 SortMergeJoin 原理 中配图所示，SortMergeJoin 需要先对 Stage 0 与 Stage 1 按同样的 Partitioner 进行 Shuffle Write。

Shuffle Write 结束后，可从每个 ShuffleMapTask 的 MapStatus 中统计得到按原计划执行时 Stage 2 各 Partition 的数据量以及 Stage 2 需要读取的总数据量。（一般来说，Partition 是 RDD 的属性而非 Stage 的属性，本文为了方便，不区分 Stage 与 RDD。可以简单认为一个 Stage 只有一个 RDD，此时 Stage 与 RDD 在本文讨论范围内等价）。

如果其中一个 Stage 的数据量较小，适合使用 BroadcastJoin，无须继续执行 Stage 2 的 Shuffle Read。相反，可利用 Stage 0 与 Stage 1 的数据进行 BroadcastJoin，如下图所示。



具体做法是：

将 Stage 1 全部 Shuffle Write 结果广播出去

启动 Stage 2，Partition 个数与 Stage 0 一样，都为 3

每个 Stage 2 每个 Task 读取 Stage 0 每个 Task 的 Shuffle Write 数据，同时与广播得到的 Stage 1 的全量数据进行 Join

注：广播数据存于每个 Executor 中，其上所有 Task 共享，无须为每个 Task 广播一份数据。上图中，为了更清晰展示为什么能够直接 Join 而将 Stage 2 每个 Task 方框内都放置了一份 Stage 1 的全量数据。

虽然 Shuffle Write 已完成，将后续的 SortMergeJoin 改为 Broadcast 仍然能提升执行效率：

SortMergeJoin 需要在 Shuffle Read 时对来自 Stage 0 与 Stage 1 的数据进行 Merge Sort，并且可能需要 Spill 到磁盘，开销较大；

SortMergeJoin 时，Stage 2 的所有 Task 需要取 Stage 0 与 Stage 1 的所有 Task 的输出数据（如果有它要的数据 ），会造成大量的网络连接。且当 Stage 2 的 Task 较多时，会造成大量的磁盘随机读操作，效率不高，且影响相同机器上其它 Job 的执行效率；

SortMergeJoin 时，Stage 2 每个 Task 需要从几乎所有 Stage 0 与 Stage 1 的 Task 取数据，无法很好利用 Locality；

Stage 2 改用 Broadcast，每个 Task 直接读取 Stage 0 的每个 Task 的数据（一对一），可很好利用 Locality 特性。最好在 Stage 0 使用的 Executor 上直接启动 Stage 2 的 Task。如果 Stage 0 的 Shuffle Write 数据并未 Spill 而是在内存中，则 Stage 2 的 Task 可直接读取内存中的数据，效率非常高。如果有 Spill，那可直接从本地文件中读取数据，且是顺序读取，效率远比通过网络随机读数据效率高。

3.5 使用与优化方法
该特性的使用方式如下：

当spark.sql.adaptive.enabled与spark.sql.adaptive.join.enabled都设置为true时，开启 Adaptive Execution 的动态调整 Join 功能；

spark.sql.adaptiveBroadcastJoinThreshold设置了 SortMergeJoin 转 BroadcastJoin 的阈值。如果不设置该参数，该阈值与spark.sql.autoBroadcastJoinThreshold的值相等；

除了本文所述 SortMergeJoin 转 BroadcastJoin，Adaptive Execution 还可提供其它 Join 优化策略。部分优化策略可能会需要增加 Shuffle。spark.sql.adaptive.allowAdditionalShuffle参数决定了是否允许为了优化 Join 而增加 Shuffle。其默认值为 false。

4 自动处理数据倾斜
4.1 解决数据倾斜典型方案
《Spark 性能优化之道——解决 Spark 数据倾斜（Data Skew）的 N 种姿势》一文讲述了数据倾斜的危害，产生原因，以及典型解决方法。

保证文件可 Split 从而避免读 HDFS 时数据倾斜；

保证 Kafka 各 Partition 数据均衡从而避免读 Kafka 引起的数据倾斜；

调整并行度或自定义 Partitioner 从而分散分配给同一 Task 的大量不同 Key；

使用 BroadcastJoin 代替 ReduceJoin 消除 Shuffle 从而避免 Shuffle 引起的数据倾斜；

对倾斜 Key 使用随机前缀或后缀从而分散大量倾斜 Key，同时将参与 Join 的小表扩容，从而保证 Join 结果的正确性。

4.2 自动解决数据倾斜
目前 Adaptive Execution 可解决 Join 时数据倾斜问题。其思路可理解为将部分倾斜的 Partition (倾斜的判断标准为该 Partition 数据是所有 Partition Shuffle Write 中位数的 N 倍) 进行单独处理，类似于 BroadcastJoin，如下图所示。



在上图中，左右两边分别是参与 Join 的 Stage 0 与 Stage 1 (实际应该是两个 RDD 进行 Join，但如同上文所述，这里不区分 RDD 与 Stage)，中间是获取 Join 结果的 Stage 2。

明显 Partition 0 的数据量较大，这里假设 Partition 0 符合“倾斜”的条件，其它 4 个 Partition 未倾斜。

以 Partition 对应的 Task 2 为例，它需获取 Stage 0 的三个 Task 中所有属于 Partition 2 的数据，并使用 MergeSort 排序。同时获取 Stage 1 的两个 Task 中所有属于 Partition 2 的数据并使用 MergeSort 排序。然后对二者进行 SortMergeJoin。

对于 Partition 0，可启动多个 Task：

在上图中，启动了两个 Task 处理 Partition 0 的数据，分别名为 Task 0-0 与 Task 0-1

Task 0-0 读取 Stage 0 Task 0 中属于 Partition 0 的数据

Task 0-1 读取 Stage 0 Task 1 与 Task 2 中属于 Partition 0 的数据，并进行 MergeSort

Task 0-0 与 Task 0-1 都从 Stage 1 的两个 Task 中所有属于 Partition 0 的数据

Task 0-0 与 Task 0-1 使用 Stage 0 中属于 Partition 0 的部分数据与 Stage 1中属于 Partition 0 的全量数据进行 Join

通过该方法，原本由一个 Task 处理的 Partition 0 的数据由多个 Task 共同处理，每个 Task 需处理的数据量减少，从而避免了 Partition 0 的倾斜。

对于 Partition 0 的处理，有点类似于 BroadcastJoin 的做法。但区别在于，Stage 2 的 Task 0-0 与 Task 0-1 同时获取 Stage 1 中属于 Partition 0 的全量数据，是通过正常的 Shuffle Read 机制实现，而非 BroadcastJoin 中的变量广播实现。

4.3 使用与优化方法
开启与调优该特性的方法如下：

将spark.sql.adaptive.skewedJoin.enabled设置为 true 即可自动处理 Join 时数据倾斜；

spark.sql.adaptive.skewedPartitionMaxSplits控制处理一个倾斜 Partition 的 Task 个数上限，默认值为 5；

spark.sql.adaptive.skewedPartitionRowCountThreshold设置了一个 Partition 被视为倾斜 Partition 的行数下限，也即行数低于该值的 Partition 不会被当作倾斜 Partition 处理。其默认值为 10L * 1000 * 1000 即一千万；

spark.sql.adaptive.skewedPartitionSizeThreshold设置了一个 Partition 被视为倾斜 Partition 的大小下限，也即大小小于该值的 Partition 不会被视作倾斜 Partition。其默认值为 64 * 1024 * 1024 也即 64MB；

spark.sql.adaptive.skewedPartitionFactor该参数设置了倾斜因子。如果一个 Partition 的大小大于spark.sql.adaptive.skewedPartitionSizeThreshold的同时大于各 Partition 大小中位数与该因子的乘积，或者行数大于spark.sql.adaptive.skewedPartitionRowCountThreshold的同时大于各 Partition 行数中位数与该因子的乘积，则它会被视为倾斜的 Partition。
=============

========== Spark SQL ==========
1、Spark SQL 是 Spark 的一个模块，可以和 RDD 进行混合编程、支持标准的数据源、可以集成和替代 Hive、可以提供 JDBC、ODBC 服务器功能。

2、Spark SQL 的特点：
  （1）和 Spark Core 的无缝集成，可以在写整个 RDD 应用的时候，配合 Spark SQL 来实现逻辑。
  （2）统一的数据访问方式，Spark SQL 提供标准化的 SQL 查询。
  （3）Hive 的集成，Spark SQL 通过内嵌的 Hive 或者连接外部已经部署好的 Hive 实例，实现了对 Hive 语法的集成和操作。
  （4）标准化的连接方式，Spark SQL 可以通过启动 thrift Server 来支持 JDBC、ODBC 的访问，即将自己作为一个 BI Server 来使用。

3、Spark SQL 可以执行 SQL 语句，也可以执行 HQL 语句，将运行的结果作为 Dataset 和 DataFrame（将查询出来的结果转换成 RDD，类似于 hive 将 sql 语句转换成 mapreduce）。

4、Spark SQL 的计算速度(Spark sql 比 Hive 快了至少一个数量级，尤其是在 Tungsten 成熟以后会更加无可匹敌)，Spark SQL 推出的 DataFrame 可以让数据仓库直接使用机器学习、图计算等复杂的算法库来对数据仓库进行复杂深度数据价值的挖掘。

5、老版本中使用 hivecontext，现在使用 sparkSession。

========== Spark SQL 的数据抽象 ==========
0、RDD（Spark1.0）-> DataFrame（Spark1.3）-> DataSet（Spark1.6）
1、Spark SQL 提供了 DataFrame 和 DataSet 数据抽象。
2、DataFrame 就是 RDD + Schema，可以认为是一张二维表格。DataFrame 也是懒执行的、不可变的。DataFrame 性能上比 RDD 要高。
3、DataFrame 是一个弱类型的数据对象，DataFrame 的劣势是在编译期不进行表格中的字段的类型检查。在运行期进行检查。类似于 java.sql.ResultSet 类，只能通过 getString 这种方式来获取具体数据。
4、DataSet 是 Spark 最新的数据抽象，Spark 的发展会逐步将 DataSet 作为主要的数据抽象，弱化 RDD 和 DataFrame。DataSet 包含了 DataFrame 所有的优化机制。除此之外提供了以样例类为 Schema 模型的强类型。
5、type DataFrame = Dataset[Row]
6、DataFrame 和 DataSet 都有可控的内存管理机制，所有数据都保存在非堆内存上，节省了大量空间之外，还摆脱了GC的限制。都使用了 catalyst 进行 SQL 的优化。可以使得不太会使用 RDD 的工程师写出相对高效的代码。
7、RDD 和 DataFrame 和 DataSet 之间可以进行数据转换。

========== Spark SQL 的初探 -- 客户端查询 ==========
1、你可以通过 spark-shell 或者 spark-sql 来操作 Spark SQL，注意：spark 作为 SparkSession 的变量名，sc 作为 SparkContext 的变量名。
2、你可以通过 Spark 提供的方法读取 JSON 文件，将 JSON 文件转换成 DataFrame。
3、你可以通过 DataFrame 提供的 API 来操作 DataFrame 里面的数据。
4、你可以通过将 DataFrame 注册成为一个临时表的方式，来通过 Spark.sql 方法运行标准的 SQL 语句来查询。

小细节：
  show() --> 表格
  collect() --> RDD 打印

========== IDEA 创建 Spark SQL 程序 ==========
1、Spark SQL 读取 json 需要 json 文件中一行是一个 json 对象。
2、通过创建 SparkSession 来使用 SparkSQL：
示例代码如下：

package com.atguigu.sparksql

import org.apache.spark.sql.SparkSession
import org.slf4j.LoggerFactory

object HelloWorld {

  val logger = LoggerFactory.getLogger(HelloWorld.getClass)

  def main(args: Array[String]) {
    // 创建 SparkSession 并设置 App 名称
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    // 通过隐式转换将 RDD 操作添加到 DataFrame 上（将 RDD 转成 DataFrame）
    import spark.implicits._

    // 通过 spark.read 操作读取 JSON 数据
    val df = spark.read.json("examples/src/main/resources/people.json")

    // show 操作类似于 Action，将 DataFrame 直接打印到 Console 上
    df.show()

    // DSL 风格的使用方式：属性的获取方法 $
    df.filter($"age" > 21).show()

    //将 DataFrame 注册为表
    df.createOrReplaceTempView("persons")

    // 执行 Spark SQL 查询操作
    spark.sql("select * from perosns where age > 21").show()

    // 关闭资源
    spark.stop()
  }
}
========== DataFrame 查询方式 ==========
1、DataFrame 支持两种查询方式：一种是 DSL 风格，另外一种是 SQL 风格。
DSL 风格：
  （1）你需要引入 import spark.implicit._ 这个隐式转换，可以将 DataFrame 隐式转换成 RDD。
示例：
  df.select("name").show()
  df.filter($"age" > 25).show()

SQL 风格：
  （1）你需要将 DataFrame 注册成一张表格，如果你通过 createOrReplaceTempView 这种方式来创建，那么该表当前 Session 有效，如果你通过 createGlobalTempView 来创建，那么该表跨 Session 有效，但是 SQL 语句访问该表的时候需要加上前缀 global_temp.xxx。
  （2）你需要通过 sparkSession.sql 方法来运行你的 SQL 语句。
示例：
  一个 SparkContext 可以多次创建 SparkSession。
  // Session 内可访问，一个 SparkSession 结束后，表自动删除。
  df.createOrReplaceTempView("persons") // 使用表名不需要任何前缀
  // 应用级别内可访问，一个 SparkContext 结束后，表自动删除。
  df.createGlobalTempView("persons") // 使用表名需要加上“global_temp.” 前缀，比如：global_temp.persons

========== DataSet 创建方式 ==========
1、定义一个 DataSet，首先你需要先定义一个 case 类。

========== RDD、DataFrame、DataSet 之间的转换总结 ==========
1、RDD -> DataFrame ： rdd.map(para => (para(0).trim(), para(1).trim().toInt)).toDF("name", "age") // RDD -> 元组 -> toDF()（注意：这是第一种方式）
2、DataFrame -> RDD ： df.rdd
注意输出类型：res2: Array[org.apache.spark.sql.Row] = Array([Michael,29], [Andy,30], [Justin,19])

1、 RDD -> DataSet ： rdd.map(para => Person(para(0).trim(), para(1).trim().toInt)).toDS() // 需要先定义样例类 -> toDS()
2、 DataSet -> RDD ： ds.rdd
注意输出类型：res5: Array[Person] = Array(Person(Michael,29), Person(Andy,30), Person(Justin,19))

1、 DataFrame -> DataSet ： df.as[Person] // 传入类型
2、 DataSet -> DataFrame ： ds.toDF()

========== DataFrame 的 Schema 的获取方式 ==========
RDD -> DataFram 的三种方式：

// 将没有包含 case 类的 RDD 转换成 DataFrame
rdd.map(para => (para(0).trim(), para(1).trim().toInt)).toDF("name", "age") // RDD -> 元组 -> toDF()（注意：这是第一种方式）

// 将包含有 case 类的 RDD 转换成 DataFrame，注意：需要我们先定义 case 类
// 通过反射的方式来设置 Schema 信息，适合于编译期能确定列的情况
rdd.map(attributes => Person(attributes(0), attributes(1).trim().toInt)).toDF() // 样例类-> RDD -> toDF()（注意：这是第二种方式）

// 通过编程的方式来设置 Schema 信息，适合于编译期不能确定列的情况（注意：这是第三种方式）
val schemaString = "name age" // 实际开发中 schemaString 是动态生成的
val fields = schemaString.map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)
val rdd[Row] = rdd.map(attributes => Row(attributes(0.trim), attributes(1).trim))
val peopeDF = spark.createDataFrame(rdd[Row], schema)
========== 对于 DataFrame Row 对象的访问方式 ==========
1、由 DataFrame = Dataset[Row] 可知， DataFrame 里面每一行都是 Row 对象。
2、如果需要访问 Row 对象中的每一个元素，可以通过索引 row(0)；也可以通过列名 row.getAsString 或者索引 row.getAsInt。

========== 应用 UDF 函数（用户自定义函数） ==========
1、通过 spark.udf.register(funcName, func) 来注册一个 UDF 函数，name 是 UDF 调用时的标识符，即函数名，fun 是一个函数，用于处理字段。
2、你需要将一个 DF 或者 DS 注册为一个临时表。
3、通过 spark.sql 去运行一个 SQL 语句，在 SQL 语句中可以通过 funcName(列名) 方式来应用 UDF 函数。
示例代码如下：

scala> val df = spark.read.json("examples/src/main/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.show()

scala> spark.udf.register("addName", (x: String) => "Name:" + x)
res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))

scala> df.createOrReplaceTempView("people")

scala> spark.sql("select addName(name), age from people").show()

scala> spark.sql("select addName(name) as newName, age from people").show()
========== 应用 UDAF 函数（用户自定义聚合函数） ==========
1、弱类型用户自定义聚合函数
步骤如下：
（1）新建一个 Class 继承UserDefinedAggregateFunction，然后复写方法：

    // 聚合函数需要输入参数的数据类型
    override def inputSchema: StructType = ???

    // 聚合缓冲区中值的数据类型
    override def bufferSchema: StructType = ???

    // 返回值的数据类型
    override def dataType: DataType = ???

    // 对于相同的输入一直有相同的输出
    override def deterministic: Boolean = true

    // 用于初始化你的数据结构
    override def initialize(buffer: MutableAggregationBuffer): Unit = ???

    // 相同 Execute 间的数据合并（同一分区）
    override def update(buffer: MutableAggregationBuffer, input: Row): Unit = ???

    // 不同 Execute 间的数据合并（不同分区）
    override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = ???

    // 计算最终结果
    override def evaluate(buffer: Row): Any = ???
（2）你需要通过 spark.udf.resigter 去注册你的 UDAF 函数。
（3）需要通过 spark.sql 去运行你的 SQL 语句，可以通过 select UDAF(列名) 来应用你的用户自定义聚合函数。

2、强类型的用户自定义聚合函数
步骤如下：
（1）新建一个class，继承Aggregator[Employee, Average, Double]
其中 Employee 是在应用聚合函数的时候传入的对象，Average 是聚合函数在运行的时候内部需要的数据结构，Double 是聚合函数最终需要输出的类型。这些可以根据自己的业务需求去调整。
复写相对应的方法：

    // 用于定义一个聚合函数内部需要的数据结构
    override def zero: Average = ???

    // 针对每个分区内部每一个输入来更新你的数据结构
    override def reduce(b: Average, a: Employee): Average = ???

    // 用于对于不同分区的结构进行聚合
    override def merge(b1: Average, b2: Average): Average = ???

    // 计算输出
    override def finish(reduction: Average): Double = ???

    // 设定之间值类型的编码器，要转换成 case 类
    // Encoders.product 是进行 scala 元组和 case 类转换的编码器
    override def bufferEncoder: Encoder[Average] = ???

    // 设定最终输出值的编码器
    override def outputEncoder: Encoder[Double] = ???
2、新建一个 UDAF 实例，通过 DF 或者 DS 的 DSL 风格语法去应用。

========== Spark SQL 的输入和输出 ==========
1、对于 Spark SQL 的输入需要使用 sparkSession.read 方法

（1）通用模式 sparkSession.read.format("json").load("path")     支持的类型有：parquet、json、text、csv、orc、jdbc、......
（2）专业模式 sparkSession.read.json("path") 或 csv 或 ...      即直接指定类型
2、对于 Spark SQL 的输出需要使用 sparkSession.write 方法

（1）通用模式 dataFrame.write.format("json").save("path")       支持的类型有：parquet、json、text、csv、orc、jdbc、......
（2）专业模式 dataFrame.write.csv("path") 或 json 或 ...   　　　即直接指定类型
3、如果使用通用模式，则 spark 默认的 parquet 是默认格式，那么 sparkSession.read.load 它加载的默认是 parquet 格式；dataFrame.write.save 也是默认保存成 parquet 格式。
4、注意：如果需要保存成一个 text 文件，那么需要 dataFrame 里面只有一列数据。

========== Spark SQL 与 Hive 的集成 ==========
内置 Hive
1、Spark 内置有 Hive，Spark 2.1.1 内置的 Hive 是 1.2.1。
2、如果要使用内嵌的 Hive，什么都不用做，直接用就可以了。但是呢，此时的我们只能创建表，如果查询表的话会报错，原因是：本地有 spark-warehouse 目录，而其他机器节点没有 spark-warehouse 目录。解决办法如下：
3、需要将 core-site.xml 和 hdfs-site.xml 拷贝到 spark 的 conf 目录下，然后分发至其他机器节点。如果 spark 路径下发现有 metastore_db 和 spark-warehouse，删除掉。然后重启集群。
4、在第一次启动创建 metastore 的时候，需要指定 spark.sql.warehouse.dir 这个参数，
比如：bin/spark-shell --conf spark.sql.warehouse.dir=hdfs://hadoop102:9000/spark_warehouse
5、注意：如果在 load 数据的时候，需要先将数据放到 HDFS 上。

外部 Hive
1、需要将 hive-site.xml 拷贝到 spark 的 conf 目录下，然后分发至其他机器节点。
2、如果 hive 的 metestore 使用的是 mysql 数据库，那么需要将 mysql 的 jdbc 驱动包放到 spark 的 jars 目录下。
3、可以通过 spark-sql 或者 spark-shell 来进行 sql 的查询，完成和 hive 的连接。

hive、spark、hdfs 关系:
  spark 文件中有两个文件夹：spark-warehouse、metastore_db，当我们拷贝 hive-site.xml 文件到 spark 的 conf 目录后，会读取 Hive 中的 warehouse 文件，获取到 hive 中的表格数据。


==============
sparksql执行流程分析
2019年03月28日 21:12:17 野狼e族 阅读数：591
版权声明：本文为博主原创文章，未经博主允许不得转载。	https://blog.csdn.net/qq_24505127/article/details/88878293
    Spark sql是spark内部最核心，也是社区最活跃的组件。Spark SQL支持在Spark中执行SQL,或者HiveQL的关系查询表达式。列式存储的类RDD（DataSet/DataFrame）数据类型以及对sql语句的支持使它更容易上手，同时，它对数据的抽取、清洗的特性，使它广泛的用于etl，甚至是机器学习领域。因此，saprk sql较其他spark组件，获得了更多的使用者。

　　下文，我们首先通过查看一个简单的sql的执行计划，对sql的执行流程有一个简单的认识，后面将通过对sql优化器catalyst的每个部分的介绍，来让大家更深入的了解sql后台的执行流程。由于此模板中代码较多，我们在此仅就执行流程中涉及到的主要代码进行介绍，方便大家更快地浏览spark sql的源码。本文中涉及到的源码spark 2.1.1的。

1. catalyst整体执行流程介绍
1.1 catalyst整体执行流程
说到spark sql不得不提的当然是Catalyst了。Catalyst是spark sql的核心，是一套针对spark sql 语句执行过程中的查询优化框架。因此要理解spark sql的执行流程，理解Catalyst的工作流程是理解spark sql的关键。而说到Catalyst，就必须得上下面这张图1了，这张图描述了spark sql执行的全流程。其中，长方形框内为catalyst的工作流程。



图1 spark sql 执行流程图

SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；此时，Optimizer再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan。

1.2 一个简单sql语句的执行
为了更好的对整个过程进行理解，下面通过一个简单的sql示例来查看采用catalyst框架执行sql的过程。示例代码如下：

object TestSql {



  case class Student(id:Long,name:String,chinese:String,math:String,English:String,age:Int)

  case class Score(sid:Long,weight1:String,weight2:String,weight3:String)



  def main(args: Array[String]): Unit = {

  //使用saprkSession初始化spark运行环境

  val spark=SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()

  //引入spark sql的隐式转换

   import sqlContext.implicits._

  //读取第一个文件的数据并转换成DataFrame

  val testP1 = spark.sparkContext.textFile("/home/dev/testP1").map(_.split(" ")).map(p=>Student(p(0).toLong,p(1),p(2),p(3),p(4),p(5).trim.toInt)).toDF()

  //注册成表

    testP1.registerTempTable("studentTable")

  //读取第二个文件的数据并转换成DataFrame

    val testp2 = spark.sparkContext.textFile("/home/dev/testP2").map(_.split(" ")).map(p=>Score(p(0).toLong,p(1),p(2),p(3))).toDF()

  //注册成表

    testp2.registerTempTable("scoreTable")

  //查看sql的逻辑执行计划

    val dataframe = spark.sql("select sum(chineseScore) from " +

      "(select x.id,x.chinese+20+30 as chineseScore,x.math from studentTable x inner join  scoreTable y on x.id=y.sid)z" +

      " where z.chineseScore <100").map(p=>(p.getLong(0))).collect.foreach(println)
此例也是针对spark2.1.1版本的，程序的入口是SparkSession。由于此例超级简单，做过spark的人一眼就能看出，而且每行代码都带有中文注释，所以在这里，我们就不做具体的介绍了。

　　这里涉及到的sql查询就是最后一句，通过spark shell可以看到该sql查询的逻辑执行计划和物理执行计划。进入sparkshell后，输入一下代码即可显示此sql查询的执行计划。



spark.sql("select sum(chineseScore) from " +
      "(select x.id,x.chinese+20+30 as chineseScore,x.math from  studentTable x inner join  scoreTable y on x.id=y.sid)z" +
      " where z.chineseScore <100").explain(true)
这里，是使用DataSet的explain函数实现逻辑执行计划和物理执行计划的打印，调用explain的源码如下：

/**
* Prints the plans (logical and physical) to the console for debugging purposes.
*
* @group basic
* @since 1.6.0
*/

def explain(extended: Boolean): Unit = {

  val explain = ExplainCommand(queryExecution.logical, extended = extended)

  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {

    // scalastyle:off println

    r => println(r.getString(0))

    // scalastyle:on println

  }

}
显示在spark shell中的unresolved logical plan、resolved logical plan、optimized logical plan和physical plan如下图2所示：







图2 spark sql 执行计划



　　将上图2中的Parsed Logical Plan表示成树结构如下图3所示。Catalyst中的parser将图左中一个sql查询的字符串解析成图右的一个AST语法树，该语法树就称为Parsed Logical Plan。解析后的逻辑计划基本形成了执行计划的基础骨架，此逻辑执行计划被称为 unresolved Logical Plan，也就是说该逻辑计划是无法执行的，系统并不知道语法树中的每个词是神马意思，如图中的filter，join，以及studentTable等。



图3 Parsed Logical Plan树



　　将上图2中的Analyzed logical plan，即Resolved logical plan表示成树结构如下图4所示。Catalyst的analyzer将unresolved Logical Plan解析成resolved Logical Plan。Analyzer借助cataLog（下文介绍）中表的结构信息、函数信息等将此逻辑计划解析成可被识别的逻辑执行计划。



图4 Analyzed logical plan树



　　optimized logical plan与physical plan的树结构跟上面两种逻辑执行计划树结构的画法相似，下面就不在画了。从optimized logical plan可出，此次优化使用了Filter下推的策略，即将Filter下推到子查询中实现，继而减少后续数据的处理量。

　　前面我们展示了catalyst执行一段sql语句的大致流程，下面我们就从源代码的角度来分析catalyst的每个部分内部如何实现，以及它们之间是如何承接的。

2. catalyst各个模块介绍
本章我们通过分析上面的例子代码的调用过程来分析catalys各个部分的主要代码模块，spark sql的入口是最后一句，SparkSession类里的sql函数，传入一个sql字符串，返回一个dataframe对象。入口调用的代码如下：

def sql(sqlText: String): DataFrame = {

  Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))

}
2.1 Parser
从入口代码可看出，首先调用sqlParser的parsePlan方法，将sql字符串解析成unresolved逻辑执行计划。parsePlan的具体实现在AbstractSqlParser类中。如下：

/** Creates LogicalPlan for a given SQL string. */

override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>

  astBuilder.visitSingleStatement(parser.singleStatement()) match {

    case plan: LogicalPlan => plan

    case _ =>

      val position = Origin(None, None)

      throw new ParseException(Option(sqlText), "Unsupported SQL statement", position, position)

  }

}

由上段代码可看出，调用的主函数是parse，继续进入到parse中，代码如下：

protected def parse[T](command: String)(toResult: SqlBaseParser => T): T = {

  logInfo(s"Parsing command: $command")



  val lexer = new SqlBaseLexer(new ANTLRNoCaseStringStream(command))

  lexer.removeErrorListeners()

  lexer.addErrorListener(ParseErrorListener)



  val tokenStream = new CommonTokenStream(lexer)

  val parser = new SqlBaseParser(tokenStream)

  parser.addParseListener(PostProcessor)

  parser.removeErrorListeners()

  parser.addErrorListener(ParseErrorListener)



  try {

    try {

      // first, try parsing with potentially faster SLL(Strong-LL) mode

      parser.getInterpreter.setPredictionMode(PredictionMode.SLL)

      toResult(parser)

    }

  }

}

从parse函数可以看出，这里对于SQL语句的解析采用的是ANTLR 4，这里使用到了两个类：词法解析器SqlBaseLexer和语法解析器SqlBaseParser

SqlBaseLexer和SqlBaseParser均是使用ANTLR 4自动生成的Java类。这里，采用这两个解析器将SQL语句解析成了ANTLR 4的语法树结构ParseTree。之后，在parsePlan（见code 2）中，使用AstBuilder（AstBuilder.scala）将ANTLR 4语法树结构转换成catalyst表达式，即logical plan。

　　此时生成的逻辑执行计划成为unresolved logical plan。只是将sql串解析成类似语法树结构的执行计划，系统并不知道每个词所表示的意思，离真正能够执行还差很远。

2.2 Analyzer
parser生成逻辑执行计划后，使用analyzer将逻辑执行计划进行分析。我们回到Dataset的ofRows函数:

def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {

  val qe = sparkSession.sessionState.executePlan(logicalPlan)

  qe.assertAnalyzed()

  new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))

}
这里首先创建了queryExecution类对象，QueryExecution中定义了sql执行过程中的关键步骤，是sql执行的关键类，返回一个dataframe类型的对象。QueryExecution类中的成员都是lazy的，被调用时才会执行。只有等到程序中出现action算子时，才会调用 queryExecution类中的executedPlan成员，原先生成的逻辑执行计划才会被优化器优化，并转换成物理执行计划真正的被系统调用执行。 　　　


//调用analyzer解析器

lazy val analyzed: LogicalPlan = {

  SparkSession.setActiveSession(sparkSession)

  sparkSession.sessionState.analyzer.execute(logical)

}



lazy val withCachedData: LogicalPlan = {

  assertAnalyzed()

  assertSupported()

  sparkSession.sharedState.cacheManager.useCachedData(analyzed)

}

//调用optimizer优化器

lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData)



//将优化后的逻辑执行计划转换成物理执行计划

lazy val sparkPlan: SparkPlan = {

  SparkSession.setActiveSession(sparkSession)

  // TODO: We use next(), i.e. take the first plan returned by the planner, here for now,

  //       but we will implement to choose the best plan.

  planner.plan(ReturnAnswer(optimizedPlan)).next()

}



// executedPlan should not be used to initialize any SparkPlan. It should be

// only used for execution.

lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan)



/**
* Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal
* row format conversions as needed.
*/

protected def prepareForExecution(plan: SparkPlan): SparkPlan = {

  preparations.foldLeft(plan) { case (sp, rule) => rule.apply(sp) }

}
　　QueryExecution类的主要成员如下所示。其中定义了解析器analyzer、优化器optimizer以及生成物理执行计划的sparkPlan。前文有介绍，analyzer的主要职责是将parser生成的unresolved logical plan解析生成logical plan。调用analyzer的代码在QueryExecution中，code 5中已经有贴出。此模块的主函数来自于analyzer的父类RuleExecutor。主函数execute实现在RuleExecutor类中，代码如下：


/**
   * Executes the batches of rules defined by the subclass. The batches are executed serially
   * using the defined execution strategy. Within each batch, rules are also executed serially.
   */

  def execute(plan: TreeType): TreeType = {

    var curPlan = plan



    batches.foreach { batch =>

      val batchStartPlan = curPlan

      var iteration = 1

      var lastPlan = curPlan

      var continue = true



      // Run until fix point (or the max number of iterations as specified in the strategy.

      while (continue) {

        curPlan = batch.rules.foldLeft(curPlan) {

          case (plan, rule) =>

            val startTime = System.nanoTime()

            val result = rule(plan)

            val runTime = System.nanoTime() - startTime

            RuleExecutor.timeMap.addAndGet(rule.ruleName, runTime)



            if (!result.fastEquals(plan)) {

              logTrace(

                s"""
                  |=== Applying Rule ${rule.ruleName} ===
                  |${sideBySide(plan.treeString, result.treeString).mkString("\n")}
                """.stripMargin)

            }



            result

        }

        iteration += 1

}
此函数实现了针对analyzer类中定义的每一个batch（类别），按照batch中定义的fix point(策略)和rule（规则）对Unresolved的逻辑计划进行解析。其中batch的结构如下：

/** A batch of rules. */

protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)
由于在analyzer的batchs中定义了多个规则，代码段很长，因此这里就不再贴出，有需要的请去spark的源码中找到Analyzer类查看。

　　在batchs里的这些batch中，Resolution是最常用的，从字面意思就可以看出其用途，就是将parser解析后的逻辑计划里的各个节点，转变成resolved节点。而其中ResolveRelations是比较好理解的一个rule（规则），这一步调用了catalog这个对象，Catalog对象里面维护了一个tableName,Logical Plan的HashMap结果。通过这个Catalog目录来寻找当前表的结构，从而从中解析出这个表的字段，如UnResolvedRelations 会得到一个tableWithQualifiers。（即表和字段）。catalog中缓存表名称和逻辑执行计划关系的代码如下：

/**
* A cache of qualified table names to table relation plans.
*/

val tableRelationCache: Cache[QualifiedTableName, LogicalPlan] = {

  val cacheSize = conf.tableRelationCacheSize

  CacheBuilder.newBuilder().maximumSize(cacheSize).build[QualifiedTableName, LogicalPlan]()

}
2.3 Optimizer
optimizer是catalyst中关键的一个部分，提供对sql查询的一个优化。optimizer的主要职责是针对Analyzer的resolved logical plan，根据不同的batch优化策略)，来对执行计划树进行优化，优化逻辑计划节点(Logical Plan)以及表达式(Expression)，同时，此部分也是转换成物理执行计划的前置。optimizer的调用在QueryExecution类中，代码code 5中已经贴出。

　　其工作方式与上面讲的Analyzer类似，因为它们的主函数executor都是继承自RuleExecutor。因此，optimizer的主函数如上面的code 6代码，这里就不在贴出。optimizer的batchs（优化策略）定义如下：

def batches: Seq[Batch] = {

  // Technically some of the rules in Finish Analysis are not optimizer rules and belong more

  // in the analyzer, because they are needed for correctness (e.g. ComputeCurrentTime).

  // However, because we also use the analyzer to canonicalized queries (for view definition),

  // we do not eliminate subqueries or compute current time in the analyzer.

  Batch("Finish Analysis", Once,

    EliminateSubqueryAliases,

    EliminateView,

    ReplaceExpressions,

    ComputeCurrentTime,

    GetCurrentDatabase(sessionCatalog),

    RewriteDistinctAggregates,

    ReplaceDeduplicateWithAggregate) ::

  //////////////////////////////////////////////////////////////////////////////////////////

  // Optimizer rules start here

  //////////////////////////////////////////////////////////////////////////////////////////

  // - Do the first call of CombineUnions before starting the major Optimizer rules,

  //   since it can reduce the number of iteration and the other rules could add/move

  //   extra operators between two adjacent Union operators.

  // - Call CombineUnions again in Batch("Operator Optimizations"),

  //   since the other rules might make two separate Unions operators adjacent.

  Batch("Union", Once,

    CombineUnions) ::

  Batch("Pullup Correlated Expressions", Once,

    PullupCorrelatedPredicates) ::

  Batch("Subquery", Once,

    OptimizeSubqueries) ::

  Batch("Replace Operators", fixedPoint,

    ReplaceIntersectWithSemiJoin,

    ReplaceExceptWithAntiJoin,

    ReplaceDistinctWithAggregate) ::

  Batch("Aggregate", fixedPoint,

    RemoveLiteralFromGroupExpressions,

    RemoveRepetitionFromGroupExpressions) ::

  Batch("Operator Optimizations", fixedPoint,

    // Operator push down

    PushProjectionThroughUnion,

    ReorderJoin,

    EliminateOuterJoin,

    PushPredicateThroughJoin,

    PushDownPredicate,

    LimitPushDown(conf),

    ColumnPruning,

    InferFiltersFromConstraints,

    // Operator combine

    CollapseRepartition,

    CollapseProject,

    CollapseWindow,

    CombineFilters,

    CombineLimits,

    CombineUnions,

    // Constant folding and strength reduction

    NullPropagation(conf),

    FoldablePropagation,

    OptimizeIn(conf),

    ConstantFolding,

    ReorderAssociativeOperator,

    LikeSimplification,

    BooleanSimplification,

    SimplifyConditionals,

    RemoveDispensableExpressions,

    SimplifyBinaryComparison,

    PruneFilters,

    EliminateSorts,

    SimplifyCasts,

    SimplifyCaseConversionExpressions,

    RewriteCorrelatedScalarSubquery,

    EliminateSerialization,

    RemoveRedundantAliases,

    RemoveRedundantProject,

    SimplifyCreateStructOps,

    SimplifyCreateArrayOps,

    SimplifyCreateMapOps) ::

  Batch("Check Cartesian Products", Once,

    CheckCartesianProducts(conf)) ::

  Batch("Join Reorder", Once,

    CostBasedJoinReorder(conf)) ::

  Batch("Decimal Optimizations", fixedPoint,

    DecimalAggregates(conf)) ::

  Batch("Typed Filter Optimization", fixedPoint,

    CombineTypedFilters) ::

  Batch("LocalRelation", fixedPoint,

    ConvertToLocalRelation,

    PropagateEmptyRelation) ::

  Batch("OptimizeCodegen", Once,

    OptimizeCodegen(conf)) ::

  Batch("RewriteSubquery", Once,

    RewritePredicateSubquery,

    CollapseProject) :: Nil

}
由此可以看出，Spark 2.1.1版本增加了更多的优化策略，因此如果要提高spark sql程序的性能，升级spark版本是非常必要的。

　　其中，"Operator Optimizations"，即操作优化是使用最多的，也是比较好理解的优化操作。"Operator Optimizations"中包括的规则有PushProjectionThroughUnion，ReorderJoin等。

PushProjectionThroughUnion策略是将左边子查询的Filter或者是projections移动到union的右边子查询中。例如针对下面代码

case class a:item1:String,item2:String,item3:String

case class b:item1:String,item2:String



select a.item1,b.item2 from a where a.item1>'example'  from a union all (select item1,item2 from b )

此时，通过PushProjectionThroughUnion规则后，查询优化器会将sql改为下面的sql，即将Filter右移到了union的右端。如下所示。

select a.item1,b.item2 from a where a.item1>’example’  union all (select item1,item2 from b where item1>’example’)

RorderJoin，顾名思义，就是对多个join操作进行重新排序。具体操作就是将一系列的带有join的子执行计划进行排序，尽可能地将带有条件过滤的子执行计划下推到执行树的最底层，这样能尽可能地减少join的数据量。

　　例如下面代码中是三个表做join操作，而过滤条件是针对表a的，但熟悉sql的人就会发现对a中字段item1的过滤可以挪到子查询中，这样可以减少join的时候数据量，如果满足此过滤条件的记录比较少，则可以大大地提高join的性能。

case class b:item1:String,item2:String

select a.item1,d.item2 from a where a.item1> ‘example’ join (select b.item1,b.item2 from b join c on b.item1=c.item1) d on a.item1= d.item1

2.4 SparkPlann
optimizer将逻辑执行计划优化后，接着该SparkPlan登场了，SparkPlann将optimized logical plan转换成physical plans。执行代码如下：

code 10

lazy val sparkPlan: SparkPlan = {

  SparkSession.setActiveSession(sparkSession)

  // TODO: We use next(), i.e. take the first plan returned by the planner, here for now,

  //       but we will implement to choose the best plan.

  planner.plan(ReturnAnswer(optimizedPlan)).next()

}

其中，planner为SparkPlanner类的对象，对象的创建如下code 11所示。该对象中定义了一系列的执行策略，包括LeftSemiJoin 、HashJoin等等，这些策略用来指定实际查询时所做的操作。SparkPlanner中定义的策略如下code 12所示：

def strategies: Seq[Strategy] =

      extraStrategies ++ (

      FileSourceStrategy ::

      DataSourceStrategy ::

      SpecialLimits ::

      Aggregation ::

      JoinSelection ::

      InMemoryScans ::

      BasicOperators :: Nil)


/**

   * Planner that converts optimized logical plans to physical plans.

   */

  def planner: SparkPlanner =

    new SparkPlanner(sparkContext, conf, experimentalMethods.extraStrategies)
plan真正的处理函数如下的code 13所示。该函数的功能是整合所有的Strategy，_(plan)每个Strategy应用plan上，得到所有Strategies执行完后生成的所有Physical Plan的集合。

def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = {

    // Obviously a lot to do here still...



    // Collect physical plan candidates.

    val candidates = strategies.iterator.flatMap(_(plan))



    // The candidates may contain placeholders marked as [[planLater]],

    // so try to replace them by their child plans.

    val plans = candidates.flatMap { candidate =>

      val placeholders = collectPlaceholders(candidate)



      if (placeholders.isEmpty) {

        // Take the candidate as is because it does not contain placeholders.

        Iterator(candidate)

      } else {

        // Plan the logical plan marked as [[planLater]] and replace the placeholders.

        placeholders.iterator.foldLeft(Iterator(candidate)) {

          case (candidatesWithPlaceholders, (placeholder, logicalPlan)) =>

            // Plan the logical plan for the placeholder.

            val childPlans = this.plan(logicalPlan)



            candidatesWithPlaceholders.flatMap { candidateWithPlaceholders =>

              childPlans.map { childPlan =>

                // Replace the placeholder by the child plan

                candidateWithPlaceholders.transformUp {

                  case p if p == placeholder => childPlan

                }

              }

            }

        }

}
3. spark2.x较spark1.x性能
对源码的分析后，可以看出spark 2.0较 spark 1.x版本性能有较大提升，具体可从以下几个方面看出：

1.parser语法解析不同

　　spark 1.x版本使用的是scala的parser语法解析器，而spark 2.x版本使用的是ANTLR4，解析性能更好

2.spark 2.x的optimizer优化策略不同

　　spark 2.x为optimizer优化器提供了更加丰富的优化策略，从两个版本里optimizer类中的batchs中可以看出。

3．spark 2.x提供了第二代Tungsten引擎

　　Spark2.x移植了第二代Tungsten引擎，这一代引擎是建立在现代编译器和MPP数据库的基础上，并且应用到数据的处理中。主要的思想是将那些拖慢整个程序执行速度的代码放到一个单独的函数中，消除虚拟函数的调用，并使用寄存器来存放中间结果。这项技术被称作“whole-stage code generation.”

　　下面通过在单机上执行10亿条数据的aggregations and joins操作，来对比spark1.6和2.0的性能。其中，ns为纳秒，表格中的处理时间为单个线程处理单行数据所用时间。由此，可看出spark2.0的处理性能远远好于1.6。


==============================================

一、从SQL到RDD

1. 一个简单的例子

样例数据 test.json

{"name":"上海滩","singer":"叶丽仪","album":"香港电视剧主题歌","path":"mp3/shanghaitan.mp3"},
{"name":"一生何求","singer":"陈百强","album":"香港电视剧主题歌","path":"mp3/shanghaitan.mp3"},
{"name":"红日","singer":"李克勤","album":"怀旧专辑","path":"mp3/shanghaitan.mp3"},
{"name":"爱如潮水","singer":"张信哲","album":"怀旧专辑","path":"mp3/airucaoshun.mp3"},
{"name":"红茶馆","singer":"陈惠嫻","album":"怀旧专辑","path":"mp3/redteabar.mp3"}

SparkSQL读进来，得到DataFrame

scala> spark.read.json("test.json")
res1: org.apache.spark.sql.DataFrame = [album: string, name: string ... 2 more fields]

scala> res1.show(false)
+--------+----+-------------------+------+
|album   |name|path               |singer|
+--------+----+-------------------+------+
|香港电视剧主题歌|上海滩 |mp3/shanghaitan.mp3|叶丽仪   |
|香港电视剧主题歌|一生何求|mp3/shanghaitan.mp3|陈百强   |
|怀旧专辑    |红日  |mp3/shanghaitan.mp3|李克勤   |
|怀旧专辑    |爱如潮水|mp3/airucaoshun.mp3|张信哲   |
|怀旧专辑    |红茶馆 |mp3/redteabar.mp3  |陈惠嫻   |
+--------+----+-------------------+------+

执行个简单的查询

scala> res1.createOrReplaceTempView("test")
scala> spark.sql("SELECT * FROM test WHERE album='香港电视剧主题歌'").show()
+--------+----+-------------------+------+
|   album|name|               path|singer|
+--------+----+-------------------+------+
|香港电视剧主题歌| 上海滩|mp3/shanghaitan.mp3|   叶丽仪|
|香港电视剧主题歌|一生何求|mp3/shanghaitan.mp3|   陈百强|
+--------+----+-------------------+------+

2. 执行过程
SQL查询 -> [逻辑计划] -> [物理计划]
1
逻辑计划将SQL语句转换成逻辑算子树；逻辑计划只是中间阶段，不会直接提交执行；物理计划对逻辑算子树进行转换，生成物理算子树，物理算子树的节点会直接生成RDD或对RDD进行transformation操作；

进一步地，逻辑计划包括三个阶段：

SQL查询 -> Unresolved 逻辑计划；
Unresolved逻辑计划 -> Analyzed逻辑计划；
Analyzed逻辑计划 -> Optimized逻辑计划
同样，物理计划也分为三个阶段：

逻辑算子树 -> Iterator[物理计划]；
Iterator[物理计划] -> SparkPlan；
SparkPlan -> Prepared SparkPlan；
打开Spark Web UI, 在SQL标签页，可以查看SQL语句编译后的结果：

== Parsed Logical Plan ==
GlobalLimit 21
+- LocalLimit 21
   +- AnalysisBarrier
         +- Project [cast(album#163 as string) AS album#215, cast(name#164 as string) AS name#216, cast(path#165 as string) AS path#217, cast(singer#166 as string) AS singer#218]
            +- Project [album#163, name#164, path#165, singer#166]
               +- Filter (album#163 = 香港电视剧主题歌)
                  +- SubqueryAlias test
                     +- Relation[album#163,name#164,path#165,singer#166] json

== Analyzed Logical Plan ==
album: string, name: string, path: string, singer: string
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(album#163 as string) AS album#215, cast(name#164 as string) AS name#216, cast(path#165 as string) AS path#217, cast(singer#166 as string) AS singer#218]
      +- Project [album#163, name#164, path#165, singer#166]
         +- Filter (album#163 = 香港电视剧主题歌)
            +- SubqueryAlias test
               +- Relation[album#163,name#164,path#165,singer#166] json

== Optimized Logical Plan ==
GlobalLimit 21
+- LocalLimit 21
   +- Filter (isnotnull(album#163) && (album#163 = 香港电视剧主题歌))
      +- Relation[album#163,name#164,path#165,singer#166] json

== Physical Plan ==
CollectLimit 21
+- *(1) Project [album#163, name#164, path#165, singer#166]
   +- *(1) Filter (isnotnull(album#163) && (album#163 = 香港电视剧主题歌))
      +- *(1) FileScan json [album#163,name#164,path#165,singer#166] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/renq/Downloads/spark-2.3.2-bin-hadoop2.6/data/test.json], PartitionFilters: [], PushedFilters: [IsNotNull(album), EqualTo(album,香港电视剧主题歌)], ReadSchema: struct<album:string,name:string,path:string,singer:string>


上面就是一条简单的SQL语句转换成RDD的流程；其他更加复杂的SQL语句（例如Join，Aggregation）的转换过程有更多的细节，但总体来看仍然遵循上述步骤。

SQL语句的整个转换过程都在Spark集群的Driver端进行，不涉及分布式环境；SparkSQL内部实现上述转换，涉及的组件包括SparkSqlParser类、Analyzer类、Optimizer类、SparkPlanner类等，最后封装成一个QueryExecution对象。更多细节不在本文展开。

二、概念和数据结构
SparkSQL内部实现上述流程的基础框架叫做Catalyst。Catalyst涉及了几个基础性概念，包括：InternalRow体系、TreeNode体系和Expression体系。

1. InternalRow
InternalRow的含义很直观，表示一行数据。物理算子树节点产生和转换的RDD的类型就是RDD[InternalRow]。InternalRow是一个抽象类，包括numFields，update方法和各列数据的get/set方法。



具体的逻辑由InternalRow的不同子类实现，目前包括BaseGenericInternalRow、UnsafeRow和JoinedRow三个子类。

BaseGenericInternalRow 同样是抽象类，实现了InternalRow定义的所有get方法，实现是通过调用类中定义的genericGet虚函数进行；
JoinedRow 用于实现Join操作，将两个InternalRow放在一起，形成新的InternalRow。
UnsafeRow 一种Spark自定义的对象存储格式，不采用Java对象存储，避免了JVM中的垃圾回收；UnsafeRow对行数据进行了编码，使得存储更加高效；
BaseGenericInternalRow的衍生出3个子类，GenericInternalRow、SpecificInternalRow和MutableUnsafeRow。而GernericInternalRow和SpecificInternalRow的区别在于构造参数的类型，其中GernericInternalRow的构造参数类型是Array[Any], 一旦创建就不允许通过set操作修改； 而SpecificInternalRow的构造参数类型是Array[MutableValue]，允许通过set操作修改。

/**
 * An internal row implementation that uses an array of objects as the underlying storage.
 * Note that, while the array is not copied, and thus could technically be mutated after creation,
 * this is not allowed.
 */
class GenericInternalRow(val values: Array[Any]) extends BaseGenericInternalRow {
  /** No-arg constructor for serialization. */
  protected def this() = this(null)

  def this(size: Int) = this(new Array[Any](size))

  override protected def genericGet(ordinal: Int) = values(ordinal)

  override def toSeq(fieldTypes: Seq[DataType]): Seq[Any] = values.clone()

  override def numFields: Int = values.length

  override def setNullAt(i: Int): Unit = { values(i) = null}

  override def update(i: Int, value: Any): Unit = { values(i) = value }
}
/**
 * A row type that holds an array specialized container objects, of type [[MutableValue]], chosen
 * based on the dataTypes of each column.  The intent is to decrease garbage when modifying the
 * values of primitive columns.
 */
final class SpecificInternalRow(val values: Array[MutableValue]) extends BaseGenericInternalRow {

  def this(dataTypes: Seq[DataType]) =
    this(
      dataTypes.map {
        case BooleanType => new MutableBoolean
        case ByteType => new MutableByte
        case ShortType => new MutableShort
        // We use INT for DATE internally
        case IntegerType | DateType => new MutableInt
        // We use Long for Timestamp internally
        case LongType | TimestampType => new MutableLong
        case FloatType => new MutableFloat
        case DoubleType => new MutableDouble
        case _ => new MutableAny
      }.toArray)

  def this() = this(Seq.empty)

  def this(schema: StructType) = this(schema.fields.map(_.dataType))

  override def numFields: Int = values.length

  // 此处省略部分代码
}
MutableUnsafeRow后面详细介绍UnsafeRow时再讲。

2. TreeNode
从文章开头的例子中，无论是物理计划还是逻辑计划都会生成表达式树，在Catalyst中，对应的是TreeNode。TreeNode是Spark SQL中所有树结构的基类，定义了树遍历的接口和一系列通用的集合操作，例如：

collectLeaves 获取当前TreeNode的所有叶子节点
collectFirst 先序遍历所有节点并返回第一个满足条件的节点
withNewChildren 将当前节点的子节点替换为新的子节点
transformDown 用先序遍历顺序将规则作用于所有节点
transformUp 用后序遍历方式将规则作用于所有节点
transformChildren 递归将规则作用到所有子节点
特别值得注意的是，Catalyst提供了定位节点功能，根据TreeNode可以定位到SQL字符串的行数和起始位置，这个功能在SQL解析发生错误时可以帮助开发者快速找到出错的位置。

case class Origin(
  line: Option[Int] = None,
  startPosition: Option[Int] = None)

/**
 * Provides a location for TreeNodes to ask about the context of their origin.  For example, which
 * line of code is currently being parsed.
 */
object CurrentOrigin {
  private val value = new ThreadLocal[Origin]() {
    override def initialValue: Origin = Origin()
  }

  def get: Origin = value.get()
  def set(o: Origin): Unit = value.set(o)

  def reset(): Unit = value.set(Origin())

  def setPosition(line: Int, start: Int): Unit = {
    value.set(
      value.get.copy(line = Some(line), startPosition = Some(start)))
  }

  def withOrigin[A](o: Origin)(f: => A): A = {
    set(o)
    val ret = try f finally { reset() }
    ret
  }
}
3. Expression


===============

今天主要是分享Spark SQL Dataset数据源的分区特性，而且是第一弹的数据格式是partquet。

常见RDD分区

Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。

举几个浪尖在星球里分享比较多的例子，比如：

Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。

Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。

普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。

这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。

准备数据

首先是由Seq数据集合生成一个Dataset

val sales = spark.createDataFrame(Seq(
     ("Warsaw", 2016, 110),
     ("Warsaw", 2017, 10),
     ("Warsaw", 2015, 100),
     ("Warsaw", 2015, 50),
     ("Warsaw", 2015, 80),
     ("Warsaw", 2015, 100),
     ("Warsaw", 2015, 130),
     ("Warsaw", 2015, 160),
     ("Warsaw", 2017, 200),
     ("Beijing", 2017, 100),
     ("Beijing", 2016, 150),
     ("Beijing", 2015, 50),
     ("Beijing", 2015, 30),
     ("Beijing", 2015, 10),
     ("Beijing", 2014, 200),
     ("Beijing", 2014, 170),
     ("Boston", 2017, 50),
     ("Boston", 2017, 70),
     ("Boston", 2017, 110),
     ("Boston", 2017, 150),
     ("Boston", 2017, 180),
     ("Boston", 2016, 30),
     ("Boston", 2015, 200),
     ("Boston", 2014, 20)
   )).toDF("city", "year", "amount")

将Dataset存处为partquet格式的hive表，分两种情况：

用city和year字段分区

sales.write.partitionBy("city","year").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCityAndYear")

用city字段分区

sales.write.partitionBy("city").mode(SaveMode.Overwrite).saveAsTable("ParquetTestCity")

读取数据采用的是

val res = spark.read.parquet("/user/hive/warehouse/parquettestcity")

直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。

1. spark.default.parallelism =40

Dataset的分区数是由参数：

println("partition size = "+res.rdd.partitions.length)
目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。



这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。

2. spark.default.parallelism =4

Dataset的分区数是由参数：

println("partition size = "+res.rdd.partitions.length)
目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。



那么数据源生成的Dataset的分区数到底是如何决定的呢？

我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。

private def createNonBucketedReadRDD(
                                       readFile: (PartitionedFile) => Iterator[InternalRow],
                                       selectedPartitions: Seq[PartitionDirectory],
                                       fsRelation: HadoopFsRelation): RDD[InternalRow] = {
   /*
     selectedPartitions 的大小代表目录数目
    */
   println("selectedPartitions.size : "+ selectedPartitions.size)
   val defaultMaxSplitBytes =
     fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes
   val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes

   // spark.default.parallelism
   val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism

   // 计算文件总大小，单位字节数
   val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum

   //计算平均每个并行度读取数据大小
   val bytesPerCore = totalBytes / defaultParallelism

   // 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值
   // 然后，比较spark.sql.files.maxPartitionBytes 取小者
   val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))
   logInfo(s"Planning scan with bin packing, max size: $maxSplitBytes bytes, " +
     s"open cost is considered as scanning $openCostInBytes bytes.")

   // 这对目录遍历
   val splitFiles = selectedPartitions.flatMap { partition =>
     partition.files.flatMap { file =>
       val blockLocations = getBlockLocations(file)

       //判断文件类型是否支持分割，以parquet为例，是支持分割的
       if (fsRelation.fileFormat.isSplitable(
         fsRelation.sparkSession, fsRelation.options, file.getPath)) {

//          eg. 0 until 2不包括 2。相当于
//        println(0 until(10) by 3) 输出 Range(0, 3, 6, 9)
         (0L until file.getLen by maxSplitBytes).map { offset =>

           // 计算文件剩余的量
           val remaining = file.getLen - offset

//            假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区
           val size = if (remaining > maxSplitBytes) maxSplitBytes else remaining

//            位置信息
           val hosts = getBlockHosts(blockLocations, offset, size)
           PartitionedFile(
             partition.values, file.getPath.toUri.toString, offset, size, hosts)
         }
       } else {
//          不可分割的话，那即是一个文件一个分区
         val hosts = getBlockHosts(blockLocations, 0, file.getLen)
         Seq(PartitionedFile(
           partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts))
       }
     }
   }.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse)

   val partitions = new ArrayBuffer[FilePartition]
   val currentFiles = new ArrayBuffer[PartitionedFile]
   var currentSize = 0L

   /** Close the current partition and move to the next. */
   def closePartition(): Unit = {
     if (currentFiles.nonEmpty) {
       val newPartition =
         FilePartition(
           partitions.size,
           currentFiles.toArray.toSeq) // Copy to a new Array.
       partitions += newPartition
     }
     currentFiles.clear()
     currentSize = 0
   }

   // Assign files to partitions using "Next Fit Decreasing"
   splitFiles.foreach { file =>
     if (currentSize + file.length > maxSplitBytes) {
       closePartition()
     }
     // Add the given file to the current partition.
     currentSize += file.length + openCostInBytes
     currentFiles += file
   }
   closePartition()

   println("FileScanRDD partitions size : "+partitions.size)
   new FileScanRDD(fsRelation.sparkSession, readFile, partitions)
 }



=======
Spark SQL从入门到精通
原创： 浪尖  Spark学习技巧  2018-09-26
本文主要是帮助大家从入门到精通掌握spark sql。篇幅较长，内容较丰富建议大家收藏，仔细阅读。

更多大数据，spark教程，请点击 阅读原文 加入浪尖知识星球获取。

微信群可以加浪尖微信 158570986 。

发家史

熟悉spark sql的都知道，spark sql是从shark发展而来。Shark为了实现Hive兼容，在HQL方面重用了Hive中HQL的解析、逻辑执行计划翻译、执行计划优化等逻辑，可以近似认为仅将物理执行计划从MR作业替换成了Spark作业（辅以内存列式存储等各种和Hive关系不大的优化）；

同时还依赖Hive Metastore和Hive SerDe（用于兼容现有的各种Hive存储格式）。
Spark SQL在Hive兼容层面仅依赖HQL parser、Hive Metastore和Hive SerDe。也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。执行计划生成和优化都由Catalyst负责。借助Scala的模式匹配等函数式语言特性，利用Catalyst开发执行计划优化策略比Hive要简洁得多。

Spark SQL



spark sql提供了多种接口：

1. 纯Sql 文本

2. dataset/dataframe api

当然，相应的，也会有各种客户端：

sql文本，可以用thriftserver/spark-sql

编码，Dataframe/dataset/sql



 Dataframe/Dataset API简介



Dataframe/Dataset也是分布式数据集，但与RDD不同的是其带有schema信息，类似一张表。

可以用下面一张图详细对比Dataset/dataframe和rdd的区别：



Dataset是在spark1.6引入的，目的是提供像RDD一样的强类型、使用强大的lambda函数，同时使用spark sql的优化执行引擎。到spark2.0以后，DataFrame变成类型为Row的Dataset，即为：

type DataFrame = Dataset[Row]




所以，很多移植spark1.6及之前的代码到spark2+的都会报错误，找不到dataframe类。

基本操作


val df = spark.read.json(“file:///opt/meitu/bigdata/src/main/data/people.json”)
df.show()
import spark.implicits._
df.printSchema()
df.select("name").show()
df.select($"name", $"age" + 1).show()
df.filter($"age" > 21).show()
df.groupBy("age").count().show()
spark.stop()
分区分桶 排序


分桶排序保存hive表
df.write.bucketBy(42,“name”).sortBy(“age”).saveAsTable(“people_bucketed”)
分区以parquet输出到指定目录
df.write.partitionBy("favorite_color").format("parquet").save("namesPartByColor.parquet")
分区分桶保存到hive表
df.write .partitionBy("favorite_color").bucketBy(42,"name").saveAsTable("users_partitioned_bucketed")
cube rullup pivot



cube
sales.cube("city", "year”).agg(sum("amount")as "amount”) .show()
rull up
sales.rollup("city", "year”).agg(sum("amount")as "amount”).show()
pivot 只能跟在groupby之后
sales.groupBy("year").pivot("city",Seq("Warsaw","Boston","Toronto")).agg(sum("amount")as "amount”).show()


SQL编程



Spark SQL允许用户提交SQL文本，支持一下三种手段编写sql文本：

1. spark 代码

2. spark-sql的shell

3. thriftserver

支持Spark SQL自身的语法，同时也兼容HSQL。

1. 编码

要先声明构建SQLContext或者SparkSession，这个是SparkSQL的编码入口。早起的版本使用的是SQLContext或者HiveContext，spark2以后，建议使用的是SparkSession。

1. SQLContext
new SQLContext(SparkContext)

2. HiveContext
new HiveContext(spark.sparkContext)

3. SparkSession
不使用hive元数据：
val spark = SparkSession.builder()
 .config(sparkConf) .getOrCreate()
使用hive元数据
val spark = SparkSession.builder()
 .config(sparkConf) .enableHiveSupport().getOrCreate()
使用

val df =spark.read.json("examples/src/main/resources/people.json")
df.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people").show()
2. spark-sql脚本

spark-sql 启动的时候类似于spark-submit 可以设置部署模式资源等，可以使用

bin/spark-sql –help 查看配置参数。

需要将hive-site.xml放到${SPARK_HOME}/conf/目录下，然后就可以测试

show tables;

select count(*) from student;
3. thriftserver

thriftserver jdbc/odbc的实现类似于hive1.2.1的hiveserver2，可以使用spark的beeline命令来测试jdbc server。

安装部署
1). 开启hive的metastore
bin/hive --service metastore
2). 将配置文件复制到spark/conf/目录下
3). thriftserver
sbin/start-thriftserver.sh --masteryarn  --deploy-mode client
对于yarn只支持client模式
4). 启动bin/beeline
5). 连接到thriftserver
!connect jdbc:hive2://localhost:10001
用户自定义函数

1. UDF

定义一个udf很简单，例如我们自定义一个求字符串长度的udf。

val len = udf{(str:String) => str.length}
spark.udf.register("len",len)
val ds =spark.read.json("file:///opt/meitu/bigdata/src/main/data/employees.json")
ds.createOrReplaceTempView("employees")
ds.show()
spark.sql("select len(name) from employees").show()
2. UserDefinedAggregateFunction

定义一个UDAF

import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.expressions.MutableAggregationBuffer
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
import org.apache.spark.sql.types._

object MyAverageUDAF extends UserDefinedAggregateFunction {
 //Data types of input arguments of this aggregate function
 definputSchema:StructType = StructType(StructField("inputColumn", LongType) :: Nil)
 //Data types of values in the aggregation buffer
 defbufferSchema:StructType = {
   StructType(StructField("sum", LongType):: StructField("count", LongType) :: Nil)
 }
 //The data type of the returned value
 defdataType:DataType = DoubleType
 //Whether this function always returns the same output on the identical input
 defdeterministic: Boolean = true
 //Initializes the given aggregation buffer. The buffer itself is a `Row` that inaddition to
 // standard methods like retrieving avalue at an index (e.g., get(), getBoolean()), provides
 // the opportunity to update itsvalues. Note that arrays and maps inside the buffer are still
 // immutable.
 definitialize(buffer:MutableAggregationBuffer): Unit = {
   buffer(0) = 0L
   buffer(1) = 0L
 }
 //Updates the given aggregation buffer `buffer` with new input data from `input`
 defupdate(buffer:MutableAggregationBuffer, input: Row): Unit ={
   if(!input.isNullAt(0)) {
     buffer(0) = buffer.getLong(0)+ input.getLong(0)
     buffer(1) = buffer.getLong(1)+ 1
   }
 }
 // Mergestwo aggregation buffers and stores the updated buffer values back to `buffer1`
 defmerge(buffer1:MutableAggregationBuffer, buffer2: Row): Unit ={
   buffer1(0) = buffer1.getLong(0)+ buffer2.getLong(0)
   buffer1(1) = buffer1.getLong(1)+ buffer2.getLong(1)
 }
 //Calculates the final result
 defevaluate(buffer:Row): Double =buffer.getLong(0).toDouble /buffer.getLong(1)
}
使用UDAF

val ds = spark.read.json("file:///opt/meitu/bigdata/src/main/data/employees.json")
ds.createOrReplaceTempView("employees")
ds.show()
spark.udf.register("myAverage", MyAverageUDAF)
val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
result.show()
3. Aggregator

定义一个Aggregator

import org.apache.spark.sql.{Encoder, Encoders, SparkSession}
import org.apache.spark.sql.expressions.Aggregator
case class Employee(name: String, salary: Long)
case class Average(var sum: Long, var count: Long)

object MyAverageAggregator extends Aggregator[Employee, Average, Double] {

 // A zero value for this aggregation. Should satisfy the property that any b + zero = b
 def zero: Average = Average(0L, 0L)
 // Combine two values to produce a new value. For performance, the function may modify `buffer`
 // and return it instead of constructing a new object
 def reduce(buffer: Average, employee: Employee): Average = {
   buffer.sum += employee.salary
   buffer.count += 1
   buffer
 }
 // Merge two intermediate values
 def merge(b1: Average, b2: Average): Average = {
   b1.sum += b2.sum
   b1.count += b2.count
   b1
 }
 // Transform the output of the reduction
 def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
 // Specifies the Encoder for the intermediate value type
 def bufferEncoder: Encoder[Average] = Encoders.product
 // Specifies the Encoder for the final output value type
 def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}
使用

spark.udf.register("myAverage2", MyAverageAggregator)
import spark.implicits._
val ds = spark.read.json("file:///opt/meitu/bigdata/src/main/data/employees.json").as[Employee]
ds.show()
val averageSalary = MyAverageAggregator.toColumn.name("average_salary")
val result = ds.select(averageSalary)
result.show()

数据源

1. 通用的laod/save函数
可支持多种数据格式：json, parquet, jdbc, orc, libsvm, csv, text
val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
peopleDF.select("name", "age").write.format("parquet").save("namesAndAges.parquet")

默认的是parquet，可以通过spark.sql.sources.default，修改默认配置。
2. Parquet 文件

val parquetFileDF =spark.read.parquet("people.parquet")
peopleDF.write.parquet("people.parquet")
3. ORC 文件

val ds = spark.read.json("file:///opt/meitu/bigdata/src/main/data/employees.json")
ds.write.mode("append").orc("/opt/outputorc/")
spark.read.orc("/opt/outputorc/*").show(1)

4. JSON

ds.write.mode("overwrite").json("/opt/outputjson/")
spark.read.json("/opt/outputjson/*").show()

5. Hive 表

spark 1.6及以前的版本使用hive表需要hivecontext。

Spark2开始只需要创建sparksession增加enableHiveSupport()即可。

val spark = SparkSession
.builder()
.config(sparkConf)
.enableHiveSupport()
.getOrCreate()

spark.sql("select count(*) from student").show()
6. JDBC

写入mysql

wcdf.repartition(1).write.mode("append").option("user", "root")
 .option("password", "mdh2018@#").jdbc("jdbc:mysql://localhost:3306/test","alluxio",new Properties())

从mysql里读

val fromMysql = spark.read.option("user", "root")
 .option("password", "mdh2018@#").jdbc("jdbc:mysql://localhost:3306/test","alluxio",new Properties())

7. 自定义数据源

自定义source比较简单，首先我们要看看source加载的方式

指定的目录下，定义一个DefaultSource类，在类里面实现自定义source。就可以实现我们的目标。

import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}

class DefaultSource  extends DataSourceV2 with ReadSupport {

 def createReader(options: DataSourceOptions) = new SimpleDataSourceReader()
}



import org.apache.spark.sql.Row
import org.apache.spark.sql.sources.v2.reader.{DataReaderFactory, DataSourceReader}
import org.apache.spark.sql.types.{StringType, StructField, StructType}

class SimpleDataSourceReader extends DataSourceReader {

 def readSchema() = StructType(Array(StructField("value", StringType)))

 def createDataReaderFactories = {
   val factoryList = new java.util.ArrayList[DataReaderFactory[Row]]
   factoryList.add(new SimpleDataSourceReaderFactory())
   factoryList
 }
}



import org.apache.spark.sql.Row
import org.apache.spark.sql.sources.v2.reader.{DataReader, DataReaderFactory}

class SimpleDataSourceReaderFactory extends
 DataReaderFactory[Row] with DataReader[Row] {
 def createDataReader = new SimpleDataSourceReaderFactory()
 val values = Array("1", "2", "3", "4", "5")

 var index = 0

 def next = index < values.length

 def get = {
   val row = Row(values(index))
   index = index + 1
   row
 }

 def close() = Unit
}

使用

val simpleDf = spark.read
 .format("bigdata.spark.SparkSQL.DataSources")
 .load()

simpleDf.show()

优化器及执行计划
1. 流程简介

整体流程如下:



总体执行流程如下：从提供的输入API（SQL，Dataset， dataframe）开始，依次经过unresolved逻辑计划，解析的逻辑计划，优化的逻辑计划，物理计划，然后根据cost based优化，选取一条物理计划进行执行.

简单化成四个部分：

1). analysis

Spark 2.0 以后语法树生成使用的是antlr4，之前是scalaparse。

2). logical optimization

常量合并，谓词下推，列裁剪，boolean表达式简化，和其它的规则

3). physical planning

eg:SortExec

4). Codegen

codegen技术是用scala的字符串插值特性生成源码，然后使用Janino，编译成java字节码。Eg： SortExec
2. 自定义优化器

1). 实现

继承Rule[LogicalPlan]

2). 注册

spark.experimental.extraOptimizations= Seq(MultiplyOptimizationRule)
3). 使用

selectExpr("amountPaid* 1")
3. 自定义执行计划

主要是实现重载count函数的功能

1).  物理计划：

继承SparkLan实现doExecute方法

2). 逻辑计划

继承SparkStrategy实现apply

3). 注册到Spark执行策略：

spark.experimental.extraStrategies =Seq(countStrategy)
4). 使用

spark.sql("select count(*) fromtest")


Spark SQL在100TB上的自适应执行实践


Spark SQL是Apache Spark最广泛使用的一个组件，它提供了非常友好的接口来分布式处理结构化数据，在很多应用领域都有成功的生产实践，但是在超大规模集群和数据集上，Spark SQL仍然遇到不少易用性和可扩展性的挑战。为了应对这些挑战，英特尔大数据技术团队和百度大数据基础架构部工程师在Spark 社区版本的基础上，改进并实现了自适应执行引擎。本文首先讨论Spark SQL在大规模数据集上遇到的挑战，然后介绍自适应执行的背景和基本架构，以及自适应执行如何应对Spark SQL这些问题，最后我们将比较自适应执行和现有的社区版本Spark SQL在100 TB 规模TPC-DS基准测试碰到的挑战和性能差异，以及自适应执行在Baidu Big SQL平台的使用情况。



挑战1：关于shuffle partition数



在Spark SQL中， shufflepartition数可以通过参数spark.sql.shuffle.partition来设置，默认值是200。这个参数决定了SQL作业每个reduce阶段任务数量，对整个查询性能有很大影响。假设一个查询运行前申请了E个Executor，每个Executor包含C个core（并发执行线程数），那么该作业在运行时可以并行执行的任务数就等于E x C个，或者说该作业的并发数是E x C。假设shuffle partition个数为P，除了map stage的任务数和原始数据的文件数量以及大小相关，后续的每个reduce stage的任务数都是P。由于Spark作业调度是抢占式的，E x C个并发任务执行单元会抢占执行P个任务，“能者多劳”，直至所有任务完成，则进入到下一个Stage。但这个过程中，如果有任务因为处理数据量过大（例如：数据倾斜导致大量数据被划分到同一个reducer partition）或者其它原因造成该任务执行时间过长，一方面会导致整个stage执行时间变长，另一方面E x C个并发执行单元大部分可能都处于空闲等待状态，集群资源整体利用率急剧下降。



那么spark.sql.shuffle.partition参数究竟是多少比较合适？如果设置过小，分配给每一个reduce任务处理的数据量就越多，在内存大小有限的情况下，不得不溢写（spill）到计算节点本地磁盘上。Spill会导致额外的磁盘读写，影响整个SQL查询的性能，更差的情况还可能导致严重的GC问题甚至是OOM。相反，如果shuffle partition设置过大。第一，每一个reduce任务处理的数据量很小并且很快结束，进而导致Spark任务调度负担变大。第二，每一个mapper任务必须把自己的shuffle输出数据分成P个hash bucket，即确定数据属于哪一个reduce partition，当shuffle partition数量太多时，hash bucket里数据量会很小，在作业并发数很大时，reduce任务shuffle拉取数据会造成一定程度的随机小数据读操作，当使用机械硬盘作为shuffle数据临时存取的时候性能下降会更加明显。最后，当最后一个stage保存数据时会写出P个文件，也可能会造成HDFS文件系统中大量的小文件。



从上，shuffle partition的设置既不能太小也不能太大。为了达到最佳的性能，往往需要经多次试验才能确定某个SQL查询最佳的shuffle partition值。然而在生产环境中，往往SQL以定时作业的方式处理不同时间段的数据，数据量大小可能变化很大，我们也无法为每一个SQL查询去做耗时的人工调优，这也意味这些SQL作业很难以最佳的性能方式运行。



Shuffle partition的另外一个问题是，同一个shuffle partition数设置将应用到所有的stage。Spark在执行一个SQL作业时，会划分成多个stage。通常情况下，每个stage的数据分布和大小可能都不太一样，全局的shuffle partition设置最多只能对某个或者某些stage最优，没有办法做到全局所有的stage设置最优。



这一系列关于shufflepartition的性能和易用性挑战，促使我们思考新的方法：我们能否根据运行时获取的shuffle数据量信息，例如数据块大小，记录行数等等，自动为每一个stage设置合适的shuffle partition值？



挑战2：Spark SQL最佳执行计划



Spark SQL在执行SQL之前，会将SQL或者Dataset程序解析成逻辑计划，然后经历一系列的优化，最后确定一个可执行的物理计划。最终选择的物理计划的不同对性能有很大的影响。如何选择最佳的执行计划，这便是Spark SQL的Catalyst优化器的核心工作。Catalyst早期主要是基于规则的优化器（RBO），在Spark 2.2中又加入了基于代价的优化（CBO）。目前执行计划的确定是在计划阶段，一旦确认以后便不再改变。然而在运行期间，当我们获取到更多运行时信息时，我们将有可能得到一个更佳的执行计划。



以join操作为例，在Spark中最常见的策略是BroadcastHashJoin和SortMergeJoin。BroadcastHashJoin属于map side join，其原理是当其中一张表存储空间大小小于broadcast阈值时，Spark选择将这张小表广播到每一个Executor上，然后在map阶段，每一个mapper读取大表的一个分片，并且和整张小表进行join，整个过程中避免了把大表的数据在集群中进行shuffle。而SortMergeJoin在map阶段2张数据表都按相同的分区方式进行shuffle写，reduce阶段每个reducer将两张表属于对应partition的数据拉取到同一个任务中做join。RBO根据数据的大小，尽可能把join操作优化成BroadcastHashJoin。Spark中使用参数spark.sql.autoBroadcastJoinThreshold来控制选择BroadcastHashJoin的阈值，默认是10MB。然而对于复杂的SQL查询，它可能使用中间结果来作为join的输入，在计划阶段，Spark并不能精确地知道join中两表的大小或者会错误地估计它们的大小，以致于错失了使用BroadcastHashJoin策略来优化join执行的机会。但是在运行时，通过从shuffle写得到的信息，我们可以动态地选用BroadcastHashJoin。以下是一个例子，join一边的输入大小只有600K，但Spark仍然规划成SortMergeJoin。





图1



这促使我们思考第二个问题：我们能否通过运行时收集到的信息，来动态地调整执行计划？



挑战3：数据倾斜



数据倾斜是常见的导致Spark SQL性能变差的问题。数据倾斜是指某一个partition的数据量远远大于其它partition的数据，导致个别任务的运行时间远远大于其它任务，因此拖累了整个SQL的运行时间。在实际SQL作业中，数据倾斜很常见，join key对应的hash bucket总是会出现记录数不太平均的情况，在极端情况下，相同join key对应的记录数特别多，大量的数据必然被分到同一个partition因而造成数据严重倾斜。如图2，可以看到大部分任务3秒左右就完成了，而最慢的任务却花了4分钟，它处理的数据量却是其它任务的若干倍。





图2



目前，处理join时数据倾斜的一些常见手段有： (1)增加shuffle partition数量，期望原本分在同一个partition中的数据可以被分散到多个partition中，但是对于同key的数据没有作用。(2)调大BroadcastHashJoin的阈值，在某些场景下可以把SortMergeJoin转化成BroadcastHashJoin而避免shuffle产生的数据倾斜。(3)手动过滤倾斜的key，并且对这些数据加入随机的前缀，在另一张表中这些key对应的数据也相应的膨胀处理，然后再做join。综上，这些手段都有各自的局限性并且涉及很多的人为处理。基于此，我们思考了第三个问题：Spark能否在运行时自动地处理join中的数据倾斜？



自适应执行背景和简介



早在2015年，Spark社区就提出了自适应执行的基本想法，在Spark的DAGScheduler中增加了提交单个map stage的接口，并且在实现运行时调整shuffle partition数量上做了尝试。但目前该实现有一定的局限性，在某些场景下会引入更多的shuffle，即更多的stage，对于三表在同一个stage中做join等情况也无法很好的处理。所以该功能一直处于实验阶段，配置参数也没有在官方文档中提及。



基于这些社区的工作，英特尔大数据技术团队对自适应执行做了重新的设计，实现了一个更为灵活的自适性执行框架。在这个框架下面，我们可以添加额外的规则，来实现更多的功能。目前，已实现的特性包括：自动设置shuffle partition数，动态调整执行计划，动态处理数据倾斜等等。



自适应执行架构



在Spark SQL中，当Spark确定最后的物理执行计划后，根据每一个operator对RDD的转换定义，它会生成一个RDD的DAG图。之后Spark基于DAG图静态划分stage并且提交执行，所以一旦执行计划确定后，在运行阶段无法再更新。自适应执行的基本思路是在执行计划中事先划分好stage，然后按stage提交执行，在运行时收集当前stage的shuffle统计信息，以此来优化下一个stage的执行计划，然后再提交执行后续的stage。





图3



从图3中我们可以看出自适应执行的工作方法，首先以Exchange节点作为分界将执行计划这棵树划分成多个QueryStage（Exchange节点在Spark SQL中代表shuffle）。每一个QueryStage都是一棵独立的子树，也是一个独立的执行单元。在加入QueryStage的同时，我们也加入一个QueryStageInput的叶子节点，作为父亲QueryStage的输入。例如对于图中两表join的执行计划来说我们会创建3个QueryStage。最后一个QueryStage中的执行计划是join本身，它有2个QueryStageInput代表它的输入，分别指向2个孩子的QueryStage。在执行QueryStage时，我们首先提交它的孩子stage，并且收集这些stage运行时的信息。当这些孩子stage运行完毕后，我们可以知道它们的大小等信息，以此来判断QueryStage中的计划是否可以优化更新。例如当我们获知某一张表的大小是5M，它小于broadcast的阈值时，我们可以将SortMergeJoin转化成BroadcastHashJoin来优化当前的执行计划。我们也可以根据孩子stage产生的shuffle数据量，来动态地调整该stage的reducer个数。在完成一系列的优化处理后，最终我们为该QueryStage生成RDD的DAG图，并且提交给DAG Scheduler来执行。



自动设置reducer个数



假设我们设置的shufflepartition个数为5，在map stage结束之后，我们知道每一个partition的大小分别是70MB，30MB，20MB，10MB和50MB。假设我们设置每一个reducer处理的目标数据量是64MB，那么在运行时，我们可以实际使用3个reducer。第一个reducer处理partition 0 (70MB)，第二个reducer处理连续的partition 1 到3，共60MB，第三个reducer处理partition 4 (50MB)，如图4所示。





图4



在自适应执行的框架中，因为每个QueryStage都知道自己所有的孩子stage，因此在调整reducer个数时，可以考虑到所有的stage输入。另外，我们也可以将记录条数作为一个reducer处理的目标值。因为shuffle的数据往往都是经过压缩的，有时partition的数据量并不大，但解压后记录条数确远远大于其它partition，造成数据不均。所以同时考虑数据大小和记录条数可以更好地决定reducer的个数。



动态调整执行计划



目前我们支持在运行时动态调整join的策略，在满足条件的情况下，即一张表小于Broadcast阈值，可以将SortMergeJoin转化成BroadcastHashJoin。由于SortMergeJoin和BroadcastHashJoin输出的partition情况并不相同，随意转换可能在下一个stage引入额外的shuffle操作。因此我们在动态调整join策略时，遵循一个规则，即在不引入额外shuffle的前提下才进行转换。



将SortMergeJoin转化成BroadcastHashJoin有哪些好处呢？因为数据已经shuffle写到磁盘上，我们仍然需要shuffle读取这些数据。我们可以看看图5的例子，假设A表和B表join，map阶段2张表各有2个map任务，并且shuffle partition个数为5。如果做SortMergeJoin，在reduce阶段需要启动5个reducer，每个reducer通过网络shuffle读取属于自己的数据。然而，当我们在运行时发现B表可以broadcast，并且将其转换成BroadcastHashJoin之后，我们只需要启动2个reducer，每一个reducer读取一个mapper的整个shuffle output文件。当我们调度这2个reducer任务时，可以优先将其调度在运行mapper的Executor上，因此整个shuffle读变成了本地读取，没有数据通过网络传输。并且读取一个文件这样的顺序读，相比原先shuffle时随机的小文件读，效率也更胜一筹。另外，SortMergeJoin过程中往往会出现不同程度的数据倾斜问题，拖慢整体的运行时间。而转换成BroadcastHashJoin后，数据量一般比较均匀，也就避免了倾斜，我们可以在下文实验结果中看到更具体的信息。





图5



动态处理数据倾斜



在自适应执行的框架下，我们可以在运行时很容易地检测出有数据倾斜的partition。当执行某个stage时，我们收集该stage每个mapper 的shuffle数据大小和记录条数。如果某一个partition的数据量或者记录条数超过中位数的N倍，并且大于某个预先配置的阈值，我们就认为这是一个数据倾斜的partition，需要进行特殊的处理。





图6



假设我们A表和B表做inner join，并且A表中第0个partition是一个倾斜的partition。一般情况下，A表和B表中partition 0的数据都会shuffle到同一个reducer中进行处理，由于这个reducer需要通过网络拉取大量的数据并且进行处理，它会成为一个最慢的任务拖慢整体的性能。在自适应执行框架下，一旦我们发现A表的partition 0发生倾斜，我们随后使用N个任务去处理该partition。每个任务只读取若干个mapper的shuffle 输出文件，然后读取B表partition 0的数据做join。最后，我们将N个任务join的结果通过Union操作合并起来。为了实现这样的处理，我们对shuffle read的接口也做了改变，允许它只读取部分mapper中某一个partition的数据。在这样的处理中，B表的partition 0会被读取N次，虽然这增加了一定的额外代价，但是通过N个任务处理倾斜数据带来的收益仍然大于这样的代价。如果B表中partition 0也发生倾斜，对于inner join来说我们也可以将B表的partition 0分成若干块，分别与A表的partition 0进行join，最终union起来。但对于其它的join类型例如Left Semi Join我们暂时不支持将B表的partition 0拆分。



自适应执行和Spark SQL在100TB上的性能比较



我们使用99台机器搭建了一个集群，使用Spark2.2在TPC-DS 100TB的数据集进行了实验，比较原版Spark和自适应执行的性能。以下是集群的详细信息：









图7



实验结果显示，在自适应执行模式下，103条SQL中有92条都得到了明显的性能提升，其中47条SQL的性能提升超过10%，最大的性能提升达到了3.8倍，并且没有出现性能下降的情况。另外在原版Spark中，有5条SQL因为OOM等原因无法顺利运行，在自适应模式下我们也对这些问题做了优化，使得103条SQL在TPC-DS 100TB数据集上全部成功运行。以下是具体的性能提升比例和性能提升最明显的几条SQL。





图8



图9



通过仔细分析了这些性能提升的SQL，我们可以看到自适应执行带来的好处。首先是自动设置reducer个数，原版Spark使用10976作为shuffle partition数，在自适应执行时，以下SQL的reducer个数自动调整为1064和1079，可以明显看到执行时间上也提升了很多。这正是因为减少了调度的负担和任务启动的时间，以及减少了磁盘IO请求。



原版Spark：





图10



自适应执行：





图11



在运行时动态调整执行计划，将SortMergeJoin转化成BroadcastHashJoin在某些SQL中也带来了很大的提升。例如在以下的例子中，原本使用SortMergeJoin因为数据倾斜等问题花费了2.5分钟。在自适应执行时，因为其中一张表的大小只有2.5k所以在运行时转化成了BroadcastHashJoin，执行时间缩短为10秒。


原版Spark：






图12

自适应执行：






图13



100 TB的挑战及优化



成功运行TPC-DS 100 TB数据集中的所有SQL，对于Apache Spark来说也是一大挑战。虽然SparkSQL官方表示支持TPC-DS所有的SQL，但这是基于小数据集。在100TB这个量级上，Spark暴露出了一些问题导致有些SQL执行效率不高，甚至无法顺利执行。在做实验的过程中，我们在自适应执行框架的基础上，对Spark也做了其它的优化改进，来确保所有SQL在100TB数据集上可以成功运行。以下是一些典型的问题。



统计map端输出数据时driver单点瓶颈的优化（SPARK-22537）



在每个map任务结束后，会有一个表示每个partition大小的数据结构（即下面提到的CompressedMapStatus或HighlyCompressedMapStatus）返回给driver。而在自适应执行中，当一次shuffle的map stage结束后，driver会聚合每个mapper给出的partition大小信息，得到在各个partition上所有mapper输出的数据总大小。该统计由单线程完成，如果mapper的数量是M，shuffle partition的数量为S，那么统计的时间复杂度在O(M x S) ~ O (M x S x log(M x S)) 之间，当CompressedMapStatus被使用时，复杂度为这个区间的下限，当HighlyCompressedMapStatus被使用时，空间有所节省，时间会更长，在几乎所有的partition数据都为空时，复杂度会接近该区间的上限。



在M x S增大时，我们会遇到driver上的单点瓶颈，一个明显的表现是UI上map stage和reduce stage之间的停顿。为了解决这个单点瓶颈，我们将任务尽量均匀地划分给多个线程，线程之间不相交地为scala Array中的不同元素赋聚合值。

在这项优化中，新的spark.shuffle.mapOutput.parallelAggregationThreshold（简称threshold）被引入，用于配置使用多线程聚合的阈值，聚合的并行度由JVM中可用core数和M * S / threshold + 1中的小值决定。



Shuffle读取连续partition时的优化 （SPARK-9853）



在自适应执行的模式下，一个reducer可能会从一个mapoutput文件中读取诺干个连续的数据块。目前的实现中，它需要拆分成许多独立的getBlockData调用，每次调用分别从硬盘读取一小块数据，这样就需要很多的磁盘IO。我们对这样的场景做了优化，使得Spark可以一次性地把这些连续数据块都读上来，这样就大大减少了磁盘的IO。在小的基准测试程序中，我们发现shuffle read的性能可以提升3倍。



BroadcastHashJoin中避免不必要的partition读的优化



自适应执行可以为现有的operator提供更多优化的可能。在SortMergeJoin中有一个基本的设计：每个reducetask会先读取左表中的记录，如果左表的 partition为空，则右表中的数据我们无需关注（对于非anti join的情况），这样的设计在左表有一些partition为空时可以节省不必要的右表读取，在SortMergeJoin中这样的实现很自然。



BroadcastHashJoin中不存在按照join key分区的过程，所以缺失了这项优化。然而在自适应执行的一些情况中，利用stage间的精确统计信息，我们可以找回这项优化：如果SortMergeJoin在运行时被转换成了BroadcastHashJoin，且我们能得到各个partition key对应partition的精确大小，则新转换成的BroadcastHashJoin将被告知：无需去读那些小表中为空的partition，因为不会join出任何结果。



Baidu真实产品线试用情况



我们将自适应执行优化应用在Baidu内部基于Spark SQL的即席查询服务BaiduBig SQL之上，做了进一步的落地验证，通过选取单日全天真实用户查询，按照原有执行顺序回放重跑和分析，得到如下几点结论：



对于秒级的简单查询，自适应版本的性能提升并不明显，这主要是因为它们的瓶颈和主要耗时集中在了IO上面，而这不是自适应执行的优化点。

按照查询复杂度维度考量测试结果发现：查询中迭代次数越多，多表join场景越复杂的情况下自适应执行效果越好。我们简单按照group by, sort, join, 子查询等操作个数来将查询分类，如上关键词大于3的查询有明显的性能提升，优化比从50%~200%不等，主要优化点来源于shuffle的动态并发数调整及join优化。

从业务使用角度来分析，前文所述SortMergeJoin转BroadcastHashJoin的优化在Big SQL场景中命中了多种典型的业务SQL模板，试考虑如下计算需求：用户期望从两张不同维度的计费信息中捞取感兴趣的user列表在两个维度的整体计费。收入信息原表大小在百T级别，用户列表只包含对应用户的元信息，大小在10M以内。两张计费信息表字段基本一致，所以我们将两张表与用户列表做inner join后union做进一步分析，SQL表达如下：



select t.c1, t.id, t.c2, t.c3, t.c4,  sum(t.num1), sum(t.num2), sum(t.num3) from

(

select  c1, t1.id as id, c2, c3, c4, sum(num1s) as num1, sum(num2) as num2, sum(num3)  as num3 from basedata.shitu_a t1 INNER JOIN basedata.user_82_1512023432000 t2  ON (t1.id = t2.id)  where  (event_day=20171107)  and flag !=  'true'  group by c1, t1.id, c2, c3, c4

union  all

select  c1, t1.id as id, c2, c3, c4, sum(num1s) as num1, sum(num2) as num2, sum(num3)  as num3 from basedata.shitu_b t1 INNER JOIN basedata.user_82_1512023432000 t2  ON (t1.id = t2.id)  where  (event_day=20171107)  and flag !=  'true'  group by c1, t1.id, c2, c3, c4

) t group by t.c1, t.id, t.c2, t.c3, c4





对应的原版Spark执行计划如下：





图14



针对于此类用户场景，可以全部命中自适应执行的join优化逻辑，执行过程中多次SortMergeJoin转为BroadcastHashJoin，减少了中间内存消耗及多轮sort，得到了近200%的性能提升。



结合上述3点，下一步自适应执行在Baidu内部的优化落地工作将进一步集中在大数据量、复杂查询的例行批量作业之上，并考虑与用户查询复杂度关联进行动态的开关控制。对于数千台的大规模集群上运行的复杂查询，自适应执行可以动态调整计算过程中的并行度，可以帮助大幅提升集群的资源利用率。另外，自适应执行可以获取到多轮stage之间更完整的统计信息，下一步我们也考虑将对应数据及Strategy接口开放给Baidu Spark平台上层用户，针对特殊作业进行进一步的定制化Strategy策略编写。



总结



随着Spark SQL广泛的使用以及业务规模的不断增长，在大规模数据集上遇到的易用性和性能方面的挑战将日益明显。本文讨论了三个典型的问题，包括调整shuffle partition数量，选择最佳执行计划和数据倾斜。这些问题在现有的框架下并不容易解决，而自适应执行可以很好地应对这些问题。我们介绍了自适应执行的基本架构以及解决这些问题的具体方法。最后我们在TPC-DS 100TB数据集上验证了自适应执行的优势，相比较原版Spark SQL，103个SQL查询中，90%的查询都得到了明显的性能提升，最大的提升达到3.8倍，并且原先失败的5个查询在自适应执行下也顺利完成。我们在百度的Big SQL平台也做了进一步的验证，对于复杂的真实查询可以达到2倍的性能提升。总之，自适应执行解决了Spark SQL在大数据规模上遇到的很多挑战，并且很大程度上改善了Spark SQL的易用性和性能，提高了超大集群中多租户多并发作业情况下集群的资源利用率。将来，我们考虑在自适应执行的框架之下，提供更多运行时可以优化的策略，并且将我们的工作贡献回馈给社区，也希望有更多的朋友可以参与进来，将其进一步完善。


==========

Spark高级操作之json复杂和嵌套数据结构的操作一
原创： 浪尖  Spark学习技巧  2017-08-05
一，基本介绍

本文主要讲spark2.0版本以后存在的Sparksql的一些实用的函数，帮助解决复杂嵌套的json数据格式，比如，map和嵌套结构。Spark2.1在spark 的Structured Streaming也可以使用这些功能函数。

下面几个是本文重点要讲的方法。

A),get_json_object()

B),from_json()

C),to_json()

D),explode()

E),selectExpr()

二，准备阶段

首先，创建一个没有任何嵌套的JSon Schema

import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
val jsonSchema = new StructType().add("battery_level", LongType).add("c02_level", LongType).add("cca3",StringType).add("cn", StringType).add("device_id", LongType).add("device_type", StringType).add("signal", LongType).add("ip", StringType).add("temp", LongType).add("timestamp", TimestampType)

使用上面的schema，我在这里创建一个Dataframe，使用的是scala 的case class，同时会产生一些json格式的数据。当然，生产中这些数据也可以来自于kafka。这个case class总共有两个字段：整型(作为device id)和一个字符串(json的数据结构，代表设备的事件)

// define a case class
case class DeviceData (id: Int, device: String)
// create some sample data
val eventsDS = Seq (
  (0, """{"device_id": 0, "device_type": "sensor-ipad", "ip": "68.161.225.1", "cca3": "USA", "cn": "United States", "temp": 25, "signal": 23, "battery_level": 8, "c02_level": 917, "timestamp" :1475600496 }"""),
  (1, """{"device_id": 1, "device_type": "sensor-igauge", "ip": "213.161.254.1", "cca3": "NOR", "cn": "Norway", "temp": 30, "signal": 18, "battery_level": 6, "c02_level": 1413, "timestamp" :1475600498 }"""),
  (2, """{"device_id": 2, "device_type": "sensor-ipad", "ip": "88.36.5.1", "cca3": "ITA", "cn": "Italy", "temp": 18, "signal": 25, "battery_level": 5, "c02_level": 1372, "timestamp" :1475600500 }"""),
  (3, """{"device_id": 3, "device_type": "sensor-inest", "ip": "66.39.173.154", "cca3": "USA", "cn": "United States", "temp": 47, "signal": 12, "battery_level": 1, "c02_level": 1447, "timestamp" :1475600502 }"""),
  (4, """{"device_id": 4, "device_type": "sensor-ipad", "ip": "203.82.41.9", "cca3": "PHL", "cn": "Philippines", "temp": 29, "signal": 11, "battery_level": 0, "c02_level": 983, "timestamp" :1475600504 }"""),
  (5, """{"device_id": 5, "device_type": "sensor-istick", "ip": "204.116.105.67", "cca3": "USA", "cn": "United States", "temp": 50, "signal": 16, "battery_level": 8, "c02_level": 1574, "timestamp" :1475600506 }"""),
  (6, """{"device_id": 6, "device_type": "sensor-ipad", "ip": "220.173.179.1", "cca3": "CHN", "cn": "China", "temp": 21, "signal": 18, "battery_level": 9, "c02_level": 1249, "timestamp" :1475600508 }"""),
  (7, """{"device_id": 7, "device_type": "sensor-ipad", "ip": "118.23.68.227", "cca3": "JPN", "cn": "Japan", "temp": 27, "signal": 15, "battery_level": 0, "c02_level": 1531, "timestamp" :1475600512 }"""),
  (8 ,""" {"device_id": 8, "device_type": "sensor-inest", "ip": "208.109.163.218", "cca3": "USA", "cn": "United States", "temp": 40, "signal": 16, "battery_level": 9, "c02_level": 1208, "timestamp" :1475600514 }"""),
  (9,"""{"device_id": 9, "device_type": "sensor-ipad", "ip": "88.213.191.34", "cca3": "ITA", "cn": "Italy", "temp": 19, "signal": 11, "battery_level": 0, "c02_level": 1171, "timestamp" :1475600516 }"""),
  (10,"""{"device_id": 10, "device_type": "sensor-igauge", "ip": "68.28.91.22", "cca3": "USA", "cn": "United States", "temp": 32, "signal": 26, "battery_level": 7, "c02_level": 886, "timestamp" :1475600518 }"""),
  (11,"""{"device_id": 11, "device_type": "sensor-ipad", "ip": "59.144.114.250", "cca3": "IND", "cn": "India", "temp": 46, "signal": 25, "battery_level": 4, "c02_level": 863, "timestamp" :1475600520 }"""),
  (12, """{"device_id": 12, "device_type": "sensor-igauge", "ip": "193.156.90.200", "cca3": "NOR", "cn": "Norway", "temp": 18, "signal": 26, "battery_level": 8, "c02_level": 1220, "timestamp" :1475600522 }"""),
  (13, """{"device_id": 13, "device_type": "sensor-ipad", "ip": "67.185.72.1", "cca3": "USA", "cn": "United States", "temp": 34, "signal": 20, "battery_level": 8, "c02_level": 1504, "timestamp" :1475600524 }"""),
  (14, """{"device_id": 14, "device_type": "sensor-inest", "ip": "68.85.85.106", "cca3": "USA", "cn": "United States", "temp": 39, "signal": 17, "battery_level": 8, "c02_level": 831, "timestamp" :1475600526 }"""),
  (15, """{"device_id": 15, "device_type": "sensor-ipad", "ip": "161.188.212.254", "cca3": "USA", "cn": "United States", "temp": 27, "signal": 26, "battery_level": 5, "c02_level": 1378, "timestamp" :1475600528 }"""),
  (16, """{"device_id": 16, "device_type": "sensor-igauge", "ip": "221.3.128.242", "cca3": "CHN", "cn": "China", "temp": 10, "signal": 24, "battery_level": 6, "c02_level": 1423, "timestamp" :1475600530 }"""),
  (17, """{"device_id": 17, "device_type": "sensor-ipad", "ip": "64.124.180.215", "cca3": "USA", "cn": "United States", "temp": 38, "signal": 17, "battery_level": 9, "c02_level": 1304, "timestamp" :1475600532 }"""),
  (18, """{"device_id": 18, "device_type": "sensor-igauge", "ip": "66.153.162.66", "cca3": "USA", "cn": "United States", "temp": 26, "signal": 10, "battery_level": 0, "c02_level": 902, "timestamp" :1475600534 }"""),
  (19, """{"device_id": 19, "device_type": "sensor-ipad", "ip": "193.200.142.254", "cca3": "AUT", "cn": "Austria", "temp": 32, "signal": 27, "battery_level": 5, "c02_level": 1282, "timestamp" :1475600536 }""")).toDF("id", "device").as[DeviceData]



三，如何使用get_json_object()

该方法从spark1.6开始就有了，从一个json 字符串中根据指定的json 路径抽取一个json 对象。从上面的dataset中取出部分数据，然后抽取部分字段组装成新的json 对象。比如，我们仅仅抽取：id，devicetype，ip，CCA3 code.

val eventsFromJSONDF = Seq (
  (0, """{"device_id": 0, "device_type": "sensor-ipad", "ip": "68.161.225.1", "cca3": "USA", "cn": "United States", "temp": 25, "signal": 23, "battery_level": 8, "c02_level": 917, "timestamp" :1475600496 }"""),
  (1, """{"device_id": 1, "device_type": "sensor-igauge", "ip": "213.161.254.1", "cca3": "NOR", "cn": "Norway", "temp": 30, "signal": 18, "battery_level": 6, "c02_level": 1413, "timestamp" :1475600498 }"""),
  (2, """{"device_id": 2, "device_type": "sensor-ipad", "ip": "88.36.5.1", "cca3": "ITA", "cn": "Italy", "temp": 18, "signal": 25, "battery_level": 5, "c02_level": 1372, "timestamp" :1475600500 }"""),
  (3, """{"device_id": 3, "device_type": "sensor-inest", "ip": "66.39.173.154", "cca3": "USA", "cn": "United States", "temp": 47, "signal": 12, "battery_level": 1, "c02_level": 1447, "timestamp" :1475600502 }"""),
  (4, """{"device_id": 4, "device_type": "sensor-ipad", "ip": "203.82.41.9", "cca3": "PHL", "cn": "Philippines", "temp": 29, "signal": 11, "battery_level": 0, "c02_level": 983, "timestamp" :1475600504 }"""),
  (5, """{"device_id": 5, "device_type": "sensor-istick", "ip": "204.116.105.67", "cca3": "USA", "cn": "United States", "temp": 50, "signal": 16, "battery_level": 8, "c02_level": 1574, "timestamp" :1475600506 }"""),
  (6, """{"device_id": 6, "device_type": "sensor-ipad", "ip": "220.173.179.1", "cca3": "CHN", "cn": "China", "temp": 21, "signal": 18, "battery_level": 9, "c02_level": 1249, "timestamp" :1475600508 }"""),
  (7, """{"device_id": 7, "device_type": "sensor-ipad", "ip": "118.23.68.227", "cca3": "JPN", "cn": "Japan", "temp": 27, "signal": 15, "battery_level": 0, "c02_level": 1531, "timestamp" :1475600512 }"""),
  (8 ,""" {"device_id": 8, "device_type": "sensor-inest", "ip": "208.109.163.218", "cca3": "USA", "cn": "United States", "temp": 40, "signal": 16, "battery_level": 9, "c02_level": 1208, "timestamp" :1475600514 }"""),
  (9,"""{"device_id": 9, "device_type": "sensor-ipad", "ip": "88.213.191.34", "cca3": "ITA", "cn": "Italy", "temp": 19, "signal": 11, "battery_level": 0, "c02_level": 1171, "timestamp" :1475600516 }""")).toDF("id", "json")

测试及输出

val jsDF = eventsFromJSONDF.select($"id", get_json_object($"json", "$.device_type").alias("device_type"),get_json_object($"json", "$.ip").alias("ip"),get_json_object($"json", "$.cca3").alias("cca3"))
jsDF.printSchema

jsDF.show




四，如何使用from_json()

与get_json_object不同的是该方法，使用schema去抽取单独列。在dataset的api select中使用from_json()方法，我可以从一个json 字符串中按照指定的schema格式抽取出来作为DataFrame的列。还有，我们也可以将所有在json中的属性和值当做一个devices的实体。我们不仅可以使用device.arrtibute去获取特定值，也可以使用*通配符。

下面的例子，主要实现如下功能：

A),使用上述schema从json字符串中抽取属性和值，并将它们视为devices的独立列。

B),select所有列

C),使用.,获取部分列。

val devicesDF = eventsDS.select(from_json($"device", jsonSchema) as "devices").select($"devices.*").filter($"devices.temp" > 10 and $"devices.signal" > 15)






五，如何使用to_json()

下面使用to_json()将获取的数据转化为json格式。将结果重新写入kafka或者保存partquet文件。

val stringJsonDF = eventsDS.select(to_json(struct($"*"))).toDF("devices")

stringJsonDF.show






保存数据到kafka

stringJsonDF.write.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("topic", "iot-devices").save()

注意依赖

groupId = org.apache.spark
artifactId = spark-sql-kafka-0-10_2.11
version = 2.1.0

六，如何使用selectExpr()

将列转化为一个JSON对象的另一种方式是使用selectExpr()功能函数。例如我们可以将device列转化为一个JSON对象。

val stringsDF = eventsDS.selectExpr("CAST(id AS INT)", "CAST(device AS STRING)")

stringsDF.show



SelectExpr()方法的另一个用法，就是使用表达式作为参数，将它们转化为指定的列。如下：

devicesDF.selectExpr("c02_level", "round(c02_level/temp) as ratio_c02_temperature").orderBy($"ratio_c02_temperature" desc).show

使用Sparksql的slq语句是很好写的

首先注册成临时表，然后写sql

devicesDF.createOrReplaceTempView("devicesDFT")
spark.sql("select c02_level,round(c02_level/temp) as ratio_c02_temperature from devicesDFT order by ratio_c02_temperature desc").show

七，验证

为了验证我们的DataFrame转化为json String是成功的我们将结果写入本地磁盘。

stringJsonDF.write.mode("overwrite").format("parquet").save("file:///opt/jules")


读入

val parquetDF = spark.read.parquet("file:///opt/jules")





八，总结

    本文主要讲 get_json_object(), from_json(), to_json(), selectExpr(), and explode()基本介绍使用，下次文章讲解更加复杂的嵌套格式的使用，欢迎关注浪尖公众号。

