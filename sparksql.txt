第 1 章 Spark SQL 背景
1．1 大数据与 Spark 系统
1．2 关系模型与 SQL 语言
1．3 Spark SQL 发展历程
1．4 本章小结
第 2 章 Spark 基础知识介绍
2．1 RDD 编程模型
2．2 DataFrame 与 Dataset
2．3 本章小结
第 3 章 Spark SQL 执行全过程概述
3．1 从 SQL 到 RDD：一个简单的案例
3．2 重要概念
3．2．1 InternalRow 体系
3．2．2 TreeNode 体系
3．2．3 Expression 体系
3．3 内部数据类型系统
3．4 本章小结
第 4 章 Spark SQL 编译器 Parser
4．1 DSL 工具之 ANTLR 简介
4．1．1 基于 ANTLR 4 的计算器
4．1．2 访问者模式
4．2 SparkSqlParser 之 AstBuilder
4．3 常见 SQL 生成的抽象语法树概览
4．4 本章小结
第 5 章 Spark SQL 逻辑计划（LogicalPlan）
5．1 Spark SQL 逻辑计划概述
5．2 LogicalPlan 简介
5．2．1 QueryPlan 概述
5．2．2 LogicalPlan 基本操作与分类
5．2．3 LeafNode 类型的 LogicalPlan
5．2．4 UnaryNode 类型的 LogicalPlan
5．2．5 BinaryNode 类型的 LogicalPlan
5．2．6 其他类型的 LogicalPlan
5．3 AstBuilder 机制：Unresolved LogicalPlan 生成
5．4 Analyzer 机制：Analyzed LogicalPlan 生成
5．4．1 Catalog 体系分析
5．4．2 Rule 体系
5．4．3 Analyzed LogicalPlan 生成过程
5．5 Spark SQL 优化器 Optimizer
5．5．1 Optimizer 概述
5．5．2 Optimizer 规则体系
5．5．3 Optimized LogicalPlan 的生成过程
5．6 本章小结
第 6 章 Spark SQL 物理计划（PhysicalPlan）
6．1 Spark SQL 物理计划概述
6．2 SparkPlan 简介
6．2．1 LeafExecNode 类型
6．2．2 UnaryExecNode 类型
6．2．3 BinaryExecNode 类型
6．2．4 其他类型的 SparkPlan
6．3 Metadata 与 Metrics 体系
6．4 Partitioning 与 Ordering 体系
6．4．1 Distribution 与 Partitioning 的概念
6．4．2 SparkPlan 的常用分区排序操作
6．5 SparkPlan 生成
6．5．1 物理计划 Strategy 体系
6．5．2 常见 Strategy 分析
6．6 执行前的准备
6．6．1 PlanSubqueries 规则
6．6．2 EnsureRequirements 规则
6．7 本章小结
第 7 章 Spark SQL 之 Aggregation 实现
7．1 Aggregation 执行概述
7．1．1 文法定义
7．1．2 聚合语句 Unresolved LogicalPlan 生成
7．1．3 从逻辑算子树到物理算子树
7．2 聚合函数（AggregateFunction）
7．2．1 聚合缓冲区与聚合模式（AggregateMode）
7．2．2 DeclarativeAggregate 聚合函数
7．2．3 ImperativeAggregate 聚合函数
7．2．4 TypedImperativeAggregate 聚合函数
7．3 聚合执行
7．3．1 执行框架 AggregationIterator
7．3．2 基于排序的聚合算子 SortAggregateExec
7．3．3 基于 Hash 的聚合算子 HashAggregateExec
7．4 窗口（Window）函数
7．4．1 窗口函数定义与简介
7．4．2 窗口函数相关表达式
7．4．3 窗口函数的逻辑计划阶段与物理计划阶段
7．4．4 窗口函数的执行
7．5 多维分析
7．5．1 OLAP 多维分析背景
7．5．2 Spark SQL 多维查询
7．5．3 多维分析 LogicalPlan 阶段
7．5．4 多维分析 PhysicalPlan 与执行
7．6 本章小结
第 8 章 Spark SQL 之 Join 实现
8．1 Join 查询概述
8．2 文法定义与抽象语法树
8．3 Join 查询逻辑计划
8．3．1 从 AST 到 Unresolved LogicalPlan
8．3．2 从 Unresolve LogicalPlan 到 Analyzed LogicalPlan
8．3．3 从 Analyzed LogicalPlan 到 Optimized LogicalPlan
8．4 Join 查询物理计划
8．4．1 Join 物理计划的生成
8．4．2 Join 物理计划的选取
8．5 Join 查询执行
8．5．1 Join 执行基本框架
8．5．2 BroadcastJoinExec 执行机制
8．5．3 ShuffledHashJoinExec 执行机制
8．5．4 SortMergeJoinExec 执行机制
8．6 本章小结
第 9 章 Tungsten 技术实现
9．1 内存管理与二进制处理
9．1．1 Spark 内存管理基础
9．1．2 Tungsten 内存管理优化基础
9．1．3 Tungsten 内存优化应用
9．2 缓存敏感计算（Cache-aware computation）
9．3 动态代码生成（Code generation）
9．3．1 漫谈代码生成
9．3．2 Janino 编译器实践
9．3．3 基本（表达式）代码生成
9．3．4 全阶段代码生成（WholeStageCodegen）
9．4 本章小结
第 10 章 Spark SQL 连接 Hive
10．1 Spark SQL 连接 Hive 概述
10．2 Hive 相关的规则和策略
10．2．1 HiveSessionCatalog 体系
10．2．2 Analyzer 之 Hive-Specific 分析规则
10．2．3 SparkPlanner 之 Hive-Specific 转换策略
10．2．4 Hive 相关的任务执行
10．3 Spark SQL 与 Hive 数据类型
10．3．1 Hive 数据类型与 SerDe 框架
10．3．2 DataTypeToInspector 与 Data Wrapping
10．3．3 InspectorToDataType 与 Data Unwrapping
10．4 Hive UDF 管理机制
10．5 Spark Thrift Server 实现
10．5．1 Service 体系
10．5．2 Operation 与 OperationManager
10．5．3 Session 与 SessionManager
10．5．4 Authentication 安全认证管理
10．5．5 Spark Thrift Server 执行流程
10．6 本章小结
第 11 章 Spark SQL 开发与实践
11．1 腾讯大数据平台（TDW）简介
11．2 腾讯大数据平台 SQL 引擎（TDW-SQL-Engine）
11．2．1 SQL-Engine 背景与演化历程
11．2．2 SQL-Engine 整体架构
11．3 TDW-Spark SQL 开发与优化
11．3．1 业务运行支撑框架
11．3．2 新功能开发案例
11．3．3 性能优化开发案例
11．4 业务实践经验与教训
11．4．1 Spark SQL 集群管理的经验
11．4．2 Spark SQL 业务层面调优
11．4．3 SQL 写法的“陷阱”

第1章 初识Spark SQL
1.1 Spark SQL的前世今生
1.2 Spark SQL能做什么
第2章 Spark安装、编程环境搭建以及打包提交
2.1 Spark的简易安装
2.2 准备编写Spark应用程序的IDEA环境
2.3 将编写好的Spark应用程序打包成jar提交到Spark上
第二部分 基础篇
第3章 Spark上的RDD编程
3.1 RDD基础
3.2 RDD简单实例—wordcount
3.3 创建RDD
3.4 RDD操作
3.5 向Spark传递函数
3.6 常见的转化操作和行动操作
3.7 深入理解RDD
3.8 RDD缓存、持久化
3.9 RDD checkpoint容错机制
第4章 Spark SQL编程入门
4.1 Spark SQL概述
4.2 Spark SQL编程入门示例
第5章 Spark SQL的DataFrame操作大全
5.1 由JSON文件生成所需的DataFrame对象
5.2 DataFrame上的行动操作
5.3 DataFrame上的转化操作
第6章 Spark SQL支持的多种数据源
6.1 概述
6.2 典型结构化数据源
第三部分 实战篇
第7章 Spark SQL工程实战之基于WiFi探针的商业大数据分析技术
7.1 功能需求
7.2 系统架构
7.3 功能设计
7.4 数据库结构
7.5 本章小结
第8章 第一个Spark SQL应用程序
8.1 完全分布式环境搭建
8.2 数据清洗
8.3 数据处理流程
8.4 Spark程序远程调试
8.5 Spark的Web界面
8.6 本章小结
第四部分 优化篇
第9章 让Spark程序再快一点
9.1 Spark执行流程
9.2 Spark内存简介
9.3 Spark的一些概念
9.4 Spark编程四大守则
9.5 Spark调优七式
9.6 解决数据倾斜问题
9.7 Spark执行引擎Tungsten简介
9.8 Spark SQL解析引擎Catalyst简介

1.1Spark SQL概述
1.1.1Spark SQL与DataFrame
1.1.2DataFrame与RDD的差异
1.1.3Spark SQL的发展历程
1.2从零起步掌握Hive
1.2.1Hive的本质是什么
1.2.2Hive安装和配置
1.2.3使用Hive分析搜索数据
1.3Spark SQL on Hive安装与配置
1.3.1安装Spark SQL
1.3.2安装MySQL
1.3.3启动Hive Metastore
1.4Spark SQL初试
1.4.1通过spark-shell来使用Spark SQL
1.4.2Spark SQL的命令终端
1.4.3Spark的Web UI
1.5本章小结
第2章DataFrame原理与常用操作
2.1DataFrame编程模型
2.2DataFrame基本操作实战
2.2.1数据准备
2.2.2启动交互式界面
2.2.3数据处理与分析
2.3通过RDD来构建DataFrame
2.4缓存表（列式存储）
2.5DataFrame API应用示例
2.6本章小结
第3章Spark SQL 操作多种数据源
3.1通用的加载/保存功能
3.1.1Spark SQL加载数据
3.1.2Spark SQL保存数据
3.1.3综合案例——电商热销商品排名
3.2Spark SQL操作Hive示例
3.3Spark SQL操作JSON数据集示例
3.4Spark SQL操作HBase示例
3.5Spark SQL操作MySQL示例
3.5.1安装并启动MySQL
3.5.2准备数据表
3.5.3操作MySQL表
3.6Spark SQL操作MongoDB示例
3.6.1安装配置MongoDB
3.6.2启动MongoDB
3.6.3准备数据
3.6.4Spark SQL操作MongoDB
3.7本章小结
第4章Parquet列式存储
4.1Parquet概述
4.1.1Parquet的基本概念
4.1.2Parquet数据列式存储格式应用举例
4.2Parquet的Block配置及数据分片
4.2.1Parquet的Block的配置
4.2.2Parquet 内部的数据分片
4.3Parquet序列化
4.3.1Spark实施序列化的目的
4.3.2Parquet两种序列化方式
4.4本章小结
第5章Spark SQL内置函数与窗口函数
5.1Spark SQL内置函数
5.1.1Spark SQL内置函数概述
5.1.2Spark SQL内置函数应用实例
5.2Spark SQL窗口函数
5.2.1Spark SQL窗口函数概述
5.2.2Spark SQL窗口函数分数查询统计案例
5.2.3Spark SQL窗口函数NBA常规赛数据统计案例
5.3本章小结
第6章Spark SQL UDF与UDAF
6.1UDF概述
6.2UDF示例
6.2.1Hobby_count函数
6.2.2Combine函数
6.2.3Str2Int函数
6.2.4Wsternstate函数
6.2.5ManyCustomers函数
6.2.6StateRegion函数
6.2.7DiscountRatio函数
6.2.8MakeStruct函数
6.2.9MyDateFilter函数
6.2.10MakeDT函数
6.3UDAF概述
6.4UDAF示例
6.4.1ScalaAggregateFunction函数
6.4.2GeometricMean函数
6.4.3CustomMean函数
6.4.4BelowThreshold函数
6.4.5YearCompare函数
6.4.6WordCount函数
6.5本章小结
第7章Thrift Server
7.1Thrift概述
7.1.1Thrift的基本概念
7.1.2Thrift的工作机制
7.1.3Thrift的运行机制
7.1.4一个简单的Thrift 实例
7.2Thrift Server的启动过程
7.2.1Thrift Sever启动详解
7.2.2HiveThriftServer2类的解析
7.3Beeline操作
7.3.1Beeline连接方式
7.3.2在Beeline中进行SQL查询操作
7.3.3通过Web控制台查看用户进行的操作
7.4Thrift Server应用示例
7.4.1示例源代码
7.4.2关键代码行解析
7.4.3测试运行
7.4.4运行结果解析
7.4.5Spark Web控制台查看运行日志
7.5本章小结
第8章Spark SQL综合应用案例
8.1综合案例实战——电商网站日志多维度数据分析
8.1.1数据准备
8.1.2数据说明
8.1.3数据创建
8.1.4数据导入
8.1.5数据测试和处理
8.2综合案例实战——电商网站搜索排名统计
8.2.1案例概述
8.2.2数据准备
8.2.3实现用户每天搜索前3名的商品排名统计
8.3本章小结

1.1 Spark SQL的前世今生 3
1.2 Spark SQL能做什么 4
第2章 Spark安装、编程环境搭建以及打包提交 6
2.1 Spark的简易安装 6
2.2 准备编写Spark应用程序的IDEA环境 10
2.3 将编写好的Spark应用程序打包成jar提交到Spark上 18
第二部分 基础篇
第3章 Spark上的RDD编程 23
3.1 RDD基础 24
3.1.1 创建RDD 24
3.1.2 RDD转化操作、行动操作 24
3.1.3 惰性求值 25
3.1.4 RDD缓存概述 26
3.1.5 RDD基本编程步骤 26
3.2 RDD简单实例—wordcount 27
3.3 创建RDD 28
3.3.1 程序内部数据作为数据源 28
3.3.2 外部数据源 29
3.4 RDD操作 33
3.4.1 转化操作 34
3.4.2 行动操作 37
3.4.3 惰性求值 38
3.5 向Spark传递函数 39
3.5.1 传入匿名函数 39
3.5.2 传入静态方法和传入方法的引用 40
3.5.3 闭包的理解 41
3.5.4 关于向Spark传递函数与闭包的总结 42
3.6 常见的转化操作和行动操作 42
3.6.1 基本RDD转化操作 43
3.6.2 基本RDD行动操作 48
3.6.3 键值对RDD 52
3.6.4 不同类型RDD之间的转换 56
3.7 深入理解RDD 57
3.8 RDD 缓存、持久化 59
3.8.1 RDD缓存 59
3.8.2 RDD持久化 61
3.8.3 持久化存储等级选取策略 63
3.9 RDD checkpoint容错机制 64
第4章 Spark SQL编程入门 66
4.1 Spark SQL概述 66
4.1.1 Spark SQL是什么 66
4.1.2 Spark SQL通过什么来实现 66
4.1.3 Spark SQL 处理数据的优势 67
4.1.4 Spark SQL数据核心抽象——DataFrame 67
4.2 Spark SQL编程入门示例 69
4.2.1 程序主入口：SparkSession 69
4.2.2 创建 DataFrame 70
4.2.3 DataFrame基本操作 70
4.2.4 执行SQL查询 72
4.2.5 全局临时表 73
4.2.6 Dataset 73
4.2.7 将RDDs转化为DataFrame 75
4.2.8 用户自定义函数 78
第5章 Spark SQL的DataFrame操作大全 82
5.1 由JSON文件生成所需的DataFrame对象 82
5.2 DataFrame上的行动操作 84
5.3 DataFrame上的转化操作 91
5.3.1 where条件相关 92
5.3.2 查询指定列 94
5.3.3 思维开拓：Column的巧妙应用 99
5.3.4 limit操作 102
5.3.5 排序操作：order by和sort 103
5.3.6 group by操作 106
5.3.7 distinct、dropDuplicates去重操作 107
5.3.8 聚合操作 109
5.3.9 union合并操作 110
5.3.10 join操作 111
5.3.11 获取指定字段统计信息 114
5.3.12 获取两个DataFrame中共有的记录 116
5.3.13 获取一个DataFrame中有另一个DataFrame中没有的记录 116
5.3.14 操作字段名 117
5.3.15 处理空值列 118
第6章 Spark SQL支持的多种数据源 121
6.1 概述 121
6.1.1 通用load/save 函数 121
6.1.2 手动指定选项 123
6.1.3 在文件上直接进行SQL查询 123
6.1.4 存储模式 123
6.1.5 持久化到表 124
6.1.6 bucket、排序、分区操作 124
6.2 典型结构化数据源 125
6.2.1 Parquet 文件 125
6.2.2 JSON 数据集 129
6.2.3 Hive表 130
6.2.4 其他数据库中的数据表 133
第三部分 实践篇
第7章 Spark SQL 工程实战之基于WiFi探针的商业大数据分析技术 139
7.1 功能需求 139
7.1.1 数据收集 139
7.1.2 数据清洗 140
7.1.3 客流数据分析 141
7.1.4 数据导出 142
7.2 系统架构 142
7.3 功能设计 143
7.4 数据库结构 144

Title Page
Copyright
Learning Spark SQL
Credits
About the Author
About the Reviewer
www.PacktPub.com
Why subscribe?
Customer Feedback
Preface
What this book covers
What you need for this book
Who this book is for
Conventions
Reader feedback
Customer support
Downloading the example code
Downloading the color images of this book
Errata
Piracy
Questions
Getting Started with Spark SQL
What is Spark SQL?
Introducing SparkSession
Understanding Spark SQL concepts
Understanding Resilient Distributed Datasets (RDDs)
Understanding DataFrames and Datasets
Understanding the Catalyst optimizer
Understanding Catalyst optimizations
Understanding Catalyst transformations
Introducing Project Tungsten
Using Spark SQL in streaming applications
Understanding Structured Streaming internals
Summary
Using Spark SQL for Processing Structured and Semistructured Data
Understanding data sources in Spark applications
Selecting Spark data sources
Using Spark with relational databases
Using Spark with MongoDB (NoSQL database)
Using Spark with JSON data
Using Spark with Avro files
Using Spark with Parquet files
Defining and using custom data sources in Spark
Summary
Using Spark SQL for Data Exploration
Introducing Exploratory Data Analysis (EDA)
Using Spark SQL for basic data analysis
Identifying missing data
Computing basic statistics
Identifying data outliers
Visualizing data with Apache Zeppelin
Sampling data with Spark SQL APIs
Sampling with the DataFrame/Dataset API
Sampling with the RDD API
Using Spark SQL for creating pivot tables
Summary
Using Spark SQL for Data Munging
Introducing data munging
Exploring data munging techniques
Pre-processing of the?household electric consumption Dataset
Computing basic statistics and aggregations
Augmenting the Dataset
Executing other miscellaneous processing steps
Pre-processing of?the weather Dataset
Analyzing missing data
Combining data using a JOIN operation
Munging textual data
Processing multiple input data files
Removing stop words
Munging time series data
Pre-processing of the?time-series Dataset
Processing date fields
Persisting and loading data
Defining a date-time index
Using the??TimeSeriesRDD?object
Handling missing time-series data
Computing basic statistics
Dealing with variable length records
Converting variable-length records to fixed-length records
Extracting data from "messy" columns
Preparing data for machine learning
Pre-processing data for machine learning
Creating and running a machine learning pipeline
Summary
Using Spark SQL in Streaming Applications
Introducing streaming data applications
Building Spark streaming applications
Implementing sliding window-based functionality
Joining a streaming Dataset with a static Dataset
Using the Dataset API in Structured Streaming
Using output sinks
Using the Foreach Sink for arbitrary computations on output
Using the Memory Sink to save output to a table
Using the File Sink to save output to a partitioned table
Monitoring streaming queries
Using Kafka with Spark Structured Streaming
Introducing Kafka concepts
Introducing ZooKeeper concepts
Introducing Kafka-Spark integration
Introducing Kafka-Spark Structured Streaming
Writing a receiver for a custom data source
Summary
Using Spark SQL in Machine Learning Applications
Introducing machine learning applications
Understanding Spark ML pipelines and their components
Understanding the steps in a pipeline application development process
Introducing feature engineering
Creating new features from raw data
Estimating the importance of a feature
Understanding dimensionality reduction
Deriving good features
Implementing a Spark ML classification model
Exploring the diabetes Dataset
Pre-processing the data
Building the Spark ML pipeline
Using StringIndexer for indexing categorical features and labels
Using VectorAssembler for assembling features into one column
Using a Spark ML classifier
Creating a Spark ML pipeline
Creating the training and test Datasets
Making predictions using the PipelineModel
Selecting the best model
Changing the ML algorithm in the pipeline
Introducing Spark ML tools and utilities
Using Principal Component Analysis to select features
Using encoders
Using Bucketizer
Using VectorSlicer
Using Chi-squared selector
Using a Normalizer
Retrieving our original labels
Implementing a Spark ML clustering model
Summary
Using Spark SQL in Graph Applications
Introducing large-scale graph applications
Exploring graphs using GraphFrames
Constructing a GraphFrame
Basic graph queries and operations
Motif analysis using GraphFrames
Processing subgraphs
Applying graph algorithms
Saving and loading GraphFrames
Analyzing JSON input modeled as a graph?
Processing graphs containing multiple types of relationships
Understanding GraphFrame internals
Viewing GraphFrame physical execution plan
Understanding partitioning in GraphFrames
Summary
Using Spark SQL with SparkR
Introducing SparkR
Understanding the SparkR architecture
Understanding SparkR DataFrames
Using SparkR for EDA and data munging tasks
Reading and writing Spark DataFrames
Exploring structure and contents of Spark DataFrames
Running basic operations on Spark DataFrames
Executing SQL statements on Spark DataFrames
Merging SparkR DataFrames
Using User Defined Functions (UDFs)
Using SparkR for computing summary statistics
Using SparkR for data visualization
Visualizing data on a map
Visualizing graph nodes and edges
Using SparkR for machine learning
Summary
Developing Applications with Spark SQL
Introducing Spark SQL applications
Understanding text analysis applications
Using Spark SQL for textual analysis
Preprocessing textual data
Computing readability
Using word lists
Creating data preprocessing pipelines
Understanding themes in document corpuses
Using Naive Bayes classifiers
Developing a machine learning application
Summary
Using Spark SQL in Deep Learning Applications
Introducing neural networks
Understanding deep learning
Understanding representation learning
Understanding stochastic gradient descent
Introducing deep learning in Spark
Introducing CaffeOnSpark
Introducing DL4J
Introducing TensorFrames
Working with BigDL
Tuning hyperparameters of deep learning models
Introducing deep learning pipelines
Understanding Supervised learning
Understanding convolutional neural networks
Using neural networks for text classification
Using deep neural networks for language processing
Understanding Recurrent Neural Networks
Introducing autoencoders
Summary
Tuning Spark SQL Components for Performance
Introducing performance tuning in Spark SQL
Understanding DataFrame/Dataset APIs
Optimizing data serialization
Understanding Catalyst optimizations
Understanding the Dataset/DataFrame API
Understanding Catalyst transformations
Visualizing Spark application execution
Exploring Spark application execution metrics
Using external tools for performance tuning
Cost-based optimizer in Apache Spark 2.2
Understanding the?CBO statistics collection
Statistics collection functions
Filter operator
Join operator
Build side selection
Understanding multi-way JOIN ordering optimization
Understanding performance improvements using whole-stage code generation
Summary
Spark SQL in Large-Scale Application Architectures
Understanding Spark-based application architectures
Using Apache Spark for batch processing
Using Apache Spark for stream processing
Understanding the Lambda architecture
Understanding the Kappa Architecture
Design considerations for building scalable stream processing applications
Building robust ETL pipelines using Spark SQL
Choosing appropriate data formats
Transforming data in ETL pipelines
Addressing errors in ETL pipelines
Implementing a scalable monitoring solution
Deploying Spark machine learning pipelines
Understanding the challenges in typical ML deployment environments
Understanding types of model scoring architectures
Using cluster managers
Summary

=============


导读：本文所述内容均基于 2018 年 9 月 17 日 Spark 最新 Spark Release 2.3.1 版本，以及截止到 2018 年 10 月 21 日 Adaptive Execution 最新开发代码。自动设置 Shuffle Partition 个数已进入 Spark Release 2.3.1 版本，动态调整执行计划与处理数据倾斜尚未进入 Spark Release 2.3.1
1 背  景
Spark SQL / Catalyst 和 CBO 的优化，从查询本身与目标数据的特点的角度尽可能保证了最终生成的执行计划的高效性。但是

执行计划一旦生成，便不可更改，即使执行过程中发现后续执行计划可以进一步优化，也只能按原计划执行；

CBO 基于统计信息生成最优执行计划，需要提前生成统计信息，成本较大，且不适合数据更新频繁的场景；

CBO 基于基础表的统计信息与操作对数据的影响推测中间结果的信息，只是估算，不够精确。

本文介绍的 Adaptive Execution 将可以根据执行过程中的中间数据优化后续执行，从而提高整体执行效率。核心在于两点：

执行计划可动态调整

调整的依据是中间结果的精确统计信息

2 动态设置 Shuffle Partition
2.1 Spark Shuffle 原理
Spark Shuffle 一般用于将上游 Stage 中的数据按 Key 分区，保证来自不同 Mapper （表示上游 Stage 的 Task）的相同的 Key 进入相同的 Reducer （表示下游 Stage 的 Task）。一般用于 group by 或者 Join 操作。



如上图所示，该 Shuffle 总共有 2 个 Mapper 与 5 个 Reducer。每个 Mapper 会按相同的规则（由 Partitioner 定义）将自己的数据分为五份。每个 Reducer 从这两个 Mapper 中拉取属于自己的那一份数据。

2.2 原有 Shuffle 的问题
使用 Spark SQL 时，可通过spark.sql.shuffle.partitions指定 Shuffle 时 Partition 个数，也即 Reducer 个数。

该参数决定了一个 Spark SQL Job 中包含的所有 Shuffle 的 Partition 个数。如下图所示，当该参数值为 3 时，所有 Shuffle 中 Reducer 个数都为 3。



这种方法有如下问题：

Partition 个数不宜设置过大；

Reducer（代指 Spark Shuffle 过程中执行 Shuffle Read 的 Task） 个数过多，每个 Reducer 处理的数据量过小。大量小 Task 造成不必要的 Task 调度开销与可能的资源调度开销（如果开启了 Dynamic Allocation）；

Reducer 个数过大，如果 Reducer 直接写 HDFS 会生成大量小文件，从而造成大量 addBlock RPC，Name node 可能成为瓶颈，并影响其它使用 HDFS 的应用；

过多 Reducer 写小文件，会造成后面读取这些小文件时产生大量 getBlock RPC，对 Name node 产生冲击；

Partition 个数不宜设置过小

每个 Reducer 处理的数据量太大，Spill 到磁盘开销增大；

Reducer GC 时间增长；

Reducer 如果写 HDFS，每个 Reducer 写入数据量较大，无法充分发挥并行处理优势；

很难保证所有 Shuffle 都最优

不同的 Shuffle 对应的数据量不一样，因此最优的 Partition 个数也不一样。使用统一的 Partition 个数很难保证所有 Shuffle 都最优；

定时任务不同时段数据量不一样，相同的 Partition 数设置无法保证所有时间段执行时都最优；

2.3 自动设置 Shuffle Partition 原理
如 Spark Shuffle 原理 一节图中所示，Stage 1 的 5 个 Partition 数据量分别为 60MB，40MB，1MB，2MB，50MB。其中 1MB 与 2MB 的 Partition 明显过小（实际场景中，部分小 Partition 只有几十 KB 及至几十字节）。

开启 Adaptive Execution 后：

Spark 在 Stage 0 的 Shuffle Write 结束后，根据各 Mapper 输出，统计得到各 Partition 的数据量，即 60MB，40MB，1MB，2MB，50MB；

通过 ExchangeCoordinator 计算出合适的 post-shuffle Partition 个数（即 Reducer）个数（本例中 Reducer 个数设置为 3）；

启动相应个数的 Reducer 任务；

每个 Reducer 读取一个或多个 Shuffle Write Partition 数据（如下图所示，Reducer 0 读取 Partition 0，Reducer 1 读取 Partition 1、2、3，Reducer 2 读取 Partition 4）。



三个 Reducer 这样分配是因为：

targetPostShuffleInputSize 默认为 64MB，每个 Reducer 读取数据量不超过 64MB；

如果 Partition 0 与 Partition 2 结合，Partition 1 与 Partition 3 结合，虽然也都不超过 64 MB。但读完 Partition 0 再读 Partition 2，对于同一个 Mapper 而言，如果每个 Partition 数据比较少，跳着读多个 Partition 相当于随机读，在HDD 上性能不高；

目前的做法是只结合相临的 Partition，从而保证顺序读，提高磁盘 IO 性能；

该方案只会合并多个小的 Partition，不会将大的 Partition 拆分，因为拆分过程需要引入一轮新的 Shuffle；

基于上面的原因，默认 Partition 个数（本例中为 5）可以大一点，然后由 ExchangeCoordinator 合并。如果设置的 Partition 个数太小，Adaptive Execution 在此场景下无法发挥作用。

由上图可见，Reducer 1 从每个 Mapper 读取 Partition 1、2、3 都有三根线，是因为原来的 Shuffle 设计中，每个 Reducer 每次通过 Fetch 请求从一个特定 Mapper 读数据时，只能读一个 Partition 的数据。也即在上图中，Reducer 1 读取 Mapper 0 的数据，需要 3 轮 Fetch 请求。对于 Mapper 而言，需要读三次磁盘，相当于随机 IO。

为了解决这个问题，Spark 新增接口，一次 Shuffle Read 可以读多个 Partition 的数据。如下图所示，Task 1 通过一轮请求即可同时读取 Task 0 内 Partition 0、1 和 2 的数据，减少了网络请求数量。同时 Mapper 0 一次性读取并返回三个 Partition 的数据，相当于顺序 IO，从而提升了性能。



由于 Adaptive Execution 的自动设置 Reducer 是由 ExchangeCoordinator 根据 Shuffle Write 统计信息决定的，因此即使在同一个 Job 中不同 Shuffle 的 Reducer 个数都可以不一样，从而使得每次 Shuffle 都尽可能最优。

上文 原有 Shuffle 的问题 一节中的例子，在启用 Adaptive Execution 后，三次 Shuffle 的 Reducer 个数从原来的全部为 3 变为 2、4、3。



2.4 使用与优化方法
可通过spark.sql.adaptive.enabled=true启用 Adaptive Execution 从而启用自动设置 Shuffle Reducer 这一特性。

通过spark.sql.adaptive.shuffle.targetPostShuffleInputSize可设置每个 Reducer 读取的目标数据量，其单位是字节，默认值为 64 MB。上文例子中，如果将该值设置为 50 MB，最终效果仍然如上文所示，而不会将 Partition 0 的 60MB 拆分。具体原因上文已说明。

3 动态调整执行计划
3.1 固定执行计划的不足
在不开启 Adaptive Execution 之前，执行计划一旦确定，即使发现后续执行计划可以优化，也不可更改。如下图所示，SortMergJoin 的 Shuffle Write 结束后，发现 Join 一方的 Shuffle 输出只有 46.9KB，仍然继续执行 SortMergeJoin。



此时完全可将 SortMergeJoin 变更为 BroadcastJoin 从而提高整体执行效率。

3.2 SortMergeJoin 原理
SortMergeJoin 是常用的分布式 Join 方式，它几乎可使用于所有需要 Join 的场景。但有些场景下，它的性能并不是最好的。

SortMergeJoin 的原理如下图所示：

将 Join 双方以 Join Key 为 Key 按照 HashPartitioner 分区，且保证分区数一致；

Stage 0 与 Stage 1 的所有 Task 在 Shuffle Write 时，都将数据分为 5 个 Partition，并且每个 Partition 内按 Join Key 排序；

Stage 2 启动 5 个 Task 分别去 Stage 0 与 Stage 1 中所有包含 Partition 分区数据的 Task 中取对应 Partition 的数据。（如果某个 Mapper 不包含该 Partition 的数据，则 Redcuer 无须向其发起读取请求）；

Stage 2 的 Task 2 分别从 Stage 0 的 Task 0、1、2 中读取 Partition 2 的数据，并且通过 MergeSort 对其进行排序；

Stage 2 的 Task 2 分别从 Stage 1 的 Task 0、1 中读取 Partition 2 的数据，且通过 MergeSort 对其进行排序；

Stage 2 的 Task 2 在上述两步 MergeSort 的同时，使用 SortMergeJoin 对二者进行 Join。



3.3 BroadcastJoin 原理
当参与 Join 的一方足够小，可全部置于 Executor 内存中时，可使用 Broadcast 机制将整个 RDD 数据广播到每一个 Executor 中，该 Executor 上运行的所有 Task 皆可直接读取其数据。（本文中，后续配图，为了方便展示，会将整个 RDD 的数据置于 Task 框内，而隐藏 Executor）。

对于大 RDD，按正常方式，每个 Task 读取并处理一个 Partition 的数据，同时读取 Executor 内的广播数据，该广播数据包含了小 RDD 的全量数据，因此可直接与每个 Task 处理的大 RDD 的部分数据直接 Join。



根据 Task 内具体的 Join 实现的不同，又可分为 BroadcastHashJoin 与 BroadcastNestedLoopJoin。后文不区分这两种实现，统称为 BroadcastJoin。

与 SortMergeJoin 相比，BroadcastJoin 不需要 Shuffle，减少了 Shuffle 带来的开销，同时也避免了 Shuffle 带来的数据倾斜，从而极大地提升了 Job 执行效率。

同时，BroadcastJoin 带来了广播小 RDD 的开销。另外，如果小 RDD 过大，无法存于 Executor 内存中，则无法使用 BroadcastJoin。

对于基础表的 Join，可在生成执行计划前，直接通过 HDFS 获取各表的大小，从而判断是否适合使用 BroadcastJoin。但对于中间表的 Join，无法提前准确判断中间表大小从而精确判断是否适合使用 BroadcastJoin。

《Spark SQL 性能优化再进一步 CBO 基于代价的优化》一文介绍的 CBO 可通过表的统计信息与各操作对数据统计信息的影响，推测出中间表的统计信息，但是该方法得到的统计信息不够准确。同时该方法要求提前分析表，具有较大开销。

而开启 Adaptive Execution 后，可直接根据 Shuffle Write 数据判断是否适用 BroadcastJoin。

3.4 动态调整执行计划原理
如上文 SortMergeJoin 原理 中配图所示，SortMergeJoin 需要先对 Stage 0 与 Stage 1 按同样的 Partitioner 进行 Shuffle Write。

Shuffle Write 结束后，可从每个 ShuffleMapTask 的 MapStatus 中统计得到按原计划执行时 Stage 2 各 Partition 的数据量以及 Stage 2 需要读取的总数据量。（一般来说，Partition 是 RDD 的属性而非 Stage 的属性，本文为了方便，不区分 Stage 与 RDD。可以简单认为一个 Stage 只有一个 RDD，此时 Stage 与 RDD 在本文讨论范围内等价）。

如果其中一个 Stage 的数据量较小，适合使用 BroadcastJoin，无须继续执行 Stage 2 的 Shuffle Read。相反，可利用 Stage 0 与 Stage 1 的数据进行 BroadcastJoin，如下图所示。



具体做法是：

将 Stage 1 全部 Shuffle Write 结果广播出去

启动 Stage 2，Partition 个数与 Stage 0 一样，都为 3

每个 Stage 2 每个 Task 读取 Stage 0 每个 Task 的 Shuffle Write 数据，同时与广播得到的 Stage 1 的全量数据进行 Join

注：广播数据存于每个 Executor 中，其上所有 Task 共享，无须为每个 Task 广播一份数据。上图中，为了更清晰展示为什么能够直接 Join 而将 Stage 2 每个 Task 方框内都放置了一份 Stage 1 的全量数据。

虽然 Shuffle Write 已完成，将后续的 SortMergeJoin 改为 Broadcast 仍然能提升执行效率：

SortMergeJoin 需要在 Shuffle Read 时对来自 Stage 0 与 Stage 1 的数据进行 Merge Sort，并且可能需要 Spill 到磁盘，开销较大；

SortMergeJoin 时，Stage 2 的所有 Task 需要取 Stage 0 与 Stage 1 的所有 Task 的输出数据（如果有它要的数据 ），会造成大量的网络连接。且当 Stage 2 的 Task 较多时，会造成大量的磁盘随机读操作，效率不高，且影响相同机器上其它 Job 的执行效率；

SortMergeJoin 时，Stage 2 每个 Task 需要从几乎所有 Stage 0 与 Stage 1 的 Task 取数据，无法很好利用 Locality；

Stage 2 改用 Broadcast，每个 Task 直接读取 Stage 0 的每个 Task 的数据（一对一），可很好利用 Locality 特性。最好在 Stage 0 使用的 Executor 上直接启动 Stage 2 的 Task。如果 Stage 0 的 Shuffle Write 数据并未 Spill 而是在内存中，则 Stage 2 的 Task 可直接读取内存中的数据，效率非常高。如果有 Spill，那可直接从本地文件中读取数据，且是顺序读取，效率远比通过网络随机读数据效率高。

3.5 使用与优化方法
该特性的使用方式如下：

当spark.sql.adaptive.enabled与spark.sql.adaptive.join.enabled都设置为true时，开启 Adaptive Execution 的动态调整 Join 功能；

spark.sql.adaptiveBroadcastJoinThreshold设置了 SortMergeJoin 转 BroadcastJoin 的阈值。如果不设置该参数，该阈值与spark.sql.autoBroadcastJoinThreshold的值相等；

除了本文所述 SortMergeJoin 转 BroadcastJoin，Adaptive Execution 还可提供其它 Join 优化策略。部分优化策略可能会需要增加 Shuffle。spark.sql.adaptive.allowAdditionalShuffle参数决定了是否允许为了优化 Join 而增加 Shuffle。其默认值为 false。

4 自动处理数据倾斜
4.1 解决数据倾斜典型方案
《Spark 性能优化之道——解决 Spark 数据倾斜（Data Skew）的 N 种姿势》一文讲述了数据倾斜的危害，产生原因，以及典型解决方法。

保证文件可 Split 从而避免读 HDFS 时数据倾斜；

保证 Kafka 各 Partition 数据均衡从而避免读 Kafka 引起的数据倾斜；

调整并行度或自定义 Partitioner 从而分散分配给同一 Task 的大量不同 Key；

使用 BroadcastJoin 代替 ReduceJoin 消除 Shuffle 从而避免 Shuffle 引起的数据倾斜；

对倾斜 Key 使用随机前缀或后缀从而分散大量倾斜 Key，同时将参与 Join 的小表扩容，从而保证 Join 结果的正确性。

4.2 自动解决数据倾斜
目前 Adaptive Execution 可解决 Join 时数据倾斜问题。其思路可理解为将部分倾斜的 Partition (倾斜的判断标准为该 Partition 数据是所有 Partition Shuffle Write 中位数的 N 倍) 进行单独处理，类似于 BroadcastJoin，如下图所示。



在上图中，左右两边分别是参与 Join 的 Stage 0 与 Stage 1 (实际应该是两个 RDD 进行 Join，但如同上文所述，这里不区分 RDD 与 Stage)，中间是获取 Join 结果的 Stage 2。

明显 Partition 0 的数据量较大，这里假设 Partition 0 符合“倾斜”的条件，其它 4 个 Partition 未倾斜。

以 Partition 对应的 Task 2 为例，它需获取 Stage 0 的三个 Task 中所有属于 Partition 2 的数据，并使用 MergeSort 排序。同时获取 Stage 1 的两个 Task 中所有属于 Partition 2 的数据并使用 MergeSort 排序。然后对二者进行 SortMergeJoin。

对于 Partition 0，可启动多个 Task：

在上图中，启动了两个 Task 处理 Partition 0 的数据，分别名为 Task 0-0 与 Task 0-1

Task 0-0 读取 Stage 0 Task 0 中属于 Partition 0 的数据

Task 0-1 读取 Stage 0 Task 1 与 Task 2 中属于 Partition 0 的数据，并进行 MergeSort

Task 0-0 与 Task 0-1 都从 Stage 1 的两个 Task 中所有属于 Partition 0 的数据

Task 0-0 与 Task 0-1 使用 Stage 0 中属于 Partition 0 的部分数据与 Stage 1中属于 Partition 0 的全量数据进行 Join

通过该方法，原本由一个 Task 处理的 Partition 0 的数据由多个 Task 共同处理，每个 Task 需处理的数据量减少，从而避免了 Partition 0 的倾斜。

对于 Partition 0 的处理，有点类似于 BroadcastJoin 的做法。但区别在于，Stage 2 的 Task 0-0 与 Task 0-1 同时获取 Stage 1 中属于 Partition 0 的全量数据，是通过正常的 Shuffle Read 机制实现，而非 BroadcastJoin 中的变量广播实现。

4.3 使用与优化方法
开启与调优该特性的方法如下：

将spark.sql.adaptive.skewedJoin.enabled设置为 true 即可自动处理 Join 时数据倾斜；

spark.sql.adaptive.skewedPartitionMaxSplits控制处理一个倾斜 Partition 的 Task 个数上限，默认值为 5；

spark.sql.adaptive.skewedPartitionRowCountThreshold设置了一个 Partition 被视为倾斜 Partition 的行数下限，也即行数低于该值的 Partition 不会被当作倾斜 Partition 处理。其默认值为 10L * 1000 * 1000 即一千万；

spark.sql.adaptive.skewedPartitionSizeThreshold设置了一个 Partition 被视为倾斜 Partition 的大小下限，也即大小小于该值的 Partition 不会被视作倾斜 Partition。其默认值为 64 * 1024 * 1024 也即 64MB；

spark.sql.adaptive.skewedPartitionFactor该参数设置了倾斜因子。如果一个 Partition 的大小大于spark.sql.adaptive.skewedPartitionSizeThreshold的同时大于各 Partition 大小中位数与该因子的乘积，或者行数大于spark.sql.adaptive.skewedPartitionRowCountThreshold的同时大于各 Partition 行数中位数与该因子的乘积，则它会被视为倾斜的 Partition。

==============
sparksql执行流程分析
2019年03月28日 21:12:17 野狼e族 阅读数：591
版权声明：本文为博主原创文章，未经博主允许不得转载。	https://blog.csdn.net/qq_24505127/article/details/88878293
    Spark sql是spark内部最核心，也是社区最活跃的组件。Spark SQL支持在Spark中执行SQL,或者HiveQL的关系查询表达式。列式存储的类RDD（DataSet/DataFrame）数据类型以及对sql语句的支持使它更容易上手，同时，它对数据的抽取、清洗的特性，使它广泛的用于etl，甚至是机器学习领域。因此，saprk sql较其他spark组件，获得了更多的使用者。

　　下文，我们首先通过查看一个简单的sql的执行计划，对sql的执行流程有一个简单的认识，后面将通过对sql优化器catalyst的每个部分的介绍，来让大家更深入的了解sql后台的执行流程。由于此模板中代码较多，我们在此仅就执行流程中涉及到的主要代码进行介绍，方便大家更快地浏览spark sql的源码。本文中涉及到的源码spark 2.1.1的。

1. catalyst整体执行流程介绍
1.1 catalyst整体执行流程
说到spark sql不得不提的当然是Catalyst了。Catalyst是spark sql的核心，是一套针对spark sql 语句执行过程中的查询优化框架。因此要理解spark sql的执行流程，理解Catalyst的工作流程是理解spark sql的关键。而说到Catalyst，就必须得上下面这张图1了，这张图描述了spark sql执行的全流程。其中，长方形框内为catalyst的工作流程。



图1 spark sql 执行流程图

SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；此时，Optimizer再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan。

1.2 一个简单sql语句的执行
为了更好的对整个过程进行理解，下面通过一个简单的sql示例来查看采用catalyst框架执行sql的过程。示例代码如下：

object TestSql {



  case class Student(id:Long,name:String,chinese:String,math:String,English:String,age:Int)

  case class Score(sid:Long,weight1:String,weight2:String,weight3:String)



  def main(args: Array[String]): Unit = {

  //使用saprkSession初始化spark运行环境

  val spark=SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()

  //引入spark sql的隐式转换

   import sqlContext.implicits._

  //读取第一个文件的数据并转换成DataFrame

  val testP1 = spark.sparkContext.textFile("/home/dev/testP1").map(_.split(" ")).map(p=>Student(p(0).toLong,p(1),p(2),p(3),p(4),p(5).trim.toInt)).toDF()

  //注册成表

    testP1.registerTempTable("studentTable")

  //读取第二个文件的数据并转换成DataFrame

    val testp2 = spark.sparkContext.textFile("/home/dev/testP2").map(_.split(" ")).map(p=>Score(p(0).toLong,p(1),p(2),p(3))).toDF()

  //注册成表

    testp2.registerTempTable("scoreTable")

  //查看sql的逻辑执行计划

    val dataframe = spark.sql("select sum(chineseScore) from " +

      "(select x.id,x.chinese+20+30 as chineseScore,x.math from studentTable x inner join  scoreTable y on x.id=y.sid)z" +

      " where z.chineseScore <100").map(p=>(p.getLong(0))).collect.foreach(println)
此例也是针对spark2.1.1版本的，程序的入口是SparkSession。由于此例超级简单，做过spark的人一眼就能看出，而且每行代码都带有中文注释，所以在这里，我们就不做具体的介绍了。

　　这里涉及到的sql查询就是最后一句，通过spark shell可以看到该sql查询的逻辑执行计划和物理执行计划。进入sparkshell后，输入一下代码即可显示此sql查询的执行计划。



spark.sql("select sum(chineseScore) from " +
      "(select x.id,x.chinese+20+30 as chineseScore,x.math from  studentTable x inner join  scoreTable y on x.id=y.sid)z" +
      " where z.chineseScore <100").explain(true)
这里，是使用DataSet的explain函数实现逻辑执行计划和物理执行计划的打印，调用explain的源码如下：

/**
* Prints the plans (logical and physical) to the console for debugging purposes.
*
* @group basic
* @since 1.6.0
*/

def explain(extended: Boolean): Unit = {

  val explain = ExplainCommand(queryExecution.logical, extended = extended)

  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {

    // scalastyle:off println

    r => println(r.getString(0))

    // scalastyle:on println

  }

}
显示在spark shell中的unresolved logical plan、resolved logical plan、optimized logical plan和physical plan如下图2所示：







图2 spark sql 执行计划



　　将上图2中的Parsed Logical Plan表示成树结构如下图3所示。Catalyst中的parser将图左中一个sql查询的字符串解析成图右的一个AST语法树，该语法树就称为Parsed Logical Plan。解析后的逻辑计划基本形成了执行计划的基础骨架，此逻辑执行计划被称为 unresolved Logical Plan，也就是说该逻辑计划是无法执行的，系统并不知道语法树中的每个词是神马意思，如图中的filter，join，以及studentTable等。



图3 Parsed Logical Plan树



　　将上图2中的Analyzed logical plan，即Resolved logical plan表示成树结构如下图4所示。Catalyst的analyzer将unresolved Logical Plan解析成resolved Logical Plan。Analyzer借助cataLog（下文介绍）中表的结构信息、函数信息等将此逻辑计划解析成可被识别的逻辑执行计划。



图4 Analyzed logical plan树



　　optimized logical plan与physical plan的树结构跟上面两种逻辑执行计划树结构的画法相似，下面就不在画了。从optimized logical plan可出，此次优化使用了Filter下推的策略，即将Filter下推到子查询中实现，继而减少后续数据的处理量。

　　前面我们展示了catalyst执行一段sql语句的大致流程，下面我们就从源代码的角度来分析catalyst的每个部分内部如何实现，以及它们之间是如何承接的。

2. catalyst各个模块介绍
本章我们通过分析上面的例子代码的调用过程来分析catalys各个部分的主要代码模块，spark sql的入口是最后一句，SparkSession类里的sql函数，传入一个sql字符串，返回一个dataframe对象。入口调用的代码如下：

def sql(sqlText: String): DataFrame = {

  Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))

}
2.1 Parser
从入口代码可看出，首先调用sqlParser的parsePlan方法，将sql字符串解析成unresolved逻辑执行计划。parsePlan的具体实现在AbstractSqlParser类中。如下：

/** Creates LogicalPlan for a given SQL string. */

override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>

  astBuilder.visitSingleStatement(parser.singleStatement()) match {

    case plan: LogicalPlan => plan

    case _ =>

      val position = Origin(None, None)

      throw new ParseException(Option(sqlText), "Unsupported SQL statement", position, position)

  }

}

由上段代码可看出，调用的主函数是parse，继续进入到parse中，代码如下：

protected def parse[T](command: String)(toResult: SqlBaseParser => T): T = {

  logInfo(s"Parsing command: $command")



  val lexer = new SqlBaseLexer(new ANTLRNoCaseStringStream(command))

  lexer.removeErrorListeners()

  lexer.addErrorListener(ParseErrorListener)



  val tokenStream = new CommonTokenStream(lexer)

  val parser = new SqlBaseParser(tokenStream)

  parser.addParseListener(PostProcessor)

  parser.removeErrorListeners()

  parser.addErrorListener(ParseErrorListener)



  try {

    try {

      // first, try parsing with potentially faster SLL(Strong-LL) mode

      parser.getInterpreter.setPredictionMode(PredictionMode.SLL)

      toResult(parser)

    }

  }

}

从parse函数可以看出，这里对于SQL语句的解析采用的是ANTLR 4，这里使用到了两个类：词法解析器SqlBaseLexer和语法解析器SqlBaseParser

SqlBaseLexer和SqlBaseParser均是使用ANTLR 4自动生成的Java类。这里，采用这两个解析器将SQL语句解析成了ANTLR 4的语法树结构ParseTree。之后，在parsePlan（见code 2）中，使用AstBuilder（AstBuilder.scala）将ANTLR 4语法树结构转换成catalyst表达式，即logical plan。

　　此时生成的逻辑执行计划成为unresolved logical plan。只是将sql串解析成类似语法树结构的执行计划，系统并不知道每个词所表示的意思，离真正能够执行还差很远。

2.2 Analyzer
parser生成逻辑执行计划后，使用analyzer将逻辑执行计划进行分析。我们回到Dataset的ofRows函数:

def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {

  val qe = sparkSession.sessionState.executePlan(logicalPlan)

  qe.assertAnalyzed()

  new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))

}
这里首先创建了queryExecution类对象，QueryExecution中定义了sql执行过程中的关键步骤，是sql执行的关键类，返回一个dataframe类型的对象。QueryExecution类中的成员都是lazy的，被调用时才会执行。只有等到程序中出现action算子时，才会调用 queryExecution类中的executedPlan成员，原先生成的逻辑执行计划才会被优化器优化，并转换成物理执行计划真正的被系统调用执行。 　　　


//调用analyzer解析器

lazy val analyzed: LogicalPlan = {

  SparkSession.setActiveSession(sparkSession)

  sparkSession.sessionState.analyzer.execute(logical)

}



lazy val withCachedData: LogicalPlan = {

  assertAnalyzed()

  assertSupported()

  sparkSession.sharedState.cacheManager.useCachedData(analyzed)

}

//调用optimizer优化器

lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData)



//将优化后的逻辑执行计划转换成物理执行计划

lazy val sparkPlan: SparkPlan = {

  SparkSession.setActiveSession(sparkSession)

  // TODO: We use next(), i.e. take the first plan returned by the planner, here for now,

  //       but we will implement to choose the best plan.

  planner.plan(ReturnAnswer(optimizedPlan)).next()

}



// executedPlan should not be used to initialize any SparkPlan. It should be

// only used for execution.

lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan)



/**
* Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal
* row format conversions as needed.
*/

protected def prepareForExecution(plan: SparkPlan): SparkPlan = {

  preparations.foldLeft(plan) { case (sp, rule) => rule.apply(sp) }

}
　　QueryExecution类的主要成员如下所示。其中定义了解析器analyzer、优化器optimizer以及生成物理执行计划的sparkPlan。前文有介绍，analyzer的主要职责是将parser生成的unresolved logical plan解析生成logical plan。调用analyzer的代码在QueryExecution中，code 5中已经有贴出。此模块的主函数来自于analyzer的父类RuleExecutor。主函数execute实现在RuleExecutor类中，代码如下：


/**
   * Executes the batches of rules defined by the subclass. The batches are executed serially
   * using the defined execution strategy. Within each batch, rules are also executed serially.
   */

  def execute(plan: TreeType): TreeType = {

    var curPlan = plan



    batches.foreach { batch =>

      val batchStartPlan = curPlan

      var iteration = 1

      var lastPlan = curPlan

      var continue = true



      // Run until fix point (or the max number of iterations as specified in the strategy.

      while (continue) {

        curPlan = batch.rules.foldLeft(curPlan) {

          case (plan, rule) =>

            val startTime = System.nanoTime()

            val result = rule(plan)

            val runTime = System.nanoTime() - startTime

            RuleExecutor.timeMap.addAndGet(rule.ruleName, runTime)



            if (!result.fastEquals(plan)) {

              logTrace(

                s"""
                  |=== Applying Rule ${rule.ruleName} ===
                  |${sideBySide(plan.treeString, result.treeString).mkString("\n")}
                """.stripMargin)

            }



            result

        }

        iteration += 1

}
此函数实现了针对analyzer类中定义的每一个batch（类别），按照batch中定义的fix point(策略)和rule（规则）对Unresolved的逻辑计划进行解析。其中batch的结构如下：

/** A batch of rules. */

protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)
由于在analyzer的batchs中定义了多个规则，代码段很长，因此这里就不再贴出，有需要的请去spark的源码中找到Analyzer类查看。

　　在batchs里的这些batch中，Resolution是最常用的，从字面意思就可以看出其用途，就是将parser解析后的逻辑计划里的各个节点，转变成resolved节点。而其中ResolveRelations是比较好理解的一个rule（规则），这一步调用了catalog这个对象，Catalog对象里面维护了一个tableName,Logical Plan的HashMap结果。通过这个Catalog目录来寻找当前表的结构，从而从中解析出这个表的字段，如UnResolvedRelations 会得到一个tableWithQualifiers。（即表和字段）。catalog中缓存表名称和逻辑执行计划关系的代码如下：

/**
* A cache of qualified table names to table relation plans.
*/

val tableRelationCache: Cache[QualifiedTableName, LogicalPlan] = {

  val cacheSize = conf.tableRelationCacheSize

  CacheBuilder.newBuilder().maximumSize(cacheSize).build[QualifiedTableName, LogicalPlan]()

}
2.3 Optimizer
optimizer是catalyst中关键的一个部分，提供对sql查询的一个优化。optimizer的主要职责是针对Analyzer的resolved logical plan，根据不同的batch优化策略)，来对执行计划树进行优化，优化逻辑计划节点(Logical Plan)以及表达式(Expression)，同时，此部分也是转换成物理执行计划的前置。optimizer的调用在QueryExecution类中，代码code 5中已经贴出。

　　其工作方式与上面讲的Analyzer类似，因为它们的主函数executor都是继承自RuleExecutor。因此，optimizer的主函数如上面的code 6代码，这里就不在贴出。optimizer的batchs（优化策略）定义如下：

def batches: Seq[Batch] = {

  // Technically some of the rules in Finish Analysis are not optimizer rules and belong more

  // in the analyzer, because they are needed for correctness (e.g. ComputeCurrentTime).

  // However, because we also use the analyzer to canonicalized queries (for view definition),

  // we do not eliminate subqueries or compute current time in the analyzer.

  Batch("Finish Analysis", Once,

    EliminateSubqueryAliases,

    EliminateView,

    ReplaceExpressions,

    ComputeCurrentTime,

    GetCurrentDatabase(sessionCatalog),

    RewriteDistinctAggregates,

    ReplaceDeduplicateWithAggregate) ::

  //////////////////////////////////////////////////////////////////////////////////////////

  // Optimizer rules start here

  //////////////////////////////////////////////////////////////////////////////////////////

  // - Do the first call of CombineUnions before starting the major Optimizer rules,

  //   since it can reduce the number of iteration and the other rules could add/move

  //   extra operators between two adjacent Union operators.

  // - Call CombineUnions again in Batch("Operator Optimizations"),

  //   since the other rules might make two separate Unions operators adjacent.

  Batch("Union", Once,

    CombineUnions) ::

  Batch("Pullup Correlated Expressions", Once,

    PullupCorrelatedPredicates) ::

  Batch("Subquery", Once,

    OptimizeSubqueries) ::

  Batch("Replace Operators", fixedPoint,

    ReplaceIntersectWithSemiJoin,

    ReplaceExceptWithAntiJoin,

    ReplaceDistinctWithAggregate) ::

  Batch("Aggregate", fixedPoint,

    RemoveLiteralFromGroupExpressions,

    RemoveRepetitionFromGroupExpressions) ::

  Batch("Operator Optimizations", fixedPoint,

    // Operator push down

    PushProjectionThroughUnion,

    ReorderJoin,

    EliminateOuterJoin,

    PushPredicateThroughJoin,

    PushDownPredicate,

    LimitPushDown(conf),

    ColumnPruning,

    InferFiltersFromConstraints,

    // Operator combine

    CollapseRepartition,

    CollapseProject,

    CollapseWindow,

    CombineFilters,

    CombineLimits,

    CombineUnions,

    // Constant folding and strength reduction

    NullPropagation(conf),

    FoldablePropagation,

    OptimizeIn(conf),

    ConstantFolding,

    ReorderAssociativeOperator,

    LikeSimplification,

    BooleanSimplification,

    SimplifyConditionals,

    RemoveDispensableExpressions,

    SimplifyBinaryComparison,

    PruneFilters,

    EliminateSorts,

    SimplifyCasts,

    SimplifyCaseConversionExpressions,

    RewriteCorrelatedScalarSubquery,

    EliminateSerialization,

    RemoveRedundantAliases,

    RemoveRedundantProject,

    SimplifyCreateStructOps,

    SimplifyCreateArrayOps,

    SimplifyCreateMapOps) ::

  Batch("Check Cartesian Products", Once,

    CheckCartesianProducts(conf)) ::

  Batch("Join Reorder", Once,

    CostBasedJoinReorder(conf)) ::

  Batch("Decimal Optimizations", fixedPoint,

    DecimalAggregates(conf)) ::

  Batch("Typed Filter Optimization", fixedPoint,

    CombineTypedFilters) ::

  Batch("LocalRelation", fixedPoint,

    ConvertToLocalRelation,

    PropagateEmptyRelation) ::

  Batch("OptimizeCodegen", Once,

    OptimizeCodegen(conf)) ::

  Batch("RewriteSubquery", Once,

    RewritePredicateSubquery,

    CollapseProject) :: Nil

}
由此可以看出，Spark 2.1.1版本增加了更多的优化策略，因此如果要提高spark sql程序的性能，升级spark版本是非常必要的。

　　其中，"Operator Optimizations"，即操作优化是使用最多的，也是比较好理解的优化操作。"Operator Optimizations"中包括的规则有PushProjectionThroughUnion，ReorderJoin等。

PushProjectionThroughUnion策略是将左边子查询的Filter或者是projections移动到union的右边子查询中。例如针对下面代码

case class a:item1:String,item2:String,item3:String

case class b:item1:String,item2:String



select a.item1,b.item2 from a where a.item1>'example'  from a union all (select item1,item2 from b )

此时，通过PushProjectionThroughUnion规则后，查询优化器会将sql改为下面的sql，即将Filter右移到了union的右端。如下所示。

select a.item1,b.item2 from a where a.item1>’example’  union all (select item1,item2 from b where item1>’example’)

RorderJoin，顾名思义，就是对多个join操作进行重新排序。具体操作就是将一系列的带有join的子执行计划进行排序，尽可能地将带有条件过滤的子执行计划下推到执行树的最底层，这样能尽可能地减少join的数据量。

　　例如下面代码中是三个表做join操作，而过滤条件是针对表a的，但熟悉sql的人就会发现对a中字段item1的过滤可以挪到子查询中，这样可以减少join的时候数据量，如果满足此过滤条件的记录比较少，则可以大大地提高join的性能。

case class b:item1:String,item2:String

select a.item1,d.item2 from a where a.item1> ‘example’ join (select b.item1,b.item2 from b join c on b.item1=c.item1) d on a.item1= d.item1

2.4 SparkPlann
optimizer将逻辑执行计划优化后，接着该SparkPlan登场了，SparkPlann将optimized logical plan转换成physical plans。执行代码如下：

code 10

lazy val sparkPlan: SparkPlan = {

  SparkSession.setActiveSession(sparkSession)

  // TODO: We use next(), i.e. take the first plan returned by the planner, here for now,

  //       but we will implement to choose the best plan.

  planner.plan(ReturnAnswer(optimizedPlan)).next()

}

其中，planner为SparkPlanner类的对象，对象的创建如下code 11所示。该对象中定义了一系列的执行策略，包括LeftSemiJoin 、HashJoin等等，这些策略用来指定实际查询时所做的操作。SparkPlanner中定义的策略如下code 12所示：

def strategies: Seq[Strategy] =

      extraStrategies ++ (

      FileSourceStrategy ::

      DataSourceStrategy ::

      SpecialLimits ::

      Aggregation ::

      JoinSelection ::

      InMemoryScans ::

      BasicOperators :: Nil)


/**

   * Planner that converts optimized logical plans to physical plans.

   */

  def planner: SparkPlanner =

    new SparkPlanner(sparkContext, conf, experimentalMethods.extraStrategies)
plan真正的处理函数如下的code 13所示。该函数的功能是整合所有的Strategy，_(plan)每个Strategy应用plan上，得到所有Strategies执行完后生成的所有Physical Plan的集合。

def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = {

    // Obviously a lot to do here still...



    // Collect physical plan candidates.

    val candidates = strategies.iterator.flatMap(_(plan))



    // The candidates may contain placeholders marked as [[planLater]],

    // so try to replace them by their child plans.

    val plans = candidates.flatMap { candidate =>

      val placeholders = collectPlaceholders(candidate)



      if (placeholders.isEmpty) {

        // Take the candidate as is because it does not contain placeholders.

        Iterator(candidate)

      } else {

        // Plan the logical plan marked as [[planLater]] and replace the placeholders.

        placeholders.iterator.foldLeft(Iterator(candidate)) {

          case (candidatesWithPlaceholders, (placeholder, logicalPlan)) =>

            // Plan the logical plan for the placeholder.

            val childPlans = this.plan(logicalPlan)



            candidatesWithPlaceholders.flatMap { candidateWithPlaceholders =>

              childPlans.map { childPlan =>

                // Replace the placeholder by the child plan

                candidateWithPlaceholders.transformUp {

                  case p if p == placeholder => childPlan

                }

              }

            }

        }

}
3. spark2.x较spark1.x性能
对源码的分析后，可以看出spark 2.0较 spark 1.x版本性能有较大提升，具体可从以下几个方面看出：

1.parser语法解析不同

　　spark 1.x版本使用的是scala的parser语法解析器，而spark 2.x版本使用的是ANTLR4，解析性能更好

2.spark 2.x的optimizer优化策略不同

　　spark 2.x为optimizer优化器提供了更加丰富的优化策略，从两个版本里optimizer类中的batchs中可以看出。

3．spark 2.x提供了第二代Tungsten引擎

　　Spark2.x移植了第二代Tungsten引擎，这一代引擎是建立在现代编译器和MPP数据库的基础上，并且应用到数据的处理中。主要的思想是将那些拖慢整个程序执行速度的代码放到一个单独的函数中，消除虚拟函数的调用，并使用寄存器来存放中间结果。这项技术被称作“whole-stage code generation.”

　　下面通过在单机上执行10亿条数据的aggregations and joins操作，来对比spark1.6和2.0的性能。其中，ns为纳秒，表格中的处理时间为单个线程处理单行数据所用时间。由此，可看出spark2.0的处理性能远远好于1.6。



图5 spark性能对比

