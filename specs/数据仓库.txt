第1章 数据仓库、商业智能及维度建模初步
1.1 数据获取与数据分析的区别
1.2 数据仓库与商业智能的目标
1.3 维度建模简介
1.3.1 星型模式与OLAP多维数据库
1.3.2 用于度量的事实表
1.3.3 用于描述环境的维度表
1.3.4 星型模式中维度与事实的连接
1.4 Kimball的DW/BI架构
1.4.1 操作型源系统
1.4.2 获取.转换_加口载(ETL)系统
1.4.3 用于支持商业智能决策的展现区
1.4.4 商业智能应用
1.4.5 以餐厅为例描述Kimball架构
1.5 其他DW/BI架构
1.5.1 独立数据集市架构]
1.5.2 辐射状企业信息工厂Inmon架构
1.5.3 混合辐射状架构与Kimball架构
1.6 维度建模神话
1.6.1 神话1：维度模型仅包含汇总数据
1.6.2 神话2：维度模型是部门级而不是企业级的
1.6.3 神话3：维度模型是不可扩展的
1.6.4 神话4：维度模型仅用于预测
1.6.5 神话5：维度模型不能被集成
1.7 考虑使用维度模型的
更多理由
1.8 本章小结

第2章 Kimball维度建模技术概述
2.1 基本概念
2.1.1 收集业务需求与数据实现
2.1.2 协作维度建模研讨
2.1.3 4步骤维度设计过程
2.1.4 业务过程
2.1.5 粒度
2.1.6 描述环境的维度
2.1.7 用于度量的事实
2.1.8 星型模式与OLAP多维数据库
2.1.9 方便地扩展到维度模型
2.2 事实表技术基础
2.2.1 事实表结构
2.2.2 可加、半可加、不可加事实
2.2.3 事实表中的空值
2.2.4 一致性事实
2.2.5 事务事实表
2.2.6 周期快照事实表
2.2.7 累积快照事实表
2.2.8 无事实的事实表
2.2.9 聚集事实表或OLAP多维数据库
2.2.1 0合并事实表
2.3 维度表技术基础
2.3.1 维度表结构
2.3.2 维度代理键
2.3.3 自然键、持久键和超自然键
2.3.4 下钻
2.3.5 退化维度
2.3.6 非规范化扁平维度
2.3.7 多层次维度
2.3.8 文档属性的标识与指示器
2.3.9 维度表中的空值属性
2.3.10 日历日期维度
2.3.11 扮演角色的维度
2.3.12 杂项维度
2.3.13 雪花维度
2.3.14 支架维度
2.4 使用一致性维度集成
2.4.1 一致性维度
2.4.2 缩减维度
2.4.3 跨表钻取
2.4.4 价值链
2.4.5 企业数据仓库总线架构
2.4.6 企业数据仓库总线矩阵
2.4.7 总线矩阵实现细节
2.4.8 机会／利益相关方矩阵
2.5 处理缓慢变化维度属性
2.5.1 类型0：原样保留
2.5.2 类型1：重写
2.5.3 类型2：增加新行
2.5.4 类型3：增加新属性
2.5.5 类型4：增加微型维度
2.5.6 类型5：增加微型维度及类型1支架
2.5.7 类型6：增加类型1属性到类型2维度
2.5.8 类型7：双类型l和类型2维度
2.6 处理维度层次关系
2.6.1 固定深度位置的层次
2.6.2 轻微参差不齐／可变深度层次
2.6.3 具有层次桥接表的参差不齐／可变深度层次
2.6.4 具有路径字符属性的可变深度层次
2.7 高级事实表技术
2.7.1 事实表代理键
2.7.2 蜈蚣事实表
2.7.3 属性或事实的数字值
2.7.4 日志／持续时间事实
2.7.5 头／行事实表
2.7.6 分配的事实
2.7.7 利用分配建立利润与损失事实表
2.7.8 多种货币事实
2.7.9 多种度量事实单位
2.7.1 0年.日事实
2.7.1 1多遍SQL以避免事实表间的连接
2.7.1 2针对事实表的时间跟踪1
2.7.1 3迟到的事实
2.8 高级维度技术
2.8.1 维度表连接
2.8.2 多值维度与桥接表
2.8.3 随时间变化的多值桥接表
2.8.4 标签的时间序列行为
2.8.5 行为研究分组
2.8.6 聚集事实作为维度属性
2.8.7 动态值范围
2.8.8 文本注释维度
2.8.9 多时区
2.8.10 度量类型维度
……
第3章 零售业务
第4章 库存
第5章 采购
第6章 订单管理
第7章 会计
第8章 客户关系管理
第9章 人力资源管理
第10章 金融服务
第11章 电信
第12章 交通运输
第13章 教育
第14章 医疗卫生
第15章 电子商务
第16章 保险业务
第17章 KimballDW/BI生命周期概述
第18章 维度建模过程与任务
第19章 ETL子系统与技术
第20章 ETL系统设计与开发过程和任务
第21章 大数据分析

第1章 读本概览
1.1 抑制住立即开始编码的冲动
1.2 设置边界
1.3 数据争夺
1.4 流言终结者
1.5 划分数据世界
1.6 集成式企业数据仓库的必要步骤
1.6.1 集成式：EDW会交付什么
1.6.2 集成的终极试金石
1.6.3 组织挑战
1.6.4 一致化维度和事实
1.6.5 使用总线矩阵与管理层交流
1.6.6 管理集成式EDW的主干
1.6.7 维度管理器
1.6.8 事实提供者
1.6.9 配置商业智能（BI）工具
1.6.10 连带责任
1.7 钻取以寻求原因
1.8 渐变维度
1.8.1 渐变维度的三种原生类型
1.8.2 高级渐变维度
1.9 通过维度评价BI工具
1.10 事实表
1.10.1 忠实于粒度
1.10.2 从最低的可能粒度进行构建
1.10.3 三类事实表
1.11 开发利用事实表
1.11.1 前端：聚合导航
1.11.2 前端：钻取不同的粒度
1.11.3 前端：将约束暴露给不同的业务过程
1.11.4 后端：事实表代理键

第2章 深入研究之前
2.1 Ralph Kimball和施乐帕克研究中心（Xerox PARC）
2.2 数据库市场分化
2.3 提出超市概念（Kimball经典）
2.3.1 危机规划
2.3.2 具有架构的数据集市
2.3.3 一致化维度的重要性
2.3.4 设计一致化维度
2.3.5 做出承诺
2.3.6 允许的一致化维度变体
2.3.7 建立标准事实定义
2.3.8 粒度的重要性
2.3.9 更高级别的数据集市
2.3.10 解决烟囱问题
2.3.11 不需要一致化维度的情形
2.3.12 清晰视角
2.4 数据仓库的全新需求
2.5 应对全新需求
2.5.1 数据集市和维度建模
2.5.2 将数据集市插入数据仓库总线架构中
2.6 挑起事端
2.7 设计约束和不可避免的现实
2.7.1 设计约束
2.7.2 不可避免的现实
2.7.3 摆脱困境
……

第3章 项目/程序规划
第4章 需求定义
第5章 数据架构
第6章 维度建模基础
第7章 维度建模任务和职责
第8章 事实表核心概念
第9章 维度表核心概念
第10章 更多的维度模式和注意事项
第11章 后台ETL和数据质量
第12章 技术架构注意事项
第13章 前台商业智能应用程序
第14章 维护和发展的注意事项


第1章 决策支持系统的发展
1.1 演化
1.2 自然演化式体系结构的问题
1.3 开发生命周期
1.4 硬件利用模式
1.5 为重建工程创造条件
1.6 监控数据仓库环境
1.7 小结
第2章 数据仓库环境
2.1 数据仓库的结构
2.2 面向主题
2.3 第1天到第n天的现象
2.4 粒度
2.5 探查与数据挖掘
2.6 活样本数据库
2.7 分区设计方法
2.8 数据仓库中的数据组织
2.9 审计与数据仓库
2.10 数据的同构/异构
2.11 数据仓库中的数据清理
2.12 报表与体系结构化环境
2.13 各种环境中的操作型窗口
2.14 数据仓库中的错误数据
第3章 设计数据仓库
3.1 从操作型数据开始
3.2 数据/过程模型与体系结构化环境
3.3 数据仓库与数据模型
3.4 数据模型与迭代式开发
3.5 规范化/反向规范化
3.6 元数据
3.7 数据周期——时间间隔
3.8 转换和集成的复杂性
3.9 数据仓库记录的触发
3.10 概要记录
3.11 管理大量数据
3.12 创建多个概要记录
3.13 从数据仓库环境到操作型环境
3.14 数据仓库数据的直拉操作型访问
3.15 数据仓库数据的间接访问
3.16 数据仓库数据的间接使用
3.17 星形连接
3.18 支持操作型数据存储
3.19 需求和Zachman框架
3.20 小结
第4章 数据仓库中的粒度
4.1 粗略估算
4.2 规划过程的输入
4.3 溢出存储器中的数据
4.4 确定粒度级别
4.5 一些反馈循环技巧
4.6 确定粒度级别的几个例子
4.7 填充数据集市
4.8 小结
第5章 数据仓库和技术
5.1 管理大量数据
5.2 管理多种介质
5.3 索引和监控数据
5.4 多种技术的接口
5.5 程序员/设计者对数据存放位置的控制
5.6 数据的并行存储和管理
5.7 语言接口
5.8 数据的有效装裁
5.9 有效利用索引
5.10 数据压缩
5.11 复合主键
5.12 变长数据
5.13 加锁管理
5.14 只涉及索引的处理
5.15 快速恢复
5.16 其他的技术特征
5.17 DBMS类型和数据仓库
5.18 改变DBMS技术
5.19 多维DBMS和数据仓库
5.20 在多种存储介质上构建数据仓库
5.21 数据仓库环境中元数据的角色
5.22 上下文和内容
5.23 刷新数据仓库
5.24 测试问题
5.25 小结
第6章 分布式数据仓库
第7章 主管信息系统和数据仓库
第8章 外部数据与数据仓库
第9章 迁移到体系结构化环境
第10章 数据仓库和Web
第11章 非结构化数据和数据仓库
第12章 大型数据仓库
第13章 关系模型和多维模型数据库设计基础
第14章 数据仓库高级话题
第15章 数据仓库的成本论证和投资回报
第16章 数据仓库和ODS
第17章 企业信息依从准则和数据仓库
第18章 最终用户社区
第19章 数据仓库设计的复查要目

第1章 数据仓库的概念与体系结构
1．1 数据仓库的兴起
1．1．1 数据管理技术的发展
1．1．2 数据仓库的萌芽
1．2 数据仓库的基本概念
1．2．1 元数据
1．2．2 数据粒度
1．2．3 数据模型
1．2．4 ETL
1．2．5 数据集市
1．3 数据仓库的特点与组成
1．3．1 数据仓库的特点
1．3．2 数据仓库的组成
1．4 数据仓库的体系结构
1．4．1 传统的数据仓库体系结构
1．4．2 传统数据仓库系统在大数据时代所面临的挑战
1．4．3 大数据时代的数据仓库
习题

第2章 数据
2．1 数据的概念与内容
2．2 数据属性与数据集
2．3 数据预处理
2．3．1 数据预处理概述
2．3．2 数据清洗
2．3．3 数据集成
2．3．4 数据变换
2．3．5 数据归约
习题

第3章 数据存储
3．1 数据仓库的数据模型
3．1．1 数据仓库的概念模型
3．1．2 数据仓库的逻辑模型
3．1．3 数据仓库的物理模型
3．2 元数据存储
3．2．1 元数据的概念
3．2．2 元数据的分类方法
3．2．3 元数据的管理
3．2．4 元数据的作用
3．3 数据集市
3．3．1 数据集市的概念
3．3．2 数据集市的类型
3．3．3 数据集市的建立
3．4 大数据存储技术
3．4．1 大数据的概念
3．4．2 传统数据库的局限
3．4．3 NoSQL数据库
3．4．4 几种主流的NoSQL数据库
习题

第4章 OLAP与数据立方体
4．1 OLAP的概念4．1．1 OLAP的定义
4．1．2 OLAP的准则
4．1．3 OLAP的特征
4．2 多维分析的基本分析动作
4．2．1 切片
4．2．2 切块
4．2．3 钻取
4．2．4 旋转
……

第5章 数据挖掘基础
第6章 关联挖掘
第7章 聚类分析
第8章 分类
第9章 神经网络
第10章 统计分析
第11章 非结构化数据挖掘
第12章 知识图谱
第13章 大数据挖掘算法


第1章 实时数据仓库技术概述
1．1 数据仓库技术
1．1．1 数据仓库的定义
1．1．2 数据仓库的特点
1．1．3 数据仓库的体系结构
1．1．4 数据仓库的模型
1．2 实时数据仓库技术
1．2．1 实时数据仓库的定义
1．2．2 实时数据仓库的新挑战
1．2．3 实时数据仓库的体系结构
1．2．4 实时数据仓库与传统数据仓库的比较
1．3 MapReduce技术
1．3．1 MapReduce编程模式
1．3．2 MapReduce框架的实现
1．3．3 Hadoop

第2章 实时数据仓库体系结构
2．1 实时数据仓库体系结构的设计
2．2 ODS分区
2．3 双镜像交替分区
2．4 数据仓库副本分区
2．5 多级缓存分区机制
2．5．1 缓存的数据新鲜度
2．5．2 缓存的更新算法
2．5．3 多级缓存分区机制的查询
2．5．4 查询冲突问题的解决
2．6 几种实时数据存储区的比较

第3章 变化数据捕获
3．1 变化数据捕获方法
3．1．1 基于数据源表的时间戳标注
3．1．2 基于日志的被动数据变化的捕获
3．1．3 基于触发器的主动数据变化的捕获
3．2 基于LogMiner的变化数据捕获
3．2．1 Oracle日志简述
3．2．2 Oracle日志的两种模式
3．2．3 LogM：iner进行日志挖掘的基本流程
3．3 基于CDC的变化数据捕获
3．3．1 CDC工具捕获变化数据概述
3．3．2 CDC相关的数据库对象
3．3．3 对变化数据处理
3．3．4 CDC捕获模块流程设计

第4章 更新查询调度技术
4．1 更新查询调度技术概述
4．2 基于优先级的更新与查询平衡调度
4．2．1 系统模型
4．2．2 在线日志捕获数据
4．2．3 系统性能参数
4．2．4 PBBS调度算法
4．2．5 并行一致性控制策略
4．2．6 小结
4．3 支持Qos的更新和查询任务调度
4．3．1 概述
4．3．2 系统模型
4．3．3 查询任务的时间估算
4．3．4 调度算法
4．3．5 小结

第5章 实时数据仓库并行查询
5．1 概述
5．2 MapRecluee的基本流程
5．3 基于MapReclUee的并行关系运算
5．3．1 选择和投影运算
5．3．2 连接运算
5．3．3 除运算
5．3．4 聚集运算
5．4 基于分块结构的分布式数据库ChunkDB
5．4．1 ChunkDB的整体架构
5．4．2 ChunkDB分布式数据库
5．5 基于ChunkDB数据库的MapRecluee计算
5．5．1 基于ChunkDB的Maptleduee计算实现流程
5．5．2 DBInputFormat数据接口扩展
5．6 ChunkDB性能评估
5．6．1 评估环境
5．6．2 查询性能评价
5．6．3 集群规模的影响

第6章 实时数据立方技术
6．1 概述
6．2 基础知识
6．2．1 数据立方Cube
6．2．2 Dwarf数据立方
6．2．3 MapRedllice
6．3 基于MapReduee的数据立方构建
6．4 Dwarf立方的分割
6．4．1 Dwarf立方的基础划分
6．4．2 Dwarf立方的多维划分
6．5 并行Dwarf数据立方
6．5．1 并行Dwarf的建立
6．5．2 并行Dwarf的查询
6．5．3 并行Dwarf的更新
6．5．4 并行Dwarf的优化
6．6 并行Dwarf性能分析
6．6．1 评估环境
6．6．2 Dwarf的建立和存储性能
6．6．3 Dwarf立方的查询性能
6．6．4 Dwarf立方的更新性能
6．6．5 集群节点数量的影响

第7章 MR-RTDWH系统
7．1 MR．RTDWH概述
7．2 MR-RTDwH系统设计
7．2．1 系统设计目标
7．2．2 系统体系结构
7．2．3 传统ETL模块
7．2．4 实时ETL模块
7．2．5 实时数据仓库存储
7．2．6 更新查询调度模块
7．2．7 M印Reduce并行计算模块
7．2．8 MR-RTDWH系统实现


1.2　大数据2
1.3　大数据的定义4
1.4　为什么需要大数据？为什么是现在4
1.5　大数据示例5
1.5.1　社交媒体的文章5
1.5.2　调查数据分析6
1.5.3　调查数据7
1.5.4　气象数据8
1.5.5　Twitter数据8
1.5.6　集成和分析8
1.5.7　附加数据的类型10
1.6　总结11
延伸阅读11
第2章　使用大数据12
2.1　引言12
2.2　数据爆炸12
2.3　数据体量13
2.3.1　机器数据14
2.3.2　应用日志14
2.3.3　点击流日志14
2.3.4　外部或第三方数据15
2.3.5　电子邮件15
2.3.6　合同15
2.3.7　地理信息系统和地理空间数据16
2.3.8　示例：Funshots公司17
2.4　数据速度19
2.4.1　Amazon、Facebook、Yahoo和Google19
2.4.2　传感器数据19
2.4.3　移动网络20
2.4.4　社交媒体20
2.5　数据多样性21
2.6　总结22
第3章　大数据处理架构23
3.1　引言23
3.2　再论数据处理23
3.3　数据处理技术24
3.4　数据处理基础设施的挑战25
3.4.1　存储25
3.4.2　传输25
3.4.3　处理26
3.4.4　速度或吞吐量26
3.5　全共享架构与无共享架构的比较26
3.5.1　全共享架构27
3.5.2　无共享架构27
3.5.3　OLTP与数据仓库28
3.6　大数据处理28
3.6.1　基础设施方面31
3.6.2　数据处理方面32
3.7　电信大数据研究32
3.7.1　基础设施34
3.7.2　数据处理34
第4章　大数据技术简介35
4.1　引言35
4.2　分布式数据处理36
4.3　大数据处理需求38
4.4　大数据处理技术39
4.5　Hadoop42
4.5.1　Hadoop核心组件43
4.5.2　Hadoop总结69
4.6　NoSQL69
4.6.1　CAP定理69
4.6.2　键-值对：Voldemort70
4.6.3　列簇存储：Cassandra70
4.6.4　文档数据库：Riak76
4.6.5　图数据库77
4.6.6　NoSQL小结78
4.7　文本ETL处理78
延伸阅读79
第5章　大数据驱动的商业价值80
5.1　引言80
5.2　案例研究1：传感器数据81
5.2.1　摘要81
5.2.2　Vestas81
5.2.3　概述81
5.2.4　利用风力发电81
5.2.5　把气候变成资本82
5.2.6　跟踪大数据的挑战83
5.2.7　维持数据中心的能源效率83
5.3　案例研究2：流数据84
5.3.1　摘要84
5.3.2　监控和安全：TerraEchos84
5.3.3　需求84
5.3.4　解决方案84
5.3.5　效益84
5.3.6　先进的光纤网结合实时流数据85
5.3.7　解决方案组件85
5.3.8　扩展安全边界创建战略优势85
5.3.9　关联传感器数据使得假阳性率为零86
5.4　案例研究3：通过大数据分析改善患者预后86
5.4.1　摘要86
5.4.2　业务目标87
5.4.3　挑战87
5.4.4　概述：给从业人员新的洞察以指导患者护理87
5.4.5　挑战：将传统数据仓库生态系统与大数据融合87
5.4.6　解决方案：为大数据分析做好准备88
5.4.7　结果：消除“数据陷阱”88
5.4.8　为什么是aster88
5.4.9　关于Aurora89
5.5　案例研究4：安大略大学技术学院—利用关键数据，提供积极的患者护理89
5.5.1　摘要89
5.5.2　概述89
5.5.3　商业上的收益90
5.5.4　更好地利用数据资源90
5.5.5　智慧医疗保健91
5.5.6　解决方案组件91
5.5.7　融合人类知识与技术92
5.5.8　扩大Artemis的影响92
5.6　案例研究5：微软SQL Server客户解决方案93
5.6.1　客户画像93
5.6.2　解决方案的亮点93
5.6.3　业务需求93
5.6.4　解决方案94
5.6.5　好处94
5.7　案例研究6：以客户为中心的数据集成95
5.7.1　概述95
5.7.2　解决方案设计98
5.7.3　促成更好的交叉销售和追加销售的机会99
5.8　总结100
第二部分　数据仓库
第6章　再论数据仓库102
6.1　引言102
6.2　传统的数据仓库或DW 1.0103
6.2.1　数据架构103
6.2.2　基础设施104
6.2.3　数据仓库的陷阱106
6.2.4　建立数据仓库的架构方法111
6.3　DW 2.0113
6.3.1　Inmon的DW 2.0概述114
6.3.2　DSS 2.0概述115
6.4　总结116
延伸阅读116
第7章　数据仓库的再造118
7.1　引言118
7.2　企业数据仓库平台118
7.2.1　事务型系统119
7.2.2　运营数据存储区119
7.2.3　分段区120
7.2.4　数据仓库120
7.2.5　数据集市120
7.2.6　分析型数据库121
7.2.7　数据仓库的问题121
7.3　再造数据仓库的选择122
7.3.1　平台再造122
7.3.2　平台工程123
7.3.3　数据工程124
7.4　使数据仓库现代化125
7.5　使数据仓库现代化的案例研究127
7.5.1　当前状态分析127
7.5.2　推荐127
7.5.3　现代化的业务收益128
7.5.4　一体机的选择过程128
7.6　总结132
第8章　数据仓库中的工作负载管理133
8.1　引言133
8.2　当前状态133
8.3　工作负载的定义134
8.4　了解工作负载135
8.4.1　数据仓库输出136
8.4.2　数据仓库输入137
8.5　查询分类138
8.5.1　宽/宽138
8.5.2　宽/窄139
8.5.3　窄/宽139
8.5.4　窄/窄139
8.5.5　非结构化/半结构化数据140
8.6　ETL和CDC的工作负载140
8.7　度量141
8.8　当前系统设计的局限142
8.9　新工作负载和大数据143
8.10　技术选择144
8.11　总结144
第9章　应用到数据仓库的新技术145

==========
https://cloud.tencent.com/document/product/878/31441


云数据仓库套件 Sparkling 简介
云数据仓库套件 Sparkling（Tencent Sparkling Data Warehouse Suite）为您提供一套全托管、简单易用的、高性能的PB级云端数据仓库解决方案。Sparkling 基于业界领先的 Apache Spark 框架，您可以在数分钟内创建数千节点的企业级云端分布式数据仓库，并高效的按需快速弹性扩缩容。通过一站式大数据开发和科学平台 DataStudio 进行集群管控、数据集成、元数据管理、工作流开发、数据加工处理、结果可视化等操作，深度集成商业智能分析 BI，构建应用数据集市，提供海量数据的离线加工、数据建模、即席查询分析、数据挖掘和可视化探查能力。还可以借助 Sparkling 跨数据源联合分析特性，轻松分析位于 COS 和 CDB 等数据引擎上的数据，帮助企业专注于数据价值的挖掘和探索。


 腾讯云 Snova 数据仓库服务简介
Snova 数据仓库（下文简称 Snova）为您提供简单、快速、经济高效的 PB 级云端数据仓库解决方案。Snova 兼容 Greenplum 开源数据仓库，是一种基于 MPP（大规模并行处理）架构的数仓服务。借助于 Snova，您可以使用丰富的 PostgreSQL 开源生态工具，实现对 Snova 中海量数据的即席查询分析、ETL 处理及可视化探索；还可以借助 Snova 云端数据无缝集成特性，轻松分析位于 COS、TencentDB、ES 等数据引擎上的 PB 级数据。


产品简介
Snova 数据仓库产品文档
 文档
腾讯云 Snova 数据仓库产品特性

弹性伸缩
提供便利的弹性扩容能力，通过云控制台或云 API 简单操作便可以实现数百节点的伸缩或变配。 根据业务需求，可选择计算单元、CPU、内存、存储空间的等比扩展，提高性能以适配业务的发展。



简单易用
通过控制台操作，即可实现集群管理、监控维护等工作，无需关注底层基础设施的繁重运维工作。完全支持 ANSI SQL 2008 标准，使用标准 SQL 即可构建企业级数据仓库。支持直接查询 COS 数据，而无需提前数据预加载。



无缝集成
支持 COS 云存储扩展，实现存储空间的无限扩展。搭配多种工具及方案支持多源数据（如传统关系型数据库、Ckafka、流计算等）高速导入，实现对云端多源数据的汇聚分析。


性能卓越
基于分布式大规模并行处理 MPP 框架，可线性扩展存储及计算能力。支持行列混合存储，可按业务需求选择最佳存储方案。查询引擎深度优化，查询效率数倍于传统数据仓库。



安全可靠
双节点同步冗余，实现用户无感的故障转移和容灾备份。分布式部署，计算单元、服务器、机柜三重防护，提高重要数据基础设施保障。用户集群独立部署，支持 VPC 隔离，数据访问安全多重保障。

应用场景
经营分析决策

海量日志分析

用户行为实时洞察
在金融、零售等领域，需要对销售、资产、供应链等业务数据进行汇总分析，以便通过数据掌握公司经营情况，提高决策精准度及效率。
通过同步或 ETL 工具将分散在 CDB、Oracle、PostgreSQL 中的数据导入到 Snova 中，利用其对多源异构数据的分析能力，辅助业务决策。


客户案例
安心保险
安心保险
互联网保险
腾讯云 Snova 数据仓库为安心保险提供了海量数据存储，并提供高性能查询能力助力挖掘数据价值。


萌蛋互动
萌蛋互动
游戏研发与运营
腾讯云 Snova 数据仓库支持了萌蛋海外游戏运营数据分析需求，使用户国内外都可使用腾讯云，降低了运维及采购成本。


微众银行
微众银行


Snova 数据仓库（下文简称 Snova）为您提供简单、快速、经济高效的 PB 级云端数据仓库解决方案。Snova 兼容 Greenplum 开源数据仓库，是一种基于 MPP（大规模并行处理）架构的数仓服务。借助于 Snova，您可以使用丰富的 PostgreSQL 开源生态工具，实现对 Snova 中海量数据的即席查询分析、ETL 处理及可视化探索；还可以借助 Snova 云端数据无缝集成特性，轻松分析位于 COS、TencentDB、ES 等数据引擎上的 PB 级数据。

Snova 以集群为基本使用单位，一个用户可以拥有多个集群，一个集群通常由2个 master 节点和不少于2个的计算节点组成。单集群随着计算节点的增加，容量和性能将线性提升。
Snova 按提供计算和存储能力的节点规格和数量进行收费，包含按量计费和包月计费两种方式。
功能
Snova 为您提供简单、快速、经济高效的 PB 级云端数据仓库解决方案。

操作简单
使用 Snova 数据仓库服务，您能够快速在云端搭建 TB 级-PB 级数据仓库，无需关注集群的管理以及繁重的运维工作。通过在控制台的操作，即可实现集群管理、监控维护等工作。Snova 支持 ANSI SQL 2008 标准，使用标准 SQL 即可对数仓中的数据以及 COS 中的数据进行分析。

弹性扩容
使用 Snova 云控制台或通过调用云 API，可对 Snova 数据仓库的节点进行扩容，提升分析能力，以应对业务增长等场景。 Snova 对节点对扩容，会提升包括 CPU、内存、存储空间的能力，并实施数据倾斜等策略，以保证新扩节点的快速使用。

数据传输
Snova 支持 COS 云存储，可对 COS 中数据直接进行分析。对云上产品如 CDB、CKafka、流计算等产品中的数据支持直接高速导入。Snova 兼容业界 PostgreSQL 生态，可使用业界的工具或方案实现数据的传输。

高性能
Snova 基于分布式大规模并行处理 MPP 框架，可线性扩展存储及计算能力。支持行列混合存储，可按业务需求选择最佳存储方案。通过对硬件、软件、算法等多维度进行加速，优化查询效率。

安全性
Snova 中节点会自动配备一份备份，以此实现故障转移以及容灾备份。同时提供了机柜、服务器、计算单元的三重防护，提高基础设施的安全性。对每个用户的集群都支持 VPC 隔离，保障数据的访问安全。



文档中心  Snova 数据仓库  快速入门
快速入门
最近更新时间：2018-12-13 20:01:47

 编辑   查看pdf
在这篇文章中：
一. 创建集群
二. 连接数据库
三. 导入数据
四. 分析数据
使用 Snova 数据仓库，您需要完成以下操作：

一. 创建集群
登录 Snova 数据仓库 创建集群。
在创建集群之前，需要明确数据量，数据所在地域，以及访问集群的网络环境。
目前只支持 VPC 网络，因此在创建集群之前需要创建好访问集群的 VPC 网络及其子网。

二. 连接数据库
创建完集群之后，在之前配置好的子网下申请一台 CVM 用于访问集群。然后在 CVM 上通过 psql 连接数据库，如果没有安装客户端，可通过以下命令安装 PostgreSQL 的客户端程序。

 yum install -y postgresql.x86_64
Snova 完全兼容 PostgreSQL 8.3.23 协议。使用 psql 连接数据库的基本语法如下：

 psql  -h 10.0.0.3 -p 5436 -d postgres -U testuser
其中 postgres 是 Snova 默认的数据库，testuser 是创建数据库的时候需要用户输入的管理员帐号，5436是数据库默认的端口号，10.0.0.3 是创建完数据库后返回的 vip，该 vip 可以在控制台查询。

三. 导入数据
使用 INSERT 导入数据。
您可以通过 INSERT 语句直接向 Snova 写入数据，适用于数据量较小的场景。

使用客户端工具 psql 连接 Snova，并使用标准 INSERT 语法写入数据。
通过 PostgreSQL JDBC 驱动书写应用程序向 Snova 写入数据。
使用 \COPY 命令导入数据。
您可以使用 \COPY 命令将客户端所在主机上的文件导入到 Snova 中，语法可以参见 PostgreSQL 相关 \COPY 语法。

从 COS 外表中导入数据。
COS 外表语法详见 导入外部数据，在创建一个可读的 COS 外表后，可以使用如下语法将 COS 外表的数据导入到一张结构相同的内表中。

 INSERT INTO cos_local_tbl SELECT * FROM cos_tbl
从公有云其它环境导入数据详见 使用外表。

四. 分析数据
在 Snova 中，语法完全兼容 Greenplum Database 5.x 语法，您可以参考其语法进行数据分析。

前提条件
使用管理员用户或者由其创建的其他用户连接到数据库。
创建了相应的数据库以及数据库表，例如 testdb 与 testtable。
向数据库表中插入了数据，具体插入数据方法参见 插入数据。
简单的 SELECT 语句
 SELECT col1,col2,col3 FROM testtable WHERE col1 = val1 AND col2 = val2;
使用上述语句可以获取数据库表 testtable 中 col1 的值为 val1 并且 col2 的值为 val2 的记录。更多分析语句可参见 Greenplum Database 5.x 官方文档。

==================

离线计算中的幂等和DataWorks中的相关事项


本页目录
离线计算与幂等
计算
ETL
幂等这个词在软件研发中经常被提到。比如消息发送时不应该同时给同个用户推送多次相同的消息，针对同一笔交易的付款也不应该在重试过程中扣多次钱。曾见过一个案例，有个对于一个单据的确认模块没有考虑到幂等性，导致对应的单据有两条确认记录。其实幂等这个词是个数学的概念，表示这个操作执行多次的结果和执行一次是完全一样的。严格的定义这里不展开讨论，有兴趣的可以到网上搜一下，会有很多介绍。通俗一些说，幂等表示这个操作可以多次重跑，不用担心重跑后到结果会乱掉。就赋值而言，i=1就是个幂等到操作，无论做多少次赋值，只要有做成功一次，i的值就是1。而i++就不是一个幂等的操作。如果多次执行这个操作，i的值会不断增加1。

从前面的示例可以看出，幂等的优势是可以屏蔽重试带来的问题。在分布式的环境里，一般会通过消息中间件、异步调用等方式实现服务之间的解耦。在此过程中，如出现系统异常状况下的状态不明确的情况，一般会进行重试。如果应用不满足幂等的要求，则会出现错误的结果。

离线计算与幂等
离线计算中的作业量较大，跑一个作业需要较多时间。而且由于其特性，经常是凌晨开始计算，在OLTP业务调用量上来以前需要产出结果。如果发现问题，经常没有太多的时间留给技术人员去详细定位问题的原因，然后清理脏数据后重新进行计算。这时候您需要计算能够进行任意次的重跑，也就是说计算需要满足幂等性。对于一个满足幂等性要求的作业，出现问题的时候，您可以首先先重跑一下作业，以期能尽快恢复业务，后续再根据之前的日志慢慢定位问题。

下面以MaxCompute+DataWorks为例，从不同的角度里讨论离线计算的典型场景——离线数仓，看看都有哪些地方需要做到幂等以及如何做到。

计算
目前的离线计算，出于开发的效率考虑，一般都会考虑使用SQL进行代码开发。SQL中包含DDL和DML两种语句。除了SQL，计算引擎一般还支持MapReduce、Graph等计算模型。

DDL
DDL语法可以通过语句里的if exists/if not exists来确保幂等性。比如创建表可以用create table if not exists xxx，删除表可以通过drop table if exists xxx来保证不报错而且可以重复执行。当然创建表也可以先删除后再创建来实现幂等性。当然，如果是建表这种一次性的操作，可以在上线的时候手工做好，但是日常的分区创建/删除等操作就需要通过写进代码里，通过if exists/if not exists来保证可以重试。

DML
DML对数据有影响的是Insert操作。目前Insert有两种模式：Insert into和Insert overwrite。

其中Insert into是把数据追加到原来的数据里，而Insert overwrite是把以前的数据直接覆盖。所以可以清楚地看到，Insert into不满足幂等性要求，而Insert overwrite满足。如果使用Dataworks的SQL节点跑一个Insert into的作业，会有如下提示：

!!!警告!!!
在SQL中使用insert into语句有可能造成不可预料的数据重复，尽管对于insert into语句已经取消SQL级别的重试，但仍然存在进行任务级别重试的可能性，请尽量避免对insert into语句的使用！
一些使用Insert into的用户，要使用这种数据更新方式的原因，除去手工数据订正，发现一般都是针对一些不会变化的数据（比如网站的日志、每天的统计结果等）每天需要追加到表中。其实更好的方法是创建一个分区表，把每天需要Insert into的数据改成Insert overwrite到每天的一个不同分区里。

MapReduce
MapReduce默认使用覆盖写入的模式。如果确实有需要追加写入，可以使用com.aliyun.odps.mapred.conf.JobConf的setOutputOverwrite(boolean isOverwrite)来实现。如果需要改成幂等的，可以使用前面SQL里提到的，把数据写入特定的分区里来实现。

ETL
ETL暂时不考虑数据清洗（一般数据清洗是通过计算来实现的），只讨论数据的同步。在Dataworks中，数据的同步通过数据集成模块来实现。在数仓中，数据同步包括数据导入到数仓和数据从数仓中导出两种场景。

数据导入的场景要实现幂等性比较容易。首先我们对于导入数据，建议把每天新增的数据导入到新的一个分区里，然后只需要设置导入的MaxCompute表的清洗规则为写入前清理已有数据Insert Overwr即可。这样数据在导入的过程中会先清空数据后再导入，从而实现幂等。



数据导出的场景，如果数据是全量导出的，也可以用类似数据导入的方法，配置导入前准备语句，把原来的数据全部删除后重新导入。另外如果数据源支持主键冲突设置时，可以通过主键冲突设置成Replace Into来实现数据的替换。



由上图可见，目前Dataworks本身就支持设置出错重试，如果同步作业满足幂等性要求的，可以大胆开启这个设置，从而降低运维成本提高稳定性。




---------

解析运行时间和定时时间的理解
更新时间：2018-03-16 14:08:08


本页目录
业务日期和定时时间结合调度参数使用
测试调度参数
业务日期和定时时间结合调度参数使用
关于调度参数的使用，可以参考一下官网文档：参数配置。现在我来给大家解析一下这篇文档：

DataWorks调度系统参数：
调度系统参数：这两个调度系统参数无需赋值，可直接使用。

${bdp.system.cyctime}：定义为一个实例的定时运行时间，默认格式为：yyyymmddhh24miss。
${bdp.system.bizdate}：定义为一个实例计算时对应的业务日期，业务日期默认为运行日期的前一天，默认以 yyyymmdd 的格式显示（业务日期不精确到时分秒）。
DataWorks 自定义调度参数：有时候我们需要对时间参数进行加减，此时使用调度系统参数已经无法满足我们的需求了。面对这种情况，DataWorks 提供了自定义调度参数，用户可根据自己的业务需求，灵活的对时间参数进行加减，完美的解决各种复杂的场景。

自定义系统参数
自定义系统参数是以 bdp.system.cyctime 为基准的，任何的时间加减都是以定时时间为基线，向上或者向下移动。

举个例子：
代码为： select ${today} from dual ;

注 ：其中 ${today} 是声明变量

调度配置为：today = $[yyyymmdd]

注：其中 $[yyyymmdd] 是给声明的变量赋值

测试运行的时候，选择的业务日期是 20180305，测试运行时，日志中打印出来的实际运行sql为：select 20180306 from dual;

附上一张步骤图
image

敲黑板：请注意调度参数的配置时 ， 声明变量的符号和赋值的符号是不一样的，详情如下：

${} 这个符号是声明变量时使用的；

$[] 这个符号是给变量赋值的时候使用的；

以下提供一些调度参数的赋值方法：
后N年：$[add_months(yyyymmdd,12*N)]

前N年：$[add_months(yyyymmdd,-12*N)]

后N月：$[add_months(yyyymmdd,N)]

前N月：$[add_months(yyyymmdd,-N)]

后N周：$[yyyymmdd+7*N]

前N周：$[yyyymmdd-7*N]

后N天：$[yyyymmdd+N]

前N天：$[yyyymmdd-N]

后N小时：$[hh24miss+N/24]

前N小时：$[hh24miss-N/24]

后N分钟：$[hh24miss+N/24/60]

前N分钟：$[hh24miss-N/24/60]

小时级调度的例子
例一
业务场景1：查看业务日期为 20180305 的小时任务，上午 3 点的实例，运行时执行的代码。

代码：select ${min} from dual ;

注：其中 ${min} 是声明变量

调度配置：min = $[yyyymmddhh24miss]

注：其中 $[yyyymmddhh24miss] 是给声明的变量赋值

测试运行时，日志中的运行代码为：select 20180306030000 from dual ;

例二
业务场景2：如何获得业务日期为 20180305 的小时任务，上午 3 点的实例，前 15 分钟的时间。

代码 ： select ${min} from dual;

注：其中 ${min} 是声明变量

调度配置：min = $[yyyymmddhh24miss-15/24/60]

注：其中 $[yyyymmddhh24miss-15/24/60] 是给声明的变量赋值

测试运行时，日志中的运行代码为：select 20180306024500 from dual ;

测试调度参数
有不少同学可能没有接触过如何测试调度参数，这里放上我之前写的一篇文章《解析Dataworks中的运行和测试运行的区别》 ，调度参数和测试运行是需要结合使用的，没有经过调度系统，调度参数是无法生效的。


解析Dataworks中的运行和测试运行的区别
更新时间：2017-09-29 10:22:29


本页目录
页面上的运行
测试运行
有很多用户在使用Dataworks的数据开发中运行SQL和在数据集成中运行同步任务时，都会有一个疑惑。我在页面上运行和测试运行有什么区别呢？为什么我明明配置了系统参数，在代码中运行时，却没有自动解析，而提醒我去填写系统变量的临时值？

image

下面我就给大家讲讲这两者的主要区别。

页面上的运行
页面上的运行是不会经过调度系统的，直接将任务下发到底层去执行。所以在使用了调度参数后，运行时，是需要指定调度参数解析出来的值的。页面上触发的运行是不会生成实例的，所以也就没有办法去指定运行任务的机器，只能下发到Dataworks的默认资源组上去执行。

数据开发在页面上运行时如何给自定义参数赋值
在数据开发中，创建了SQL节点任务时，在SQL中使用了自定义参数。点击页面上的运行，会弹出一个提示框，在这个提示框里一定要填一个具体的值，而不要填$[yyyymmdd] 这种，不然在代码中$[yyyymmdd]是不会识别出来的。1imageimage

数据集成在页面上运行时如何给自定义参数赋值
在数据集成中，创建脚本模式的任务时。在脚本中使用了自定义参数，保存后，点击页面上的运行，提示我需要给自定义参数赋值。我填了一个值以后，却没有解析出来呢？

imageimage

原因是因为：系统参数和自定义系统参数，是调度系统的参数，只有通过调度系统后，才会解析出来。而我们点击的运行，是没有经过调度系统的，所以提示你输入的自定义变量参数 是需要填一个具体的值才行，这样在执行任务的时候，才会直接替换掉。

image

image

测试运行
测试运行会通过调度系统，去生成实例的，所以在使用了调度参数后，运行时，调度参数就会自动解析出来了，而且可以指定实例运行所在的资源组。

安全的数据开发模式
更新时间：2017-11-07 11:49:22


本页目录
实验背景
解决方案
操作步骤
实验背景
因为开发角色拥有删除表的权限，有用户质疑如果让其直接操作生产环境的表，会导致数据不安全。本文将为您介绍如何保证生产环境的数据安全。

解决方案
解决数据安全问题的解决方案的整体流程，如下图所示：



操作步骤
前期准备
创建两个项目，一个作为开发项目，一个作为生产项目，比如：Project_A 和 Project_B。

进入 Project_A 的 项目管理 页面，指定此项目发布到 Project_B 下。指定后，这两个项目便具有了关联关系，可以通过发布功能，将任务发布。



在项目管理中 Maxcompute 配置下，配置使用个人账号访问 Maxcompute 资源。如下图所示：



代码编辑
在 Project_A 中进行编辑代码，配置任务等操作。

将编辑好的代码和任务，通过 发布 功能，发布到 Project_B 中。

查询生产项目下的数据
如果需要操作生产项目下的表，可以进入 数据管理 页面，申请生产项目表的权限，这样开发角色便可在 Project_A 项目中通过 Project_B.table 的方式来查询生产项目中表的数据（申请的只有查询表权限，没有 drop 表权限）。

在 数据管理 页面，您不仅可以申请表的权限，还可以申请资源以及函数的权限。


注意：

从 Project_A 发布到 Project_B 后，有一些项目级别的配置是不会发布过去的，比如说数据源、表、资源、函数等，都需要在 Project_B中重新建立。

=================

配置不同周期任务依赖

本页目录
天任务依赖小时任务
小时任务依赖分钟任务
总结
大数据开发过程中常遇到不同运行周期的任务进行依赖，常见的有天任务依赖小时任务和小时任务依赖分钟任务。那么如何通过DataWorks开发这两种场景呢？

本文将从上述两种场景出发，结合调度依赖/参数/调度执行等，为您介绍不同周期调度依赖的最佳实践。

在开始操作前，为您介绍以下几个概念：

业务日期：业务数据产生的日期，这里指完整一天的业务数据。在DataWorks中，任务每天能处理的最近的完整一天的业务数据是昨天的数据，所以业务日期=日常调度日期-1天。

依赖关系：依赖关系是描述两个或多个节点/工作流之间的语义连接关系，其中上游节点/工作流的运行状态可以影响下游节点/工作流的运行状态，反之则不成立。

调度实例：DataWorks的调度系统对周期任务进行调度执行时，会先根据任务的配置进行实例化，每个实例带上具体的定时时间、状态、上下游依赖等属性。

注意：

目前数加DataWorks每天自动调度的实例都是在昨天晚上23:30生成。

调度规则：调度任务是否能运行起来需要满足以下条件。

确认上游任务实例是否都运行成功。若所有上游任务实例都运行成功则触发任务进入等待时间状态。

确认是否到任务实例的定时时间。任务实例进入等待时间状态后会check是否到达本身的定时时间，如果时间到了则进入等待资源状态。

确认当前调度资源是否充足。任务实例进入等待资源状态后，check当前本项目调度资源是否充足，若充足即可成功运行。

天任务依赖小时任务
业务场景
系统需求统计截止到每小时的业务数据增量，然后在最后一个小时的数据汇总完成后需要一个任务进行一整天的汇总 。

需求分析
每个小时的增量，即每整点起任务统计上个小时时间段的数据量。需要配置一个每天每整点调度一次的任务，每天最后一个小时的数据是在第二天的第一个实例进行统计。

最后的汇总任务为每天执行一次，且必须是在每天最后一个小时的数据统计完成之后才能执行，那么需要配置一个天任务，依赖小时任务的第一个实例 。

分析得出的调度形态如下图所示：

1

但是，真正如上图调度任务定义那样配置调度依赖后，调度任务实例并没有得到上图的效果，而是如下图所示：

1

上图中，天任务必须等小时任务当天的其它所有实例也执行完成才能执行，而需求是天任务只需依赖小时任务第一个实例，此效果明显不能满足需求。

要满足该场景的需求，需要结合任务的跨周期依赖进行配置，可以将小时任务的跨周期依赖属性配置为自依赖，然后天任务配置定时时间为零点整，且依赖属性配置为依赖小时任务。

分析得出的最终方案调度形态如下图所示：

1

此时，小时任务的实例为串行执行，第一个实例能执行成功，可保证它前面（昨天）的实例都已经执行成功，因此天任务可以只需要依赖第一个实例。

配置实践
小时任务的调度配置如下图所示：

1

1

参数配置：小时任务每整点实例处理前一小时的数据，如可以用$[yyyy-mm-dd-hh24-1/24]。天任务：若时间格式为yyyymmdd，用${bdp.system.bizdate}；若时间格式为yyyy-mm-dd，用自定义参数$[yyyy-mm-dd-1]，具体视详细设计而定。参数配置如下图所示：

1

测试/补数据/自动调度
天任务实例的定时时间为2017-01-11 00:00:00。

小时实例的定时时间为2017-01-11 00:00:00至2017-01-11 23:00:00。

${bdp.system.bizdate}赋值结果为20170110（实例定时间年月日减1天）。

$[yyyy-mm-dd-hh24-1/24]赋值结果为2017-01-10-23至2017-01-11-22（实例定时间年月日时减1小时）。

自动调度：调度系统自动生成的实例，每天的实例定时时间都是当天，如需求分析中的最终方案效果图。

小时任务依赖分钟任务
业务场景
已经有任务每30分钟进行一次同步，将前30分钟的系统数据增量导入到MaxCompute，任务定时为每天的每个整点和整点30分运行。现在需要配置一个小时任务，每6个小时进行一次统计，即每天分别统计0点到6点之间、6点到12点之间、12点到18点之间、18点到明天0点整之间的数据。

需求分析
分钟任务

00:00实例同步的是昨天最后30分钟的数据，产出的表分区如：昨天日期年-月-日-23:30。

00:30实例同步的是今天00:00-00:30之间的数据，产出的分区如：今天日期年-月-日-00:00。

01:00实例同步的是今天00:30-01:00之间的数据，产出的分区如：今天日期年-月-日-00:30。

以此类推，23:30实例同步的是今天23:00-23:30之间的数据，产出的分区如：今天日期年-月-日-23:00。

小时任务

每6个小时进行一次统计，则一天调度4次。

统计0点到6点之间的数据，则依赖分钟任务当天的00:30—6:00，共12个实例。

统计6点到12点之间的数据，则依赖分钟任务当天的6:30—12:00，共12个实例。

统计12点到18点之间的数据，则依赖分钟任务当天的12:30—18:00，共12个实例。

统计18点到第二天0点之间的数据，则依赖分钟任务当天的18:30—23:30以及第二天00:00，共12个实例。

分析得出的调度形态如下图所示：

1

但是，真正如上图调度任务定义那样配置调度依赖后，调度任务实例并没有得到上图的效果，而是如下图所示：

1

如上图所示，10日18点到11日0点之间的数据，11日小时任务0点，整点实例只依赖了分钟任务11日0点整实例，不能确保分钟任务10日18:30至23:30的实例可以执行成功。

要达到该场景需求，此时就需要结合任务的跨周期依赖进行配置，可以将分钟任务跨周期依赖属性配置成自依赖，然后小时任务依赖属性配置依赖小时任务。

分析得出的最终方案调度形态如下图所示：


配置实践
分钟任务的调度配置如下图所示：


小时任务调度配置如下图所示：


参数配置：分钟任务每个实例处理前面30分钟数据产出的分区可以用参数如$[yyyy-mm-dd-hh24:mi-30/24/60]，具体视详细设计而定。配置如下图所示：


测试/补数据/自动调度
测试和补数据：都是手动生成的调度实例，选择的是业务日期。如选择业务日期为2017-01-10。

分钟任务实例的定时时间是2017-01-11 00:00:00至2017-01-11 23:30:00，共48个实例。

小时实例的定时时间是2017-01-11 00:00:00、06:00:00、12:00:00、18:00:00，共4个实例。

$[yyyy-mm-dd-hh24:mi-30/24/60]赋值结果为2017-01-10-23:30至2017-01-11-23:00（实例定时间年月日时分减30分钟）。

自动调度：调度系统自动生成的实例，每天都实例定时时间都是当天，如需求分析中的最终方案效果图。

总结
长周期任务依赖短周期任务时，如果短周期有自依赖：当天的调度实例中，长周期任务的每个实例只依赖短周期实例中定时时间与它最近（且小于）的一个实例。

长周期任务（小时）依赖短周期任务（分钟）时，如果短周期无自依赖：当天的调度实例中，长周期任务的每个实例会依赖定时时间小于等于且没被本任务其他实例依赖的短周期实例。天/周/月依赖小时/分钟任务例外，因为天任务实例会依赖所有小时/分钟任务。

调度周期和调度时间参数配合使用，最终调度参数替换的值取决于每次调度的实例定时时间，而调度上看到的业务日期=实例定时时间年月日减1天。

==========
实验涉及大数据产品
大数据计算服务 MaxCompute
大数据开发套件 DataWorks
实验环境准备
必备条件：首先需要确保自己有阿里云云账号并已实名认证。详细请参见：

注册阿里云账号
企业实名认证
个人实名认证
开通大数据计算服务MaxCompute
若已经开通和购买了MaxCompute，请忽略次步骤直接进入创建DataWorks项目空间。

step1：进入阿里云官网并单击右上角登录阿里云账号。
step1_

step2：点击进入大数据计算服务产品详情页，单击立即开通。
step2_

step2_2_

step3：选择按量付费并单击立即购买。
step3_
创建DataWorks项目空间
确保阿里云账号处于登录状态。

step1：点击进入大数据（数加）管理控制台>大数据开发套件tab页面下。
step2：单击右上角创建项目或者直接在项目列表—>创建项目，跳出创建项目对话框。
1
选择相应的服务器时如果没有购买是选择不了会提示您去开通购买。数据开发、运维中心、数据管理默认是被选择中。

step3：勾选相应的服务单击 确认，跳转到下面的界面，填写相应的信息单击确认，创建项目完成。
2

项目名需要字母或下划线开头，只能包含字母下划线和数字。
【注意】项目名称全局唯一，建议大家采用自己容易区分的名称来作为本次workshop的项目空间名称。

step4：单击进入项目跳转到下面的界面：
step2_2_

新建数据源
根据workshop模拟的场景，需要分别创建FTP数据源和RDS数据源。

1.新建FTP数据源
step1：选择数据集成>数据源，单击新增数据源。
step1_

step2：选择数据源类型ftp，同时Protocol选择为sftp，其他配置项如下。
1

FTP数据源配置信息如下：

数据源类型：有公网ip
数据源名称：ftp_workshop_log
数据源描述：ftp日志文件同步
Protocol：sftp
Host：10.80.177.33（内网）/118.31.238.64（公网）
Port：22
用户名/密码：workshop/workshop

注：若项目创建在华东2，建议使用内网Host。由于跨region可能会出现网络不可达，所以项目创建在其他region的同学请使用公网Host。

step3：单击测试连通性，连通性测试通过后，单击确定保存配置。
step3_

2.新建RDS数据源
step1：选择数据集成>数据源，单击新增数据源。
step1_
step2：选择数据源类型为RDS>mysql并完成相关配置项。
step2_
RDS数据源配置信息如下：

数据源类型：阿里云数据库（RDS）
数据源名称：rds_workshop_log
数据源描述：rds日志数据同步
RDS实例名称：rm-bp1z69dodhh85z9qa
RDS实例购买者ID：1156529087455811
数据库名：workshop
用户名/密码：workshop/workshop#2017

step3：单击测试连通性，连通性测试通过后，单击确定保存配置。
step3_

创建目标表
step1：单击数据开发，进入数据开发首页中单击新建脚本。
step1_
step2：配置文件名称为create_table_ddl，类型选择为ODPS SQL，单击提交。
1
step3：编写DDL创建表语句，如下分别创建FTP日志对应目标表和RDS对应目标表。
step3_DDL_FTP_RDS_
DDL语句如下：

--创建ftp日志对应目标表
DROP TABLE IF EXISTS ods_raw_log_d;
CREATE TABLE ods_raw_log_d (
  col STRING
)
PARTITIONED BY (
  dt STRING
);
--创建RDS对应目标表
DROP TABLE IF EXISTS ods_user_info_d;
CREATE TABLE ods_user_info_d (
  uid STRING COMMENT '用户ID',
  gender STRING COMMENT '性别',
  age_range STRING COMMENT '年龄段',
  zodiac STRING COMMENT '星座'
)
PARTITIONED BY (
  dt STRING
);
step3：单击运行，直至日志信息返回成功表示两张目标表创建成功。

运行DDL

step4：可以使用desc语法来确认创建表是否成功。

DESC

step5：单击保存，保存编写的SQL建表语句。
保存DDL

新建工作流任务
step1：单击新建并选择新建任务。
新建任务

step2：选择工作流任务，调度类型选择为周期调度，其他配置项如下。

配置任务

step3：点击创建。

step4：进入工作流配置面板，并向面板中拖入一个虚节点（命名为workshopstart）和两个数据同步节点（分别命名为ftp数据同步和rds_数据同步）：
1
1
1

step5：拖拽连线将workshop_start虚节点设置为两个数据同步节点的上游节点，如下所示：
1

step6：点击保存（或直接快捷键ctrl+s）。

配置数据同步任务
1）配置ftp_数据同步节点
step1：双击ftp_数据同步节点，进入节点配置界面。选择来源：并选择数据来源事先配置好的ftp数据源，为ftp_workshop_log，文件路径为/home/workshop/user_log.txt。可以对非压缩文件进行数据预览。
配置同步

同步预览

数据来源配置项具体说明如下：

数据来源：ftp_workshop_ftp
文件路径：/home/workshop/user_log.txt*
列分隔符：|
step2：选择目标。点击下一步。

数据流向选择数据源为odps_first，表名为ods_raw_log_d。分区信息和清理规则都采取系统默认，即清理规则为写入前清理已有数据，分区按照${bdp.system.bizdate}。

step3：配置字段映射。连接要同步的字段。如下：
字段映射

step4：在下一步操作中配置通道控制，作业速率上限为10MB/s，进入下一步。
通道控制

可在预览保存页面中，预览上述的配置情况，也可以进行修改，确认无误后，点击保存。

step5：点击返回工作流面板。
返回工作流

2）配置rds_数据同步节点
step1：双击rds_数据同步节点进入配置界面。选择来源：选择数据来源为rds_workshop_log，表名为ods_user_info_d；切分键为使用默认生成列即可。点击数据预览，可以看到表中数据样例。
RDS选择来源

step2：进入下一步，选择目标数据源和表名。
RDS选择目标

step3：进入下一步，配置字段映射。默认会同名映射，字段映射关系采用默认即可，如下所示：
RDS字段映射

step4：进入下一步，配置作业速率上限。
RDS通道控制

step5：在预览保存页面中确认配置信息，无误后点击保存配置。
RDS预览保存

配置调度、提交工作流任务
step1：点击调度配置，配置调度参数
调度配置

step2：点击提交，提交已经配置的工作流任务。
提交工作流任务

step3：在变更节点列表弹出框中点击确定提交。
确定提交任务

提交成功后工作流任务处于只读状态，如下：
只读状态

测试运行工作流任务
step1：点击测试运行。
测试运行

step2：在周期任务运行提醒弹出框点击确定。
周期任务运行提醒

step3：在测试运行弹出框中，实例名称和业务日期都保持默认，点击运行。
测试运行按钮

step4：在工作流任务测试运行弹出框中，点击前往运维中心。

在运维中心可以查看任务视图，如下图表示该工作流任务（名称为workshop_start）正在运行。运维中心测试
直至所有节点都运行返回成功状态即可（需要点击运维视窗中的刷新按钮查看实时状态）。如下所示：
数据同步测试成功

step5：点击节点，查看运行日志。
日志界面

确认数据是否成功导入MaxCompute
step1：返回到create_table_ddl脚本文件中。

step2：编写并执行sql语句查看导入ods_raw_log_d记录数。
数据预览

step3：同样编写并执行sql语句查看导入ods_user_info_d记录数。

附录：SQL语句如下，其中分区键需要更新为业务日期，如测试运行任务的日期为20171011，那么业务日期为20171010。

--查看是否成功写入MaxCompute
select count(*) from ods_raw_log_d where dt=业务日期;
select count(*) from ods_user_info_d where dt=业务日期;

实验背景介绍
本手册为阿里云MVP Meetup Workshop《云计算·大数据：海量日志数据分析与应用》的《数据加工：用户画像》篇而准备。主要阐述在使用大数据开发套件过程中如何将已经采集至MaxCompute上的日志数据进行加工并进行用户画像，学员可以根据本实验手册，去学习如何创建SQL任务、如何处理原始日志数据。

实验涉及大数据产品
大数据计算服务 MaxCompute
大数据开发套件 DataWorks
实验环境准备
必备条件：

开通大数据计算服务MaxCompute
创建大数据开发套件项目空间
进入大数据开发套件，创建DataWorks项目空间
确保阿里云账号处于登录状态。

step1：点击进入大数据（数加）管理控制台>大数据开发套件tab页面下。
step2：点击右上角创建项目或者直接在项目列表—>创建项目，跳出创建项目对话框。
1
选择相应的服务器时如果没有购买是选择不了会提示您去开通购买。数据开发、运维中心、数据管理默认是被选择中。

step3：勾选相应的服务单击 确认，跳转到下面的界面，填写相应的信息单击确认，创建项目完成。
2
项目名需要字母或下划线开头，只能包含字母下划线和数字。【注意】项目名称全局唯一，建议大家采用自己容易区分的名称来作为本次workshop的项目空间名称。

step4：单击进入项目跳转到下面的界面：
进入大数据开发套件
新建数据表
若在实验《数据采集：日志数据上传》中已经新建脚本文件，可以直接切换至脚本开发tab下，双击打开create_table_ddl脚本文件。若无新建脚本文件可通过如下详细步骤进行创建脚本文件。

1.新建ods_log_info_d表
step1：点击数据开发，进入数据开发首页中点击新建脚本。
新建脚本

step2：配置文件名称为create_table_ddl，类型选择为ODPS SQL，点击提交。
配置脚本

step3：编写DDL创建表语句。
编写DDL

DDL建表语句如下：

CREATE TABLE ods_log_info_d (
  ip STRING COMMENT 'ip地址',
  uid STRING COMMENT '用户ID',
  time STRING COMMENT '时间yyyymmddhh:mi:ss',
  status STRING COMMENT '服务器返回状态码',
  bytes STRING COMMENT '返回给客户端的字节数',
  region STRING COMMENT '地域，根据ip得到',
  method STRING COMMENT 'http请求类型',
  url STRING COMMENT 'url',
  protocol STRING COMMENT 'http协议版本号',
  referer STRING COMMENT '来源url',
  device STRING COMMENT '终端类型 ',
  identity STRING COMMENT '访问类型 crawler feed user unknown'
)
PARTITIONED BY (
  dt STRING
);
step4：选择需要执行的SQL语句，点击运行，直至日志信息返回成功表示表创建成功。
运行DDL

step5：可以使用desc语法来确认创建表是否成功。
DESC

step6：点击保存，保存编写的SQL建表语句。
保存DDL

2.新建dw_user_info_all_d表
创建表方法同上，本小节附建表语句:

--创建dw_user_info_all_d表
drop table if exists dw_user_info_all_d;
CREATE TABLE dw_user_info_all_d (
  uid STRING COMMENT '用户ID',
  gender STRING COMMENT '性别',
  age_range STRING COMMENT '年龄段',
  zodiac STRING COMMENT '星座',
  region STRING COMMENT '地域，根据ip得到',
  device STRING COMMENT '终端类型 ',
  identity STRING COMMENT '访问类型 crawler feed user unknown',
  method STRING COMMENT 'http请求类型',
  url STRING COMMENT 'url',
  referer STRING COMMENT '来源url',
  time STRING COMMENT '时间yyyymmddhh:mi:ss'
)
PARTITIONED BY (
  dt STRING
);
3.新建rpt_user_info_d表
创建表方法同上，本小节附建表语句:

--创建rpt_user_info_d表
DROP TABLE IF EXISTS rpt_user_info_d;
CREATE TABLE rpt_user_info_d (
  uid STRING COMMENT '用户ID',
  region STRING COMMENT '地域，根据ip得到',
  device STRING COMMENT '终端类型 ',
  pv BIGINT COMMENT 'pv',
  gender STRING COMMENT '性别',
  age_range STRING COMMENT '年龄段',
  zodiac STRING COMMENT '星座'
)
PARTITIONED BY (
  dt STRING
);
上述三张表创建成功后，保存脚本文件。
保存脚本文件

工作流设计
若成功完成实验《数据采集：日志数据上传》，即可切换至任务开发tab中，双击打开workshop工作流任务。

打开工作流任务

向画布中拖入三个ODPS SQL节点，依次命名为ods_log_info_d、dw_user_info_all_d、rpt_user_info_d，并配置依赖关系如下：

SQL依赖关系

若未完成实验《数据采集：日志数据上传》篇，可通过进入查看如何创建工作流任务。

创建自定义函数
step1：点击下载
ip2region.jar

step2：切换至资源管理tab页，点击上传按钮。

进入资源管理

step3：点击选择文件，选择已经下载到本地的ip2region.jar。
资源上传

step4：点击提交。

step5：切换至函数管理tab，点击创建函数按钮。
进入函数管理

step6：资源选择ip2region.jar，其他配置项如下所示。
新建函数

配置项说明如下：

函数名：getregion
类名：org.alidata.odps.udf.Ip2Region
资源：ip2region.jar
step7：点击提交。
配置ODPS SQL节点
1）配置ods_log_info_d节点：
step1：双击ods_log_info_d节点，进入节点配置界面，编写处理逻辑。
ODS
附SQL逻辑如下：

INSERT OVERWRITE TABLE ods_log_info_d PARTITION (dt=${bdp.system.bizdate})
SELECT ip
  , uid
  , time
  , status
  , bytes --使用自定义UDF通过ip得到地域
  , getregion(ip) AS region --通过正则把request差分为三个字段
  , regexp_substr(request, '(^[^ ]+ )') AS method
  , regexp_extract(request, '^[^ ]+ (.*) [^ ]+$') AS url
  , regexp_substr(request, '([^ ]+$)') AS protocol --通过正则清晰refer，得到更精准的url
  , regexp_extract(referer, '^[^/]+://([^/]+){1}') AS referer --通过agent得到终端信息和访问形式
  , CASE
    WHEN TOLOWER(agent) RLIKE 'android' THEN 'android'
    WHEN TOLOWER(agent) RLIKE 'iphone' THEN 'iphone'
    WHEN TOLOWER(agent) RLIKE 'ipad' THEN 'ipad'
    WHEN TOLOWER(agent) RLIKE 'macintosh' THEN 'macintosh'
    WHEN TOLOWER(agent) RLIKE 'windows phone' THEN 'windows_phone'
    WHEN TOLOWER(agent) RLIKE 'windows' THEN 'windows_pc'
    ELSE 'unknown'
  END AS device
  , CASE
    WHEN TOLOWER(agent) RLIKE '(bot|spider|crawler|slurp)' THEN 'crawler'
    WHEN TOLOWER(agent) RLIKE 'feed'
    OR regexp_extract(request, '^[^ ]+ (.*) [^ ]+$') RLIKE 'feed' THEN 'feed'
    WHEN TOLOWER(agent) NOT RLIKE '(bot|spider|crawler|feed|slurp)'
    AND agent RLIKE '^[Mozilla|Opera]'
    AND regexp_extract(request, '^[^ ]+ (.*) [^ ]+$') NOT RLIKE 'feed' THEN 'user'
    ELSE 'unknown'
  END AS identity
  FROM (
    SELECT SPLIT(col, '##@@')[0] AS ip
    , SPLIT(col, '##@@')[1] AS uid
    , SPLIT(col, '##@@')[2] AS time
    , SPLIT(col, '##@@')[3] AS request
    , SPLIT(col, '##@@')[4] AS status
    , SPLIT(col, '##@@')[5] AS bytes
    , SPLIT(col, '##@@')[6] AS referer
    , SPLIT(col, '##@@')[7] AS agent
  FROM ods_raw_log_d
  WHERE dt = ${bdp.system.bizdate}
) a;
step2：点击保存。
保存ODS

step3：点击返回，返回至工作流开发面板。
返回工作流任务

2）配置dw_user_info_all_d节点：
step1：双击dw_user_info_all_d节点，进入节点配置界面，编写处理逻辑。
DW

附SQL语句如下：

INSERT OVERWRITE TABLE dw_user_info_all_d PARTITION (dt='${bdp.system.bizdate}')
SELECT COALESCE(a.uid, b.uid) AS uid
  , b.gender
  , b.age_range
  , b.zodiac
  , a.region
  , a.device
  , a.identity
  , a.method
  , a.url
  , a.referer
  , a.time
FROM (
  SELECT *
  FROM ods_log_info_d
  WHERE dt = ${bdp.system.bizdate}
) a
LEFT OUTER JOIN (
  SELECT *
  FROM ods_user_info_d
  WHERE dt = ${bdp.system.bizdate}
) b
ON a.uid = b.uid;
step2：点击保存。

step3：点击返回，返回至工作流开发面板。

配置rpt_user_info_d节点
step1：双击进入rpt_user_info_d节点进入配置界面。rpt
附SQL代码如下：

INSERT OVERWRITE TABLE rpt_user_info_d PARTITION (dt='${bdp.system.bizdate}')
SELECT uid
  , MAX(region)
  , MAX(device)
  , COUNT(0) AS pv
  , MAX(gender)
  , MAX(age_range)
  , MAX(zodiac)
FROM dw_user_info_all_d
WHERE dt = ${bdp.system.bizdate}
GROUP BY uid;
step2：点击保存。

step3：点击返回，返回至工作流开发面板。

提交工作流任务
step1：点击提交，提交已配置的工作流任务。
提交工作流

step2：在变更节点列表弹出框中点击确定提交。
变更节点列表

提交成功后工作流任务处于只读状态，如下：

只读状态

通过补数据功能测试新建的SQL任务
鉴于在数据采集阶段已经测试了数据同步任务，本节中直接测试下游SQL任务即可，也保证了时效性。

step1：进入运维中心>任务列表，找到workshop工作流任务。
工作流任务

step2：单击名称展开工作流。
![进入节点试图]image

step3：选中ods_log_info_d节点，单击补数据。
![选择补数据节点]image

step4：在补数据节点对话框中全选节点名称，选择业务日期，点击运行选中节点。
补数据节点列表

自动跳转到补数据任务实例页面。

step5：输入字母‘d’，通过过滤条件刷新，直至SQL任务都运行成功即可。
展开子节点
确认数据是否成功写入MaxCompute相关表
step1：返回到create_table_ddl脚本文件中。

step2：编写并执行sql语句查看rpt_user_info_d数据情况。
数据预览

附录：SQL语句如下。

---查看rpt_user_info_d数据情况
select * from rpt_user_info_d where dt=业务日期 limit 10;


实验环境准备
必备条件：

开通大数据计算服务MaxCompute
创建大数据开发套件项目空间
进入大数据开发套件，创建DataWorks项目空间
确保阿里云账号处于登录状态。

step1：点击进入大数据（数加）管理控制台>大数据开发套件tab页面下。
step2：点击右上角创建项目或者直接在项目列表—>创建项目，跳出创建项目对话框。1
选择相应的服务器时如果没有购买是选择不了会提示您去开通购买。数据开发、运维中心、数据管理默认是被选择中。

step3：勾选相应的服务单击 确认，跳转到下面的界面，填写相应的信息单击确认，创建项目完成。2
项目名需要字母或下划线开头，只能包含字母下划线和数字。【注意】项目名称全局唯一，建议大家采用自己容易区分的名称来作为本次workshop的项目空间名称。

step4：单击进入项目跳转到下面的界面：进入大数据开发套件
数据质量
数据质量（DQC），是支持多种异构数据源的质量校验、通知、管理服务的一站式平台。数据质量以数据集（DataSet）为监控对象，目前支持MaxCompute数据表和DataHub实时数据流的监控，当离线MaxCompute数据发生变化时，数据质量会对数据进行校验，并阻塞生产链路，以避免问题数据污染扩散。同时，数据质量提供了历史校验结果的管理，以便您对数据质量分析和定级。在流式数据场景下，数据质量能够基于Datahub数据通道进行断流监控，第一时间告警给订阅用户，并且支持橙色、红色告警等级，以及告警频次设置，以最大限度的减少冗余报警。

数据质量的使用流程是，针对已有的表进行监控规则配置，配置完规则后可以进行试跑，验证此规则是否试用。当试跑成功后，可将此规则和调度任务进行关联。关联成功后，每次调度任务代码运行完毕，都会触发数据质量的校验规则，以提升任务准确性。在关联调度后，可根据业务情况，对重要的表进行订阅。订阅成功后，此表的数据质量一旦出问题，都会有邮件或者报警进行通知。

注：数据质量会产生额外的计算费用，在使用时请注意。

新增表规则配置
若已完成《日志数据上传》、《用户画像》实验，我们会得到表：ods_raw_log_d、ods_user_info_d、ods_log_info_d、dw_user_info_all_d、rpt_user_info_d。

数据质量最重要的就是表规则的配置，那么如何配置表规则才是合理的呢？我们来看一下上面这几张表应该如何配置表规则。

ods_raw_log_d
在数据质量中可以看到该项目下的所有表信息，现在我们来给 ods_raw_log_d 表进行数据质量的监控规则配置。

image

选择ods_raw_log_d表，点击配置监控规则，将会进入如下页面。

image

我们可以回顾一下 ods_raw_log_d 这张表的数据来源，ods_raw_log_d 这张表的数据是从ftp中获取到的日志数据，其分区是以${bdp.system.bizdate}格式写入进表中（”dbp.system.bizdate” 是获取到前一天的日期）。

image

对于这种每日的日志数据，我们可以配置一下表的分区表达式，分区表达式有如下几种，我们选择 dt=$[yyyymmdd-1] 这种表达式，有关调度表达式的详细解读，请参考文档调度参数。

imageimage

注：若表中无分区列，可以配置无分区，请根据真实的分区值，来配置对应的分区表达式。

确认以后，可以见到如下界面，我们可以选择创建规则。

image

选择创建规则后，出现如下界面：

image

点击添加监控规则，会出现一个提示窗，来配置规则。

image

这张表里的数据来源于FTP上传的日志文件，作为源头表，我们需要尽早判断此表分区中是否有数据。如果这张表中没有数据，那么就需要阻止后面的任务运行，因为来源表没有数据，后面的任务运行是没有意义的。

注：只有强规则下红色报警会导致任务阻塞，阻塞会将任务的实例状态置为失败。

我们在配置规则的时候，选择模板类型为表行数，将规则的强度设置为强，比较方式设置为期望值不等于0，设置完毕后点击批量保存按钮即可。

image

此配置主要是为了避免分区中没有数据，导致下游任务的数据来源为空的问题。

规则试跑
右上角有一个节点试跑的按钮，可以在规则配置完毕后，进行规则校验，试跑按钮可立即触发数据质量的校验规则。

image

点击试跑按钮后，会提示一个弹窗，确认试跑日期。点击试跑后，下方会有一个提示信息，点击提示信息，可跳转至试跑结果中。

image

image

可根据试跑结果，来确认此次任务产出的数据是否符合预期。建议每个表规则配置完毕后，都进行一次试跑操作，以验证表规则的适用性。

在规则配置完毕，且试跑又都成功的情况下。我们需要将表和其产出任务进行关联，这样每次表的产出任务运行完毕后，都会触发数据质量规则的校验，以保证数据的准确性。

关联调度
数据质量支持任务关联调度，在表规则和调度任务绑定后，每次任务运行完毕，都会触发数据质量的检查。可以在表规则配置界面，点击关联调度，配置规则与任务的绑定关系。

image

点击关联调度，可以与已提交到调度的节点任务进行绑定，我们会根据血缘关系给出推荐绑定的任务，也支持自定义绑定。

image

选中搜索结果后，点击添加，添加完毕后即可完成与调度节点任务的绑定。

image

关联调度后，表名后面的小图标会变成蓝色。

image

配置任务订阅
关联调度后，每次调度任务运行完毕，都会触发数据质量的校验，但是我们如何去跟进校验结果呢？数据质量支持设置规则订阅，可以针对重要的表及其规则设置订阅，设置订阅后会根据数据质量的校验结果，进行告警。若数据质量校验结果异常，则会根据配置的告警策略进行通知。

点击订阅管理，设置接收人以及订阅方式，目前支持邮件通知及邮件和短信通知。

imageimageimage

订阅管理设置完毕后，可以在我的订阅中进行查看及修改。

image

建议将全部规则订阅，避免校验结果无法及时通知。

ods_user_info_d
ods_user_info_d 表的数据来至于rds的数据库，为用户信息表。我们在配置规则的时候，需要配置表的行数校验；还需要配置主键唯一的校验，避免数据重复。

同样，我们还是需要先配置一个分区字段的监控规则，监控的时间表达式为：dt=$[yyyymmdd-1]，配置成功后，在已添加的分区表达式中可以看到成功的分区配置记录。

image

分区表达式配置完毕后，点击右侧的创建规则，进行数据质量的校验规则配置。添加表行数的监控规则，规则强度设置为强，比较方式设置为期望值不等于0。

image

添加列级规则，设置主键列(uid)为监控列，模板类型为：字段重复值个数校验，规则设置为弱，比较方式设置为字段重复值个数小于1，设置完毕后，点击批量保存按钮即可。

image

此配置主要是为了避免数据重复，导致下游数据被污染的情况。

请不要忘记试跑->关联调度->规则订阅。

ods_log_info_d
ods_log_info_d 这张表的数据，主要是解析ods_raw_log_d 表里的数据，鉴于日志中的数据无法配置过多监控，只需配置表数据不为空的校验规则即可。先配置表的分区表达式为：dt=$[yyyymmdd-1]

image

配置表数据不为空的校验规则，规则强度设置为强，比较方式设置为期望值不等于0，设置完毕后，点击批量保存按钮即可。

image

请不要忘记试跑->关联调度->规则订阅。

dw_user_info_all_d
dw_user_info_all_d 这个表是针对ods_user_info_d 和 ods_log_info_d 表的数据汇总，由于此流程较为简单，ods层又都已配置了表行数不为空的规则，所以此表不进行数据质量监控规则的配置，以节省计算资源。

rpt_user_info_d
rpt_user_info_d 表是数据汇总后的结果表，根据此表的数据，我们可以进行表行数波动监测，针对主键进行唯一值校验等。先配置表的分区表达式：dt=$[yyyymmdd-1]

image

然后配置监控规则，单击右侧创建规则，点击添加监控规则。添加列级规则，设置主键列(uid)为监控列，模板类型为：字段重复值个数校验，规则设置为弱，比较方式设置为字段重复值个数小于1。

image

继续添加监控规则，添加表级规则，模板类型为：SQL任务表行数，7天波动检测；规则强度设置为弱，橙色阈值设置成0%，红色阈值设置成50%（此处阈值范围根据业务逻辑进行设置），配置完毕后，点击批量保存即可。

image

注：此处我们监控表行数主要是为了查看每日uv的波动，好及时了解应用动态。

请不要忘记试跑->关联调度->规则订阅。

大家可能注意到了，我们在设置表规则强度的时候，数据仓库中越底层的表，设置强规则的次数越多。那是因为ods层的数据作为数仓中的原始数据，一定要保证其数据的准确性，避免因ods层的数据质量太差而影响其他层的数据，及时止损。

数据质量还提供了一个任务查询的界面，在此界面上，我们可以查看已配置规则的校验结果。

