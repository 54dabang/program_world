1.1　人工智能、机器学习与深度学习　2
1.1.1　人工智能　3
1.1.2　机器学习　3
1.1.3　从数据中学习表示　4
1.1.4　深度学习之“深度”　6
1.1.5　用三张图理解深度学习的工作原理　7
1.1.6　深度学习已经取得的进展　9
1.1.7　不要相信短期炒作　9
1.1.8　人工智能的未来　10
1.2　深度学习之前：机器学习简史　11
1.2.1　概率建模　11
1.2.2　早期神经网络　11
1.2.3　核方法　12
1.2.4　决策树、随机森林与梯度提升机　13
1.2.5　回到神经网络　14
1.2.6　深度学习有何不同　14
1.2.7　机器学习现状　15
1.3　为什么是深度学习，为什么是现在　15
1.3.1　硬件　16
1.3.2　数据　17
1.3.3　算法　17
1.3.4　新的投资热潮　17
1.3.5　深度学习的大众化　18
1.3.6　这种趋势会持续吗　18
第　2章 神经网络的数学基础　20
2.1　初识神经网络　20
2.2　神经网络的数据表示　23
2.2.1　标量（0D张量）　23
2.2.2　向量（1D张量）　24
2.2.3　矩阵（2D张量）　24
2.2.4　3D张量与更高维张量　24
2.2.5　关键属性　25
2.2.6　在Numpy中操作张量　26
2.2.7　数据批量的概念　27
2.2.8　现实世界中的数据张量　27
2.2.9　向量数据　27
2.2.10　时间序列数据或序列数据　28
2.2.11　图像数据　28
2.2.12　视频数据　29
2.3　神经网络的“齿轮”：张量运算　29
2.3.1　逐元素运算　30
2.3.2　广播　31
2.3.3　张量点积　32
2.3.4　张量变形　34
2.3.5　张量运算的几何解释　34
2.3.6　深度学习的几何解释　35
2.4　神经网络的“引擎”：基于梯度的优化　36
2.4.1　什么是导数　37
2.4.2　张量运算的导数：梯度　38
2.4.3　随机梯度下降　38
2.4.4　链式求导：反向传播算法　41
2.5　回顾第 一个例子　41
第3章　神经网络入门　43
3.1　神经网络剖析　43
3.1.1　层：深度学习的基础组件　44
3.1.2　模型：层构成的网络　45
3.1.3　损失函数与优化器：配置学习过程的关键　45
3.2　Keras简介　46
3.2.1　Keras、TensorFlow、Theano 和CNTK　47
3.2.2　使用Keras 开发：概述　48
3.3　建立深度学习工作站　49
3.3.1　Jupyter笔记本：运行深度学习实验的首选方法　49
3.3.2　运行Keras：两种选择　50
3.3.3　在云端运行深度学习任务：优点和缺点　50
3.3.4　深度学习的最佳GPU　50
3.4　电影评论分类：二分类问题　51
3.4.1　IMDB 数据集　51
3.4.2　准备数据　52
3.4.3　构建网络　52
3.4.4　验证你的方法　56
3.4.5　使用训练好的网络在新数据上生成预测结果　59
3.4.6　进一步的实验　59
3.4.7　小结　59
3.5　新闻分类：多分类问题　59
3.5.1　路透社数据集　60
3.5.2　准备数据　61
3.5.3　构建网络　61
3.5.4　验证你的方法　62
3.5.5　在新数据上生成预测结果　65
3.5.6　处理标签和损失的另一种方法　65
3.5.7　中间层维度足够大的重要性　65
3.5.8　进一步的实验　66
3.5.9　小结　66
3.6　预测房价：回归问题　66
3.6.1　波士顿房价数据集　67
3.6.2　准备数据　67
3.6.3　构建网络　68
3.6.4　利用K折验证来验证你的方法　68
第4章　机器学习基础　74
4.1　机器学习的四个分支　74
4.1.1　监督学习　74
4.1.2　无监督学习　75
4.1.3　自监督学习　75
4.1.4　强化学习　75
4.2　评估机器学习模型　76
4.2.1　训练集、验证集和测试集　77
4.2.2　评估模型的注意事项　80
4.3　数据预处理、特征工程和特征学习　80
4.3.1　神经网络的数据预处理　80
4.3.2　特征工程　81
4.4　过拟合与欠拟合　83
4.4.1　减小网络大小　83
4.4.2　添加权重正则化　85
4.4.3　添加dropout正则化　87
4.5　机器学习的通用工作流程　89
4.5.1　定义问题，收集数据集　89
4.5.2　选择衡量成功的指标　89
4.5.3　确定评估方法　90
4.5.4　准备数据　90
4.5.5　开发比基准更好的模型　90
4.5.6　扩大模型规模：开发过拟合的模型　91
4.5.7　模型正则化与调节超参数　92
第5章　深度学习用于计算机视觉　94
5.1　卷积神经网络简介　94
5.1.1　卷积运算　96
5.1.2　最大池化运算　101
5.2　在小型数据集上从头开始训练一个卷积神经网络　102
5.2.1　深度学习与小数据问题的相关性　103
5.2.2　下载数据　103
5.2.3　构建网络　106
5.2.4　数据预处理　107
5.2.5　使用数据增强　111
5.3　使用预训练的卷积神经网络　115
5.3.1　特征提取　116
5.3.2　微调模型　124
5.4　卷积神经网络的可视化　130
5.4.1　可视化中间激活　131
5.4.2　可视化卷积神经网络的过滤器　136
5.4.3　可视化类激活的热力图　142
第6章　深度学习用于文本和序列　147
6.1　处理文本数据　147
6.1.1　单词和字符的one-hot编码　149
6.1.2　使用词嵌入　151
6.1.3　整合在一起：从原始文本到词嵌入　155
6.2　理解循环神经网络　162
6.2.1　Keras中的循环层　164
6.2.2　理解LSTM层和GRU层　168
6.2.3　Keras中一个LSTM的具体例子　170
6.3　循环神经网络的高级用法　172
6.3.1　温度预测问题　172
6.3.2　准备数据　175
6.3.3　一种基于常识的、非机器学习的基准方法　177
6.3.4　一种基本的机器学习方法　178
6.3.5　第 一个循环网络基准　180
6.3.6　使用循环dropout来降低过拟合　181
6.3.7　循环层堆叠　182
6.3.8　使用双向RNN　184
6.4　用卷积神经网络处理序列　188
6.4.1　理解序列数据的一维卷积　188
6.4.2　序列数据的一维池化　189
6.4.3　实现一维卷积神经网络　189
6.4.4　结合CNN和RNN来处理长序列　191
第7章　高级的深度学习最佳实践　196
7.1　不用Sequential模型的解决方案：Keras 函数式API　196
7.1.1　函数式API简介　199
7.1.2　多输入模型　200
7.1.3　多输出模型　202
7.1.4　层组成的有向无环图　204
7.1.5　共享层权重　208
7.1.6　将模型作为层　208
7.2　使用Keras回调函数和TensorBoard来检查并监控深度学习模型　210
7.2.1　训练过程中将回调函数作用于模型　210
7.2.2　TensorBoard简介：TensorFlow的可视化框架　212
7.3　让模型性能发挥到极致　219
7.3.1　高级架构模式　219
7.3.2　超参数优化　222
7.3.3　模型集成　223
第8章　生成式深度学习　226
8.1　使用LSTM生成文本　227
8.1.1　生成式循环网络简史　227
8.1.2　如何生成序列数据　228
8.1.3　采样策略的重要性　229
8.1.4　实现字符级的LSTM文本生成　230
8.2　DeepDream　235
8.2.1　用Keras实现DeepDream　236
8.3　神经风格迁移　241
8.3.1　内容损失　242
8.3.2　风格损失　243
8.3.3　用Keras实现神经风格迁移　243
8.4　用变分自编码器生成图像　249
8.4.1　从图像的潜在空间中采样　249
8.4.2　图像编辑的概念向量　250
8.4.3　变分自编码器　251
8.5　生成式对抗网络简介　257
8.5.1　GAN 的简要实现流程　258
8.5.2　大量技巧　259
8.5.3　生成器　260
8.5.4　判别器　261
8.5.5　对抗网络　261
8.5.6　如何训练DCGAN　262
9.1　重点内容回顾　265
9.1.1　人工智能的各种方法　265
9.1.2　深度学习在机器学习领域中的特殊之处　266
9.1.3　如何看待深度学习　266
9.1.4　关键的推动技术　267
9.1.5　机器学习的通用工作流程　268
9.1.6　关键网络架构　268
9.1.7　可能性空间　272
9.2　深度学习的局限性　273
9.2.1　将机器学习模型拟人化的风险　273
9.2.2　局部泛化与极端泛化　275
9.2.3　小结　276
9.3　深度学习的未来　277
9.3.1　模型即程序　277
9.3.2　超越反向传播和可微层　278
9.3.3　自动化机器学习　279
9.3.4　终身学习与模块化子程序复用　279
9.3.5　长期愿景　281
9.4　了解一个快速发展领域的最新进展　281
9.4.1　使用Kaggle练习解决现实世界的问题　281
9.4.2　在arXiv阅读最新进展　282
9.4.3　探索Keras生态系统　282
附录A　在Ubuntu上安装Keras及其依赖　283
附录B　在EC2 GPU实例上运行Jupyter笔记本　287

1．1　Python是什么　1
1．2　Python的安装　2
1．2．1　Python版本　2
1．2．2　使用的外部库　2
1．2．3　Anaconda发行版　3
1．3　Python解释器　4
1．3．1　算术计算　4
1．3．2　数据类型　5
1．3．3　变量　5
1．3．4　列表　6
1．3．5　字典　7
1．3．6　布尔型　7
1．3．7　if 语句　8
1．3．8　for 语句　8
1．3．9　函数　9
1．4　Python脚本文件　9
1．4．1　保存为文件　9
1．4．2　类　10
1．5　NumPy　11
1．5．1　导入NumPy　11
1．5．2　生成NumPy数组　12
1．5．3　NumPy 的算术运算　12
1．5．4　NumPy的N维数组　13
1．5．5　广播　14
1．5．6　访问元素　15
1．6　Matplotlib　16
1．6．1　绘制简单图形　16
1．6．2　pyplot 的功能　17
1．6．3　显示图像　18
第　2 章 感知机　21
2．1　感知机是什么　21
2．2　简单逻辑电路　23
2．2．1　与门　23
2．2．2　与非门和或门　23
2．3　感知机的实现　25
2．3．1　简单的实现　25
2．3．2　导入权重和偏置　26
2．3．3　使用权重和偏置的实现　26
2．4　感知机的局限性　28
2．4．1　异或门　28
2．4．2　线性和非线性　30
2．5　多层感知机　31
2．5．1　已有门电路的组合　31
2．5．2　异或门的实现　33
2．6　从与非门到计算机　35
第3　章 神经网络　37
3．1　从感知机到神经网络　37
3．1．1　神经网络的例子　37
3．1．2　复习感知机　38
3．1．3　激活函数登场　40
3．2　激活函数　42
3．2．1　sigmoid 函数　42
3．2．2　阶跃函数的实现　43
3．2．3　阶跃函数的图形　44
3．2．4　sigmoid 函数的实现　45
3．2．5　sigmoid 函数和阶跃函数的比较　46
3．2．6　非线性函数　48
3．2．7　ReLU函数　49
3．3　多维数组的运算　50
3．3．1　多维数组　50
3．3．2　矩阵乘法　51
3．3．3　神经网络的内积　55
3．4　3 层神经网络的实现　56
3．4．1　符号确认　57
3．4．2　各层间信号传递的实现　58
3．4．3　代码实现小结　62
3．5　输出层的设计　63
3．5．1　恒等函数和softmax 函数　64
3．5．2　实现softmax 函数时的注意事项　66
3．5．3　softmax 函数的特征　67
3．5．4　输出层的神经元数量　68
3．6　手写数字识别　69
3．6．1　MNIST数据集　70
3．6．2　神经网络的推理处理　73
3．6．3　批处理　75
第4　章 神经网络的学习　81
4．1　从数据中学习　81
4．1．1　数据驱动　82
4．1．2　训练数据和测试数据　84
4．2　损失函数　85
4．2．1　均方误差　85
4．2．2　交叉熵误差　87
4．2．3　mini-batch 学习　88
4．2．4　mini-batch 版交叉熵误差的实现　91
4．2．5　为何要设定损失函数　92
4．3　数值微分　94
4．3．1　导数　94
4．3．2　数值微分的例子　96
4．3．3　偏导数　98
4．4　梯度　100
4．4．1　梯度法　102
4．4．2　神经网络的梯度　106
4．5　学习算法的实现　109
4．5．1　2 层神经网络的类　110
4．5．2　mini-batch 的实现　114
4．5．3　基于测试数据的评价　116
第5　章 误差反向传播法　121
5．1　计算图　121
5．1．1　用计算图求解　122
5．1．2　局部计算　124
5．1．3　为何用计算图解题　125
5．2　链式法则　126
5．2．1　计算图的反向传播　127
5．2．2　什么是链式法则　127
5．2．3　链式法则和计算图　129
5．3　反向传播　130
5．3．1　加法节点的反向传播　130
5．3．2　乘法节点的反向传播　132
5．3．3　苹果的例子　133
5．4　简单层的实现　135
5．4．1　乘法层的实现　135
5．4．2　加法层的实现　137
5．5　激活函数层的实现　139
5．5．1　ReLU层　139
5．5．2　Sigmoid 层　141
5．6　Affine/Softmax层的实现　144
5．6．1　Affine层　144
5．6．2　批版本的Affine层　148
5．6．3　Softmax-with-Loss 层　150
5．7　误差反向传播法的实现　154
5．7．1　神经网络学习的全貌图　154
5．7．2　对应误差反向传播法的神经网络的实现　155
5．7．3　误差反向传播法的梯度确认　158
5．7．4　使用误差反向传播法的学习　159
第6　章 与学习相关的技巧　163
6．1　参数的更新　163
6．1．1　探险家的故事　164
6．1．2　SGD　164
6．1．3　SGD的缺点　166
6．1．4　Momentum　168
6．1．5　AdaGrad　170
6．1．6　Adam　172
6．1．7　使用哪种更新方法呢　174
6．1．8　基于MNIST数据集的更新方法的比较　175
6．2　权重的初始值　176
6．2．1　可以将权重初始值设为0 吗　176
6．2．2　隐藏层的激活值的分布　177
6．2．3　ReLU的权重初始值　181
6．2．4　基于MNIST数据集的权重初始值的比较　183
6．3　Batch Normalization　184
6．3．1　Batch Normalization 的算法　184
6．3．2　Batch Normalization 的评估　186
6．4　正则化　188
6．4．1　过拟合　189
6．4．2　权值衰减　191
6．4．3　Dropout　192
6．5　超参数的验证　195
6．5．1　验证数据　195
6．5．2　超参数的最优化　196
6．5．3　超参数最优化的实现　198
第7　章 卷积神经网络　201
7．1　整体结构　201
7．2　卷积层　202
7．2．1　全连接层存在的问题　203
7．2．2　卷积运算　203
7．2．3　填充　206
7．2．4　步幅　207
7．2．5　3 维数据的卷积运算　209
7．2．6　结合方块思考　211
7．2．7　批处理　213
7．3　池化层　214
7．4　卷积层和池化层的实现　216
7．4．1　4 维数组　216
7．4．2　基于im2col 的展开　217
7．4．3　卷积层的实现　219
7．4．4　池化层的实现　222
7．5　CNN的实现　224
7．6　CNN的可视化　228
7．6．1　第 1 层权重的可视化　228
7．6．2　基于分层结构的信息提取　230
7．7　具有代表性的CNN　231
7．7．1　LeNet　231
7．7．2　AlexNet　232
8．1　加深网络　235
8．1．1　向更深的网络出发　235
8．1．2　进一步提高识别精度　238
8．1．3　加深层的动机　240
8．2　深度学习的小历史　242
8．2．1　ImageNet　243
8．2．2　VGG　244
8．2．3　GoogLeNet　245
8．2．4　ResNet　246
8．3　深度学习的高速化　248
8．3．1　需要努力解决的问题　248
8．3．2　基于GPU的高速化　249
8．3．3　分布式学习　250
8．3．4　运算精度的位数缩减　252
8．4　深度学习的应用案例　253
8．4．1　物体检测　253
8．4．2　图像分割　255
8．4．3　图像标题的生成　256
8．5　深度学习的未来　258
8．5．1　图像风格变换　258
8．5．2　图像的生成　259
8．5．3　自动驾驶　261
8．5．4　Deep Q-Network（强化学习）　262
附录A　Softmax-with-Loss 层的计算图　267
A．1　正向传播　268
A．2　反向传播　270

第　1章 什么是深度学习　2
1.1　人工智能、机器学习与深度学习　2
1.1.1　人工智能　3
1.1.2　机器学习　3
1.1.3　从数据中学习表示　4
1.1.4　深度学习之“深度”　6
1.1.5　用三张图理解深度学习的工作原理　7
1.1.6　深度学习已经取得的进展　9
1.1.7　不要相信短期炒作　9
1.1.8　人工智能的未来　10
1.2　深度学习之前：机器学习简史　11
1.2.1　概率建模　11
1.2.2　早期神经网络　11
1.2.3　核方法　12
1.2.4　决策树、随机森林与梯度提升机　13
1.2.5　回到神经网络　14
1.2.6　深度学习有何不同　14
1.2.7　机器学习现状　15
1.3　为什么是深度学习，为什么是现在　15
1.3.1　硬件　16
1.3.2　数据　17
1.3.3　算法　17
1.3.4　新的投资热潮　17
1.3.5　深度学习的大众化　18
1.3.6　这种趋势会持续吗　18
第　2章 神经网络的数学基础　20
2.1　初识神经网络　20
2.2　神经网络的数据表示　23
2.2.1　标量（0D张量）　23
2.2.2　向量（1D张量）　24
2.2.3　矩阵（2D张量）　24
2.2.4　3D张量与更高维张量　24
2.2.5　关键属性　25
2.2.6　在Numpy中操作张量　26
2.2.7　数据批量的概念　27
2.2.8　现实世界中的数据张量　27
2.2.9　向量数据　27
2.2.10　时间序列数据或序列数据　28
2.2.11　图像数据　28
2.2.12　视频数据　29
2.3　神经网络的“齿轮”：张量运算　29
2.3.1　逐元素运算　30
2.3.2　广播　31
2.3.3　张量点积　32
2.3.4　张量变形　34
2.3.5　张量运算的几何解释　34
2.3.6　深度学习的几何解释　35
2.4　神经网络的“引擎”：基于梯度的优化　36
2.4.1　什么是导数　37
2.4.2　张量运算的导数：梯度　38
2.4.3　随机梯度下降　38
2.4.4　链式求导：反向传播算法　41
2.5　回顾第 一个例子　41
本章小结　42
第3章　神经网络入门　43
3.1　神经网络剖析　43
3.1.1　层：深度学习的基础组件　44
3.1.2　模型：层构成的网络　45
3.1.3　损失函数与优化器：配置学习过程的关键　45
3.2　Keras简介　46
3.2.1　Keras、TensorFlow、Theano 和CNTK　47
3.2.2　使用Keras 开发：概述　48
3.3　建立深度学习工作站　49
3.3.1　Jupyter笔记本：运行深度学习实验的首选方法　49
3.3.2　运行Keras：两种选择　50
3.3.3　在云端运行深度学习任务：优点和缺点　50
3.3.4　深度学习的最佳GPU　50
3.4　电影评论分类：二分类问题　51
3.4.1　IMDB 数据集　51
3.4.2　准备数据　52
3.4.3　构建网络　52
3.4.4　验证你的方法　56
3.4.5　使用训练好的网络在新数据上生成预测结果　59
3.4.6　进一步的实验　59
3.4.7　小结　59
3.5　新闻分类：多分类问题　59
3.5.1　路透社数据集　60
3.5.2　准备数据　61
3.5.3　构建网络　61
3.5.4　验证你的方法　62
3.5.5　在新数据上生成预测结果　65
3.5.6　处理标签和损失的另一种方法　65
3.5.7　中间层维度足够大的重要性　65
3.5.8　进一步的实验　66
3.5.9　小结　66
3.6　预测房价：回归问题　66
3.6.1　波士顿房价数据集　67
3.6.2　准备数据　67
3.6.3　构建网络　68
3.6.4　利用K折验证来验证你的方法　68
3.6.5　小结　72
本章小结　73
第4章　机器学习基础　74
4.1　机器学习的四个分支　74
4.1.1　监督学习　74
4.1.2　无监督学习　75
4.1.3　自监督学习　75
4.1.4　强化学习　75
4.2　评估机器学习模型　76
4.2.1　训练集、验证集和测试集　77
4.2.2　评估模型的注意事项　80
4.3　数据预处理、特征工程和特征学习　80
4.3.1　神经网络的数据预处理　80
4.3.2　特征工程　81
4.4　过拟合与欠拟合　83
4.4.1　减小网络大小　83
4.4.2　添加权重正则化　85
4.4.3　添加dropout正则化　87
4.5　机器学习的通用工作流程　89
4.5.1　定义问题，收集数据集　89
4.5.2　选择衡量成功的指标　89
4.5.3　确定评估方法　90
4.5.4　准备数据　90
4.5.5　开发比基准更好的模型　90
4.5.6　扩大模型规模：开发过拟合的模型　91
4.5.7　模型正则化与调节超参数　92
本章小结　92
第二部分　深度学习实践
第5章　深度学习用于计算机视觉　94
5.1　卷积神经网络简介　94
5.1.1　卷积运算　96
5.1.2　最大池化运算　101
5.2　在小型数据集上从头开始训练一个卷积神经网络　102
5.2.1　深度学习与小数据问题的相关性　103
5.2.2　下载数据　103
5.2.3　构建网络　106
5.2.4　数据预处理　107
5.2.5　使用数据增强　111
5.3　使用预训练的卷积神经网络　115
5.3.1　特征提取　116
5.3.2　微调模型　124
5.3.3　小结　130
5.4　卷积神经网络的可视化　130
5.4.1　可视化中间激活　131
5.4.2　可视化卷积神经网络的过滤器　136
5.4.3　可视化类激活的热力图　142
本章小结　146
第6章　深度学习用于文本和序列　147
6.1　处理文本数据　147
6.1.1　单词和字符的one-hot编码　149
6.1.2　使用词嵌入　151
6.1.3　整合在一起：从原始文本到词嵌入　155
6.1.4　小结　162
6.2　理解循环神经网络　162
6.2.1　Keras中的循环层　164
6.2.2　理解LSTM层和GRU层　168
6.2.3　Keras中一个LSTM的具体例子　170
6.2.4　小结　172
6.3　循环神经网络的高级用法　172
6.3.1　温度预测问题　172
6.3.2　准备数据　175
6.3.3　一种基于常识的、非机器学习的基准方法　177
6.3.4　一种基本的机器学习方法　178
6.3.5　第 一个循环网络基准　180
6.3.6　使用循环dropout来降低过拟合　181
6.3.7　循环层堆叠　182
6.3.8　使用双向RNN　184
6.3.9　更多尝试　187
6.3.10　小结　187
6.4　用卷积神经网络处理序列　188
6.4.1　理解序列数据的一维卷积　188
6.4.2　序列数据的一维池化　189
6.4.3　实现一维卷积神经网络　189
6.4.4　结合CNN和RNN来处理长序列　191
6.4.5　小结　195
本章总结　195
第7章　高级的深度学习最佳实践　196
7.1　不用Sequential模型的解决方案：Keras 函数式API　196
7.1.1　函数式API简介　199
7.1.2　多输入模型　200
7.1.3　多输出模型　202
7.1.4　层组成的有向无环图　204
7.1.5　共享层权重　208
7.1.6　将模型作为层　208
7.1.7　小结　209
7.2　使用Keras回调函数和TensorBoard来检查并监控深度学习模型　210
7.2.1　训练过程中将回调函数作用于模型　210
7.2.2　TensorBoard简介：TensorFlow的可视化框架　212
7.2.3　小结　219
7.3　让模型性能发挥到极致　219
7.3.1　高级架构模式　219
7.3.2　超参数优化　222
7.3.3　模型集成　223
7.3.4　小结　224
本章总结　225
第8章　生成式深度学习　226
8.1　使用LSTM生成文本　227
8.1.1　生成式循环网络简史　227
8.1.2　如何生成序列数据　228
8.1.3　采样策略的重要性　229
8.1.4　实现字符级的LSTM文本生成　230
8.1.5　小结　234
8.2　DeepDream　235
8.2.1　用Keras实现DeepDream　236
8.2.2　小结　241
8.3　神经风格迁移　241
8.3.1　内容损失　242
8.3.2　风格损失　243
8.3.3　用Keras实现神经风格迁移　243
8.3.4　小结　249
8.4　用变分自编码器生成图像　249
8.4.1　从图像的潜在空间中采样　249
8.4.2　图像编辑的概念向量　250
8.4.3　变分自编码器　251
8.4.4　小结　256
8.5　生成式对抗网络简介　257
8.5.1　GAN 的简要实现流程　258
8.5.2　大量技巧　259
8.5.3　生成器　260
8.5.4　判别器　261
8.5.5　对抗网络　261
8.5.6　如何训练DCGAN　262
8.5.7　小结　264
本章总结　264
第9章　总结　265
9.1　重点内容回顾　265
9.1.1　人工智能的各种方法　265
9.1.2　深度学习在机器学习领域中的特殊之处　266
9.1.3　如何看待深度学习　266
9.1.4　关键的推动技术　267
9.1.5　机器学习的通用工作流程　268
9.1.6　关键网络架构　268
9.1.7　可能性空间　272
9.2　深度学习的局限性　273
9.2.1　将机器学习模型拟人化的风险　273
9.2.2　局部泛化与极端泛化　275
9.2.3　小结　276
9.3　深度学习的未来　277
9.3.1　模型即程序　277
9.3.2　超越反向传播和可微层　278
9.3.3　自动化机器学习　279
9.3.4　终身学习与模块化子程序复用　279
9.3.5　长期愿景　281
9.4　了解一个快速发展领域的最新进展　281
9.4.1　使用Kaggle练习解决现实世界的问题　281
9.4.2　在arXiv阅读最新进展　282
9.4.3　探索Keras生态系统　282
9.5　结束语　282
附录A　在Ubuntu上安装Keras及其依赖　283
附录B　在EC2 GPU实例上运行Jupyter笔记本　287

