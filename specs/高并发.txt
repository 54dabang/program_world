第1章　并发编程的挑战 1
1.1　上下文切换 1
1.1.1　多线程一定快吗 1
1.1.2　测试上下文切换次数和时长 3
1.1.3　如何减少上下文切换 3
1.1.4　减少上下文切换实战 4
1.2　死锁 5
1.3　资源限制的挑战 6
1.4　本章小结 7

第2章　Java并发机制的底层实现原理 8
2.1　volatile的应用 8
2.2　synchronized的实现原理与应用 11
2.2.1　Java对象头 12
2.2.2　锁的升级与对比 13
2.3　原子操作的实现原理 16
2.4　本章小结 20

第3章　Java内存模型 21
3.1　Java内存模型的基础 21
3.1.1　并发编程模型的两个关键问题 21
3.1.2　Java内存模型的抽象结构 22
3.1.3　从源代码到指令序列的重排序 23
3.1.4　并发编程模型的分类 24
3.1.5　happens-before简介 26

3.2　重排序 27
3.2.1　数据依赖性 28
3.2.2　as-if-serial语义 28
3.2.3　程序顺序规则 29
3.2.4　重排序对多线程的影响 29
3.3　顺序一致性 31
3.3.1　数据竞争与顺序一致性 31
3.3.2　顺序一致性内存模型 32
3.3.3　同步程序的顺序一致性效果 34
3.3.4　未同步程序的执行特性 35
3.4　volatile的内存语义 38
3.4.1　volatile的特性 38
3.4.2　volatile写-读建立的happens-before关系 39
3.4.3　volatile写-读的内存语义 40
3.4.4　volatile内存语义的实现 42
3.4.5　JSR-133为什么要增强volatile的内存语义 46
3.5　锁的内存语义 47
3.5.1　锁的释放-获取建立的
　　　happens-before关系 47
3.5.2　锁的释放和获取的内存语义 48
3.5.3　锁内存语义的实现 50
3.5.4　concurrent包的实现 54
3.6　final域的内存语义 55
3.6.1　final域的重排序规则 55
3.6.2　写final域的重排序规则 56
3.6.3　读final域的重排序规则 57
3.6.4　final域为引用类型 58
3.6.5　为什么final引用不能从构造函数内“溢出” 59
3.6.6　final语义在处理器中的实现 61
3.6.7　JSR-133为什么要增强f?inal的语义 62
3.7　happens-before 62
3.7.1　JMM的设计 62
3.7.2　happens-before的定义 64
3.7.3　happens-before规则 65
3.8　双重检查锁定与延迟初始化 67
3.8.1　双重检查锁定的由来 67
3.8.2　问题的根源 69
3.8.3　基于volatile的解决方案 71
3.8.4　基于类初始化的解决方案 72
3.9　Java内存模型综述 78
3.9.1　处理器的内存模型 78
3.9.2　各种内存模型之间的关系 80
3.9.3　JMM的内存可见性保证 80
3.9.4　JSR-133对旧内存模型的修补 81
3.10　本章小结 82
第4章　Java并发编程基础 83
4.1　线程简介 83
4.1.1　什么是线程 83
4.1.2　为什么要使用多线程 84
4.1.3　线程优先级 85
4.1.4　线程的状态 87
4.1.5　Daemon线程 90
4.2　启动和终止线程 91
4.2.1　构造线程 91
4.2.2　启动线程 92
4.2.3　理解中断 92
4.2.4　过期的suspend()、resume()和stop() 93
4.2.5　安全地终止线程 95
4.3　线程间通信 96
4.3.1　volatile和synchronized关键字 96
4.3.2　等待/通知机制 98
4.3.3　等待/通知的经典范式 101
4.3.4　管道输入/输出流 102
4.3.5　Thread.join()的使用 103
4.3.6　ThreadLocal的使用 105
4.4　线程应用实例 106
4.4.1　等待超时模式 106
4.4.2　一个简单的数据库连接池示例 106
4.4.3　线程池技术及其示例 110
4.4.4　一个基于线程池技术的简单Web服务器 114

第5章　Java中的锁 119
5.1　Lock接口 119
5.2　队列同步器 121
5.2.1　队列同步器的接口与示例 121
5.2.2　队列同步器的实现分析 124
5.3　重入锁 136
5.4　读写锁 140
5.4.1　读写锁的接口与示例 141
5.4.2　读写锁的实现分析 142
5.5　LockSupport工具 146
5.6　Condition接口 147
5.6.1　Condition接口与示例 148
5.6.2　Condition的实现分析 150
5.7　本章小结 154
第6章　Java并发容器和框架 155
6.1　ConcurrentHashMap的实现原理与使用 155
6.1.1　为什么要使用ConcurrentHashMap 155
6.1.2　ConcurrentHashMap的结构 156
6.1.3　ConcurrentHashMap的初始化 157
6.1.4　定位Segment 159
6.1.5　ConcurrentHashMap的操作 160
6.2　ConcurrentLinkedQueue 161
6.2.1　ConcurrentLinkedQueue的结构 162
6.2.2　入队列 162
6.2.3　出队列 165
6.3　Java中的阻塞队列 167
6.3.1　什么是阻塞队列 167
6.3.2　Java里的阻塞队列 168
6.3.3　阻塞队列的实现原理 172
6.4　Fork/Join框架 175
6.4.1　什么是Fork/Join框架 175
6.4.2　工作窃取算法 176
6.4.3　Fork/Join框架的设计 177
6.4.4　使用Fork/Join框架 177
6.4.5　Fork/Join框架的异常处理 179
6.4.6　Fork/Join框架的实现原理 179
6.5　本章小结 181
第7章　Java中的13个原子操作类 182
7.1　原子更新基本类型类 182
7.2　原子更新数组 184
7.3　原子更新引用类型 185
7.4　原子更新字段类 187
7.5　本章小结 188
第8章　Java中的并发工具类 189
8.1　等待多线程完成的CountDownLatch 189
8.2　同步屏障CyclicBarrier 191
8.2.1　CyclicBarrier简介 191
8.2.2　CyclicBarrier的应用场景 193
8.2.3　CyclicBarrier和CountDownLatch的区别 195
8.3　控制并发线程数的Semaphore 196
8.4　线程间交换数据的Exchanger 198
8.5　本章小结 199
第9章　Java中的线程池 200
9.1　线程池的实现原理 200
9.2　线程池的使用 203
9.2.1　线程池的创建 203
9.2.2　向线程池提交任务 205
9.2.3　关闭线程池 205
9.2.4　合理地配置线程池 206
9.2.5　线程池的监控 206
9.3　本章小结 207
第10章　Executor框架 208
10.1　Executor框架简介 208
10.1.1　Executor框架的两级调度模型 208
10.1.2　Executor框架的结构与成员 208
10.2　ThreadPoolExecutor详解 213
10.2.1　FixedThreadPool详解 213
10.2.2　SingleThreadExecutor详解 214
10.2.3　CachedThreadPool详解 215
10.3　ScheduledThreadPoolExecutor详解 217
10.3.1　ScheduledThreadPoolExecutor的运行机制 217
10.3.2　ScheduledThreadPoolExecutor的实现 218
10.4　FutureTask详解 221
10.4.1　FutureTask简介 222
10.4.2　FutureTask的使用 222
10.4.3　FutureTask的实现 224

第11章　Java并发编程实践 228
11.1　生产者和消费者模式 228
11.1.1　生产者消费者模式实战 229
11.1.2　多生产者和多消费者场景 231
11.1.3　线程池与生产消费者模式 234
11.2　线上问题定位 234
11.3　性能测试 236
11.4　异步任务池 238

1.1　并发简史
1.2　线程的优势
1.2.1　发挥多处理器的强大能力
1.2.2　建模的简单性
1.2.3　异步事件的简化处理
1.2.4　响应更灵敏的用户界面
1.3　线程带来的风险
1.3.1　安全性问题
1.3.2　活跃性问题
1.3.3　性能问题
1.4　线程无处不在

第一部分　基础知识
第2章　线程安全性
2.1　什么是线程安全性
2.2　原子性
2.2.1　竞态条件
2.2.2　示例：延迟初始化中的竞态条件
2.2.3　复合操作
2.3　加锁机制
2.3.1　内置锁
2.3.2　重入
2.4　用锁来保护状态
2.5　活跃性与性能
第3章　对象的共享
3.1　可见性
3.1.1　失效数据
3.1.2　非原子的64位操作
3.1.3　加锁与可见性
3.1.4　Volatile变量
3.2　发布与逸出
3.3　线程封闭
3.3.1　Ad-hoc线程封闭
3.3.2　栈封闭
3.3.3　ThreadLocal类
3.4　不变性
3.4.1　Final域
3.4.2　示例：使用Volatile类型来发布不可变对象
3.5　安全发布
3.5.1　不正确的发布：正确的对象被破坏
3.5.2 　不可变对象与初始化安全性
3.5.3　安全发布的常用模式
3.5.4　事实不可变对象
3.5.5　可变对象
3.5.6　安全地共享对象
第4章　对象的组合
4.1　设计线程安全的类
4.1.1　收集同步需求
4.1.2　依赖状态的操作
4.1.3　状态的所有权
4.2　实例封闭
4.2.1　Java监视器模式
4.2.2　示例：车辆追踪
4.3　线程安全性的委托
4.3.1　示例：基于委托的车辆追踪器
4.3.2　独立的状态变量
4.3.3　当委托失效时
4.3.4　发布底层的状态变量
4.3.5　示例：发布状态的车辆追踪器
4.4　在现有的线程安全类中添加功能
4.4.1　客户端加锁机制
4.4.2　组合
4.5　将同步策略文档化
第5章　基础构建模块
5.1　同步容器类
5.1.1　同步容器类的问题
5.1.2　迭代器与Concurrent-ModificationException
5.1.3　隐藏迭代器
5.2　并发容器
5.2.1　ConcurrentHashMap
5.2.2　额外的原子Map操作
5.2.3　CopyOnWriteArrayList
5.3　阻塞队列和生产者-消费者模式
5.3.1　示例：桌面搜索
5.3.2　串行线程封闭
5.3.3　双端队列与工作密取
5.4　阻塞方法与中断方法
5.5　同步工具类
5.5.1　闭锁
5.5.2　FutureTask
5.5.3　信号量
5.5.4　栅栏
5.6　构建高效且可伸缩的结果缓存
第二部分　结构化并发应用程序
第6章　任务执行
6.1　在线程中执行任务
6.1.1　串行地执行任务
6.1.2　显式地为任务创建线程
6.1.3　无限制创建线程的不足
6.2　Executor框架
6.2.1　示例：基于Executor的Web服务器
6.2.2　执行策略
6.2.3　线程池
6.2.4　Executor的生命周期
6.2.5　延迟任务与周期任务
6.3　找出可利用的并行性
6.3.1　示例：串行的页面渲染器
6.3.2　携带结果的任务Callable与Future
6.3.3　示例：使用Future实现页面渲染器
6.3.4　在异构任务并行化中存在的局限
6.3.5　CompletionService:Executor与BlockingQueue
6.3.6　示例：使用CompletionService实现页面渲染器
6.3.7　为任务设置时限
6.3.8　示例：旅行预定门户网站
第7章　取消与关闭
第8章　线程池的使用
第9章　图形用户界面应用程序
第三部分　活跃性、性能与测试
第10章　避免活跃性危险
第11章　性能与可伸缩性
第12章　并发程序的测试
第四部分　高级主题
第13章　显式锁
第14章　构建自定义的同步工具
第15章　原子变量与非阻塞同步机制
第16章　Java内存模型
附录A　并发性标注

第1章：快速认识线程 22
1.1 线程的介绍 22
1.2 快速创建并启动一个线程 22
1.2.1 尝试并行运行 23
1.2.2 并发运行交替输出 24
1.2.3 使用Jconsole观察线程 25
1.3 线程的生命周期详解 26
1.3.1 线程的NEW状态 27
1.3.2 线程的RUNNABLE状态 28
1.3.3 线程的 RUNNING状态 28
1.3.4 线程的BLOCKED状态 29
1.3.5 线程的TERMINATED状态 29
1.4 线程的start方法剖析--模板设计模式在Thread中的应用 30
1.4.1 Thread start方法源码分析以及注意事项 30
1.4.2 模板设计模式在Thread中的应用 33
1.4.3 Thread模拟营业大厅叫号机程序 34
1.5 Runnable接口的引入以及策略模式在Thread中的使用 39
1.5.1 Runnable的职责 39
1.5.2 策略模式在Thread中的应用 40
1.5.3 模拟营业大厅叫号机程序 42
1.6 本章总结 43
第2章：深入理解Thread构造函数 45
2.1 线程的命名 45
2.1.1 线程的默认命名 45
2.1.2 命名线程 46
2.1.3 修改线程的名字 47
2.2 线程的父子关系 48
2.3 Thread与ThreadGroup 48
2.4 Thread与Runnable 50
2.5 Thread与JVM虚拟机栈 50
2.5.1 Thread与Stacksize 51
2.5.2 JVM内存结构 53
2.5.3 Thread与虚拟机栈 58
2.6 守护线程 62
2.6.1 什么是守护线程 62
2.6.2 守护线程的作用 64
2.7 本章总结 64
第3章：Thread API的详细介绍 66
3.1 线程sleep 66
3.1.1 sleep方法介绍 66
3.1.2 使用TimeUnit替代Thread.sleep 67
3.2 线程yield 68
3.2.1 yield方法介绍 68
3.2.2 yield vs sleep 69
3.3 设置线程的优先级 69
3.3.1 线程优先级介绍 70
3.3.2 线程优先级源码分析 71
3.3.3 关于优先级的一些总结 72
3.4 获取线程ID 73
3.5 获取当前线程 73
3.6 设置线程上下文类加载器 74
3.7 线程interrupt 75
3.7.1 interrupt 75
3.7.2 isInterrupted 77
3.7.3 interrupted 79
3.7.4 interrupt注意事项 81
3.8 线程join 82
3.8.1 线程join方法详解 83
3.8.2 join方法结合实战 85
3.9 如何关闭一个线程 90
3.9.1 正常关闭 90
3.9.2 异常退出 94
3.9.3 进程假死 94
3.10 本章总结 95
第4章：线程安全与数据同步 97
4.1 数据同步 97
4.1.1 数据不一致问题的引入 97
4.1.2 数据不一致问题原因分析 99
4.2 初识 synchronized关键字 101
4.2.1 什么是synchronized 102
4.2.2 synchronized关键字的用法 103
4.3 深入synchronized关键字 105
4.3.1 线程堆栈分析 105
4.3.2 JVM指令分析 108
4.3.3 使用synchronized需要注意的问题 112
4.4 This Monitor和Class Monitor的详细介绍 114
4.4.1 this monitor 114
4.4.2 class monitor 117

4.5 程序死锁的原因以及如何诊断 120
4.5.1 程序死锁 120
4.5.2 程序死锁举例 121
4.5.3 死锁诊断 124
4.6 本章总结 126

第5章：线程间通信 127
5.1 同步阻塞与异步非阻塞 127
5.1.1 同步阻塞消息处理 127
5.1.2 异步非阻塞消息处理 128
5.2 单线程间通信 129
5.2.1 初识wait和notify 129
5.2.2 wait和notify方法详解 133
5.2.3 wait和notify注意事项 135
5.2.4 wait vs sleep 137
5.3 多线程间通信 137
5.3.1 生产者消费者 137
5.3.2 线程休息室wait set 140
5.4 自定义显式锁BooleanLock 141
5.4.1 synchronized关键字的缺陷 142
5.4.2 显式锁BooleanLock 143
5.5 本章总结 153
第6章：ThreadGroup详细讲解 155
6.1 ThreadGroup与Thread 155
6.2 创建Thread Group 155
6.3 拷贝Thread数组和ThreadGroup数组 157
6.3.1 拷贝Thread数组 157
6.3.2 拷贝ThreadGroup数组 159
6.4 ThreadGroup操作 160
6.4.1 ThreadGroup的基本操作 161
6.4.2 ThreadGroup的interrupt 164
6.4.3 ThreadGroup的destroy 166
6.4.4 守护ThreadGroup 168
6.5 本章总结 169
第7章：Hook线程以及捕获线程执行异常 170
7.1 获取线程运行时异常 170
7.1.1 UncaughtExceptionHandler介绍 170
7.1.2 UncaughtExceptionHandler实例 171
7.1.3 UncaughtExceptionHandler源码分析 173
7.2 注入钩子线程（Hook） 175
7.2.1 Hook线程介绍 175
7.2.2 Hook线程实战 177
7.2.3 Hook线程应用场景以及注意事项 179
7.3 本章总结 179
第8章：线程池原理以及自定义线程池 180
8.1 线程池原理 180
8.2 线程池实现 181
8.2.1 线程池接口定义 182
8.2.2 线程池详细实现 188
8.3 线程池应用 198
8.4 本章总结 202
第二部分：Java ClassLoader 204
第9章 类的加载过程 205
9.1 类的加载过程介绍 205
9.2 类的主动使用和被动使用 206
9.3 类加载过程详解 209
9.3.1 类的加载阶段 210
9.3.2 类的连接阶段 212
9.3.3 类的初始化阶段 219
9.4 本章总结 221
第10章 JVM类加载器 224
10.1 JVM内置三大类加载器 224
10.1.1 根类加载器介绍 225
10.1.2 扩展类加载器介绍 226
10.1.3 系统类加载器介绍 227
10.2 自定义类加载器 227
10.2.1 自定义类加载器，问候世界 228
10.2.2 双亲委托机制详细介绍 233
10.2.3 破坏双亲委托机制 236
10.2.4 类加载器命名空间，运行时包，类的卸载等 239
10.3 本章总结 246
第11章 线程上下文类加载器 249
11.1 为什么需要线程上下文类加载器 249
11.2 数据库驱动的初始化源码分析 250
第三部分 深入理解volatile关键字 254
第12章 volatile关键字的介绍 255
12.1 初识volatile关键字 255
12.2 机器硬件CPU 257
12.3 Java 内存模型 262
第13章 深入volatile关键字 265
13.1 并发编程的三个重要特性 265
13.1.1 原子性 265
13.1.2 可见性 266
13.1.3 有序性 266
13.2 JMM如何保证三大特性 268
13.2.1 JMM与原子性 269
13.2.2 JMM与可见性 271
13.2.3 JMM与有序性 272
13.3 volatile关键字深入解析 273
13.3.1 volatile关键字的语义 274
13.3.2 volatile的原理和实现机制 277
13.3.3 volatile的使用场景 278

第1 章 并发编程线程基础 2

1.1 什么是线程 2

1.2 线程创建与运行 3

1.3 线程通知与等待 6

1.4 等待线程执行终止的join 方法 16

1.5 让线程睡眠的sleep 方法 19

1.6 让出CPU 执行权的yield 方法 23

1.7 线程中断 24

1.8 理解线程上下文切换 30

1.9 线程死锁 30

1.9.1 什么是线程死锁 30

1.9.2 如何避免线程死锁 33

1.10 守护线程与用户线程 35

1.11 ThreadLocal 39

1.11.1 ThreadLocal 使用示例 40

1.11.2 ThreadLocal 的实现原理 42

1.11.3 ThreadLocal 不支持继承性 45

1.11.4 InheritableThreadLocal 类 46

第2 章 并发编程的其他基础知识 50

2.1 什么是多线程并发编程 50

2.2 为什么要进行多线程并发编程 51

2.3 Java 中的线程安全问题 51

?2.4 Java 中共享变量的内存可见性问题 52

2.5 Java 中的synchronized 关键字 54

2.5.1 synchronized 关键字介绍 54

2.5.2 synchronized 的内存语义 55

2.6 Java 中的volatile 关键字 55

2.7 Java 中的原子性操作 57

2.8 Java 中的CAS 操作 59

2.9 Unsafe 类 59

2.9.1 Unsafe 类中的重要方法 59

2.9.2 如何使用Unsafe 类 61

2.10 Java 指令重排序 65

2.11 伪共享 67

2.11.1 什么是伪共享 67

2.11.2 为何会出现伪共享 68

2.11.3 如何避免伪共享 70

2.11.4 小结 72

2.12 锁的概述 72

2.12.1 乐观锁与悲观锁 72

2.12.2 公平锁与非公平锁 75

2.12.3 独占锁与共享锁 75

2.12.4 什么是可重入锁 76

2.12.5 自旋锁 77

2.13 总结 77

第二部分 Java 并发编程高级篇

第3 章 Java 并发包中ThreadLocalRandom 类原理剖析 80

3.1 Random 类及其局限性 80

3.2 ThreadLocalRandom 82

3.3 源码分析 84

3.4 总结 87

第4 章 Java 并发包中原子操作类原理剖析 88

4.1 原子变量操作类 88

4.2 JDK 8 新增的原子操作类LongAdder 93

4.2.1 LongAdder 简单介绍 93

4.2.2 LongAdder 代码分析 95

4.2.3 小结 101

4.3 LongAccumulator 类原理探究 102

4.4 总结 104

第5 章 Java 并发包中并发List 源码剖析 105

5.1 介绍 105

5.2 主要方法源码解析 106

5.2.1 初始化 106

5.2.2 添加元素 106

5.2.3 获取指定位置元素 108

5.2.4 修改指定元素 109

5.2.5 删除元素 110

5.2.6 弱一致性的迭代器 111

5.3 总结 114

第6 章 Java 并发包中锁原理剖析 115

6.1 LockSupport 工具类 115

6.2 抽象同步队列AQS 概述 122

6.2.1 AQS——锁的底层支持 122

6.2.2 AQS——条件变量的支持 128

6.2.3 基于AQS 实现自定义同步器 131

6.3 独占锁ReentrantLock 的原理 136

6.3.1 类图结构 136

6.3.2 获取锁 137

6.3.3 释放锁 142

6.3.4 案例介绍 143

6.3.5 小结 145

?6.4 读写锁ReentrantReadWriteLock 的原理 145

6.4.1 类图结构 145

6.4.2 写锁的获取与释放 147

6.4.3 读锁的获取与释放 151

6.4.4 案例介绍 156

6.4.5 小结 158

6.5 JDK 8 中新增的StampedLock 锁探究 158

6.5.1 概述 158

6.5.2 案例介绍 160

6.5.3 小结 164

第7 章 Java 并发包中并发队列原理剖析 165

7.1 ConcurrentLinkedQueue 原理探究 165

7.1.1 类图结构 165

7.1.2 ConcurrentLinkedQueue 原理介绍 166

7.1.3 小结 181

7.2 LinkedBlockingQueue 原理探究 182

7.2.1 类图结构 182

7.2.2 LinkedBlockingQueue 原理介绍 185

7.2.3 小结 194

7.3 ArrayBlockingQueue 原理探究 195

7.3.1 类图结构 195

7.3.2 ArrayBlockingQueue 原理介绍 197

7.3.3 小结 202

7.4 PriorityBlockingQueue 原理探究 203

7.4.1 介绍 203

7.4.2 PriorityBlockingQueue 类图结构 203

7.4.3 原理介绍 205

7.4.4 案例介绍 214

7.4.5 小结 216

7.5 DelayQueue 原理探究 217

7.5.1 DelayQueue 类图结构 217

7.5.2 主要函数原理讲解 219

7.5.3 案例介绍 222

7.5.4 小结 224

第8 章 Java 并发包中线程池ThreadPoolExecutor 原理探究 225

8.1 介绍 225

8.2 类图介绍 225

8.3 源码分析 230

8.3.1 public void execute(Runnable command) 230

8.3.2 工作线程Worker 的执行 235

8.3.3 shutdown 操作 238

8.3.4 shutdownNow 操作 240

8.3.5 awaitTermination 操作 241

8.4 总结 242

第9 章 Java 并发包中ScheduledThreadPoolExecutor 原理探究 243

9.1 介绍 243

9.2 类图介绍 243

9.3 原理剖析 245

9.3.1 schedule(Runnable command, long delay,TimeUnit unit) 方法 246

9.3.2 scheduleWithFixedDelay(Runnable command,long initialDelay, long delay,TimeUnit unit) 方法 252

9.3.3 scheduleAtFixedRate(Runnable command,long initialDelay,long period,TimeUnit unit) 方法 254

9.4 总结 255

第10 章 Java 并发包中线程同步器原理剖析 256

10.1 CountDownLatch 原理剖析 256

10.1.1 案例介绍 256

10.1.2 实现原理探究 259

10.1.3 小结 263

10.2 回环屏障CyclicBarrier 原理探究 264

10.2.1 案例介绍 264

10.2.2 实现原理探究 268

10.2.3 小结 272

?10.3 信号量Semaphore 原理探究 272

10.3.1 案例介绍 272

10.3.2 实现原理探究 276

10.3.3 小结 281

10.4 总结 281

第三部分 Java 并发编程实践篇

第11 章 并发编程实践 284

11.1 ArrayBlockingQueue 的使用 284

11.1.1 异步日志打印模型概述 284

11.1.2 异步日志与具体实现 285

11.1.3 小结 293

11.2 Tomcat 的NioEndPoint 中ConcurrentLinkedQueue 的使用 293

11.2.1 生产者——Acceptor 线程 294

11.2.2 消费者——Poller 线程 298

11.2.3 小结 300

11.3 并发组件ConcurrentHashMap 使用注意事项 300

11.4 SimpleDateFormat 是线程不安全的 304

11.4.1 问题复现 304

11.4.2 问题分析 305

11.4.3 小结 309

11.5 使用Timer 时需要注意的事情 309

11.5.1 问题的产生 309

11.5.2 Timer 实现原理分析 310

11.5.3 小结 313

11.6 对需要复用但是会被下游修改的参数要进行深复制 314

11.6.1 问题的产生 314

11.6.2 问题分析 316

11.6.3 小结 318

11.7 创建线程和线程池时要指定与业务相关的名称 319

11.7.1 创建线程需要有线程名 319

11.7.2 创建线程池时也需要指定线程池的名称 321

11.7.3 小结 325

11.8 使用线程池的情况下当程序结束时记得调用shutdown 关闭线程池 325

11.8.1 问题复现 325

11.8.2 问题分析 327

11.8.3 小结 329

11.9 线程池使用FutureTask 时需要注意的事情 329

11.9.1 问题复现 329

11.9.2 问题分析 332

11.9.3 小结 335

11.10 使用ThreadLocal 不当可能会导致内存泄漏 336

11.10.1 为何会出现内存泄漏 336

11.10.2 在线程池中使用ThreadLocal 导致的内存泄漏 339

11.10.3 在Tomcat 的Servlet 中使用ThreadLocal 导致内存泄漏 341

第 1 章 第 一步：并发设计原理 　　 1
1．1　基本的并发概念　1
1．1．1　并发与并行　1
1．1．2　同步　2
1．1．3　不可变对象　2
1．1．4　原子操作和原子变量　3
1．1．5　共享内存与消息传递　3
1．2　并发应用程序中可能出现的问题　3
1．2．1　数据竞争　3
1．2．2　死锁　4
1．2．3　活锁　4
1．2．4　资源不足　4
1．2．5　优先权反转　5
1．3　设计并发算法的方法论　5
1．3．1　起点：算法的一个串行版本　5
1．3．2　第 1 步：分析　5
1．3．3　第 2 步：设计　5
1．3．4　第3 步：实现　6
1．3．5　第4 步：测试　6
1．3．6　第5 步：调整　6
1．3．7　结论　7
1．4　Java 并发API　8
1．4．1　基本并发类　8
1．4．2　同步机制　8
1．4．3　执行器　9
1．4．4　Fork/Join 框架　9
1．4．5　并行流　9
1．4．6　并发数据结构　9
1．5　并发设计模式　10
1．5．1　信号模式　10
1．5．2　会合模式　11
1．5．3　互斥模式　11
1．5．4　多元复用模式　12
1．5．5　栅栏模式　12
1．5．6　双重检查锁定模式　12
1．5．7　读 写锁模式　13
1．5．8　线程池模式　14
1．5．9　线程局部存储模式　14
1．6　设计并发算法的提示和技巧　14
1．6．1　正确识别独立任务　14
1．6．2　在尽可能高的层面上实施并发处理　15
1．6．3　考虑伸缩性　15
1．6．4　使用线程安全API　15
1．6．5　绝不要假定执行顺序　16
1．6．6　在静态和共享场合尽可能使用局部线程变量　16
1．6．7　寻找更易于并行处理的算法版本　17
1．6．8　尽可能使用不可变对象　17
1．6．9　通过对锁排序来避免死锁　17
1．6．10　使用原子变量代替同步　18
1．6．11　占有锁的时间尽可能短　19
1．6．12　谨慎使用延迟初始化　19
1．6．13　避免在临界段中使用阻塞操作　19
1．7　小结　20
第　2 章 使用基本元素：Thread 和Runnable　21
2．1　Java 中的线程　21
2．1．1　Java 中的线程：特征和状态　22
2．1．2　Thread 类和Runnable 接口　23
2．2　第 一个例子：矩阵乘法　24
2．2．1　公共类　24
2．2．2　串行版本　25
2．2．3　并行版本　25
2．3　第二个例子：文件搜索　32
2．3．1　公共类　32
2．3．2　串行版本　32
2．3．3　并发版本　33
2．3．4　对比解决方案　37
2．4　小结　38
第3　章 管理大量线程：执行器　39
3．1　执行器简介　39
3．1．1　执行器的基本特征　39
3．1．2　执行器框架的基本组件　40
3．2　第 一个例子：k-最近邻算法　40
3．2．1　k-最近邻算法：串行版本　41
3．2．2　k-最近邻算法：细粒度并发版本　42
3．2．3　k-最近邻算法：粗粒度并发版本　45
3．2．4　对比解决方案　46
3．3　第二个例子：客户端/服务器环境下的并发处理　48
3．3．1　客户端/服务器：串行版　48
3．3．2　客户端/服务器：并行版本　51
3．3．3　额外的并发服务器组件　54
3．3．4　对比两种解决方案　59
3．3．5　其他重要方法　61
3．4　小结　62
第4　章 充分利用执行器　63
4．1　执行器的高级特性　63
4．1．1　任务的撤销　63
4．1．2　任务执行调度　64
4．1．3　重载执行器方法　64
4．1．4　更改一些初始化参数　64
4．2　第 一个例子：高级服务器应用程序　65
4．2．1　ServerExecutor 类　65
4．2．2　命令类　70
4．2．3　服务器部件　72
4．2．4　客户端部件　78
4．3　第二个例子：执行周期性任务　79
4．3．1　公共部件　79
4．3．2　基础阅读器　81
4．3．3　高级阅读器　84
4．4　有关执行器的其他信息　87
4．5　小结　87
第5　章 从任务获取数据：Callable接口与Future 接口　88
5．1　Callable 接口和Future 接口简介　88
5．1．1　Callable 接口　88
5．1．2　Future 接口　89
5．2　第 一个例子：单词最佳匹配算法　89
5．2．1　公共类　90
5．2．2　最佳匹配算法：串行版本　91
5．2．3　最佳匹配算法：第 一个并发版本　92
5．2．4　最佳匹配算法：第二个并发版本　95
5．2．5　单词存在算法：串行版本　96
5．2．6　单词存在算法：并行版本　98
5．2．7　对比解决方案　100
5．3　第二个例子：为文档集创建倒排索引　102
5．3．1　公共类　103
5．3．2　串行版本　104
5．3．3　第 一个并发版本：每个文档一个任务　105
5．3．4　第二个并发版本：每个任务多个文档　109
5．3．5　对比解决方案　112
5．3．6　其他相关方法　113
5．4　小结　113
第6　章 运行分为多阶段的任务：Phaser 类　115
6．1　Phaser 类简介　115
6．1．1　参与者的注册与注销　116
6．1．2　同步阶段变更　116
6．1．3　其他功能　116
6．2　第 一个例子：关键字抽取算法　117
6．2．1　公共类　118
6．2．2　串行版本　121
6．2．3　并发版本　123
6．2．4　对比两种解决方案　128
6．3　第二个例子：遗传算法　129
6．3．1　公共类　130
6．3．2　串行版本　132
6．3．3　并发版本　134
6．3．4　对比两种解决方案　139
6．4　小结　141
第7　章 优化分治解决方案：
Fork/Join　框架　142
7．1　Fork/Join 框架简介　142
7．1．1　Fork/Join 框架的基本特征　143
7．1．2　Fork/Join 框架的局限性　143
7．1．3　Fork/Join 框架的组件　144
7．2　第 一个例子：k-means 聚类算法　144
7．2．1　公共类　145
7．2．2　串行版本　149
7．2．3　并发版本　151
7．2．4　对比解决方案　155
7．3　第二个例子：数据筛选算法　157
7．3．1　公共特性　157
7．3．2　串行版　157
7．3．3　并发版本　159
7．3．4　对比两个版本　165
7．4　第三个例子：归并排序算法　166
7．4．1　共享类　166
7．4．2　串行版本　167
7．4．3　并发版本　169
7．4．4　对比两个版本　172
7．5　Fork/Join 框架的其他方法　172
7．6　小结　173
第8　章 使用并行流处理大规模数据集：MapReduce 模型　174
8．1　流的简介　174
8．1．1　流的基本特征　174
8．1．2　流的组成部分　175
8．1．3　MapReduce 与MapCollect　177
8．2　第 一个例子：数值综合分析应用程序　178
8．2．1　并发版本　178
8．2．2　串行版本　185
8．2．3　对比两个版本　186
8．3　第二个例子：信息检索工具　186
8．3．1　约简操作简介　187
8．3．2　第 一种方式：全文档查询　188
8．3．3　第二种方式：约简的文档查询　191
8．3．4　第三种方式：生成一个含有结果的HTML 文件　191
8．3．5　第四种方式：预先载入倒排索引　194
8．3．6　第五种方式：使用我们的执行器　195
8．3．7　从倒排索引获取数据：ConcurrentData 类　196
8．3．8　获取文件中的单词数　196
8．3．9　获取文件的平均tfxidf 值　196
8．3．10　获取索引中的最大tfxidf值和最小tfxidf 值　197
8．3．11　ConcurrentMain 类　198
8．3．12　串行版　199
8．3．13　对比两种解决方案　199
8．4　小结　202
第9　章 使用并行流处理大规模数据集：MapCollect 模型　203
9．1　使用流收集数据　203
9．2　第 一个例子：无索引条件下的数据搜索　205
9．2．1　基本类　205
9．2．2　第 一种方式：基本搜索　207
9．2．3　第二种方式：高级搜索　209
9．2．4　本例的串行实现　211
9．2．5　对比实现方案　211
9．3　第二个例子：推荐系统　212
9．3．1　公共类　212
9．3．2　推荐系统：主类　213
9．3．3　ConcurrentLoaderAccumulator 类　215
9．3．4　串行版　216
9．3．5　对比两个版本　216
9．4　第三个例子：社交网络中的共同联系人　217
9．4．1　基本类　218
9．4．2　并发版本　219
9．4．3　串行版本　223
9．4．4　对比两个版本　223
9．5　小结　224
第　10 章 异步流处理：反应流　225
10．1　Java 反应流简介　225
10．1．1　Flow．Publisher 接口　226
10．1．2　Flow．Subscriber 接口　226
10．1．3　Flow．Subscription 接口　226
10．1．4　SubmissionPublisher 类　226
10．2　第 一个例子：面向事件通知的集中式系统　227
10．2．1　Event 类　227
10．2．2　Producer 类　227
10．2．3　Consumer 类　228
10．2．4　Main 类　230
10．3　第二个例子：新闻系统　231
10．3．1　News 类　232
10．3．2　发布者相关的类　232
10．3．3　Consumer 类　235
10．3．4　Main 类　236
10．4　小结　238
第　11 章 探究并发数据结构和同步工具　240
11．1　并发数据结构　240
11．1．1　阻塞型数据结构和非阻塞型数据结构　241
11．1．2　并发数据结构　241
11．1．3　使用新特性　244
11．1．4　原子变量　251
11．1．5　变量句柄　252
11．2　同步机制　254
11．2．1　CommonTask 类　255
11．2．2　Lock 接口　255
11．2．3　Semaphore 类　256
11．2．4　CountDownLatch 类　258
11．2．5　CyclicBarrier 类　259
11．2．6　CompletableFuture 类　261
11．3　小结　268
第　12 章 测试与监视并发应用程序　269
12．1　监视并发对象　269
12．1．1　监视线程　269
12．1．2　监视锁　270
12．1．3　监视执行器　272
12．1．4　监视Fork/Join 框架　273
12．1．5　监视Phaser　274
12．1．6　监视流API　275
12．2　监视并发应用程序　276
12．2．1　Overview 选项卡　278
12．2．2　Memory 选项卡　279
12．2．3　Threads 选项卡　280
12．2．4　Classes 选项卡　280
12．2．5　VM Summary 选项卡　281
12．2．6　MBeans 选项卡　283
12．2．7　About 选项卡　284
12．3　测试并发应用程序　284
12．3．1　使用MultithreadedTC 测试并发应用程序　285
12．3．2　使用Java Pathfinder 测试并发应用程序　288
12．4　小结　293
第　13 章 JVM 中的并发处理：Clojure、带有GPars 库的Groovy 以及Scala　294
13．1　Clojure 的并发处理　294
13．1．1　使用Java 元素　295
13．1．2　引用类型　295
13．1．3　Ref 对象　298
13．1．4　Delay　299
13．1．5　Future　300
13．1．6　Promise　301
13．2　Groovy 及其GPars 库的并发处理　302
13．3　软件事务性内存　302
13．3．1　使用Java 元素　302
13．3．2　数据并行处理　303
13．3．3　Fork/Join 处理　307
13．3．4　Actor　308
13．3．5　Agent　315
13．3．6　Dataf low　316
13．4　Scala 的并发处理　322
13．4．1　Scala 中的Future 对象　322
13．4．2　Promise　328

第1章　概述　　1

1.1 并发还是并行？ 　　1

1.2 并行架构　　3

1.3 并发：不只是多核　　5

1.4 七个模型　　6

第2章　线程与锁　　7

2.1 简单粗暴　　7

2.2 第一天：互斥和内存模型　　8

2.3 第二天：超越内置锁　　17

2.4 第三天：站在巨人的肩膀上　　27

2.5 复习　　38

第3章　函数式编程　　41

3.1 若不爽，就另辟蹊径　　41

3.2 第一天：抛弃可变状态　　42

3.3 第二天：函数式并行　　51

3.4 第三天：函数式并发　　61

3.5 复习　　70

第4章　Clojure 之道——分离标识与状态　　73

4.1 混搭的力量　　73

4.2 第一天：原子变量与持久数据结构　　73

4.3 第二天：代理和软件事务内存　　84

4.4 第三天：深入学习　　92

4.5 复习　　98

第5章　Actor　　100

5.1 更加面向对象　　100

5.2 第一天：消息和信箱　　101

5.3 第二天：错误处理和容错性　　111

5.4 第三天：分布式　　120

5.5 复习　　132

第6章　通信顺序进程　　135

6.1 万物皆通信　　135

6.2 第一天：channel 和go 块　　136

6.3 第二天：多个channel 与IO　　146

6.4 第三天：客户端CSP　　157

6.5 复习　　164

第7章　数据并行　　167

7.1 隐藏在笔记本电脑中的超级计算机　　167

7.2 第一天：GPGPU编程　　167

7.3 第二天：多维空间与工作组　　177

7.4 第三天：OpenCL和OpenGL——全部在GPU 上运行　　187

7.5 复习　　194

第8章　Lambda架构　　196

8.1 并行计算搞定大数据　　196

8.2 第一天：MapReduce　　197

8.3 第二天：批处理层　　208

8.4 第三天：加速层　　218

8.5 复习　　229

第9章　圆满结束　　231

9.1 君欲何往　　231

9.2 未尽之路　　232

9.3 越过山丘　　234

第1部分 概述 / 1
1 交易型系统设计的一些原则 / 2
1.1 高并发原则 / 3
1.1.1 无状态 / 3
1.1.2 拆分 / 3
1.1.3 服务化 / 4
1.1.4 消息队列 / 4
1.1.5 数据异构 / 6
1.1.6 缓存银弹 / 7
1.1.7 并发化 / 9
1.2 高可用原则 / 10
1.2.1 降级 / 10
1.2.2 限流 / 11
1.2.3 切流量 / 12
1.2.4 可回滚 / 12
1.3 业务设计原则 / 12
1.3.1 防重设计 / 13
1.3.2 幂等设计 / 13
1.3.3 流程可定义 / 13
1.3.4 状态与状态机 / 13
1.3.5 后台系统操作可反馈 / 14
1.3.6 后台系统审批化 / 14
1.3.7 文档和注释 / 14
1.3.8 备份 / 14
1.4 总结 / 14
第2部分 高可用 / 17
2 负载均衡与反向代理 / 18
2.1 upstream配置 / 20
2.2 负载均衡算法 / 21
2.3 失败重试 / 23
2.4 健康检查 / 24
2.4.1 TCP心跳检查 / 24
2.4.2 HTTP心跳检查 / 25
2.5 其他配置 / 25
2.5.1 域名上游服务器 / 25
2.5.2 备份上游服务器 / 26
2.5.3 不可用上游服务器 / 26
2.6 长连接 / 26
2.7 HTTP反向代理示例 / 29
2.8 HTTP动态负载均衡 / 30
2.8.1 Consul+Consul-template / 31
2.8.2 Consul+OpenResty / 35
2.9 Nginx四层负载均衡 / 39
2.9.1 静态负载均衡 / 39
2.9.2 动态负载均衡 / 41
参考资料 / 42
3 隔离术 / 43
3.1 线程隔离 / 43
3.2 进程隔离 / 45
3.3 集群隔离 / 45
3.4 机房隔离 / 46
3.5 读写隔离 / 47
3.6 动静隔离 / 48
3.7 爬虫隔离 / 49
3.8 热点隔离 / 50
3.9 资源隔离 / 50
3.10 使用Hystrix实现隔离 / 51
3.10.1 Hystrix简介 / 51
3.10.2 隔离示例 / 52
3.11 基于Servlet 3实现请求隔离 / 56
3.11.1 请求解析和业务处理线程池分离 / 57
3.11.2 业务线程池隔离 / 58
3.11.3 业务线程池监控/运维/降级 / 58
3.11.4 如何使用Servlet 3异步化 / 59
3.11.5 一些Servlet 3异步化压测数据 / 64
4 限流详解 / 66
4.1 限流算法 / 67
4.1.1 令牌桶算法 / 67
4.1.2 漏桶算法 / 68
4.2 应用级限流 / 69
4.2.1 限流总并发/连接/请求数 / 69
4.2.2 限流总资源数 / 70
4.2.3 限流某个接口的总并发/请求数 / 70
4.2.4 限流某个接口的时间窗请求数 / 70
4.2.5 平滑限流某个接口的请求数 / 71
4.3 分布式限流 / 75
4.3.1 Redis+Lua实现 / 76
4.3.2 Nginx+Lua实现 / 77
4.4 接入层限流 / 78
4.4.1 ngx_http_limit_conn_module / 78
4.4.2 ngx_http_limit_req_module / 80
4.4.3 lua-resty-limit-traffic / 88
4.5 节流 / 90
4.5.1 throttleFirst/throttleLast / 90
4.5.2 throttleWithTimeout / 91
参考资料 / 92
5 降级特技 / 93
5.1 降级预案 / 93
5.2 自动开关降级 / 95
5.2.1 超时降级 / 95
5.2.2 统计失败次数降级 / 95
5.2.3 故障降级 / 95
5.2.4 限流降级 / 95
5.3 人工开关降级 / 96
5.4 读服务降级 / 96
5.5 写服务降级 / 97
5.6 多级降级 / 98
5.7 配置中心 / 100
5.7.1 应用层API封装 / 100
5.7.2 配置文件实现开关配置 / 101
5.7.3 配置中心实现开关配置 / 102
5.8 使用Hystrix实现降级 / 106
5.9 使用Hystrix实现熔断 / 108
5.9.1 熔断机制实现 / 108
5.9.2 配置示例 / 112
5.9.3 采样统计 / 113
6 超时与重试机制 / 117
6.1 简介 / 117
6.2 代理层超时与重试 / 119
6.2.1 Nginx / 119
6.2.2 Twemproxy / 126
6.3 Web容器超时 / 127
6.4 中间件客户端超时与重试 / 127
6.5 数据库客户端超时 / 131
6.6 NoSQL客户端超时 / 134
6.7 业务超时 / 135
6.8 前端Ajax超时 / 135
6.9 总结 / 136
6.10 参考资料 / 137
7 回滚机制 / 139
7.1 事务回滚 / 139
7.2 代码库回滚 / 140
7.3 部署版本回滚 / 141
7.4 数据版本回滚 / 142
7.5 静态资源版本回滚 / 143
8 压测与预案 / 145
8.1 系统压测 / 145
8.1.1 线下压测 / 146
8.1.2 线上压测 / 146
8.2 系统优化和容灾 / 147
8.3 应急预案 / 148
第3部分 高并发 / 153
9 应用级缓存 / 154
9.1 缓存简介 / 154
9.2 缓存命中率 / 155
9.3 缓存回收策略 / 155
9.3.1 基于空间 / 155
9.3.2 基于容量 / 155
9.3.3 基于时间 / 155
9.3.4 基于Java对象引用 / 156
9.3.5 回收算法 / 156
9.4 Java缓存类型 / 156
9.4.1 堆缓存 / 158
9.4.2 堆外缓存 / 162
9.4.3 磁盘缓存 / 162
9.4.4 分布式缓存 / 164
9.4.5 多级缓存 / 166
9.5 应用级缓存示例 / 167
9.5.1 多级缓存API封装 / 167
9.5.2 NULL Cache / 170
9.5.3 强制获取最新数据 / 170
9.5.4 失败统计 / 171
9.5.5 延迟报警 / 171
9.6 缓存使用模式实践 / 172
9.6.1 Cache-Aside / 173
9.6.2 Cache-As-SoR / 174
9.6.3 Read-Through / 174
9.6.4 Write-Through / 176
9.6.5 Write-Behind / 177
9.6.6 Copy Pattern / 181
9.7 性能测试 / 181
9.8 参考资料 / 182
10 HTTP缓存 / 183
10.1 简介 / 183
10.2 HTTP缓存 / 184
10.2.1 Last-Modified / 184
10.2.2 ETag / 190
10.2.3 总结 / 192
10.3 HttpClient客户端缓存 / 192
10.3.1 主流程 / 195
10.3.2 清除无效缓存 / 195
10.3.3 查找缓存 / 196
10.3.4 缓存未命中 / 198
10.3.5 缓存命中 / 198
10.3.6 缓存内容陈旧需重新验证 / 202
10.3.7 缓存内容无效需重新执行请求 / 205
10.3.8 缓存响应 / 206
10.3.9 缓存头总结 / 207
10.4 Nginx HTTP缓存设置 / 208
10.4.1 expires / 208
10.4.2 if-modified-since / 209
10.4.3 nginx proxy_pass / 209
10.5 Nginx代理层缓存 / 212
10.5.1 Nginx代理层缓存配置 / 212
10.5.2 清理缓存 / 215
10.6 一些经验 / 216
参考资料 / 217
11 多级缓存 / 218
11.1 多级缓存介绍 / 218
11.2 如何缓存数据 / 220
11.2.1 过期与不过期 / 220
11.2.2 维度化缓存与增量缓存 / 221
11.2.3 大Value缓存 / 221
11.2.4 热点缓存 / 221
11.3 分布式缓存与应用负载均衡 / 222
11.3.1 缓存分布式 / 222
11.3.2 应用负载均衡 / 222
11.4 热点数据与更新缓存 / 223
11.4.1 单机全量缓存+主从 / 223
11.4.2 分布式缓存+应用本地热点 / 224
11.5 更新缓存与原子性 / 225
11.6 缓存崩溃与快速修复 / 226
11.6.1 取模 / 226
11.6.2 一致性哈希 / 226
11.6.3 快速恢复 / 226
12 连接池线程池详解 / 227
12.1 数据库连接池 / 227
12.1.1 DBCP连接池配置 / 228
12.1.2 DBCP配置建议 / 233
12.1.3 数据库驱动超时实现 / 234
12.1.4 连接池使用的一些建议 / 235
12.2 HttpClient连接池 / 236
12.2.1 HttpClient 4.5.2配置 / 236
12.2.2 HttpClient连接池源码分析 / 240
12.2.3 HttpClient 4.2.3配置 / 241
12.2.4 问题示例 / 243
12.3 线程池 / 244
12.3.1 Java线程池 / 245
12.3.2 Tomcat线程池配置 / 248
13 异步并发实战 / 250
13.1 同步阻塞调用 / 251
13.2 异步Future / 252
13.3 异步Callback / 253
13.4 异步编排CompletableFuture / 254
13.5 异步Web服务实现 / 257
13.6 请求缓存 / 259
13.7 请求合并 / 261
14 如何扩容 / 266
14.1 单体应用垂直扩容 / 267
14.2 单体应用水平扩容 / 267
14.3 应用拆分 / 268
14.4 数据库拆分 / 271
14.5 数据库分库分表示例 / 275
14.5.1 应用层还是中间件层 / 275
14.5.2 分库分表策略 / 277
14.5.3 使用sharding-jdbc分库分表 / 279
14.5.4 sharding-jdbc分库分表配置 / 279
14.5.5 使用sharding-jdbc读写分离 / 283
14.6 数据异构 / 284
14.6.1 查询维度异构 / 284
14.6.2 聚合数据异构 / 285
14.7 任务系统扩容 / 285
14.7.1 简单任务 / 285
14.7.2 分布式任务 / 287
14.7.3 Elastic-Job简介 / 287
14.7.4 Elastic-Job-Lite功能与架构 / 287
14.7.5 Elastic-Job-Lite示例 / 288
15 队列术 / 295
15.1 应用场景 / 295
15.2 缓冲队列 / 296
15.3 任务队列 / 297
15.4 消息队列 / 297
15.5 请求队列 / 299
15.6 数据总线队列 / 300
15.7 混合队列 / 301
15.8 其他队列 / 302
15.9 Disruptor+Redis队列 / 303
15.10 下单系统水平可扩展架构 / 311
第4部分 案例 / 323
16 构建需求响应式亿级商品详情页 / 324
16.1 商品详情页是什么 / 324
16.2 商品详情页前端结构 / 325
16.3 我们的性能数据 / 327
16.4 单品页流量特点 / 327
16.5 单品页技术架构发展 / 327
16.5.1 架构1.0 / 328
16.5.2 架构2.0 / 328
16.5.3 架构3.0 / 330
16.6 详情页架构设计原则 / 332
16.7 遇到的一些坑和问题 / 339
16.8 其他 / 347
17 京东商品详情页服务闭环实践 / 348
17.1 为什么需要统一服务 / 348
17.2 整体架构 / 349
17.3 一些架构思路和总结 / 350
17.4 引入Nginx接入层 / 354
17.5 前端业务逻辑后置 / 356
17.6 前端接口服务端聚合 / 357
17.7 服务隔离 / 359
18 使用OpenResty开发高性能Web应用 / 360
18.1 OpenResty简介 / 361
18.1.1 Nginx优点 / 361
18.1.2 Lua的优点 / 361
18.1.3 什么是ngx_lua / 361
18.1.4 开发环境 / 362
18.1.5 OpenResty生态 / 362
18.1.6 场景 / 362
18.2 基于OpenResty的常用架构模式 / 363
18.3 如何使用OpenResty开发Web应用 / 371
18.4 基于OpenResty的常用功能总结 / 375
18.5 一些问题 / 376
19 应用数据静态化架构高性能单页Web应用 / 377
19.1 整体架构 / 378
19.2 数据和模板动态化 / 381
19.3 多版本机制 / 381
19.4 异常问题 / 382
20 使用OpenResty开发Web服务 / 383
20.1 架构 / 383
20.2 单DB架构 / 384
20.3 实现 / 387
21 使用OpenResty开发商品详情页 / 405
21.1 技术选型 / 407
21.2 核心流程 / 408
21.3 项目搭建 / 408
21.4 数据存储实现 / 410
21.5 动态服务实现 / 422
21.6 前端展示实现 / 430

第1章　走入并行世界 1
1．1　何去何从的并行计算 1
1．1．1　忘掉那该死的并行 2
1．1．2　可怕的现实：摩尔定律的失效 4
1．1．3　柳暗花明：不断地前进 5
1．1．4　光明或是黑暗 6
1．2　你必须知道的几个概念 6
1．2．1　同步（Synchronous）和异步（Asynchronous） 7
1．2．2　并发（Concurrency）和并行（Parallelism） 8
1．2．3　临界区 9
1．2．4　阻塞（Blocking）和非阻塞（Non-Blocking） 9
1．2．5　死锁（Deadlock）、饥饿（Starvation）和活锁（Livelock） 9
1．3　并发级别 11
1．3．1　阻塞（Blocking） 11
1．3．2　无饥饿（Starvation-Free） 11
1．3．3　无障碍（Obstruction-Free） 12
1．3．4　无锁（Lock-Free） 12
1．3．5　无等待（Wait-Free） 13
1．4　有关并行的两个重要定律 13
1．4．1　Amdahl定律 13
1．4．2　Gustafson定律 16
1．4．3　Amdahl定律和Gustafson定律是否相互矛盾 16
1．5　回到Java：JMM 17
1．5．1　原子性（Atomicity） 18
1．5．2　可见性（Visibility） 20
1．5．3　有序性（Ordering） 22
1．5．4　哪些指令不能重排：Happen-Before规则 27
1．6　参考文献 27
第2章　Java并行程序基础 29
2．1　有关线程你必须知道的事 29
2．2　初始线程：线程的基本操作 32
2．2．1　新建线程 32
2．2．2　终止线程 34
2．2．3　线程中断 38
2．2．4　等待（wait）和通知（notify） 41
2．2．5　挂起（suspend）和继续执行（resume）线程 44
2．2．6　等待线程结束（join）和谦让（yield） 48
2．3　volatile与Java内存模型（JMM） 50
2．4　分门别类的管理：线程组 52
2．5　驻守后台：守护线程（Daemon） 54
2．6　先干重要的事：线程优先级 55
2．7　线程安全的概念与synchronized 57
2．8　程序中的幽灵：隐蔽的错误 61
2．8．1　无提示的错误案例 61
2．8．2　并发下的ArrayList 62
2．8．3　并发下诡异的HashMap 63
2．8．4　初学者常见问题：错误的加锁 66
2．9　参考文献 68
第3章　JDK并发包 70
3．1　多线程的团队协作：同步控制 70
3．1．1　synchronized的功能扩展：重入锁 71
3．1．2　重入锁的好搭档：Condition条件 80
3．1．3　允许多个线程同时访问：信号量（Semaphore） 83
3．1．4　ReadWriteLock读写锁 85
3．1．5　倒计时器：CountDownLatch 87
3．1．6　循环栅栏：CyclicBarrier 89
3．1．7　线程阻塞工具类：LockSupport 92
3．2　线程复用：线程池 95
3．2．1　什么是线程池 96
3．2．2　不要重复发明轮子：JDK对线程池的支持 97
3．2．3　刨根究底：核心线程池的内部实现 102
3．2．4　超负载了怎么办：拒绝策略 106
3．2．5　自定义线程创建：ThreadFactory 109
3．2．6　我的应用我做主：扩展线程池 110
3．2．7　合理的选择：优化线程池线程数量 112
3．2．8　堆栈去哪里了：在线程池中寻找堆栈 113
3．2．9　分而治之：Fork/Join框架 117
3．3　不要重复发明轮子：JDK的并发容器 121
3．3．1　超好用的工具类：并发集合简介 121
3．3．2　线程安全的HashMap 122
3．3．3　有关List的线程安全 123
3．3．4　高效读写的队列：深度剖析ConcurrentLinkedQueue 123
3．3．5　高效读取：不变模式下的CopyOnWriteArrayList 129
3．3．6　数据共享通道：BlockingQueue 130
3．3．7　随机数据结构：跳表（SkipList） 134
3．4　参考资料 136

第4章　锁的优化及注意事项 138
4．1　有助于提高“锁”性能的几点建议 139
4．1．1　减小锁持有时间 139
4．1．2　减小锁粒度 140
4．1．3　读写分离锁来替换独占锁 142
4．1．4　锁分离 142
4．1．5　锁粗化 144
4．2　Java虚拟机对锁优化所做的努力 146
4．2．1　锁偏向 146
4．2．2　轻量级锁 146
4．2．3　自旋锁 146
4．2．4　锁消除 146
4．3　人手一支笔：ThreadLocal 147
4．3．1　ThreadLocal的简单使用 148
4．3．2　ThreadLocal的实现原理 149
4．3．3　对性能有何帮助 155
4．4　无锁 157
4．4．1　与众不同的并发策略：比较交换（CAS） 158
4．4．2　无锁的线程安全整数：AtomicInteger 159
4．4．3　Java中的指针：Unsafe类 161
4．4．4　无锁的对象引用：AtomicReference 162
4．4．5　带有时间戳的对象引用：AtomicStampedReference 165
4．4．6　数组也能无锁：AtomicIntegerArray 168
4．4．7　让普通变量也享受原子操作：AtomicIntegerFieldUpdater 169
4．4．8　挑战无锁算法：无锁的Vector实现 171
4．4．9　让线程之间互相帮助：细看SynchronousQueue的实现 176
4．5　有关死锁的问题 179
4．6　参考文献 183

第5章　并行模式与算法 184
5．1　探讨单例模式 184
5．2　不变模式 187
5．3　生产者-消费者模式 190
5．4　高性能的生产者-消费者：无锁的实现 194
5．4．1　无锁的缓存框架：Disruptor 195
5．4．2　用Disruptor实现生产者-消费者案例 196
5．4．3　提高消费者的响应时间：选择合适的策略 199
5．4．4　CPU Cache的优化：解决伪共享问题 200
5．5　Future模式 204
5．5．1　Future模式的主要角色 206
5．5．2　Future模式的简单实现 207
5．5．3　JDK中的Future模式 210
5．6　并行流水线 212
5．7　并行搜索 216
5．8　并行排序 218
5．8．1　分离数据相关性：奇偶交换排序 218
5．8．2　改进的插入排序：希尔排序 221
5．9　并行算法：矩阵乘法 226
5．10　准备好了再通知我：网络NIO 230
5．10．1　基于Socket的服务端的多线程模式 230
5．10．2　使用NIO进行网络编程 235
5．10．3　使用NIO来实现客户端 243
5．11　读完了再通知我：AIO 245
5．11．1　AIO EchoServer的实现 245
5．11．2　AIO Echo客户端实现 248
5．12　参考文献 249

第6章　Java 8与并发 251
6．1　Java 8的函数式编程简介 251
6．1．1　函数作为一等公民 252
6．1．2　无副作用 252
6．1．3　申明式的（Declarative） 253
6．1．4　不变的对象 254
6．1．5　易于并行 254
6．1．6　更少的代码 254
6．2　函数式编程基础 255
6．2．1　FunctionalInterface注释 255
6．2．2　接口默认方法 256
6．2．3　lambda表达式 259
6．2．4　方法引用 260
6．3　一步一步走入函数式编程 263
6．4　并行流与并行排序 267
6．4．1　使用并行流过滤数据 267
6．4．2　从集合得到并行流 268
6．4．3　并行排序 268
6．5　增强的Future：CompletableFuture 269
6．5．1　完成了就通知我 269
6．5．2　异步执行任务 270
6．5．3　流式调用 272
6．5．4　CompletableFuture中的异常处理 272
6．5．5　组合多个CompletableFuture 273

6．6　读写锁的改进：StampedLock 274
6．6．1　StampedLock使用示例 275
6．6．2　StampedLock的小陷阱 276
6．6．3　有关StampedLock的实现思想 278
6．7　原子类的增强 281
6．7．1　更快的原子类：LongAdder 281
6．7．2　LongAdder的功能增强版：LongAccumulator 287

第7章　使用Akka构建高并发程序 289
7．1　新并发模型：Actor 290
7．2　Akka之Hello World 290
7．3　有关消息投递的一些说明 293
7．4　Actor的生命周期 295
7．5　监督策略 298
7．6　选择Actor 303
7．7　消息收件箱（Inbox） 303
7．8　消息路由 305

7．9　Actor的内置状态转换 308
7．10　询问模式：Actor中的Future 311
7．11　多个Actor同时修改数据：Agent 313
7．12　像数据库一样操作内存数据：软件事务内存 316
7．13　一个有趣的例子：并发粒子群的实现 319
7．13．1　什么是粒子群算法 320
7．13．2　粒子群算法的计算过程 320
7．13．3　粒子群算法能做什么 322
7．13．4　使用Akka实现粒子群 323
7．14　参考文献 330

第8章　并行程序调试 331
8．1　准备实验样本 331
8．2　正式起航 332
8．3　挂起整个虚拟机 334
8．4　调试进入ArrayList内部 336

CLH锁是一种基于链表的可扩展、高性能、公平的自旋锁，申请线程只在本地变量上自旋，它不断轮询前驱的状态，如果发现前驱释放了锁就结束自旋，获得锁。
* @author EX-ZHANGYONGTIAN001
* @version 1.0.0
* @date 2019-3-13  13:46
*/
public class CLHLock {

    /**
     * 定义一个节点，默认的lock状态为true
     */
    public static class CLHNode {
        private volatile boolean isLocked = true;
    }
    /**
     * 尾部节点,只用一个节点即可
     */
    private volatile CLHNode tail;
    private static final ThreadLocal<CLHNode> LOCAL = new ThreadLocal<CLHNode>();
    private static final AtomicReferenceFieldUpdater<CLHLock, CLHNode> UPDATER = AtomicReferenceFieldUpdater.newUpdater(CLHLock.class, CLHNode.class,
            "tail");
    public void lock() {
        // 新建节点并将节点与当前线程保存起来
        CLHNode node = new CLHNode();
        LOCAL.set(node);
        // 将新建的节点设置为尾部节点，并返回旧的节点（原子操作），这里旧的节点实际上就是当前节点的前驱节点
        CLHNode preNode = UPDATER.getAndSet(this, node);
        if (preNode != null) {
            // 前驱节点不为null表示当锁被其他线程占用，通过不断轮询判断前驱节点的锁标志位等待前驱节点释放锁
            while (preNode.isLocked) {
            }
            preNode = null;
            LOCAL.set(node);
        }
        // 如果不存在前驱节点，表示该锁没有被其他线程占用，则当前线程获得锁
    }
    public void unlock() {
        // 获取当前线程对应的节点
        CLHNode node = LOCAL.get();
        // 如果tail节点等于node，则将tail节点更新为null，同时将node的lock状态职位false，表示当前线程释放了锁
        if (!UPDATER.compareAndSet(this, node, null)) {
            node.isLocked = false;
        }
        node = null;
    }


}




提到 JAVA 加锁，我们通常会想到 synchronized 关键字或者是 Java Concurrent Util（后面简称JCU）包下面的 Lock，今天就来扒一扒 Lock 是如何实现的，比如我们可以先提出一些问题：当我们通实例化一个 ReentrantLock 并且调用它的 lock 或 unlock 的时候，这其中发生了什么？如果多个线程同时对同一个锁实例进行 lock 或 unlcok 操作，这其中又发生了什么？

什么是可重入锁？

ReentrantLock 是可重入锁，什么是可重入锁呢？可重入锁就是当前持有该锁的线程能够多次获取该锁，无需等待。可重入锁是如何实现的呢？这要从 ReentrantLock 的一个内部类 Sync 的父类说起，Sync 的父类是 AbstractQueuedSynchronizer（后面简称AQS）。

什么是AQS？

AQS 是 JDK1.5 提供的一个基于 FIFO 等待队列实现的一个用于实现同步器的基础框架，这个基础框架的重要性可以这么说，JCU 包里面几乎所有的有关锁、多线程并发以及线程同步器等重要组件的实现都是基于 AQS 这个框架。AQS 的核心思想是基于 volatile int state 这样的一个属性同时配合 Unsafe 工具对其原子性的操作来实现对当前锁的状态进行修改。当 state 的值为 0 的时候，标识改 Lock 不被任何线程所占有。

ReentrantLock 锁的架构

ReentrantLock 的架构相对简单，主要包括一个 Sync 的内部抽象类以及 Sync 抽象类的两个实现类。上面已经说过了 Sync 继承自 AQS，他们的结构示意图如下：



上图除了 AQS 之外，我把 AQS 的父类 AbstractOwnableSynchronizer（后面简称AOS）也画了进来，可以稍微提一下，AOS 主要提供一个 exclusiveOwnerThread 属性，用于关联当前持有该所的线程。另外、Sync 的两个实现类分别是 NonfairSync 和 FairSync，由名字大概可以猜到，一个是用于实现公平锁、一个是用于实现非公平锁。那么 Sync 为什么要被设计成内部类呢？我们可以看看 AQS 主要提供了哪些 protect 的方法用于修改 state 的状态，我们发现 Sync 被设计成为安全的外部不可访问的内部类。ReentrantLock 中所有涉及对 AQS 的访问都要经过 Sync，其实，Sync 被设计成为内部类主要是为了安全性考虑，这也是作者在 AQS 的 comments 上强调的一点。

AQS 的等待队列

作为 AQS 的核心实现的一部分，举个例子来描述一下这个队列长什么样子，我们假设目前有三个线程 Thread1、Thread2、Thread3 同时去竞争锁，如果结果是 Thread1 获取了锁，Thread2 和 Thread3 进入了等待队列，那么他们的样子如下：



AQS 的等待队列基于一个双向链表实现的，HEAD 节点不关联线程，后面两个节点分别关联 Thread2 和 Thread3，他们将会按照先后顺序被串联在这个队列上。这个时候如果后面再有线程进来的话将会被当做队列的 TAIL。

1）入队列

我们来看看，当这三个线程同时去竞争锁的时候发生了什么？代码：

public final void acquire(int arg) {
    if (!tryAcquire(arg) &&
        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
        selfInterrupt();
}
解读：三个线程同时进来，他们会首先会通过 CAS 去修改 state 的状态，如果修改成功，那么竞争成功，因此这个时候三个线程只有一个 CAS 成功，其他两个线程失败，也就是 tryAcquire 返回 false。接下来，addWaiter 会把将当前线程关联的 EXCLUSIVE 类型的节点入队列：

private Node addWaiter(Node mode) {
    Node node = new Node(Thread.currentThread(), mode);
    Node pred = tail;
    if (pred != null) {
        node.prev = pred;
        if (compareAndSetTail(pred, node)) {
            pred.next = node;
            return node;
        }
    }
    enq(node);
    return node;
}
解读：如果队尾节点不为 null，则说明队列中已经有线程在等待了，那么直接入队尾。对于我们举的例子，这边的逻辑应该是走 enq，也就是开始队尾是 null，其实这个时候整个队列都是 null 的。代码：

private Node enq(final Node node) {
    for (;;) {
        Node t = tail;
        if (t == null) { // Must initialize
            if (compareAndSetHead(new Node()))
                tail = head;
        } else {
            node.prev = t;
            if (compareAndSetTail(t, node)) {
                t.next = node;
                return t;
            }
        }
    }
}
解读：如果 Thread2 和 Thread3 同时进入了 enq，同时 t==null，则进行 CAS 操作对队列进行初始化，这个时候只有一个线程能够成功，然后他们继续进入循环，第二次都进入了 else 代码块，这个时候又要进行 CAS 操作，将自己放在队尾，因此这个时候又是只有一个线程成功，我们假设是 Thread2 成功，哈哈，Thread2 开心的返回了，Thread3 失落的再进行下一次的循环，最终入队列成功，返回自己。

2）并发问题

基于上面两段代码，他们是如何实现不进行加锁，当有多个线程，或者说很多很多的线程同时执行的时候，怎么能保证最终他们都能够乖乖的入队列而不会出现并发问题的呢？这也是这部分代码的经典之处，多线程竞争，热点、单点在队列尾部，多个线程都通过【CAS+死循环】这个free-lock黄金搭档来对队列进行修改，每次能够保证只有一个成功，如果失败下次重试，如果是N个线程，那么每个线程最多 loop N 次，最终都能够成功。

3）挂起等待线程

上面只是 addWaiter 的实现部分，那么节点入队列之后会继续发生什么呢？那就要看看 acquireQueued 是怎么实现的了，为保证文章整洁，代码我就不贴了，同志们自行查阅，我们还是以上面的例子来看看，Thread2 和 Thread3 已经被放入队列了，进入 acquireQueued 之后：

对于 Thread2 来说，它的 prev 指向 HEAD，因此会首先再尝试获取锁一次，如果失败，则会将 HEAD 的 waitStatus 值为 SIGNAL，下次循环的时候再去尝试获取锁，如果还是失败，且这个时候 prev 节点的 waitStatus 已经是 SIGNAL，则这个时候线程会被通过 LockSupport 挂起。

对于 Thread3 来说，它的 prev 指向 Thread2，因此直接看看 Thread2 对应的节点的 waitStatus 是否为 SIGNAL，如果不是则将它设置为 SIGNAL，再给自己一次去看看自己有没有资格获取锁，如果 Thread2 还是挡在前面，且它的 waitStatus 是 SIGNAL，则将自己挂起。

如果 Thread1 死死的握住锁不放，那么 Thread2 和 Thread3 现在的状态就是挂起状态啦，而且 HEAD，以及 Thread 的 waitStatus 都是 SIGNAL，尽管他们在整个过程中曾经数次去尝试获取锁，但是都失败了，失败了不能死循环呀，所以就被挂起了。当前状态如下：



锁释放-等待线程唤起

我们来看看当 Thread1 这个时候终于做完了事情，调用了 unlock 准备释放锁，这个时候发生了什么。代码：

public final boolean release(int arg) {
    if (tryRelease(arg)) {
        Node h = head;
        if (h != null && h.waitStatus != 0)
            unparkSuccessor(h);
        return true;
    }
    return false;
}
解读：首先，Thread1 会修改AQS的state状态，加入之前是 1，则变为 0，注意这个时候对于非公平锁来说是个很好的插入机会，举个例子，如果锁是公平锁，这个时候来了 Thread4，那么这个锁将会被 Thread4 抢去。。。

我们继续走常规路线来分析，当 Thread1 修改完状态了，判断队列是否为 null，以及队头的 waitStatus 是否为 0，如果 waitStatus 为 0，说明队列无等待线程，按照我们的例子来说，队头的 waitStatus 为 SIGNAL=-1，因此这个时候要通知队列的等待线程，可以来拿锁啦，这也是 unparkSuccessor 做的事情，unparkSuccessor 主要做三件事情：

将队头的 waitStatus 设置为 0。

通过从队列尾部向队列头部移动，找到最后一个 waitStatus<=0 的那个节点，也就是离队头最近的没有被cancelled的那个节点，队头这个时候指向这个节点。

将这个节点唤醒，其实这个时候 Thread1 已经出队列了。

还记得线程在哪里挂起的么，上面说过了，在 acquireQueued 里面，我没有贴代码，自己去看哦。这里我们也大概能理解 AQS 的这个队列为什么叫 FIFO 队列了，因此每次唤醒仅仅唤醒队头等待线程，让队头等待线程先出。

羊群效应

这里说一下羊群效应，当有多个线程去竞争同一个锁的时候，假设锁被某个线程占用，那么如果有成千上万个线程在等待锁，有一种做法是同时唤醒这成千上万个线程去去竞争锁，这个时候就发生了羊群效应，海量的竞争必然造成资源的剧增和浪费，因此终究只能有一个线程竞争成功，其他线程还是要老老实实的回去等待。AQS 的 FIFO 的等待队列给解决在锁竞争方面的羊群效应问题提供了一个思路：保持一个 FIFO 队列，队列每个节点只关心其前一个节点的状态，线程唤醒也只唤醒队头等待线程。其实这个思路已经被应用到了分布式锁的实践中，见：Zookeeper 分布式锁的改进实现方案。

==========

线程池的设计原理是什么？
无敌码农  Java技术江湖  昨天
点击关注上方“Java技术江湖”，设为“置顶或星标”，第一时间送达技术干货。





作者：无敌码农

文章来源：无敌码农





导读





线程池相关的知识点是面试中非常高频的问题，掌握线程及线程池相关的知识点也是程序员向高段位进阶的必由之路。由于线程池涉及线程、并发、编程语言内存模型等多方面的知识，历来也不是一块特别好掌握的内容。因此，小码哥决定好好梳理下这方面的知识，希望能够对你有所帮助。在本文中，作者将以JAVA语言中的线程池设计为基础，从原理分析及代码实践两个方面来进行梳理。



线程的概念





在了解线程池的相关的知识之前，我们有必要再次深入理解下线程的基本概念。在这里，也许会有很多同学质疑，线程的基本概念我们都懂，为什么还需要重复提起呢？



在回答这个问题之前，我们还是先回到实际的编程语言中来看看线程到底是什么？以JAVA为例，在JAVA中如何实现一个线程呢？



public class ThreadDemo01 {
    public static void main(String args[]) {
        //通过匿名内部类的方式创建线程，并且重写其中的run方法
        new Thread() {
            public void run() {
                while (true) {
                    System.out.println("线程->" + Thread.currentThread().getName() + " 运行中！");
                    try {
                        sleep(100);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
            }
        }.start();
    }
}




通过上面的代码示例，我们知道在JAVA中要实现一个线程可以通过构造Thread类来实现。之后，通过重写run()方法来让线程执行我们想要让它执行的逻辑。然而，为了让线程生效，我们还需要通过调用start()方法来启动它。那么为什么我们重写了run()方法，但是却还需要调用start()方法呢？run()方法和start()方法有什么关系？到底那个方法才是真正代表了线程这个存在呢？



要搞清楚这个问题，需要我们明确“线程的执行单元”与“线程”是两个不同的概念。在JAVA中通过Thread类重写的run()方法是线程的执行单元，而通过调用start()方法才是真正启动了一个线程。这一点对后面我们理解线程池的作用会比较有用，因为只有从概念上剥离线程的执行单元与线程本身才能更深入的理解线程池存在的意义。



为了更加深入的说明这一点，我们可以来具体分析下上面例子中start()方法在JDK中的源码：



 public synchronized void start() {
        group.add(this);
        boolean started = false;
        try {
            start0();
            started = true;
        } finally {
            try {
                if (!started) {
                    group.threadStartFailed(this);
                }
            } catch (Throwable ignore) {
            }
        }
    }



在start()方法的源码中，最核心的部分其实就是start0()这个JNI本地方法：



private native void start0();




也就是说在start方法中会调用start0这个本地方法，但是从源码上这么看又看不出start0的具体逻辑。为此，作者特地翻了下JDK的官方文档，其中关于start方法的说明如下：

Causes this thread to begin execution; the Java Virtual Machine calls the run method of this thread.

上面这句话的意思是：在开始执行这个线程的时候，JVM将会调用该线程的run方法，而实际上run方法是被本地方法start0()调用的。也就是说，在JAVA中由于语言的约定，我们需要在使用线程时重写线程中的执行单元方法来实现业务逻辑，而真正开启线程资源的则是start方法。



在不少关于JAVA线程的软文或者书籍中，经常会提到，创建线程有两种方式：第一种是构造一个Thread；第二种是实现Runnable接口。通过上面的分析，这种说法其实是不严谨的。在JDK中代表线程的只有Thread类，而Runnable接口只是简单定义了一个无参数返回值的run方法。而我们知道run方法只是定义了线程的执行单元，而并非直接开启了线程资源，只有Thread方法的start()方法才可以启动一个线程。



所以，如果面试中有人问你在JAVA中实现线程的方式有哪些？应该告诉他准确答案：“在JAVA中创建线程只有一种方式，那就是构造Thread类。而实现线程的执行单元则有两种方式，第一种是重写Thread类的run方法；第二种是实现Runnable接口的run方法，并且将Runnable实例用作构造Thread的参数”。



接下来让我们再来回顾下线程的定义：“线程是一种轻量级的进程，是由进程派生出来的子任务，它是程序执行的一个路径；每个线程都有自己的局部变量表、程序计数器（指向真正执行的指令指针）以及各自的生命周期”。例如，当启动了一个JVM时，从操作系统开始就会创建一个新的JVM进程，之后JVM进程中将会派生或者创建很多线程。



线程知识涉及编程语言特性的面非常广泛，以JAVA语言为例，作者梳理了一份有关线程的知识图谱，如下：





要掌握JAVA中的线程，需要我们理解线程的生命周期、Thread类提供的方法细节、线程安全问题等多方面的知识点。而其中线程安全相关的问题又涉及JVM的内存模型、线程同步及锁相关的知识。由于篇幅的关系，这里作者也只能给出一个大致的提纲，更细节的内容在后面有时间再和大家一起细化同步。



以上就是在具体讲述线程池之前有关线程知识的回顾了，接下来就让我们进入本篇文章的主题“线程池”相关的内容吧！



线程池原理





在上节关于线程知识的回顾中，我们知道创建一个线程Thread其实是比较耗费操作系统资源的，况且系统中可创建的线程数量也是有限的，如果创建的线程资源数量不能够很好的加以限制，反而会导致系统性能的下降。因此我们在进行多线程编程时，对线程资源的重复利将是一种非常好的程序设计习惯。



那么我们在编程时如何才能实现线程资源的重复利用呢？答案就是使用线程池！所谓的线程池，通俗的理解就是有一个池子，里面存放着已经创建好的线程资源，当有任务提交给线程池执行时，池中的某个线程就会主动执行该任务，执行完任务后该线程就会继续回到池子中等待下次任务的执行。下面我们就来看一下线程池的基本原理图，如下：







线程池中的线程资源是Thread类代表的，而具体的执行任务是由实现Runnable接口的线程执行单元类组成。线程的执行单元逻辑随业务的变化而有所不同，而线程则是一个公共资源，所以可以复用，这一点也是我们在前面内容中特别强调的，因为如果我们将线程的执行单元中的逻辑与线程本身混在一起理解的话就很容易产生疑惑。



那么如何实现一个线程池呢？一个完整的线程池应该具备如下要素：

任务队列：用于缓存提交的任务。

线程数量管理功能：一个线程池必须能够很好地管理和控制线程的数量。大致会有三个参数，创建线程池时的初始线程数量init；自动扩充时的最大线程数量max；在线程池空闲时需要释放资源但是也要维持一定数量的核心线程数量core。通过这三个基本参数维持好线程池中数量的合理范围，一般来说它们之间的关系是“init<=core<=max”。

任务拒绝策略：如果线程数量已达到上限且任务队列已满，则需要有相应的拒绝策略来通知任务的提交者。

线程工厂：主要用于个性化定制线程，如设置线程的名称或者将线程设置为守护线程等。

QueueSize：任务队列主要存放提交的Runnable,但是为了防止内存溢出，需要有limit数量对其进行限制。

Keepedalive时间：该时间主要决定线程各个重要参数自动维护的时间间隔。


通过上面对线程池组成部分及原理的分析，为了更加深刻地理解下线程池，下面我们手工实现一个线程池！UML类图如下：







ThreadPool（接口）：主要定义一个线程池应该具备的基本操作和方法。

RunnableQueue（接口）：定义存放提交的线程执行单元Runnable的队列。

ThreadFactory（接口）：定义创建线程的接口，便于个性化地定制Thread。

DenyPolicy（接口）：拒绝策略接口，主要用于Queue中当runnable达到limit上限后所采用的拒绝策略。

Internaltask（类）：Runnable的实现，用于线程池内部，该类通过沦陷RunnableQueue队列，不断从队列中取出任务进行执行。

LinkedRunnableQueue（类）：队列接口的具体实现。

BasicThreadPool（类）：线程池的核心实现类。

手工编写完线程池后，我们看看怎么使用：



public class ThreadPoolTest
    public static void main(String args[]) throws InterruptedException {
        //定义线程池，初始化线程数为2，核心线程数为4，最大线程数为6，任务队列最多允许1000个任务
        final ThreadPool threadPool = new BasicThreadPool(2, 6, 4, 1000);
        //定义20个任务并提交给线程池
        for (int i = 0; i < 20; i++) {
            threadPool.execute(() -> {
                try {
                    TimeUnit.SECONDS.sleep(10);
                    System.out.println(Thread.currentThread().getName() + " is running and done.");
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            });
        }
    }
}


以上测试代码，我们通过初始化2个线程、核心线程数为4，最大为6，然后向该线程池提交20个任务，执行结果如下：



thread-pool-0 is running and done.
thread-pool--1 is running and done.
thread-pool--2 is running and done.
thread-pool--3 is running and done.
thread-pool-0 is running and done.
thread-pool--1 is running and done.
thread-pool--2 is running and done.
thread-pool--3 is running and done.
thread-pool--1 is running and done.
thread-pool-0 is running and done.
thread-pool--3 is running and done.
thread-pool--2 is running and done.
thread-pool--1 is running and done.
thread-pool-0 is running and done.
thread-pool--3 is running and done.
thread-pool--2 is running and done.
thread-pool-0 is running and done.
thread-pool--1 is running and done.
thread-pool--2 is running and done.
thread-pool--3 is running and done.


从运行结果看，由于提交速度比较快，线程池扩容到了其核心线程的数量，总共4个线程，然后这些线程逐步完成了20个任务的执行，从而实现了线程的重复使用。



以上代码的github地址如下：

https://github.com/manongwudi/java-thread.git。

通过手工编写线程池的目的只是为了让大家更好地理解线程池的实现原理，实际上在JDK1.5以后在"java.util.concurrent(简称JUC)"中已经提供了多种版本的线程池实现，所以在JAVA中使用线程池时，我们只需要选择合适的线程池类型即可，而这些线程池的实现也基本上与我们手工编写的线程池原理类似。



Java自带线程池





在Java中通过Executor框架提供线程池支持，通过该框架我们可以创建出如下几类线程池：





按照线程池的核心实现类的不同派生，Java中共提供了5种现成的线程池。



1、newSingleThreadExecutor



是单个工作线程的Executor，它的corePoolSize和maximumPoolSize被设置为1。采用的是无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为Interger.MAX_VALUE）。由于使用了无界队列，如果请求过多会导致OOM，在并发请求量比较大的系统中，使用此线程池需要注意。



public class SingleThreadExecutorDemo {

    public static void main(String args[]) {
        ExecutorService pool = Executors.newSingleThreadExecutor();
        for (int i = 0; i <= 20; i++) {
            pool.execute(() -> System.out.println(Thread.currentThread().getName() + "[running done]"));
        }
    }
}


2、newFixedThreadPool



被称为可重用固定线程数线程池。与SingleThreadExecutor一样它也使用了无界队列作为工作队列，如果没有执行方法shutdown()的话也是不会拒绝任务的。



public class FixThreadPoolDemo {
    public static void main(String args[]) {
        ExecutorService pool = Executors.newFixedThreadPool(10);
        for (int i = 0; i <= 100; i++) {
            pool.execute(() -> {
                try {
                    Thread.sleep(10);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                System.out.println(Thread.currentThread().getName() + "[runing done]");
            });
        }
    }
}


3、newCachedThreadPool



是一个会根据需要创建新线程的线程池。它的corePoolSize被设置为0，即corePool为空；maximumPoolSize被设置为Integer.MAX_VALUE,即maximumPool是无界的，正因为如此，如果主线程提交任务的速度高于线程池中线程处理任务的速度的话，线程池就会不断创建新的线程，极端情况下就可能导致线程创建过多而耗尽CPU和内存资源。



public class CacheThreadPoolDemo {
    public static void main(String args[]) {
        ExecutorService pool = Executors.newCachedThreadPool();
        for (int i = 0; i <= 20; i++) {
            pool.execute(() -> {
                System.out.println(Thread.currentThread().getName() + "[runing done]");
            });
        }
    }
}


4、newScheduledThreadPool



用于实现多个线程的周期性任务，它会把待调度的任务放到延迟队列DelayQueue中。与CacheThreadPool一样，它允许创建的最大线程数也是Interger.MAX_VALUE。



public class ScheduledThreadPoolExecutorDemo {
    public static void main(String args[]) {
        ScheduledExecutorService pool = Executors.newScheduledThreadPool(10);
        for (int i = 0; i <= 20; i++) {
            pool.schedule(() -> {
                System.out.println(Thread.currentThread().getName() + "[runing done]");
            }, 10, TimeUnit.SECONDS);
        }
    }
}
以上逻辑实现的是：延迟10秒后开始执行任务。



5、newSingleThreadScheduledExecutor



只包含一个线程的ThreadScheduleExecutor。



public class SingleThreadScheduledExecutorDemo {
    public static void main(String args[]) {
        ScheduledExecutorService pool = Executors.newSingleThreadScheduledExecutor();
        for (int i = 0; i <= 20; i++) {
            pool.scheduleAtFixedRate(() -> {
                System.out.println(Thread.currentThread().getName() + "[runing done]");
            }, 1, 1, TimeUnit.SECONDS);
        }
    }
}
以上逻辑实现的是：每一秒钟执行一次任务。



6、newWorkStealingPool



该线程池是jdk1.8以后新增的，底层采用ForkJoinPool来实现，类似于Fork-Join框架所支持的功能。



public class WorkStealingPoolDemo {
    public static void main(String args[]) {
        ExecutorService pool = Executors.newWorkStealingPool(10);
        for (int i = 0; i <= 20; i++) {
            pool.execute(() -> {
                System.out.println(Thread.currentThread().getName() + "[running done]");
            });
        }
    }
}


JDK自带的线程池大家可以根据场景选用，在阿里的开发手册中要求在实现线程池时明确的通过ThreadPoolExecutor去自行创建，并要求使用有界队列作为线程池的工作队列，同时对线程池允许创建的最大线程数也要限制，因为以上几个线程池都存在对资源使用没有限制的问题，所以大家还是根据实际情况来判断吧！


==========
【干货走一波】千万级用户的大型网站，应该如何设计其高并发架构？
原创： 原子弹大侠  狸猫技术窝  前天
目录

（1）单块架构

（2）初步的高可用架构

（3）千万级用户量的压力预估

（4）服务器压力预估

（5）业务垂直拆分

（6）用分布式缓存抗下读请求

（7）基于数据库主从架构做读写分离

（8）总结

本文将会从一个大型的网站发展历程出发，一步一步的探索这个网站的架构是如何从单体架构，演化到分布式架构，然后演化到高并发架构的。






（1）单块架构



一般一个网站刚开始建立的时候，用户量是很少的，大概可能就几万或者几十万的用户量，每天活跃的用户可能就几百或者几千个。



这个时候一般网站架构都是采用单体架构来设计的，总共就部署3台服务器，1台应用服务器，1台数据库服务器，1台图片服务器。



研发团队通常都在10人以内，就是在一个单块应用里写代码，然后写好之后合并代码，接着就是直接在线上的应用服务器上发布。很可能就是手动把应用服务器上的Tomcat给关掉，然后替换系统的代码war包，接着重新启动Tomcat。



数据库一般就部署在一台独立的服务器上，存放网站的全部核心数据。



然后在另外一台独立的服务器上部署NFS作为图片服务器，存放网站的全部图片。应用服务器上的代码会连接以及操作数据库以及图片服务器。如下图所示：







（2）初步的高可用架构



但是这种纯单块系统架构下，有高可用问题存在，最大的问题就是应用服务器可能会故障，或者是数据库可能会故障



所以在这个时期，一般稍微预算充足一点的公司，都会做一个初步的高可用架构出来。



对于应用服务器而言，一般会集群化部署。当然所谓的集群化部署，在初期用户量很少的情况下，其实一般也就是部署两台应用服务器而已，然后前面会放一台服务器部署负载均衡设备，比如说LVS，均匀的把用户请求打到两台应用服务器上去。



如果此时某台应用服务器故障了，还有另外一台应用服务器是可以使用的，这样就避免了单点故障问题。如下图所示：



对于数据库服务器而言，此时一般也会使用主从架构，部署一台从库来从主库同步数据，这样一旦主库出现问题，可以迅速使用从库继续提供数据库服务，避免数据库故障导致整个系统都彻底故障不可用。如下图：







（3）千万级用户量的压力预估



这个假设这个网站预估的用户数是1000万，那么根据28法则，每天会来访问这个网站的用户占到20%，也就是200万用户每天会过来访问。



通常假设平均每个用户每次过来会有30次的点击，那么总共就有6000万的点击（PV）。



每天24小时，根据28法则，每天大部分用户最活跃的时间集中在（24小时 * 0.2）≈ 5小时内，而大部分用户指的是（6000万点击 * 0.8 ≈ 5000万点击）



也就是说，在5小时内会有5000万点击进来。



换算下来，在那5小时的活跃访问期内，大概每秒钟会有3000左右的请求量，然后这5小时中可能又会出现大量用户集中访问的高峰时间段。



比如在集中半个小时内大量用户涌入形成高峰访问。根据线上经验，一般高峰访问是活跃访问的2~3倍。假设我们按照3倍来计算，那么5小时内可能有短暂的峰值会出现每秒有10000左右的请求。







（4）服务器压力预估



大概知道了高峰期每秒钟可能会有1万左右的请求量之后，来看一下系统中各个服务器的压力预估。



一般来说一台虚拟机部署的应用服务器，上面放一个Tomcat，也就支撑最多每秒几百的请求。



按每秒支撑500的请求来计算，那么支撑高峰期的每秒1万访问量，需要部署20台应用服务。



而且应用服务器对数据库的访问量又是要翻几倍的，因为假设一秒钟应用服务器接收到1万个请求，但是应用服务器为了处理每个请求可能要涉及到平均3~5次数据库的访问。



按照3次数据库访问来算，那么每秒会对数据库形成3万次的请求。



按照一台数据库服务器最高支撑每秒5000左右的请求量，此时需要通过6台数据库服务器才能支撑每秒3万左右的请求。



图片服务器的压力同样会很大，因为需要大量的读取图片展示页面，这个不太好估算，但是大致可以推算出来每秒至少也会有几千次请求，因此也需要多台图片服务器来支撑图片访问的请求。







（5）业务垂直拆分



一般来说在这个阶段要做的第一件事儿就是业务的垂直拆分



因为如果所有业务代码都混合在一起部署，会导致多人协作开发时难以维护。在网站到了千万级用户的时候，研发团队一般都有几十人甚至上百人。



所以这时如果还是在一个单块系统里做开发，是一件非常痛苦的事情，此时需要做的就是进行业务的垂直拆分，把一个单块系统拆分为多个业务系统，然后一个小团队10个人左右就专门负责维护一个业务系统。如下图







（6）分布式缓存扛下读请求



这个时候应用服务器层面一般没什么大问题，因为无非就是加机器就可以抗住更高的并发请求。



现在估算出来每秒钟是1万左右的请求，部署个二三十台机器就没问题了。



但是目前上述系统架构中压力最大的，其实是数据库层面 ，因为估算出来可能高峰期对数据库的读写并发会有3万左右的请求。



此时就需要引入分布式缓存来抗下对数据库的读请求压力了，也就是引入Redis集群。



一般来说对数据库的读写请求也大致遵循28法则，所以每秒3万的读写请求中，大概有2.4万左右是读请求



这些读请求基本上90%都可以通过分布式缓存集群来抗下来，也就是大概2万左右的读请求可以通过 Redis集群来抗住。



我们完全可以把热点的、常见的数据都在Redis集群里放一份作为缓存，然后对外提供缓存服务。



在读数据的时候优先从缓存里读，如果缓存里没有，再从数据库里读取。这样2万读请求就落到Redis上了，1万读写请求继续落在数据库上。



Redis一般单台服务器抗每秒几万请求是没问题的，所以Redis集群一般就部署3台机器，抗下每秒2万读请求是绝对没问题的。如下图所示：





（7）基于数据库主从架构做读写分离



此时数据库服务器还是存在每秒1万的请求，对于单台服务器来说压力还是过大。



但是数据库一般都支持主从架构，也就是有一个从库一直从主库同步数据过去。此时可以基于主从架构做读写分离。



也就是说，每秒大概6000写请求是进入主库，大概还有4000个读请求是在从库上去读，这样就可以把1万读写请求压力分摊到两台服务器上去。

这么分摊过后，主库每秒最多6000写请求，从库每秒最多4000读请求，基本上可以勉强把压力给抗住。如下图：


（8）总结



本文主要是探讨在千万级用户场景下的大型网站的高并发架构设计，也就是预估出了千万级用户的访问压力以及对应的后台系统为了要抗住高并发，在业务系统、缓存、数据库几个层面的架构设计以及抗高并发的分析。


MQ、CDN、静态化、分库分表、NoSQL、搜索、分布式文件系统、反向代理，



为什么我如此注重并发？



因为随着公司发展，并发量及数据量肯定会增加，高性能、高并发的问题就避免不了。而且工作中，你总是绕不开并发编程的任务，比如说，你想写个程序，一边从文件中读取数据，一边还要做实时计算.....



另外，并发编程是Java语言中最为晦涩的知识点，它涉及操作系统、内存、CPU、编程语言等多方面的基础能力，而这些知识点看上去非常的零散、独立，可实则关联性又比较强，更为考验一个程序员的内功。



甚至可以说，只要你想进大厂，并发是必须跨过的一道“坎”。



不过，因为网上学习资料非常零散，也很少有人能系统讲清楚并发，所以想掌握并发，只能靠“自学”，却往往不得要领。



说到这里，我想给你推荐一张“并发编程”全景图。是由京东资深架构师王宝令凝聚他十几年经验制成的，从三个核心问题：分工、互斥、协作，全面且系统地涵盖了Java并发编程的技术难点。



我认为，这个全景图是回到并发的源头思考来问题，根据这个思路，可以举一反三，融会贯通。



并发编程全景图



这张全景图，出自宝令老师《Java并发编程实战》专栏，我也一直在跟着学习，让我觉得很厉害的是，他把并发编程涉及到的Java内存模型、管程、多线程等关键性问题讲得通俗易懂、深入浅出，时常让你有种“原来如此”的豁然开朗。甚至即使你是初学者，也能看懂每篇内容的逻辑及要点。



而且，每篇文章下的留言也特别精彩，有些是总结笔记，有些是提问，还有一些是课后作业的回答代码，跟着大家一起查缺补漏，我也收获良多。




其实，这个专栏上新的时候，我就推荐过了。有好的学习资料，我希望可以让粉丝中想想好好并发的用户知道。特意给大伙争取了《Java并发编程实战》专栏的24小时限时福利，参团价仅79元，原价99元，立省20元。








👆👆👆

扫描上方二维码，立即参团

立省20元，限时24小时





另外，这个专栏不断有用户呼吁涨价，据运营小姐姐说，涨价时间不确定，但这个拼团价格绝对是最低了。





也截了一些留言，供你参考。






第二个推荐的硬核课程：《深入拆解Java虚拟机》，原价99，限时优惠79，立省20。



JVM也是大厂常见的面试题之一，而且如果你想进阶资深Java程序员，虚拟机必然是你要深耕的一块内容。



郑雨迪这个专栏，我看到很多Java领域大神都推荐过，比如你假笨、江南白衣、开涛等等，自己也在跟着学，确实很硬核。



我记得，这个专栏上线不到3天，专栏订阅量都已经破1W订阅了，异常火爆。目前为止，这个专栏订阅量一直稳居极客时间 top 5，现在快2.5w+订阅。大概原因，我总结了两点：

1、Java虚拟机确实是面试大题；

2、作者是郑雨迪，Oracle Labs高级研究员，专攻Graal编译器，也在研究HotSpot虚拟机项目。



既然你要学JVM，那跟着Oracle内部专家学，是我想到最高效的方式。想认真进阶Java的同学，请不要错过这次的福利，也是专栏最优惠的时候了。



雨迪从底层出发，通过揭秘 Java虚拟机的工作原理，掌握诊断手法和调优方式。通过这个专栏的学习，你将了解如何编写高效的代码，如何对Bug达到最优处理，以及如何针对自己的应用调整虚拟机的运营参数。



如何模拟超过 5 万的并发用户
程序猿DD  1周前


来源：http://t.cn/ES7KBkW



本文将从负载测试的角度，描述了做一次流畅的5万用户并发测试需要做的事情.

你可以在本文的结尾部分看到讨论的记录.

快速的步骤概要

编写你的脚本

使用JMeter进行本地测试

BlazeMeter沙箱测试

使用一个控制台和一个引擎设置Users-per-Engine的数量

设置并测试你的集合 (1个控制台和10-14 引擎)

使用 Master / Slave 特性来达成你的最大CC目标



步骤1 : 编写你的脚本
开始之前，请确定从JMeter的Apache社区jmeter.apache.org 获得了最新的版本.

你也会要下载这些附加的插件 ，因为它们可以让你的工作更轻松.

有许多方法可以获得脚本:

使用 BlazeMeter 的 Chrome 扩展 来记录你的方案

使用 JMeter HTTP(S) 测试脚本记录器 来设置一个代理，那样你就可以运行你的测试并记录下所有的东西

从头开始全部手工构建(可能是功能/QA测试)

如果你的脚本是一份记录的结果(像步骤1&2), 请牢记:

你需要改变诸如Username & Password这样的特定参数，或者你也许会想要设置一个CSV文件，有了里面的值每个用户就可以是不同的.

为了完成诸如“添加到购物车”，“登录”还有其它这样的请求，你也许要使用正则表达式，JSON路径提取器，XPath提取器，来提取诸如Token字符串，表单构建ID还有其它要素

保持你的脚本参数化，并使用配置元素，诸如默认HTTP请求，来使得在环境之间切换时你的工作更轻松.

步骤2 : 使用JMeter进行本地测试
在1个线程的1个迭代中使用查看结果树要素，调试样本，虚拟样本还有打开的日志查看器（一些JMeter的错误会在里面报告），来调试你的脚本.

遍历所有的场景(包括True 或者 False的回应) 来确保脚本行为确如预期...

在成功使用一个线程测试之后——将其提高到10分钟10到20个线程继续测试:

如果你想要每个用户独立——是那样的么?

有没有收到错误?

如果你在做一个注册过程，那就看看你的后台 - 账户是不是照你的模板创建好了? 它们是不是独立的呢?

从总结报告中，你可以看到对测试的统计 - 它们有点用么? (平均响应时间, 错误, 每秒命中率)

一旦你准备好了脚本:

通过移除任何调试和虚拟样本来清理脚本，并删除你的脚本侦听器

如果你使用了侦听器(诸如 "将响应保存到一个文件")，请确保你没有使用任何路径! , 而如果他是一个侦听器或者一个CSV数据集配置——请确保你没有使用你在本地使用的路径 - 而只要文件名(就好像跟你的脚本在同一个文件夹)

如果你使用了自己专有的JAR文件，请确保它也被上传了.

如果你使用了超过一个线程组（不是默认的那个) - 请确保在将其上传到BlazeMeter之前设置了这个值.

步骤3 : BlazeMeter沙箱测试
如果那时你的第一个测试——你应该温习一下 这篇 有关如何在BlazeMeter中创建测试的文章.

将沙箱的测试配置设置成，用户300，1个控制台, 时间50分钟.

对沙箱进行这样的配置让你可以在后台测试你的脚本，并确保上的BlazeMeter的一切都运行完好.

为此，先按下灰色的按钮: 告诉JMeter引擎我想要完全控制! - 来获得对你的测试参数的完全控制

通常你将会遇到的问题:

防火墙 - 确保你的环境对BlazeMeter的CIDR 列表 (它们会实时更新)开发，并把它们放入白名单中

确保你所有的测试文件, 比如: CSVs, JAR, JSON, User.properties 等等.. 都可以使用

确保你没有使用任何路径

如果仍然有问题，那就看看错误日志吧(你应该可以把整个日志都下载下来).

一个沙箱的配置可以是这样的:

引擎: 是能使控制台(1 个控制台 , 0 个引擎)

线程: 50-300

产能提升: 20 分钟

迭代: 一直测试下去

时间: 30-50 分钟

这可以让你在产能提升期间获得足够多的数据(以防你遇到问题) ，而你将可以对结果进行分析，以确保脚本的执行确如预期.

你应该观察下Waterfall / WebDriver 选项卡来看看请求是否正常，你不应该在这一点上出任何问题（除非你是故意的).

你应该盯着监控选项卡，观察期内存和CPU消耗 - 这对你在步骤4中尝试设置每一个引擎的用户数量.

步骤4 : 使用1个控制台和1个引擎来设置每个引擎用户的数量
现在我们可以肯定脚本能在BlazeMeter中完美运行了——我们需要计算出要多少用户放到一个引擎中.

如果你能用户沙箱中的数据来做这个决定，那就太棒了!

在这里，我会给出一种不用回头去查看沙箱测试数据就能计算出这个数的方法.

设置你的测试配置:

线程数: 500

产能提升：40 分钟

迭代: 永久

时长: 50 分钟

使用一个控制台和一个引擎.

运行测试并(通过监视选项卡)对你的测试引擎进行监视.

如果你的引擎对于75%的CPI使用率和85%的内存使用率都没有达到(一次性的峰值可以忽略) 的话:

将线程数调整到700在测试一次

提交线程的数量直到线程数达到1000或者60%的CPU或内存使用

如果你的引擎过了75%的CPU使用率或者85%的内存使用率(一次性的峰值可以忽略 :

看看你第一次达到75%的点，在那个点有多少并发用户.

在运行一次测试, 而不是提高你之前500个用户数量的产能

这一次将产能提升放到真实的测试中(5-15 分钟是一个好的开始) 并将时长设置为50分钟.

确保整个测试过程中没有超过75%的CPU使用率或者85%的内存使用率...

为安全起见，你可以把每个引擎的线程数降低10%的.

步骤5：安装并测试集群
我们现在知道了从一个引擎中我们得到了多少线程，在该章节的最后，我们将会知道一个集群能给我们提供多少用户。

一个集群是指具有一个控制台（仅有一个）和0-14个引擎的逻辑容器。

即使你可以创建一个使用超过14个引擎的测试案例——但实际上是创建了两个集群（你可以注意到控制台的数量增加了），并且克隆了你的测试案例……

每个集群具有最多14个引擎，是基于BlazeMeter自己本身的测试，以确保控制台可以控制这14台引擎对新建的大量数据处理的压力。

所以在这一步骤中，我们会用步骤4种的测试，并且仅仅修改引擎数量，将其增加到14.

将该测试按照最终测试的全部时长运行。当测试在运行时，打开监听标签，并且检验：



没有一个引擎超过CPU75%的占有率和内存85%占有率的上限；





定位你的控制台标签（你可以通过一次点击Logs Tab->Network Information，查看控制台私有IP地址来找到它的名字）——它不应该达到CPU75%占有率和内存85%占有率的上限。



如果你的控制台达到了该上限——减少引擎数量并重新运行直到控制台在该上限之下。

在这个步骤的最后，你会发现：

每个集群的用户数量；

每个集群的命中率。

查看Aggretate Table中的其他统计信息，并找到本地结果统计图来获得有关你集群吞吐量的更多信息。

步骤 6 : 使用 Master / Slave 特性来达成你的最大CC目标
我们到了最后一步了。

我们知道脚本正在运行，我们也知道一个引擎可以支持多少用户以及一个集群可以支持多少用户。

让我们做一下假设：

一个引擎支持500用户

一个集群可以用户12个引擎

我们的目标是5万用户测试

因此为了完成这些，我们需要8.3 个集群..

我们可以用8个12台引擎的集群和一个4太引擎的集群 - 但是像下面这样分散负载应该会更好：

每个集群我们用10台引擎而不是12，那么每个集群可以支持 10*500 = 5K 用户并且我们需要10个集群来支持5万用户。

这样可以得到如下好处：

不用维护两个不同的测试类型

我们可以通过简单的复制现有集群来增加5K用户（5K比6K更常见）

只要需要我们可以一直增加

现在，我们已经准备好创建最终的5万用户级别的Master / Slave测试了：

将测试的名称从"My prod test" 改为"My prod test - slave 1"。

我们回到步骤5，将高级测试属性(Advanced Test Properties)下的Standalone修改为Slave。

按保存按钮——现在我们有了一个Master和9个Slave中的一个。

返回你的 "My prod test -slave 1".

按复制按钮

接下来重复步骤1-5直到你创建了9个slave。

回到你的 "My prod test -salve 9" 并按复制按钮.

将测试的名称改为 "My prod test -Master".

将高级测试属性(Advanced Test Properties) 下的Slave改为Master。

检查我们刚才创建的所有的Slave(My prod test -salve 1..9)并按保存。

你的5万用户级别的Master-Slave测试已经准备好了。通过按master上的开始按钮来运行10个测试，每个测试5千用户。

你可以修改任意一个测试（salve或master），让它们来自不同的区域，有不同的脚本/csv/以及其他文件，使用不同的网络模拟器，不同的参数等。

你可以在一个叫“Master load results”的master报告中的一个新tab页中找到生成的聚合结果的报告，你还可以通过打开单个的报告来独立的查看每一个测试结果。



90%的Java工程师都不了解的线程池细节问题！
crossoverJie  石杉的架构笔记  5天前
公众号后台回复 “资料”

获取作者独家秘制学习资料

线程池的工作原理
首先复习下线程池的基本原理,我认为线程池它就是一个调度任务的工具。



众所周知，在初始化线程池会给定线程池的大小，假设现在我们有 1000 个线程任务需要运行，而线程池的大小为 10~20。



在真正运行任务的过程中他肯定不会创建这1000个线程同时运行，而是充分利用线程池里这 10~20 个线程来调度这1000个任务。



而这里的 10~20 个线程最后会由线程池封装为 ThreadPoolExecutor.Worker 对象，而这个 Worker 是实现了 Runnable 接口的，所以他自己本身就是一个线程。



深入分析


这里我们来做一个模拟，创建了一个核心线程、最大线程数、阻塞队列都为2的线程池。



这里假设线程池已经完成了预热，也就是线程池内部已经创建好了两个线程 Worker。



当我们往一个线程池丢一个任务会发生什么事呢？



第一步是生产者，也就是任务提供者他执行了一个 execute() 方法，本质上就是往这个内部队列里放了一个任务。


之前已经创建好了的 Worker 线程会执行一个 while 循环 ---> 不停的从这个 内部队列里获取任务。(这一步是竞争的关系，都会抢着从队列里获取任务，由这个队列内部实现了线程安全)


获取得到一个任务后，其实也就是拿到了一个 Runnable 对象(也就是 execute(Runnabletask) 这里所提交的任务)，接着执行这个 Runnable 的 run() 方法，而不是 start()。这点需要注意。后文分析原因。



结合源码来看：



从图中其实就对应了刚才提到的二三两步：

while 循环，从 getTask() 方法中一直不停的获取任务。

拿到任务后，执行它的 run() 方法。


这样一个线程就调度完毕，然后再次进入循环从队列里取任务并不断的进行调度。



再次解释之前的问题
接下来回顾一下以前提到的问题：导致一个线程没有运行的根本原因是？

在单个线程的线程池中一旦抛出了未被捕获的异常时，线程池会回收当前的线程，并创建一个新的 Worker



它也会一直不断的从队列里获取任务来执行，但由于这是一个消费线程，根本没有生产者往里边丢任务，因此它会一直 waiting 在从队列里获取任务处。



所以也就造成了线上的队列没有消费，业务线程池没有执行的问题。



结合之前的那张图来看：



这里大家问的最多的一个点是，为什么会没有生产者往里面丢任务？图中不是明明画的有一个 product 嘛？



这里确实是有些不太清楚，再次强调一次：图中的 product 是往内部队列里写消息的生产者，并不是往这个 Consumer 所在的线程池中写任务的生产者。



因为即便 Consumer 是一个单线程的线程池，它依然具有一个常规线程池所具备的所有条件：

Worker 调度线程，也就是线程池运行的线程；虽然只有一个。

内部的阻塞队列；虽然长度只有1。


再次结合图来看：



所以之前提到的【没有生产者往里边丢任务】是指右图放大后的那一块，也就是内部队列并没有其他线程往里边丢任务执行 execute() 方法。



而一旦发生未捕获的异常后， Worker1 被回收，顺带的它所调度的线程 task1（这个task1 也就是在执行一个 while 循环消费左图中的那个队列） 也会被回收掉。



新创建的 Worker2 会取代 Worker1 继续执行 while 循环从内部队列里获取任务，但此时这个队列就一直会是空的，所以也就是处于 Waiting 状态。



为什是 run() 而不是 start() ？

问题搞清楚后来想想为什么线程池在调度的时候执行的是 Runnable 的 run() 方法，而不是 start() 方法呢？



我相信大部分没有看过源码的同学心中第一个印象就应该是执行的 start() 方法；



因为不管是学校老师，还是网上大牛讲的都是只有执行了 start() 方法后操作系统才会给我们创建一个独立的线程来运行，而 run() 方法只是一个普通的方法调用。



而在线程池这个场景中却恰好就是要利用它只是一个普通方法调用。



回到我在文初中所提到的：我认为线程池它就是一个调度任务的工具。



假设这里是调用的 Runnable 的 start 方法，那会发生什么事情？



如果我们往一个核心、最大线程数为 2 的线程池里丢了 1000 个任务，那么它会额外的创建 1000 个线程，同时每个任务都是异步执行的，一下子就执行完毕了。



这样就没法做到由这两个 Worker 线程来调度这 1000 个任务，而只有当做一个同步阻塞的 run() 方法调用时才能满足这个要求。






迄今为止把同步/异步/阻塞/非阻塞/BIO/NIO/AIO讲的这么清楚的好文章
编程新说李新杰  Java团长  前天


来源：编程新说

网上有很多讲同步/异步/阻塞/非阻塞/BIO/NIO/AIO的文章，但是都没有达到我的心里预期，于是自己写一篇出来。






常规的误区


假设有一个展示用户详情的需求，分两步，先调用一个HTTP接口拿到详情数据，然后使用适合的视图展示详情数据。

如果网速很慢，代码发起一个HTTP请求后，就卡住不动了，直到十几秒后才拿到HTTP响应，然后继续往下执行。

这个时候你问别人，刚刚代码发起的这个请求是不是一个同步请求，对方一定回答是。这是对的，它确实是。

但你要问它为什么是呢？对方一定是这样回答的，“因为发起请求后，代码就卡住不动了，直到拿到响应后才可以继续往下执行”。

我相信很多人也都是这样认为的，其实这是不对的，是把因果关系搞反了：

不是因为代码卡住不动了才叫同步请求，而是因为它是同步请求所以代码才卡住不动了。

至于为什么能卡住不动，这是由操作系统和CPU决定的：

因为内核空间里的对应函数会卡住不动，造成用户空间发起的系统调用卡住不动，继而使程序里的用户代码卡住不动了。

因此卡住不动了只是同步请求的一个副作用，并不能用它来定义同步请求，那该如何定义呢？


同步和异步


所谓同步，指的是协同步调。既然叫协同，所以至少要有2个以上的事物存在。协同的结果就是：

多个事物不能同时进行，必须一个一个的来，上一个事物结束后，下一个事物才开始。

那当一个事物正在进行时，其它事物都在干嘛呢？

严格来讲这个并没有要求，但一般都是处于一种“等待”的状态，因为通常后面事物的正常进行都需要依赖前面事物的结果或前面事物正在使用的资源。

因此，可以认为，同步更希望关注的是从宏观整体来看，多个事物是一种逐个逐个的串行化关系，绝对不会出现交叉的情况。

所以，自然也不太会去关注某个瞬间某个具体事物是处于一个什么状态。

把这个理论应用的出神入化的非“排队”莫属。凡是在资源少需求多的场景下都会用到排队。

比如排队买火车票这件事：

其实售票大厅更在意的是旅客一个一个的到窗口去买票，因为一次只能卖一张票。

即使大家一窝蜂的都围上去，还是一次只能卖一张票，何必呢？挤在一起又不安全。

只是有些人素质太差，非要往上挤，售票大厅迫不得已，采用排队这种形式来达到自己的目的，即一个一个的买票。

至于每个旅客排队时的状态，是看手机呀还是说话呀，根本不用去在意。


除了这种由于资源导致的同步外，还存在一种由于逻辑上的先后顺序导致的同步。

比如，先更新代码，然后再编译，接着再打包。这些操作由于后一步要使用上一步的结果，所以只能按照这种顺序一个一个的执行。

关于同步还需知道两个小的点：

一是范围，并不需要在全局范围内都去同步，只需要在某些关键的点执行同步即可。

比如食堂只有一个卖饭窗口，肯定是同步的，一个人买完，下一个人再买。但吃饭的时候也是一个人吃完，下一个人才开始吃吗？当然不是啦。

二是粒度，并不是只有大粒度的事物才有同步，小粒度的事物也有同步。



只不过小粒度的事物同步通常是天然支持的，而大粒度的事物同步往往需要手工处理。

比如两个线程的同步就需要手工处理，但一个线程里的两个语句天然就是同步的。

所谓异步，就是步调各异。既然是各异，那就是都不相同。所以结果就是：

多个事物可以你进行你的、我进行我的，谁都不用管谁，所有的事物都在同时进行中。



一言以蔽之，同步就是多个事物不能同时开工，异步就是多个事物可以同时开工。



注：一定要去体会“多个事物”，多个线程是多个事物，多个方法是多个事物，多个语句是多个事物，多个CPU指令是多个事物。等等等等。





阻塞和非阻塞





所谓阻塞，指的是阻碍堵塞。它的本意可以理解为由于遇到了障碍而造成的动弹不得。



所谓非阻塞，自然是和阻塞相对，可以理解为由于没有遇到障碍而继续畅通无阻。

对这两个词最好的诠释就是，当今中国一大交通难题，堵车：

汽车可以正常通行时，就是非阻塞。一旦堵上了，全部趴窝，一动不动，就是阻塞。



因此阻塞关注的是不能动，非阻塞关注的是可以动。



不能动的结果就是只能等待，可以动的结果就是继续前行。

因此和阻塞搭配的词一定是等待，和非阻塞搭配的词一定是进行。

回到程序里，阻塞同样意味着停下来等待，非阻塞表明可以继续向下执行。


阻塞和等待





等待只是阻塞的一个副作用而已，表明随着时间的流逝，没有任何有意义的事物发生或进行。



阻塞的真正含义是你关心的事物由于某些原因无法继续进行，因此让你等待。但没必要干等，你可以做一些其它无关的事物，因为这并不影响你对相关事物的等待。

在堵车时，你可以干等。也可以玩手机、和别人聊天，或者打牌、甚至先去吃饭都行。因为这些事物并不影响你对堵车的等待。不过你的车必须呆在原地。

在计算机里，是没有人这么灵活的，一般在阻塞时，选在干等，因为这最容易实现，只需要挂起线程，让出CPU即可。在条件满足时，会重新调度该线程。


两两组合


所谓同步/异步，关注的是能不能同时开工。

所谓阻塞/非阻塞，关注的是能不能动。

通过推理进行组合：

同步阻塞，不能同时开工，也不能动。只有一条小道，一次只能过一辆车，可悲的是还TMD的堵上了。

同步非阻塞，不能同时开工，但可以动。只有一条小道，一次只能过一辆车，幸运的是可以正常通行。

异步阻塞，可以同时开工，但不可以动。有多条路，每条路都可以跑车，可气的是全都TMD的堵上了。

异步非阻塞，可以工时开工，也可以动。有多条路，每条路都可以跑车，很爽的是全都可以正常通行。

是不是很容易理解啊。其实它们的关注点是不同的，只要搞明白了这点，组合起来也不是事儿。

回到程序里，把它们和线程关联起来：

同步阻塞，相当于一个线程在等待。

同步非阻塞，相当于一个线程在正常运行。

异步阻塞，相当于多个线程都在等待。

异步非阻塞，相当于多个线程都在正常运行。





I/O





IO指的就是读入/写出数据的过程，和等待读入/写出数据的过程。一旦拿到数据后就变成了数据操作了，就不是IO了。

拿网络IO来说，等待的过程就是数据从网络到网卡再到内核空间。读写的过程就是内核空间和用户空间的相互拷贝。



所以IO就包括两个过程，一个是等待数据的过程，一个是读写（拷贝）数据的过程。而且还要明白，一定不能包括操作数据的过程。





阻塞IO和非阻塞IO





应用程序都是运行在用户空间的，所以它们能操作的数据也都在用户空间。按照这样子来理解，只要数据没有到达用户空间，用户线程就操作不了。



如果此时用户线程已经参与，那它一定会被阻塞在IO上。这就是常说的阻塞IO。用户线程被阻塞在等待数据上或拷贝数据上。



非阻塞IO就是用户线程不参与以上两个过程，即数据已经拷贝到用户空间后，才去通知用户线程，一上来就可以直接操作数据了。



用户线程没有因为IO的事情出现阻塞，这就是常说的非阻塞IO。





同步IO和同步阻塞IO





按照上文中对同步的理解，同步IO是指发起IO请求后，必须拿到IO的数据才可以继续执行。

按照程序的表现形式又分为两种：

在等待数据的过程中，和拷贝数据的过程中，线程都在阻塞，这就是同步阻塞IO。

在等待数据的过程中，线程采用死循环式轮询，在拷贝数据的过程中，线程在阻塞，这其实还是同步阻塞IO。



网上很多文章把第二种归为同步非阻塞IO，这肯定是错误的，它一定是阻塞IO，因为拷贝数据的过程，线程是阻塞的。

严格来讲，在IO的概念上，同步和非阻塞是不可能搭配的，因为它们是一对相悖的概念。

同步IO意味着必须拿到IO的数据，才可以继续执行。因为后续操作依赖IO数据，所以它必须是阻塞的。

非阻塞IO意味着发起IO请求后，可以继续往下执行。说明后续执行不依赖于IO数据，所以它肯定不是同步的。

因此，在IO上，同步和非阻塞是互斥的，所以不存在同步非阻塞IO。但同步非阻塞是存在的，那不叫IO，叫操作数据了。



所以，同步IO一定是阻塞IO，同步IO也就是同步阻塞IO。





异步IO和异步阻塞/非阻塞IO




按照上文中对异步的理解，异步IO是指发起IO请求后，不用拿到IO的数据就可以继续执行。



用户线程的继续执行，和操作系统准备IO数据的过程是同时进行的，因此才叫做异步IO。

按照IO数据的两个过程，又可以分为两种：

在等待数据的过程中，用户线程继续执行，在拷贝数据的过程中，线程在阻塞，这就是异步阻塞IO。

在等待数据的过程中，和拷贝数据的过程中，用户线程都在继续执行，这就是异步非阻塞IO。



第一种情况是，用户线程没有参与数据等待的过程，所以它是异步的。但用户线程参与了数据拷贝的过程，所以它又是阻塞的。合起来就是异步阻塞IO。

第二种情况是，用户线程既没有参与等待过程也没有参与拷贝过程，所以它是异步的。当它接到通知时，数据已经准备好了，它没有因为IO数据而阻塞过，所以它又是非阻塞的。合起来就是异步非阻塞IO。



PS：聪明的你或许发现了我没有提多路复用IO，因为它值得专门撰文一篇。

======================

java并发编程系列：wait/notify机制
小孩子4919  程序员小灰  今天
本文转载自公众号  我们都是小青蛙



如果一个线程从头到尾执行完也不和别的线程打交道的话，那就不会有各种安全性问题了。但是协作越来越成为社会发展的大势，一个大任务拆成若干个小任务之后，各个小任务之间可能也需要相互协作最终才能执行完整个大任务。所以各个线程在执行过程中可以相互通信，所谓通信就是指相互交换一些数据或者发送一些控制指令，比如一个线程给另一个暂停执行的线程发送一个恢复执行的指令，下边详细看都有哪些通信方式。



volatile和synchronized


可变共享变量是天然的通信媒介，也就是说一个线程如果想和另一个线程通信的话，可以修改某个在多线程间共享的变量，另一个线程通过读取这个共享变量来获取通信的内容。



由于原子性操作、内存可见性和指令重排序的存在，java提供了volatile和synchronized的同步手段来保证通信内容的正确性，假如没有这些同步手段，一个线程的写入不能被另一个线程立即观测到，那这种通信就是不靠谱的～



wait/notify机制


故事背景


也不知道是那个遭天杀的给我们学校厕所的坑里塞了个塑料瓶，导致楼道里如黄河泛滥一般，臭味熏天。更加悲催的是整个楼只有这么一个厕所，比这个更悲催的是这个厕所里只有一个坑！！！！！好吧，让我们用java来描述一下这个厕所：



public class Washroom {

    private volatile boolean isAvailable = false;    //表示厕所是否是可用的状态

    private Object lock = new Object(); //厕所门的锁

    public boolean isAvailable() {
        return isAvailable;
    }

    public void setAvailable(boolean available) {
        this.isAvailable = available;
    }

    public Object getLock() {
        return lock;
    }
}


isAvailable字段代表厕所是否可用，由于厕所损坏，默认是false的，lock字段代表这个厕所门的锁。需要注意的是isAvailable字段被volatile修饰，也就是说有一个线程修改了它的值，它可以立即对别的线程可见～



由于厕所资源宝贵，英明的学校领导立即拟定了一个修复任务：



public class RepairTask implements Runnable {

    private Washroom washroom;

    public RepairTask(Washroom washroom) {
        this.washroom = washroom;
    }

    @Override
    public void run() {

        synchronized (washroom.getLock()) {
            System.out.println("维修工 获取了厕所的锁");
            System.out.println("厕所维修中，维修厕所是一件辛苦活，需要很长时间。。。");

            try {
                Thread.sleep(5000L);    //用线程sleep表示维修的过程
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
            washroom.setAvailable(true);        //维修结束把厕所置为可用状态
            System.out.println("维修工把厕所修好了，准备释放锁了");
        }
    }
}


这个维修计划的内容就是当维修工进入厕所之后，先把门锁上，然后开始维修，维修结束之后把Washroom的isAvailable字段设置为true，以表示厕所可用。



与此同时，一群急得像热锅上的蚂蚁的家伙在厕所门前打转转，他们想做神马不用我明说了吧😏😏：



public class ShitTask implements Runnable {

    private Washroom washroom;

    private String name;

    public ShitTask(Washroom washroom, String name) {
        this.washroom = washroom;
        this.name = name;
    }

    @Override
    public void run() {
        synchronized (washroom.getLock()) {
            System.out.println(name + " 获取了厕所的锁");
            while (!washroom.isAvailable()) {
                // 一直等
            }
            System.out.println(name + " 上完了厕所");
        }
    }
}


这个ShitTask描述了上厕所的一个流程，先获取到厕所的锁，然后判断厕所是否可用，如果不可用，则在一个死循环里不断的判断厕所是否可用，直到厕所可用为止，然后上完厕所释放锁走人。



然后我们看看现实世界都发生了什么吧：



public class Test {
    public static void main(String[] args) {
        Washroom washroom = new Washroom();
        new Thread(new RepairTask(washroom), "REPAIR-THREAD").start();

        try {
            Thread.sleep(1000L);
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }

        new Thread(new ShitTask(washroom, "狗哥"), "BROTHER-DOG-THREAD").start();
        new Thread(new ShitTask(washroom, "猫爷"), "GRANDPA-CAT-THREAD").start();
        new Thread(new ShitTask(washroom, "王尼妹"), "WANG-NI-MEI-THREAD").start();
    }
}


学校先让维修工进入厕所维修，然后包括狗哥、猫爷、王尼妹在内的上厕所大军就开始围着厕所打转转的旅程，我们看一下执行结果：



维修工 获取了厕所的锁
厕所维修中，维修厕所是一件辛苦活，需要很长时间。。。
维修工把厕所修好了，准备释放锁了
王尼妹 获取了厕所的锁
王尼妹 上完了厕所
猫爷 获取了厕所的锁
猫爷 上完了厕所
狗哥 获取了厕所的锁
狗哥 上完了厕所


看起来没有神马问题，但是再回头看看代码，发现有两处特别别扭的地方：



在main线程开启REPAIR-THREAD线程后，必须调用sleep方法等待一段时间才允许上厕所线程开启。



如果REPAIR-THREAD线程和其他上厕所线程一块儿开启的话，就有可能上厕所的人，比如狗哥先获取到厕所的锁，然后维修工压根儿连厕所也进不去。但是真实情况可能真的这样的，狗哥先到了厕所，然后维修工才到。不过狗哥的处理应该不是一直待在厕所里，而是先出来等着，啥时候维修工说修好了他再进去。所以这点有些别扭～



在一个上厕所的人获取到厕所的锁的时候，必须不断判断Washroom的isAvailable字段是否为true。



如果一个人进入到厕所发现厕所仍然处在不可用状态的话，那它应该在某个地方休息，啥时候维修工把厕所修好了，再叫一下等着上厕所的人就好了嘛，没必要自己不停的去检查厕所是否被修好了。



总结一下，就是一个线程在获取到锁之后，如果指定条件不满足的话，应该主动让出锁，然后到专门的等待区等待，直到某个线程完成了指定的条件，再通知一下在等待这个条件完成的线程，让它们继续执行。



如果你觉得上边这句话比较绕的话，我来给你翻译一下：当上狗哥获取到厕所门锁之后，如果厕所处于不可用状态，那就主动让出锁，然后到等待上厕所的队伍里排队`等待`，直到维修工把厕所修理好，把厕所的状态置为可用后，维修工再通知需要上厕所的人，然他们正常上厕所。


具体使用方式


为了实现这个构想，java里提出了一套叫wait/notify的机制。当一个线程获取到锁之后，如果发现条件不满足，那就主动让出锁，然后把这个线程放到一个等待队列里等待去，等到某个线程把这个条件完成后，就通知等待队列里的线程他们等待的条件满足了，可以继续运行啦！



如果不同线程有不同的等待条件肿么办，总不能都塞到同一个等待队列里吧？是的，java里规定了每一个锁都对应了一个等待队列，也就是说如果一个线程在获取到锁之后发现某个条件不满足，就主动让出锁然后把这个线程放到与它获取到的锁对应的那个等待队列里，另一个线程在完成对应条件时需要获取同一个锁，在条件完成后通知它获取的锁对应的等待队列。这个过程意味着锁和等待队列建立了一对一关联。



怎么让出锁并且把线程放到与锁关联的等待队列中以及怎么通知等待队列中的线程相关条件已经完成java已经为我们规定好了。我们知道，锁其实就是个对象而已，在所有对象的老祖宗类Object中定义了这么几个方法：



public final void wait() throws InterruptedException
public final void wait(long timeout) throws InterruptedException
public final void wait(long timeout, int nanos) throws InterruptedException

public final void notify();
public final void notifyAll();

各个方法的详细说明如下：



方法名	说明
wait()	在线程获取到锁后，调用锁对象的本方法，线程释放锁并且把该线程放置到与锁对象关联的等待队列
wait(long timeout)	与wait()方法相似，只不过等待指定的毫秒数，如果超过指定时间则自动把该线程从等待队列中移出
wait(long timeout, int nanos)	与上边的一样，只不过超时时间粒度更小，即指定的毫秒数加纳秒数
notify()	通知一个在与该锁对象关联的等待队列的线程，使它从wait()方法中返回继续往下执行
notifyAll()	与上边的类似，只不过通知该等待队列中的所有线程


了解了这些方法的意思以后我们再来改写一下ShitTask：



public class ShitTask implements Runnable {

    // ... 为节省篇幅，省略相关字段和构造方法

    @Override
    public void run() {
        synchronized (washroom.getLock()) {
            System.out.println(name + " 获取了厕所的锁");
            while (!washroom.isAvailable()) {
                try {
                    washroom.getLock().wait();  //调用锁对象的wait()方法，让出锁，并把当前线程放到与锁关联的等待队列
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            }
            System.out.println(name + " 上完了厕所");
        }
    }
}


看，原来我们在判断厕所是否可用的死循环里加了这么一段代码：



washroom.getLock().wait();


这段代码的意思就是让出厕所的锁，并且把当前线程放到与厕所的锁相关联的等待队列里。



然后我们也需要修改一下维修任务：



public class RepairTask implements Runnable {

    // ... 为节省篇幅，省略相关字段和构造方法

    @Override
    public void run() {

        synchronized (washroom.getLock()) {
            System.out.println("维修工 获取了厕所的锁");
            System.out.println("厕所维修中，维修厕所是一件辛苦活，需要很长时间。。。");

            try {
                Thread.sleep(5000L);    //用线程sleep表示维修的过程
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
            washroom.setAvailable(true);    //维修结束把厕所置为可用状态

            washroom.getLock().notifyAll(); //通知所有在与锁对象关联的等待队列里的线程，它们可以继续执行了
            System.out.println("维修工把厕所修好了，准备释放锁了");
        }
    }
}


大家可以看出来，我们在维修结束后加了这么一行代码：



washroom.getLock().notifyAll();


这个代码表示将通知所有在与锁对象关联的等待队列里的线程，它们可以继续执行了。



在使用java的wait/notify机制修改了ShitTask和RepairTask后，我们在复原一下整个现实场景：



public class Test {
    public static void main(String[] args) {
        Washroom washroom = new Washroom();
        new Thread(new ShitTask(washroom, "狗哥"), "BROTHER-DOG-THREAD").start();
        new Thread(new ShitTask(washroom, "猫爷"), "GRANDPA-CAT-THREAD").start();
        new Thread(new ShitTask(washroom, "王尼妹"), "WANG-NI-MEI-THREAD").start();

        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }

        new Thread(new RepairTask(washroom), "REPAIR-THREAD").start();
    }
}


在这个场景中，我们可以刻意让着急上厕所的先到达了厕所，维修工最后抵达厕所，来看一下加了wait/notify机制的代码的执行结果是：



狗哥 获取了厕所的锁
猫爷 获取了厕所的锁
王尼妹 获取了厕所的锁
维修工 获取了厕所的锁
厕所维修中，维修厕所是一件辛苦活，需要很长时间。。。
维修工把厕所修好了，准备释放锁了
王尼妹 上完了厕所
猫爷 上完了厕所
狗哥 上完了厕所


从执行结果可以看出来，狗哥、猫爷、王尼妹虽然先到达了厕所并且获取到锁，但是由于厕所处于不可用状态，所以都先调用wait()方法让出了自己获得的锁，然后躲到与这个锁关联的等待队列里，直到维修工修完了厕所，通知了在等待队列中的狗哥、猫爷、王尼妹，他们才又开始继续执行上厕所的程序～



通用模式


经过上边的厕所案例，大家应该对wait/notify机制有了大致了解，下边我们总结一下这个机制的通用模式。首先看一下等待线程的通用模式：

获取对象锁。

如果某个条件不满足的话，调用锁对象的wait方法，被通知后仍要检查条件是否满足。

条件满足则继续执行代码。



通用的代码如下：



synchronized (对象) {
    处理逻辑（可选）
    while(条件不满足) {
        对象.wait();
    }
    处理逻辑（可选）
}


除了判断条件是否满足和调用wait方法以外的代码，其他的处理逻辑是可选的。



下边再来看通知线程的通用模式：

获得对象的锁。

完成条件。

通知在等待队列中的等待线程。



synchronized (对象) {
    完成条件
    对象.notifyAll();、
}
小贴士：别忘了同步方法也是使用锁的喔，静态同步方法的锁对象是该类的`Class对象`，成员同步方法的锁对象是`this对象`。所以如果没有刻意强调，下边所说的同步代码块也包含同步方法。



了解了wait/notify的通用模式之后，使用的时候需要特别小心，需要注意下边这些方面：



必须在同步代码块中调用wait、 notify或者notifyAll方法。



有的童鞋会有疑问，为啥wait/notify机制的这些方法必须都放在同步代码块中才能调用呢？wait方法的意思只是让当前线程停止执行，把当前线程放在等待队列里，notify方法的意思只是从等待队列里移除一个线程而已，跟加锁有什么关系？



答：因为wait方法是运行在等待线程里的，notify或者notifyAll是运行在通知线程里的。而执行wait方法前需要判断一下某个条件是否满足，如果不满足才会执行wait方法，这是一个先检查后执行的操作，不是一个原子性操作，所以如果不加锁的话，在多线程环境下等待线程和通知线程的执行顺序可能是这样的：


也就是说当等待线程已经判断条件不满足，正要执行wait方法，此时通知线程抢先把条件完成并且调用了notify方法，之后等待线程才执行到wait方法，这会导致等待线程永远停留在等待队列而没有人再去notify它。所以等待线程中的判断条件是否满足、调用wait方法和通知线程中完成条件、调用notify方法都应该是原子性操作，彼此之间是互斥的，所以用同一个锁来对这两个原子性操作进行同步，从而避免出现等待线程永久等待的尴尬局面。



如果不在同步代码块中调用wait、notify或者notifyAll方法，也就是说没有获取锁就调用wait方法，就像这样：



对象.wait();


是会抛出IllegalMonitorStateException异常的。



在同步代码块中，必须调用获取的锁对象的wait、 notify或者notifyAll方法。



也就是说不能随便调用一个对象的wait、notify或者notifyAll方法。比如等待线程中的代码是这样的：




synchronized (对象1) {
    while(条件不满足) {
        对象2.wait();    //随便调用一个对象的wait方法
    }
}


通知线程中的代码是这样的：





synchronized (对象1) {
    完成条件
    对象2.notifyAll();
}


对于代码对象2.wait()，表示让出当前线程持有的对象2的锁，而当前线程持有的是对象1的锁，所以这么写是错误的，也会抛出IllegalMonitorStateException异常的。意思就是如果当前线程不持有某个对象的锁，那它就不能调用该对象的wait方法来让出该锁。所以如果想让等待线程让出当前持有的锁，只能调用对象1.wait()。然后这个线程就被放置到与对象1相关联的等待队列中，在通知线程中只能调用对象1.notifyAll()来通知这些等待的线程了。



在等待线程判断条件是否满足时，应该使用while，而不是if。



也就是说在判断条件是否满足的时候要使用while：


while(条件不满足) { //正确✅
    对象.wait();
}


而不是使用if：





if(条件不满足) { //错误❌
    对象.wait();
}


这个是因为在多线程条件下，可能在一个线程调用notify之后立即又有一个线程把条件改成了不满足的状态，比如在维修工把厕所修好之后通知大家上厕所吧的瞬间，有一个小屁孩以迅雷不及掩耳之势又给厕所坑里塞了个瓶子，厕所又被置为不可用状态，等待上厕所的还是需要再判断一下条件是否满足才能继续执行。



在调用完锁对象的notify或者notifyAll方法后，等待线程并不会立即从wait()方法返回，需要调用notify()或者notifyAll()的线程释放锁之后，等待线程才从wait()返回继续执行。






也就是说如果通知线程在调用完锁对象的notify或者notifyAll方法后还有需要执行的代码，就像这样：



synchronized (对象) {
    完成条件
    对象.notifyAll();
    ... 通知后的处理逻辑
}


需要把通知后的处理逻辑执行完成后，把锁释放掉，其他线程才可以从wait状态恢复过来，重新竞争锁来执行代码。比方说在维修工修好厕所并通知了等待上厕所的人们之后，他还没有从厕所出来，而是在厕所的墙上写了 "XXX到此一游"之类的话之后才从厕所出来，从厕所出来才代表着释放了锁，狗哥、猫爷、王尼妹才开始争抢进入厕所的机会。



notify方法只会将等待队列中的一个线程移出，而notifyAll方法会将等待队列中的所有线程移出。



大家可以把上边代码中的notifyAll方法替换称notify方法，看看执行结果～


wait和sleep的区别


眼尖的小伙伴肯定发现，wait和sleep这两个方法都可以让线程暂停执行，而且都有InterruptedException的异常说明，那么它们的区别是啥呢？



wait是Object的成员方法，而sleep是Thread的静态方法。



只要是作为锁的对象都可以在同步代码块中调用自己的wait方法，sleep是Thread的静态方法，表示的是让当前线程休眠指定的时间。



调用wait方法需要先获得锁，而调用sleep方法是不需要的。



再一次强调，一定要在同步代码块中调用锁对象的wait方法，前提是要获得锁！前提是要获得锁！前提是要获得锁！而sleep方法随时调用～



调用wait方法的线程需要用notify来唤醒，而sleep必须设置超时值。



线程在调用wait方法之后会先释放锁，而sleep不会释放锁。



这一点可能是最重要的一点不同点了吧，狗哥、猫爷、王尼妹这些线程一开始是获取到厕所的锁了，但是调用了wait方法之后主动把锁让出，从而让维修工得以进入厕所维修。如果狗哥在发现厕所是不可用的条件时选择调用sleep方法的话，线程是不会释放锁的，也就是说维修工无法获得厕所的锁，也就修不了厕所了～ 大家一定要谨记这一点啊！



总结


线程间需要通过通信才能协作解决某个复杂的问题。



可变共享变量是天然的通信媒介，但是使用的时候一定要保证线程安全性，通常使用volatile变量或synchronized来保证线程安全性。



一个线程在获取到锁之后，如果指定条件不满足的话，应该主动让出锁，然后到专门的等待区等待，直到某个线程完成了指定的条件，再通知一下在等待这个条件完成的线程，让它们继续执行。这个机制就是wait/notify机制。



等待线程的通用模式：





synchronized (对象) {
    处理逻辑（可选）
    while(条件不满足) {
        对象.wait();
    }
    处理逻辑（可选）
}




可以分为下边几个步骤：



获取对象锁。

如果某个条件不满足的话，调用锁对象的wait方法，被通知后仍要检查条件是否满足。

条件满足则继续执行代码。







通知线程的通用模式：





synchronized (对象) {
    完成条件
    对象.notifyAll();、
}




可以分为下边几个步骤：

获得对象的锁。

完成条件。

通知在等待队列中的等待线程。



wait和sleep的区别

wait是Object的成员方法，而sleep是Thread的静态方法。

调用wait方法需要先获得锁，而调用sleep方法是不需要的。

调用wait方法的线程需要用notify来唤醒，而sleep必须设置超时值。

线程在调用wait方法之后会先释放锁，而sleep不会释放锁。






