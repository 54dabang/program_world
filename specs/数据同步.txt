读数据表
读取Maxcompute的表数据组件，默认读取本工程下的数据。若读取其他工程的表数据且拥有该工程的操作权限，只需在表名前添加工程名，格式：工程名.表名，如：tianchi_project.weibo_data。

读MaxCompute表的输入框：当输入表名后，会自动读取表的结构数据，可单击字段信息查看。


注意：MaxCompute表字段修改后，如果增加或删除某个字段，在算法平台中是无法感知的，需要用户重新设置一下MaxCompute源，重新加载这个表信息。

分区功能介绍。

若输入表是分区表，后台会自动勾选分区框，用户可选择或输入分区参数，目前仅支持输入单个分区。
不勾选分区框或勾选后不输入分区参数均默认为输入全表。
若输入表是非分区表，分区框不可勾选。
PAI的读数据组件包含读取分区表的功能，在日期定义上与大数据开发套件略有不同。
PAI在读取分区表时需要指定dt=@@{yyyyMMdd}，其中@@{yyyyMMdd}表示当前日期，@@{yyyyMMdd-1d}表示当前日期前一天。
写数据表
写入MaxCompute表的数据组件，同样支持写入其他工程的表数据。
写入表数据不支持分区操作。

Mysql数据同步
功能说明
同步Mysql数据到MaxCompute项目。

参数说明
参数名称	参数描述	取值范围	默认值/行为
实例名称	必填，RDS的实例名称	不涉及	不涉及
数据库	必填，RDS数据库名称	不涉及	不涉及
数据表	必填，欲同步的数据表	不涉及	不涉及
用户名	必填，RDS数据库的用户名	不涉及	不涉及
密码	必填，RDS数据库密码	不涉及	不涉及
同步的字段	选填，默认同步该数据库所有字段	不涉及	所有字段
允许脏数据阈值	选填，数据错误数，默认0容忍数据错误	不涉及	0
同步数据宽带	选填，数据同步带宽	单位 MB/s	1
注意：由于CDP服务是对外服务，不支持集团内部数据同步，集团内部数据同步请走数据同步中心或者DataX。

如何获取组件参数
使用主账号登录阿里云官网，切换到RDS控制台，如下图所示，获取RDS的实例名称。


添加白名单。由于RDS对访问的IP有限制，需要单击实例链接，选择数据安全性，添加白名单。其中 0.0.0.0/0 表示运行任意IP访问。


单击实例链接，可以查看实例的详细信息，比如账号信息（如果没有账号，可以新建一个账号）、数据库信息等。


在左侧的菜单栏中选择数据库连接，单击登录数据库。



登录后可以查看数据库database，数据库下对应的table和schema。
image

OSS数据同步
功能说明
同步OSS的文本到MaxCompute数据源。

说明：CDP服务不提供命令行执行语句。

参数说明
参数名称	参数描述	取值范围	默认值/行为
OSSendpoint	必填，OSS存储服务所在的 Endpoint	oss-cn-xxxx.aliyuncs.com	oss-cn-shanghai.aliyuncs.com
OSSaccessId	必填，OSS服务的 AccessId	不涉及	不涉及
OSSaccessKey	必填，OSS服务的 AccessKey	不涉及	不涉及
bucket	必填，OSS服务的 Bucket	不涉及	不涉及
object	必填，欲同步的 OSS object	不涉及	不涉及
OSScolumn映射	必填，同步的字段映射格式是 index:name，表示OSS第index列同步到Maxcompute字段名为name的字段中，字段类型默认string，比如0:label,1:s_width,2:s_length,3:v_width,4:v_length	不涉及	不涉及
OSS文本分隔符	必填，OSS object 的文本分隔符(列分隔符)	逗号	,
OSS文本压缩格式	选填，OSS 文本压缩格式	gzip，zip，bzip2	无
OSS文本编码	选填，OSS 文本的编码	utf-8	utf-8
同步数据带宽	选填，数据同步带宽	单位 MB/s	1
允许脏数据阈值	选填，数据错误数，默认0容忍数据错误	不涉及	0
注意：由于CDP服务是对外服务，不支持集团内部数据同步，集团内部数据同步请走数据同步中心或者DataX。

如何获取组件参数
使用主账号登录阿里云官网，切换到OSS控制台。单击界面右侧的Access key，获取 AccessId 和 AccessKey，如下图所示。


在OSS控制台，可以在左侧列表中搜索用户拥有的 Bucket，如果没有可以参考创建OSS存储空间创建 Bucket。

单击 Bucket 实例链接，进入概览页面，可以获取该 OSS Bucket 所在的 Endpoint。


单击文件管理可以获取 Bucket，Object 等信息。


DataWorks数据集成支持任意位置任意网络环境下的任意数据源之间的实时、离线数据互通，是一站式数据同步的全栈平台，并允许用户在各种云和本地数据存储中每天复制数十TB的数据。

速度超快的数据传输性能以及400+对异构数据源之间的数据互通是确保用户能专注于核心“大数据”问题的关键：构建高级分析解决方案并从所有数据获得深入洞察。

本文将为您介绍数据同步速度的影响因素，如何通过调整同步作业的DMU配置、并发配置来达到最大化同步速度，作业限速和不限速的区别，以及自定义资源组的注意事项。

数据同步速度的影响因素
影响数据同步速度的因素，可从以下几方面进行考虑：

来源端数据源

数据库的性能：CPU、内存、SSD 硬盘、网络、硬盘等。

并发数：如果数据源并发数高，数据库负载便高。

网络：网络的带宽（吞吐量），网速。一般来说，数据库的性能越好，其可承载的并发数越高，可为数据同步作业设置更高的并发进行数据的抽取。
数据集成的同步任务配置

传输速度：是否设置任务同步速度上限值
DMU：任务运行所需要的资源量
并发：从源并行读取或并行写入数据存储端的最大线程数
WAIT 资源
Bytes 的设置：单个线程的 Bytes = 1048576，在网速比较敏感时，会出现超时现象，建议设置小一些。
查询语句是否建索引。
关于同步任务配置的相关概念，下文有详细说明。

目的端数据源
性能：CPU、内存、SSD 硬盘、网络、硬盘。
负载：目的数据库负载过高会影响同步任务数据写入效率。
网络：网络的带宽（吞吐量），网速。
数据源端和目的端数据库的性能、负载和网络情况主要由用户自己关注和调优，下文重点介绍在数据集成产品中配置同步任务的核心概念。

DMU数据移动单位
概念&配置
数据移动单位 (DMU) 是数据集成消耗资源（包含 CPU、内存、网络等资源分配）的度量单位。一个DMU描述了一个数据集成作业最小运行能力，即在限定的CPU、内存、网络等资源情况下对于数据同步的处理能力。

一个同步任务可以在1个或多个DMU上运行，向导模式最高可选上限为20个DMU；

如果通过脚本模式配置DMU，示例如下：

"setting": {
      "speed": {
        "dmu": 10
      }
    }
注：脚本模式虽然可以配置超过20DMU，系统上仍然会进行一定的资源限制。

DMU与作业速度的关系
DMU代表了资源能力，给同步任务配置较高DMU，可以分配得到更多的资源占用，但并不等于同步任务的速度一定能得到提升；速度调优需要结合并发、DMU两者之间的配比调优。举例来说，一个同步任务，配置3个并发，所需3DMU，同步速度为10MB/s;此时3并发所需资源量为3DMU，任务无需使用更多的资源，因此增加DMU并不能同步增加同步任务的速度。

并发
概念&配置
可将此属性视为数据同步任务内，可从源并行读取或并行写入数据存储端的最大线程数。向导模式通过界面化配置并发数，指定任务所使用的并行度；脚本模式通过如下示例代码：

"setting": {
      "speed": {
        "concurrent": 10
      }
    }
并发与DMU的关系
并发是指同步任务读取或写入的并行度，并发配置的越大，则所需要的系统资源（DMU）越多，在网络状况和读写端数据源能力满足的情况下，同步速度可以呈线性增长。但需注意以下几点：

为了保障高并发状态下，任务可正常执行，在产品设计上，DMU和并发有联动关系：当DMU配置为10时，最高并发数不可超过10
配置高并发，需要考虑读写端数据源的能力，并发过高可能会对源端数据库造成性能影响，请注意调优
脚本模式中，并发值虽然可以设置较高，但单任务所分配数据移动单元（DMU）会有控制，请注意调优
限速
商业化之后，数据集成同步任务默认不限速，任务将在所配置的并发数、DMU数的限制上以最高能达到的速度进行同步。另一方面，考虑到速度过高可能对数据库造成过大的压力从而影响生产，数据集成同时提供了限速选项，用户可以按照实际情况调优配置（我们建议选择限速之后，最高速度上限不应超过30MB/s）。脚本模式通过如下示例代码配置限速：

"setting": {
      "speed": {
         "throttle": true // 限流
        "mbps": 1,　// 具体速率值
      }
    }
注：当throttle设置为false时，表示不限速，则mbps的配置无意义；

自定义资源组注意事项
自定义资源组指的是将用户的同步任务运行在用户自己的机器上，特别是在网络不通情况下解决数据传输问题，详见文档：https://help.aliyun.com/document_detail/60429.html

任务运行在用户自己机器的自定义资源组上与运行在阿里云机器上的运行机制相似，均需要配置任务所需占用的DMU、并发数以及是否限速等；所不同的是，数据集成不会针对自定义资源消耗的DMU收费，而仅针对任务运行时长收费，详见数据集成计量计费文档。

为了保障任务在自定义资源组上顺畅的运行，推荐的自定义资源组机器配置至少为：2GHz，4核，8GB RAM和80GB磁盘。

数据同步过慢的场景
同步任务使用公共调度（WAIT）资源时一直在等待状态
场景示例

在 DataWorks（数据工场，原大数据开发套件）中对任务运行测试时，出现任务一直等待的状态，或好多测试任务都处于等待状态，而且还提示了系统内部错误。示例如下：

比如一个数据同步任务执行完成，共等待了大概 800s，但是日志显示任务只运行了 18s，使用的是默认资源组，现在运行其他同步任务，也是 RDS 到 MaxCompute ，一共几百条数据，一直处于等待中 。

显示等待日志：

2017-01-03 07:16:54 : State: 2(WAIT) | Total: 0R 0B | Speed: 0R/s 0B/s | Error: 0R 0B | Stage: 0.0%
解决方法

因为您使用的是公共调度资源，公共资源能力是受限的有很多项目都在使用，不只是单个用户的 2-3 个任务，任务实际运行 10 秒，但是延长到 800 秒，是因为您的任务下发执行时，发现资源不足，需等待获取资源。

如果对于同步速度和等待时间比较敏感，建议在低峰期配置同步任务，一般晚上零点到 3 点同步任务比较多，这样可以避开零点到 3 点的时间段，便可相对减少等待资源的情况。

提高多个任务导入数据到同一张表的同步速度
场景示例

想要将多个数据源的表同步到一张表里，所以将同步任务设置成串行任务，但是最后发现同步时间很长。

解决方法

可以同时启动多个任务，同时往一个数据库进行写入，注意以下几方面：

确保目标数据库负载能力是能够承受，避免不能正常工作。

在配置工作流任务，可以选择单个任务节点，配置分库分表任务或者在一个工作流中设置多个节点同时执行。

如果任务执行时，出现等待资源（WAIT）情况，可以低峰期配置同步任务，这样任务有较高的执行优先级。

数据同步任务 where 条件没有索引，导致全表扫描同步变慢
场景示例

执行的 SQL 如下所示：

select bid,inviter,uid,createTime from `relatives` where createTime>='2016-10-2300:00:00'and reateTime<'2016-10-24 00:00:00';
从 2016-10-25 11:01:24.875 开始执行，到 2016-10-25 11:11:05.489 开始返回结果。同步程序在等待数据库返回 SQL 查询结果，MaxCompute 需等待很久才能执行。

分析原因

where 条件查询时，createTime 列没有索引，导致查询全表扫描。

解决方法

建议 where 条件使用有索引相关的列，提高性能，索引也可以补充添加。


====================

FTP日志数据上传
更新时间：2017-11-08 13:40:05


本页目录
操作步骤
本文将以 FTP 数据源为例，说明如何利用数据集成功能将 FTP 数据源中的日志数据上传到 DataWorks（数据工场，原大数据开发套件）中。

操作步骤
新增数据源
进入项目空间后，导航至 数据集成 > 数据源 页面，单击右上角的 新增数据源。



在新增数据源页面填写相关信息，选择数据源类型为 ftp，配置如下：

1

FTP 数据源配置信息如下：

数据源名称：ftp_workshop_log

数据源描述：ftp日志文件同步

数据源类型：ftp

网络类型：经典网络

Protocol：sftp

Host：10.80.177.33

Port：22

用户名/密码：workshop/workshop

单击 测试连通性，如果测试成功，单击 确定，即成功新增数据源。



创建目标表
单击顶部导航栏中的 数据开发，进入数据开发首页后单击 新建 > 新建脚本文件 或 新建脚本。

1

配置新建脚本文件弹出框中的相关信息，填写文件名称，选择类型为 ODPS SQL 后，单击提交。如下图所示：

1

输入创建 FTP 日志对应目标表的语句，如下所示：

 DROP TABLE IF EXISTS ods_raw_log_d;
 CREATE TABLE ods_raw_log_d (
     col STRING
 )
 PARTITIONED BY (
     dt STRING
 );
单击 运行，直至日志信息返回成功表示目标表创建成功。

1

注意：

可以使用 desc 语法来确认创建表是否成功。

1

单击 保存，保存输入的 SQL 建表语句。

1

新建数据同步任务
单击 新建 并选择 新建任务。

1

配置新建的节点任务，单击 创建。配置项如下图所示：

1

配置数据同步任务
进入节点配置页面，选择来源。如下图所示：

1

数据来源配置项说明：

数据源：选择已创建好的 ftp 数据源。

文件路径：/home/workshop/user_log.txt

列分隔符：|

单击 下一步，选择目标。如下图所示：

1

数据目标配置项说明：

数据源：数据存放目标源选择 odps_first。

表：数据存放目标表选择 ods_raw_log_d。

分区信息：${bdp.system.bizdate}。

清理规则：写入前清理已有数据。

单击 下一步，连接要同步的数据，配置字段映射。如下图所示：

1

单击 下一步，配置通道控制，作业速率上限为 10MB/s。如下图所示：

1

单击 下一步，进入预览保存页面中预览上述的配置情况，也可以进行修改，确认无误后，单击 保存。

提交数据同步任务
单击 提交，提交已经配置的数据同步任务。

1

在 提交新版本 弹出框中单击 确认提交，即可将数据同步任务提交到调度系统中。

1

测试运行数据同步任务
单击工具栏中的 测试运行。

在 周期运行任务 弹出框中单击 确定。

1

在 测试运行 弹出框中，实例名称和业务日期都保持默认，单击运行。

1

在 工作流任务测试运行 弹出框中单击 前往运维中心。

1

在运维中心即可查看实例运行状态，如下图所示：

1

确认数据是否成功导入 MaxCompute
返回到 create_table_ddl 脚本文件中。

编写并执行 SQL 语句查看导入 ods_raw_log_d 的记录数。

1

SQL 语句如下，其中分区键需要更新为业务日期，如测试运行任务的日期为 20170712，那么业务日期为 20170711。

---查看是否成功写入MaxCompute
select count(*) from ods_raw_log_d where dt=业务日期;

=======================================================


同步日志排查
更新时间：2018-02-26 16:10:59


简介
数据集成，是阿里巴巴对外提供的稳定高效、弹性伸缩的数据同步平台。 致力于提供复杂网络环境下、丰富的异构数据源之间数据高速稳定的数据移动及同步能力。丰富的数据源支持:文本存储(FTP/SFTP/OSS/多媒体文件 等)、数据库(RDS/DRDS/MySQL/PostgreSQL 等)、NoSQL(Memcache/Redis/MongoDB/HBase 等)、大数据(MaxCompute/ AnalyticDB/HDFS 等) 、MPP数据库（HybridDB for MySQL等）。正因为数据集成兼容了复杂网络环境下，多种数据库之间共通，所以在使用的时候，难免会遇到出错的情况，那么下面我们来解析一下数据集成的日志组成。

任务是从哪里开始的
1

如图所示：“Start Job” 表示开始这个任务；Start Job 下面会有段日志 “running in Pipeline[XXXXX]” 主要是用来区分任务是跑在什么机器上，如果XXXXX中含有“ basecommon_group_XXXX ”等字样，说明是跑在公共资源组的机器上，如果不包含 “ basecommon_group_XXXX ” 的字样，说明在您的自定义资源组上运行。如何查看具体执行任务的机器名，请参考“任务运行情况这节的介绍”。

实际运行的任务代码
在任务开始以后，日志中会打印出来实际执行的任务代码（出于安全考虑，我们会将敏感信息以*号表示），如图所示：2

图示这个任务的实际代码样例如下：

Reader: odps
                       shared=[false                         ]
          bindingCalcEngineId=[9617                          ]
                       column=[["t_name","t_password","pt"]  ]
                  description=[connection from odps calc engine 9617]
                      project=[XXXXXXXXX                     ]
                   *accessKey=[********                      ]
                    gmtCreate=[2016-10-13 16:42:19           ]
                         type=[odps                          ]
                     accessId=[XXXXXXXXX                     ]
               datasourceType=[odps                          ]
                   odpsServer=[http://service.xxx.aliyun.com/api]
                     endpoint=[http://service.xxx.aliyun.com/api]
                    partition=[pt=20170425                   ]
             datasourceBackUp=[odps_first                    ]
                         name=[odps_first                    ]
                     tenantId=[168418089343600               ]
                      subType=[                              ]
                           id=[30525                         ]
                     authType=[1                             ]
                    projectId=[27474                         ]
                        table=[t_name                        ]
                       status=[1                             ]
Writer: odps
                       shared=[false                         ]
          bindingCalcEngineId=[9617                          ]
                       column=[["id","name","pt"]            ]
                  description=[connection from odps calc engine 9617]
                      project=[XXXXXXXXX                     ]
                   *accessKey=[********                      ]
                    gmtCreate=[2016-10-13 16:42:19           ]
                         type=[odps                          ]
                     accessId=[XXXXXXXXX                     ]
               datasourceType=[odps                          ]
                   odpsServer=[http://service.xxx.aliyun.com/api]
                     endpoint=[http://service.xxx.aliyun.com/api]
                    partition=[                              ]
                     truncate=[true                          ]
             datasourceBackUp=[odps_first                    ]
                         name=[odps_first                    ]
                     tenantId=[XXXXXXXXX                     ]
                      subType=[                              ]
                           id=[30525                         ]
                     authType=[1                             ]
                    projectId=[27474                         ]
                        table=[test_pm                       ]
                       status=[1                             ]
这是一个典型的 Maxcompute（原ODPS）数据源同步到 Maxcompute 数据源的任务代码，关于这段任务代码的解析，请参考MaxCompute Reader和 MaxCompute Writer

任务运行情况
上面我们介绍了实际运行的任务代码，在实际运行的任务代码下，会打印出来该任务的运行情况，如图所示：

3

图中用红框标记出来的内容，记录了这个任务何时开始运行，何时运行结束。当：“State: 2(WAIT)” State 状态为2的时候，还在等待任务运行；
当：“State: 3(RUN) ” State 状态为3的时候，表示任务正在运行；
当：“State: 0(SUCCESS)” State 状态为0的时候，表示任务已经成功运行完毕；

注意：在任务运行完毕的记录下面，有“INFO Start execute shell on node XXXXXXX” 这段话表示，该任务实际运行在 XXXXXXX 这台机器上。

排错小助手：当有脏数据的时候，无法将数据写入进去，日志就会报如下错误：

4

详细运行日志
其实数据同步的任务日志，到上节为止，就结束了，下面的一长串日志，是DataX的详细执行日志（数据集成功能是针对阿里巴巴开源项目DataX做了一层封装），如图所示：

1

很多同学运行数据同步任务会报错，可以参考如下文档先进行错误排查：常见报错。

若常见报错无法解决您的问题，请带上完整的日志提工单反馈给我们。



数据同步任务调优
更新时间：2019-04-03 13:57:16

编辑 ·
 · 我的收藏
本页目录
场景分类
前提条件
场景一：任务开始运行时间和调度时间差异比较大
场景二：任务长时间处于WAIT状态
场景三：任务同步速率慢
数据同步任务调度运行时，您可能会遇到实例的执行时间超过预期的情况。本文为您介绍如何在数据同步任务实例执行慢、时间差异大等不满足预期的情况下进行调优的方法。

场景分类
通常数据同步任务执行慢的场景分为以下三种：

任务开始运行的时间和调度时间差异比较大。
任务长时间处于WAIT状态。
任务同步的速率慢。
前提条件
正式开始数据同步任务调优前，请首先收集下列信息：
任务运行日志（从日志开始打印到结束）
任务的属性标签页信息
针对数据同步任务，DataWorks的调度资源分为一级调度资源和二级运行资源。
一级调度资源： 可以在运维中心 > 周期任务 > 属性的调度资源组中查看或配置。

二级运行资源：可以在数据开发 > 任务资源组中配置 。任务资源组下的可选自定义资源组 需要在数据集成的资源组中完成配置。

场景一：任务开始运行时间和调度时间差异比较大
在该场景下，您首先需要任务运行日志和任务属性标签页信息 。对比分析发现， 任务运行日志中开始running的时间和属性节点的调度时间是有差异的，时间主要耗费在等待调度上。

问题示例
在运维中心中的周期任务页面查看用户任务的属性标签页查，发现调度时间在00:00， 但是开始运行时间在00:29，怀疑时间主要消耗在等待调度上。

在实例页面右键查看用户任务运行日志，任务从00:29分开始运行，00:30执行结束，整个任务执行仅仅花费了1分钟。说明本次任务本身执行无问题。


问题解法
首先建议您观察您的项目下是否有较多的任务同时调度。默认资源组下的一级调度资源有限， 同时调度的任务较多会有其他任务排队等待。
说明 数据集成默认资源组在默认情况下为您提供50个槽位，每个DMU占据2个槽位，因此默认资源组在默认情况下支持25个DMU。如果您需要默认资源组支持更多槽位，请提交工单申请。
通常每天0点-2点是 业务调度的高峰期， 建议您的业务运行时间尽量避开高峰期 。
场景二：任务长时间处于WAIT状态
在该场景下，通过任务运行日志分析会发现， 任务运行主要时间耗费在WAIT的状态上。WAIT说明任务在等待二级运行资源。
问题示例
查看任务运行日志，任务长时间处于WAIT状态。

您可以通过如下图所示的方法确认资源调用的资源组，在ALISA_TASK_EXEC_TARGET:字段之后，如果是group_xxxxxx字样则说明是 默认资源组，如果是xxxxxx字样说明是自定义资源组 。

问题解法
如果您使用的是默认资源组，首先需观察资源组下是否有较多的任务在运行。默认资源组的槽位资源是有限的，如果有较多的任务需要跑，建议使用自定义资源组。
如果您使用的是自定义资源组，请在数据集成 > 资源组 > 管理 > 已使用DMU处观察自定义资源组的DMU已使用量是否比较高。如果是，需要考虑添加更多的ECS到自定义资源组下，提升资源池能力。

自定义资源组下槽位能力估算示例如下（假设自定义资源组下有2个ECS，ECS添加时填写的内存数为8，数据同步任务的concurrent数为2，即1个任务占用2个DMU）：
资源组支持的槽位数=2*8*3/2=24个槽位
数据同步任务槽位和任务、DMU换算关系： 1个任务占用2个DMU，1个DMU对应2个槽位， DMU数=24/2=12个， 可运行的周期任务数=24/2/2(concurrent数)=6个。 即自定义资源组最多有6个任务可以并发运行。
如果同时您还运行了其他任务，例如Shell任务。一个并发进程占用一个槽位，一个DMU对应一个槽位，DMU数=24个，则可以并发运行的Shell任务是 24个。您可以参考这个数值估算运行Shell任务之后还剩余多少资源给数据同步任务。
如果运行的任务数较少，您需要查看运行中的任务配置的 concurrent信息 和DMU信息， 观察是否配置了较大的concurrent（例如10），这样任务运行时会申请较多的资源，导致其他任务申请不到运行资源，处于WAIT状态。您可以通过调小任务的concurrent和DMU数来增加可以运行的任务并发数。
concurrent和DMU数调整策略：concurrent和DMU数保持一致， 不建议超过5。

场景三：任务同步速率慢
在该场景下，通过 任务运行日志分析，通常有两种情况：
任务一直在运行，但速率是0。
任务速率较低。
任务速率为0

查看运行日志，看到任务长时间处于run的状态，速率为0。通常是由于拉取的SQL执行比较慢（源数据库CPU负载高或网络流量占用高），或在拉取SQL前进行truncate等操作，导致处理时间较长。

问题示例
查看任务运行日志，任务长时间在run，速率为0。 从18:00开始到21:13结束。


查看运行日志信息有truncate操作记录，从18:00开始到21:13 truncate操作结束。

问题解法

综上，可以推断是truncate操作导致的同步任务慢，您可能需要检查源数据库truncate慢的原因。

任务速率慢

查看运行日志，看到任务同步速率不为0，但是速率慢。

问题示例
获取运行日志后，查看日志中信息同步速率确实比较慢， 约为1.93kb/s。

查看运行日志中同步时间消耗字段 WaitWriterTime、WaitReaderTime的信息， 发现WaitReaderTime时长较长，主要在等待读数据。

问题解法
针对速率比较慢的情况， 可以看下主要在等Writer还是Reader，如果是读或写慢，需要查看对应的源数据库或目的数据库的负载情况。
如果负载正常，可以查看任务配置的concurrent和DMU，如果比较小，例如配置concurrent和DMU为1 ，可以调整大，建议配置不超过5。


日志服务（Loghub）通过数据集成投递数据
更新时间：2019-02-15 13:19:27

编辑 ·
 · 我的收藏
本页目录
支持场景
跨阿里云账号的特别说明
新增数据源
通过向导模式配置同步任务
通过脚本模式配置同步任务
本文将以LogHub数据同步至MaxCompute为例，为您介绍如何通过数据集成同步LogHub数据至数据集成已支持的目的端数据源（如MaxCompute、OSS、OTS、RDBMS、DataHub等）。

说明 此功能已在华北2、华东2、华南1、香港、美西1、亚太东南1、欧洲中部1、亚太东南2、亚太东南3、亚太东北1、亚太南部1等多个Region发布上线。
支持场景
支持跨Region的LogHub与MaxCompute等数据源的数据同步。
支持不同阿里云账号下的LogHub与MaxCompute等数据源间的数据同步。
支持同一阿里云账号下的LogHub与MaxCompute等数据源间的数据同步。
支持公共云与金融云账号下的LogHub与MaxCompute等数据源间的数据同步。
跨阿里云账号的特别说明
以B账号进入数据集成配置同步任务，将A账号的LogHub数据同步至B账号的MaxCompute为例。

用A账号的AccessId和Accesskey建LogHub数据源。
此时B账号可以拖A账号下所有sls project的数据。

用A账号下子账号A1的AccessId和Accesskey创建LogHub数据源。
A给A1赋权日志服务的通用权限，即AliyunLogFullAccess和AliyunLogReadOnlyAccess，详情请参见授权RAM子用户访问日志服务资源。
A给A1赋权日志服务的自定义权限。
主账号A进入RAM控制台 > 策略管理页面，选择自定义授权策略 > 新建授权 > 空白模板。

相关的授权请参见访问控制RAM和RAM子用户访问。

根据下述策略进行授权后，B账号通过子账号A1只能同步日志服务project_name1以及project_name2的数据。
{
"Version": "1",
"Statement": [
{
"Action": [
"log:Get*",
"log:List*",
"log:CreateConsumerGroup",
"log:UpdateConsumerGroup",
"log:DeleteConsumerGroup",
"log:ListConsumerGroup",
"log:ConsumerGroupUpdateCheckPoint",
"log:ConsumerGroupHeartBeat",
"log:GetConsumerGroupCheckPoint"
],
"Resource": [
"acs:log:*:*:project/project_name1",
"acs:log:*:*:project/project_name1/*",
"acs:log:*:*:project/project_name2",
"acs:log:*:*:project/project_name2/*"
],
"Effect": "Allow"
}
]
}
新增数据源
B账号或B的子账号以开发者身份登录DataWorks控制台，单击对应项目下的进入数据集成。
进入同步资源管理 > 数据源页面，单击右上角的新增数据源。
选择数据源类型为LogHub，填写新增LogHub数据源对话框中的配置。

配置	说明
数据源名称	数据源名称必须以字母、数字、下划线组合，且不能以数字和下划线开头。
数据源描述	对数据源进行简单描述，不得超过80个字符。
LogHub Endpoint	LogHub的Endpoint，格式为http://yyy.com。
Project
详情请参见服务入口。

Access Id/Access Key	即访问密钥，相当于登录密码。您可以填写主账号或子账号的Access Id和Access Key。
单击测试连通性。
测试连通性通过后，单击确定。
通过向导模式配置同步任务
进入数据开发 > 业务流程页面，单击左上角的新建数据同步节点。
填写新建数据同步节点对话框中的配置，单击提交，进入数据同步任务配置页面。
选择数据来源。

配置	说明
数据源	填写LogHub数据源的名称。
Logstore	导出增量数据的表的名称。该表需要开启Stream，可以在建表时开启，或者使用UpdateTable接口开启。
日志开始时间	数据消费的开始时间位点，为时间范围（左闭右开）的左边界，为yyyyMMddHHmmss格式的时间字符串（比如20180111013000），可以和DataWorks的调度时间参数配合使用。
日志结束时间	数据消费的结束时间位点，为时间范围（左闭右开）的右边界，为yyyyMMddHHmmss格式的时间字符串（比如20180111013010），可以和DataWorks的调度时间参数配合使用。
批量条数	一次读取的数据条数，默认为256。
数据预览默认收起，您可单击进行预览。
说明 数据预览是选择LogHub中的几条数据展现在预览框，可能您同步的数据会跟您的预览的结果不一样，因为您同步的数据会指定开始时间可结束时间。
选择数据去向。
选择MaxCompute数据源及目标表ok。

配置	说明
数据源	填写配置的数据源名称。
表	选择需要同步的表。
分区信息	此处需同步的表是非分区表，所以无分区信息。
清理规则
写入前清理已有数据：导数据之前，清空表或者分区的所有数据，相当于insert overwrite。
写入前保留已有数据：导数据之前不清理任何数据，每次运行数据都是追加进去的，相当于insert into。
压缩	默认选择不压缩。
空字符串作为null	默认选择否。
字段映射。
选择字段的映射关系。需对字段映射关系进行配置，左侧源头表字段和右侧目标表字段为一一对应的关系。

通道控制。
配置作业速率上限和脏数据检查规则。

配置	说明
DMU	数据集成的计费单位。
说明 设置DMU时，需注意DMU的值限制了最大并发数的值，请合理配置。
作业并发数	配置时会结合读取端指定的切分建，将数据分成多个Task，多个Task同时运行，以达到提速的效果。
同步速率	设置同步速率可保护读取端数据库，以避免抽取速度过大，给源库造成太大的压力。同步速率建议限流，结合源库的配置，请合理配置抽取速率。
错误记录数超过	脏数据，类似源端是Varchar类型的数据，写到Int类型的目标列中，导致因为转换不合理而无法写入的数据。同步脏数据的设置，主要在于控制同步数据的质量问题。建议根据业务情况，合理配置脏数据条数。
任务资源组	配置同步任务时，指定任务运行所在的资源组，默认运行在默认资源组上。当项目调度资源紧张时，也可以通过新增自定义资源组的方式来给调度资源进行扩容，然后将同步任务指定在自定义资源组上运行，新增自定义资源组的操作请参见新增任务资源。
您可根据数据源网络情况、项目调度资源情况和业务重要程度，进行合理配置。

运行任务。
您可通过以下两种方式运行任务。

直接运行（一次性运行）
单击任务上方的 运行按钮，将直接在数据集成页面运行任务，运行之前需要配置自定义参数的具体数值。

如上图所示，代表同步10:10到17:30这段时间的LogHub记录到MaxCompute。

调度运行
单击提交按钮，将同步任务提交到调度系统中，调度系统会按照配置属性在从第二天开始自动定时执行。


如上图，设置开始时间和结束时间：startTime=$[yyyymmddhh24miss-10/24/60]系统前10分钟到 endTime=$[yyyymmddhh24miss-5/24/60]系统前5分钟时间。

如上图所示，设置按分钟调度，从00：00到23:59每5分钟调度一次。

通过脚本模式配置同步任务
如果您需要通过脚本模式配置此任务，单击工具栏中的转换脚本，选择 确认即可进入脚本模式。

您可根据自身进行配置，示例脚本如下。

{
"type": "job",
"version": "1.0",
"configuration": {
"reader": {
"plugin": "loghub",
"parameter": {
"datasource": "loghub_lzz",//数据源名，保持跟您添加的数据源名一致
"logstore": "logstore-ut2",//目标日志库的名字，logstore是日志服务中日志数据的采集、存储和查询单元。
"beginDateTime": "${startTime}",//数据消费的开始时间位点，为时间范围（左闭右开）的左边界
"endDateTime": "${endTime}",//数据消费的开始时间位点，为时间范围（左闭右开）的右边界
"batchSize": 256,//一次读取的数据条数，默认为256。
"splitPk": "",
"column": [
"key1",
"key2",
"key3"
]
}
},
"writer": {
"plugin": "odps",
"parameter": {
"datasource": "odps_first",//数据源名，保持跟您添加的数据源名一致
"table": "ok",//目标表名
"truncate": true,
"partition": "",//分区信息
"column": [//目标列名
"key1",
"key2",
"key3"
]
}
},
"setting": {
"speed": {
"mbps": 8,/作业速率上限
"concurrent": 7//并发数
}
}
}
}