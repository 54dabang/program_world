需求综合
ETL的34个子系统
将数据插入数据仓库中
清洗与整合数据
发布 准备展现
管理ETL环境

ETL过程概览
ETL开发规划
开发一次性的历史加载过程
开发增量式ETL过程
实时影响




采样与过滤

随机采样

加权采样

过滤与映射

分层采样

数据合并

join

合并列

合并行（UNION）

增加序号列

拆分

缺失值填充

归一化

标准化

类型转换

KV2Table

Table2KV



------------------------------------------------------------------------------------------------------

采样与过滤
随机采样
以随机方式生成采样数据，每次采样是各自独立的。

参数设置
random_sample_param_setting random_sample_opt

可手动输入采样个数（或者采样比例）。
可以选择放回采样或者不放回采样，默认为不放回，勾选后变为放回。
可配置随机数种子，默认系统自动分配。
可以配置并发计算核心数目与内存，默认系统自动分配。
PAI命令
PAI -name RandomSample
    -project algo_public \
    -Dlifecycle="28"\
    -DrandomSeed=1007\
    -DoutputTableName=wpbc_sample \
    -Dreplace=false \
    -DsampleSize=100 \
    -DinputTableName=wbpc;
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表的表名	NA	NA
inputTablePartitions	可选，输入表中指定哪些分区参与训练，格式为： partition_name = value。
如果是多级，格式为name1 = value1/name2 = value2。
如果指定多个分区，中间用“,”分开。
NA	输入表的所有partition
outputTableName	必选，输出结果表	NA	NA
sampleSize	可选，采样个数	正整数	默认为空
sampleRatio	可选，采样比例	浮点数，范围(0,1)	默认为空
replace	可选，是否放回，boolean类型	true / false	false（默认不放回）
randomSeed	可选，随机数种子	正整数	默认系统自动生成
lifecycle	可选，指定输出表生命周期	正整数，[1,3650]	输出表没有生命周期
coreNum	可选，计算的核心数目	正整数	系统自动分配
memSizePerCore	可选，每个核心的内存（单位是MB）	正整数，(1, 65536)	系统自动分配
注意

当sampleSize与sampleRatio都为空时，系统会报错。
当sampleSize与sampleRatio都不为空时，以sampleSize为准。
加权采样
以加权方式生成采样数据。权重列必须为double或bigint类型，按照该列值的大小采样。比如所选权重列的值是1.2和1.0，则值为1.2所属样本的被采样的概率就大一些。

参数设置
weighted_sample_param

可手动输入采样个数（或者采样比例）。
可以选择放回采样或者不放回采样，默认为不放回，勾选后变为放回。
下拉框选择加权列，加权列支持double型和bigint型。
可配置随机数种子，默认系统自动分配。
PAI命令
PAI -name WeightedSample
    -project algo_public \
    -Dlifecycle="28" \
    -DoutputTableName="test2" \
    -DprobCol="previous" \
    -Dreplace="false" \
    -DsampleSize="500" \
    -DinputPartitions="pt=20150501" \
    -DinputTableName="bank_data_partition";
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表的表名	NA	NA
inputTablePartitions	可选，输入表中指定哪些分区参与训练，格式为： partition_name = value。
如果是多级，格式为name1 = value1/name2 = value2。
如果指定多个分区，中间用“,”分开。
NA	输入表的所有partition
outputTableName	必选，输出结果表	NA	NA
sampleSize	可选，采样个数	正整数	默认为空
sampleRatio	可选，采样比例	浮点数，范围(0,1)	默认为空
probCol	必选，选择的要加权的列。每个值代表所在记录出现的权重，不需要归一化	double型和bigint型	NA
replace	可选，是否放回，boolean类型	true / false	false(默认不放回)
randomSeed	可选，随机数种子	正整数	默认系统自动生成
lifecycle	可选，指定输出表生命周期	正整数，[1,3650]	输出表没有生命周期
coreNum	可选，计算的核心数目	正整数	系统自动分配
memSizePerCore	可选，每个核心的内存（单位是MB）	正整数，(1, 65536)	系统自动分配
注意

当sampleSize与sampleRatio都为空时，系统会报错。
当sampleSize与sampleRatio都不为空时，以sampleSize为准。
过滤与映射
对数据按照过滤表达式进行筛选，可以修改输出字段名称。

参数设置
选择要筛选的列，默认选择全部列。也可以修改输出字段名称。
dd

通过where条件实现数据过滤，与SQL类似，如下图所示。
screenshot
筛选条件：目前操作符支持”=”，”!=”，”>”，”<”，“>=”和“<=”，like，rlike

PAI命令
PAI -name Filter
    -project algo_public \
    -DoutTableName="test_9" \
    -DinputPartitions="pt=20150501" \
    -DinputTableName="bank_data_partition" \
    -Dfilter="age>=40";
name：组件名字。
project：工程名字，用于指定算法所在空间。系统默认是algo_public，用户自己更改后系统会报错。
outTableName：输出表的名字。
inputPartitions：（可选）训练输入表分区。输入表对应的输入分区，选中全表则为None。
inputTableName：输入表的名字。
filter：where筛选条件，目前操作符支持”=”，”!=”，”>”，”<”，“>=”，“<=”,“like”和“rlike”。


分层采样
数据集分层抽取一定比例或者一定数据的随机样本。

参数设置

下拉框选择分组列（最大支持100个分组）。
可手动输入分组的采样个数（或者采样比例）。
可配置随机数种子，默认1234567。
可以自己配置并发计算核心数目与内存，默认系统自动分配。
PAI命令
PAI -name StratifiedSample
    -project algo_public \
    -DinputTableName="test_input" \
    -DoutputTableName="test_output" \
    -DstrataColName="label" \
    -DsampleSize="A:200,B:300,C:500" \
    -DrandomSeed=1007 \
    -Dlifecycle=30;
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表	NA	NA
inputTablePartitions	可选，训练输入表分区。	NA	默认选中全表
outputTableName	必选，输出表	NA	NA
strataColName	必选，层次列，分层就是按照此列作为key分层的	NA	NA
sampleSize	可选，采样大小
正整数：表示每个层的采样个数。
字符串：格式为strata0:n0，strata1:n1，表示每个层分别配置的采样个数。
正整数或字符串	默认为空
sampleRatio	可选，采样比例
数字：范围(0,1)，表示每个stratum的采样比例。
字符串：格式为strata0:r0,strata1:r1 ，表示每个层分别配置采样比例。
范围为(0,1)的浮点数或字符串	默认为空
randomSeed	可选, 随机种子	正整数	默认123456
lifecycle	可选，输出表生命周期	正整数，[1,3650]	默认不设置
coreNum	可选，核心个数	正整数	系统自动分配
memSizePerCore	可选，每个核心的内存（单位是MB）	正整数，(1, 65536)	系统自动分配
注意

当sampleSize与sampleRatio都为空时，系统会报错。
当sampleSize与sampleRatio都不为空时，以sampleSize为准。
示例
源数据
id	outlook	temperature	humidity	windy	play
0	Sunny	85	85	false	No
1	Sunny	80	90	true	No
2	Overcast	83	86	false	Yes
3	Rainy	70	96	false	Yes
4	Rainy	68	80	false	Yes
5	Rainy	65	70	true	No
6	Overcast	64	65	true	Yes
7	Sunny	72	95	false	No
8	Sunny	69	70	false	Yes
9	Rainy	75	80	false	Yes
10	Sunny	75	70	true	Yes
11	Overcast	72	90	true	Yes
12	Overcast	81	75	false	Yes
13	Rainy	71	91	true	No
创建实验。
stratified_sampling_demo_exp

选取分组列。
stratified_sampling_demo_exp_select_strata_column
选择play列做为分组列。

配置采样个数。
stratified_sampling_demo_exp_config_param
play = Yes的分组采4条，play = No的分组采3条。

采样结果。
stratified_sampling_demo_exp_result

数据合并
join
两张表通过关联信息，合成一张表，并决定输出的字段，与SQL的join语句功能类似。

参数设置
screenshot

连接类型支持：左连接，内连接，右连接，全连接。
关联条件目前只支持等式。
可手动添加或删除关联条件。
PAI命令
不提供PAI命令。

合并列
将两张表的数据按列合并，需要表的行数保持一致。

参数设置
screenshot
左表选择输入列
screenshot
右表选择输入列
screenshot

选择的两张表行数需保持一致。
左表与右表选择的输出列名不能重复。
选择输出字段列时可手动更改输出字段名。
若左表右表均不选择输出列，则默认输出全表。此时勾选“是否自动重命名输出列”，会重命名重复列后输出。
PAI命令
PAI -name AppendColumns
   -project algo_public \
   -DoutputTableColNames="petal_length,petal_width,petal_length2,petal_width2" \
    -DautoRenameCol="false" \
    -DoutputTableName="pai_temp_770_6840_1" \
    -DinputTableNames="iris_twopartition,iris_twopartition" \
    -DinputPartitionsInfoList="dt=20150125/dp=20150124;dt=20150124/dp=20150123" \
    -DselectedColNamesList="petal_length,petal_width;sepal_length,sepal_width";
name：组件名字。
project：工程名字，用于指定算法所在空间。系统默认是algo_public，用户自己更改后系统会报错。
outputTableColNames：新表中各列的列名，用逗号分隔。若autoRenameCol为true，则此参数无效。
autoRenameCol：（可选）输出表是否自动重命名列，true为重命名，false不进行重命名，默认false。
outputTableName：输出的表名。
inputTableNames：输入的表名，如果有多个，逗号分隔。
inputPartitionsInfoList：（可选）输入表对应选择的partition列表，同一个表的各partition按逗号分隔，不同表的partition分号分隔。
selectedColNamesList：选择输入的列名，同张表之间列名用逗号分隔，不同表之间用分号分隔。
合并行（UNION）
将两张表的数据按行合并，左表及右表选择输出的字段个数以及类型应保持一致。整合了union和union all的功能。

参数设置
调整参数，如下：
screenshot

进行联合操作时，左右表选择的列数需相同，对应列的类型需保证一致。
可根据实际情况在条件框中手动输入已选字段的过滤条件，默认情况下全表，目前操作符支持“=”，“!=”，“>”，“<”， “>=”，“<=”，“like” 和 “rlike”。
系统默认勾选去重，对生成的数据表的重复行进行去重操作。
左表联合列
screenshot
右表联合列
screenshot
PAI命令
不提供PAI命令。

增加序号列
在数据表第一列追加ID列，并将append id后的内容存为新表，增加的ID列为BigInt类型。

参数设置
略

PAI命令
PAI -name AppendId -project algo_public -DIDColName="append_id" -DoutputTableName="test_11" -DinputTableName="bank_data" \
    -DselectedColNames="age,campaign,cons_conf_idx,cons_price_idx,emp_var_rate,euribor3m,nr_employed,pdays,poutcome,previous,y";
name：组件名字。
project：project名字，用于指定算法所在空间。系统默认是algo_public，用户自己更改后系统会报错。
IDColName：追加的ID列列名，从0开始编号，依次为0,1,2,3…。
outputTableNames：输出表的名字。
inputTableNmae：输入表的名字。
selectedColNames：要保留的字段名，多个用逗号分隔。
拆分
背景
对输入表或分区进行按比例拆分，分别写入两张输出表。

算法组件
split_icon_intro splig_param_setting

拆分组件，对应两个输出桩。
如图所示配置0.6，则左边的输出桩对应60%的数据，右边对应40%的数据。
PAI命令
pai -name split -project algo_public \
    -DinputTableName=wbpc \
    -Doutput1TableName=wpbc_split1 \
    -Doutput2TableName=wpbc_split2 \
    -Dfraction=0.25;
参数设置
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表的表名	-	-
inputTablePartitions	可选，输入表中指定哪些分区参与训练，格式为：partition_name=value。如果是多级，格式为name1=value1/name2=value2；如果指定多个分区，中间用’,’分开	-	输入表的所有partition
output1TableName	必选，输出结果表1	-	-
output1TablePartition	可选，输出结果表1分区名	-	输出表1为非分区表
output2TableName	必选，输出结果表2	-	-
output2TablePartition	可选，输出结果表2分区名	-	输出表2为非分区表
fraction	必选，切分至输出表1的数据比例	(0,1)	-
lifecycle	可选，指定输出表生命周期	正整数，[1,3650]	输出表没有生命周期
缺失值填充
缺失值填充用来将空值或者一个指定的值替换为最大值，最小值，均值或者一个自定义的值。可以通过给定一个缺失值的配置列表，来实现将输入表的缺失值用指定的值来填充。

可以将数值型的空值替换为最大值，最小值，均值或者一个自定义的值。
可以将字符型的空值，空字符串，空值和空字符串，指定值替换为一个自定义的值。
待填充的缺失值可以选择空值或空字符，也可以自定义。
缺失值若选择空字符，则填充的目标列应是string型。
数值型替换可以自定义，也可以直接选择替换成数值最大值，最小值或者均值。
缺失值填充界面
image

两个输入桩依次对应参数为：

inputTableName：输入表，即要填充的表。

inputParaTableName：配置输入表，即缺失值填充节点生成的参数列表，通过此参数，可以将一张表的配置参数应用到一张新的表。

两个输出桩依次对应参数为：

outputTableName：输出表，即填充完成的表。

outputParaTableName：输出配置表，用于应用到其他的数据集上。

缺失值填充参数界面
image

填充的字段，原值，替换为三个部分组成了config参数，分别对应config参数的三个部分：列名，原值，替换值。
PAI命令
PAI -name FillMissingValues -project algo_public -Dconfigs="poutcome,null-empty,testing" \
    -DoutputTableName="test_3" -DinputPartitions="pt=20150501" -DinputTableName="bank_data_partition";
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表	-	-
inputTablePartitions	可选，训练输入表分区。	-	默认选中全表
outputTableName	必选，输出表	-	-
configs	必选，缺失值填充的配置。格式如 “col1, null, 3.14; col2, empty, hello; col3, empty-null, world”，其中null表示空值，empty表示空字符。缺失值若选择空字符，则填充的目标列应是string型。若采用最大值、最小值、均值，可以采用变量，其命名规范形如：min, max, mean。若用户自定义替换值，则使用user-defined，格式如”col4,user-defined,str,str123”	-	-
outputParaTableName	必选，配置输出表	-	-
inputParaTableName	可选，配置输入表	-	默认不输入
lifecycle	可选，输出表生命周期	-	默认不设置
coreNum	可选，核心个数	-	默认不设置，系统自动分配
memSizePerCore	可选，单个核心使用的内存数	-	默认不设置，系统自动分配
实例
测试数据

新建数据SQL

drop table if exists fill_missing_values_test_input;
create table fill_missing_values_test_input(
    col_string string,
    col_bigint bigint,
    col_double double,
    col_boolean boolean,
    col_datetime datetime);
insert overwrite table fill_missing_values_test_input
select
    *
from
(
    select
        '01' as col_string,
        10 as col_bigint,
        10.1 as col_double,
        True as col_boolean,
        cast('2016-07-01 10:00:00' as datetime) as col_datetime
    from dual
    union all
        select
            cast(null as string) as col_string,
            11 as col_bigint,
            10.2 as col_double,
            False as col_boolean,
            cast('2016-07-02 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '02' as col_string,
            cast(null as bigint) as col_bigint,
            10.3 as col_double,
            True as col_boolean,
            cast('2016-07-03 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '03' as col_string,
            12 as col_bigint,
            cast(null as double) as col_double,
            False as col_boolean,
            cast('2016-07-04 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '04' as col_string,
            13 as col_bigint,
            10.4 as col_double,
            cast(null as boolean) as col_boolean,
            cast('2016-07-05 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '05' as col_string,
            14 as col_bigint,
            10.5 as col_double,
            True as col_boolean,
            cast(null as datetime) as col_datetime
        from dual
) tmp;
输入数据说明

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 04         | 13         | 10.4       | NULL        | 2016-07-05 10:00:00 |
| 02         | NULL       | 10.3       | true        | 2016-07-03 10:00:00 |
| 03         | 12         | NULL       | false       | 2016-07-04 10:00:00 |
| NULL       | 11         | 10.2       | false       | 2016-07-02 10:00:00 |
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 |
| 05         | 14         | 10.5       | true        | NULL         |
+------------+------------+------------+-------------+--------------+
运行命令

drop table if exists fill_missing_values_test_input_output;
drop table if exists fill_missing_values_test_input_model_output;
PAI -name FillMissingValues
-project algo_public
-Dconfigs="col_double,null,mean;col_string,null-empty,str_type_empty;col_bigint,null,max;col_boolean,null,true;col_datetime,null,2016-07-06 10:00:00"
-DoutputParaTableName="fill_missing_values_test_input_model_output"
-Dlifecycle="28"
-DoutputTableName="fill_missing_values_test_input_output"
-DinputTableName="fill_missing_values_test_input";
drop table if exists fill_missing_values_test_input_output_using_model;
drop table if exists fill_missing_values_test_input_output_using_model_model_output;
PAI -name FillMissingValues
-project algo_public
-DoutputParaTableName="fill_missing_values_test_input_output_using_model_model_output"
-DinputParaTableName="fill_missing_values_test_input_model_output"
-Dlifecycle="28"
-DoutputTableName="fill_missing_values_test_input_output_using_model"
-DinputTableName="fill_missing_values_test_input";
运行结果

fill_missing_values_test_input_output

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 04         | 13         | 10.4       | true        | 2016-07-05 10:00:00 |
| 02         | 14         | 10.3       | true        | 2016-07-03 10:00:00 |
| 03         | 12         | 10.3       | false       | 2016-07-04 10:00:00 |
| str_type_empty | 11         | 10.2       | false       | 2016-07-02 10:00:00 |
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 |
| 05         | 14         | 10.5       | true        | 2016-07-06 10:00:00 |
+------------+------------+------------+-------------+--------------+
fill_missing_values_test_input_model_output

+------------+------------+
| feature    | json       |
+------------+------------+
| col_string | {"name": "fillMissingValues", "type": "string", "paras":{"missing_value_type": "null-empty",  "replaced_value": "str_type_empty"}} |
| col_bigint | {"name": "fillMissingValues", "type": "bigint", "paras":{"missing_value_type": "null",  "replaced_value": 14}} |
| col_double | {"name": "fillMissingValues", "type": "double", "paras":{"missing_value_type": "null",  "replaced_value": 10.3}} |
| col_boolean | {"name": "fillMissingValues", "type": "boolean", "paras":{"missing_value_type": "null",  "replaced_value": 1}} |
| col_datetime | {"name": "fillMissingValues", "type": "datetime", "paras":{"missing_value_type": "null",  "replaced_value": 1467770400000}} |
+------------+------------+
fill_missing_values_test_input_output_using_model

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 04         | 13         | 10.4       | true        | 2016-07-05 10:00:00 |
| 02         | 14         | 10.3       | true        | 2016-07-03 10:00:00 |
| 03         | 12         | 10.3       | false       | 2016-07-04 10:00:00 |
| str_type_empty | 11         | 10.2       | false       | 2016-07-02 10:00:00 |
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 |
| 05         | 14         | 10.5       | true        | 2016-07-06 10:00:00 |
+------------+------------+------------+-------------+--------------+
fill_missing_values_test_input_output_using_model_model_output

+------------+------------+
| feature    | json       |
+------------+------------+
| col_string | {"name": "fillMissingValues", "type": "string", "paras":{"missing_value_type": "null-empty",  "replaced_value": "str_type_empty"}} |
| col_bigint | {"name": "fillMissingValues", "type": "bigint", "paras":{"missing_value_type": "null",  "replaced_value": 14}} |
| col_double | {"name": "fillMissingValues", "type": "double", "paras":{"missing_value_type": "null",  "replaced_value": 10.3}} |
| col_boolean | {"name": "fillMissingValues", "type": "boolean", "paras":{"missing_value_type": "null",  "replaced_value": 1}} |
| col_datetime | {"name": "fillMissingValues", "type": "datetime", "paras":{"missing_value_type": "null",  "replaced_value": 1467770400000}} |
+------------+------------+
归一化
对一个表的某一列或多列，进行归一化处理，产生的数据存入新表中。
目前支持的是线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)，MaxValue、MinValue分别为样本的最大值和最小值。
可以选择是否保留原始列，勾选后原始列会被保留，处理过的列重命名。
点击选择字段按钮可以选择想要归一化的列，目前支持double类型与bigint类型。
归一化界面
image

两个输入桩依次对应参数为：

inputTableName：输入表，即要归一化的表。

inputParaTableName：配置输入表，即归一化节点生成的参数列表，通过此参数，可以将一张表的配置参数应用到一张新的表。

两个输出桩依次对应参数为：

outputTableName：输出表，即归一化完成的表。

outputParaTableName：输出配置表，用于应用到其他的数据集上。

归一化参数界面
image

保留原始列对应参数keepOriginal。
PAI命令
PAI -name Normalize -project algo_public -DkeepOriginal="true" -DoutputTableName="test_4" -DinputPartitions="pt=20150501" \
    -DinputTableName="bank_data_partition" -DselectedColNames="emp_var_rate,euribor3m";
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表	-	-
selectedColNames	可选，输入表选择列	-	默认选中全部列
inputTablePartitions	可选，训练输入表分区	-	默认选中全表
outputTableName	必选，输出表	-	-
outputParaTableName	必选，配置输出表	-	-
inputParaTableName	可选，配置输入表	-	默认不输入
keepOriginal	可选，是否保留原始列(keepOriginal =true时，处理过的列重命名（”normalized_”前缀），原始列保留，keepOriginal=false时，全部列保留且不重命名)	-	默认为false
lifecycle	可选，输出表生命周期	-	默认不设置
coreNum	可选，核心个数	-	默认不设置，系统自动分配
memSizePerCore	可选，单个核心使用的内存数	-	默认不设置，系统自动分配
实例
测试数据

新建数据SQL

drop table if exists normalize_test_input;
create table normalize_test_input(
    col_string string,
    col_bigint bigint,
    col_double double,
    col_boolean boolean,
    col_datetime datetime);
insert overwrite table normalize_test_input
select
    *
from
(
    select
        '01' as col_string,
        10 as col_bigint,
        10.1 as col_double,
        True as col_boolean,
        cast('2016-07-01 10:00:00' as datetime) as col_datetime
    from dual
    union all
        select
            cast(null as string) as col_string,
            11 as col_bigint,
            10.2 as col_double,
            False as col_boolean,
            cast('2016-07-02 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '02' as col_string,
            cast(null as bigint) as col_bigint,
            10.3 as col_double,
            True as col_boolean,
            cast('2016-07-03 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '03' as col_string,
            12 as col_bigint,
            cast(null as double) as col_double,
            False as col_boolean,
            cast('2016-07-04 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '04' as col_string,
            13 as col_bigint,
            10.4 as col_double,
            cast(null as boolean) as col_boolean,
            cast('2016-07-05 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '05' as col_string,
            14 as col_bigint,
            10.5 as col_double,
            True as col_boolean,
            cast(null as datetime) as col_datetime
        from dual
) tmp;
输入数据说明

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 |
| NULL       | 11         | 10.2       | false       | 2016-07-02 10:00:00 |
| 02         | NULL       | 10.3       | true        | 2016-07-03 10:00:00 |
| 03         | 12         | NULL       | false       | 2016-07-04 10:00:00 |
| 04         | 13         | 10.4       | NULL        | 2016-07-05 10:00:00 |
| 05         | 14         | 10.5       | true        | NULL         |
+------------+------------+------------+-------------+--------------+
运行命令

drop table if exists normalize_test_input_output;
drop table if exists normalize_test_input_model_output;
PAI -name Normalize
-project algo_public
-DoutputParaTableName="normalize_test_input_model_output"
-Dlifecycle="28"
-DoutputTableName="normalize_test_input_output"
-DinputTableName="normalize_test_input"
-DselectedColNames="col_double,col_bigint"
-DkeepOriginal="true";
drop table if exists normalize_test_input_output_using_model;
drop table if exists normalize_test_input_output_using_model_model_output;
PAI -name Normalize
-project algo_public
-DoutputParaTableName="normalize_test_input_output_using_model_model_output"
-DinputParaTableName="normalize_test_input_model_output"
-Dlifecycle="28"
-DoutputTableName="normalize_test_input_output_using_model"
-DinputTableName="normalize_test_input";
运行结果

normalize_test_input_output

+------------+------------+------------+-------------+--------------+-----------------------+-----------------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime | normalized_col_bigint | normalized_col_double |
+------------+------------+------------+-------------+--------------+-----------------------+-----------------------+
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 | 0.0                   | 0.0                   |
| NULL       | 11         | 10.2       | false       | 2016-07-02 10:00:00 | 0.25                  | 0.2499999999999989    |
| 02         | NULL       | 10.3       | true        | 2016-07-03 10:00:00 | NULL                  | 0.5000000000000022    |
| 03         | 12         | NULL       | false       | 2016-07-04 10:00:00 | 0.5                   | NULL                  |
| 04         | 13         | 10.4       | NULL        | 2016-07-05 10:00:00 | 0.75                  | 0.7500000000000011    |
| 05         | 14         | 10.5       | true        | NULL         | 1.0                   | 1.0                   |
+------------+------------+------------+-------------+--------------+-----------------------+-----------------------+
normalize_test_input_model_output

+------------+------------+
| feature    | json       |
+------------+------------+
| col_bigint | {"name": "normalize", "type":"bigint", "paras":{"min":10, "max": 14}} |
| col_double | {"name": "normalize", "type":"double", "paras":{"min":10.1, "max": 10.5}} |
+------------+------------+
normalize_test_input_output_using_model

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 01         | 0.0        | 0.0        | true        | 2016-07-01 10:00:00 |
| NULL       | 0.25       | 0.2499999999999989 | false       | 2016-07-02 10:00:00 |
| 02         | NULL       | 0.5000000000000022 | true        | 2016-07-03 10:00:00 |
| 03         | 0.5        | NULL       | false       | 2016-07-04 10:00:00 |
| 04         | 0.75       | 0.7500000000000011 | NULL        | 2016-07-05 10:00:00 |
| 05         | 1.0        | 1.0        | true        | NULL         |
+------------+------------+------------+-------------+--------------+
normalize_test_input_output_using_model_model_output

+------------+------------+
| feature    | json       |
+------------+------------+
| col_bigint | {"name": "normalize", "type":"bigint", "paras":{"min":10, "max": 14}} |
| col_double | {"name": "normalize", "type":"double", "paras":{"min":10.1, "max": 10.5}} |
+------------+------------+
标准化
对一个表的某一列或多列，进行标准化处理，产生的数据存入新表中。
标准化所使用的公式 :（X - Mean）/（standard deviation）。
Mean：样本平均值。
standard deviation：样本标准偏差，针对从总体抽样，利用样本来计算总体偏差，为了使算出的值与总体水平更接近，就必须将算出的标准偏差的值适度放大，即 img。
样本标准偏差公式：img  img 代表所采用的样本X1，X2，…，Xn的均值。
可以选择是否保留原始列，勾选后原始列会被保留，处理过的列重命名。
点击选择字段按钮可以选择想要标准化的列，目前支持double类型与bigint类型。
标准化界面
image

两个输入桩依次对应参数为：

inputTableName：输入表，即要标准化的表。

inputParaTableName：配置输入表，即标准化节点生成的参数列表，通过此参数，可以将一张表的配置参数应用到一张新的表。

两个输出桩依次对应参数为：

outputTableName：输出表，即标准化完成的表。

outputParaTableName：输出配置表，用于应用到其他的数据集上。

标准化参数界面
image

保留原始列对应参数keepOriginal。
PAI命令
PAI -name Standardize -project algo_public -DkeepOriginal="false" -DoutputTableName="test_5" \
    -DinputPartitions="pt=20150501" -DinputTableName="bank_data_partition" -DselectedColNames="euribor3m,pdays";
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTableName	必选，输入表	-	-
selectedColNames	可选，输入表选择列	-	默认选中全部列
inputTablePartitions	可选，训练输入表分区	-	默认选中全表
outputTableName	必选，输出表	-	-
outputParaTableName	必选，配置输出表	-	-
inputParaTableName	可选，配置输入表	-	默认不输入
keepOriginal	可选，是否保留原始列(keepOriginal=true时，处理过的列重命名（”stdized_”前缀），原始列保留，keepOriginal=false时，全部列保留且不重命名)	-	默认为false
lifecycle	可选，输出表生命周期	-	默认不设置
coreNum	可选，核心个数	-	默认不设置，系统自动分配
memSizePerCore	可选，单个核心使用的内存数	-	默认不设置，系统自动分配
实例
测试数据

新建数据SQL

drop table if exists standardize_test_input;
create table standardize_test_input(
    col_string string,
    col_bigint bigint,
    col_double double,
    col_boolean boolean,
    col_datetime datetime);
insert overwrite table standardize_test_input
select
    *
from
(
    select
        '01' as col_string,
        10 as col_bigint,
        10.1 as col_double,
        True as col_boolean,
        cast('2016-07-01 10:00:00' as datetime) as col_datetime
    from dual
    union all
        select
            cast(null as string) as col_string,
            11 as col_bigint,
            10.2 as col_double,
            False as col_boolean,
            cast('2016-07-02 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '02' as col_string,
            cast(null as bigint) as col_bigint,
            10.3 as col_double,
            True as col_boolean,
            cast('2016-07-03 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '03' as col_string,
            12 as col_bigint,
            cast(null as double) as col_double,
            False as col_boolean,
            cast('2016-07-04 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '04' as col_string,
            13 as col_bigint,
            10.4 as col_double,
            cast(null as boolean) as col_boolean,
            cast('2016-07-05 10:00:00' as datetime) as col_datetime
        from dual
    union all
        select
            '05' as col_string,
            14 as col_bigint,
            10.5 as col_double,
            True as col_boolean,
            cast(null as datetime) as col_datetime
        from dual
) tmp;
输入数据说明

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 |
| NULL       | 11         | 10.2       | false       | 2016-07-02 10:00:00 |
| 02         | NULL       | 10.3       | true        | 2016-07-03 10:00:00 |
| 03         | 12         | NULL       | false       | 2016-07-04 10:00:00 |
| 04         | 13         | 10.4       | NULL        | 2016-07-05 10:00:00 |
| 05         | 14         | 10.5       | true        | NULL         |
+------------+------------+------------+-------------+--------------+
运行命令

drop table if exists standardize_test_input_output;
drop table if exists standardize_test_input_model_output;
PAI -name Standardize
-project algo_public
-DoutputParaTableName="standardize_test_input_model_output"
-Dlifecycle="28"
-DoutputTableName="standardize_test_input_output"
-DinputTableName="standardize_test_input"
-DselectedColNames="col_double,col_bigint"
-DkeepOriginal="true";
drop table if exists standardize_test_input_output_using_model;
drop table if exists standardize_test_input_output_using_model_model_output;
PAI -name Standardize
-project algo_public
-DoutputParaTableName="standardize_test_input_output_using_model_model_output"
-DinputParaTableName="standardize_test_input_model_output"
-Dlifecycle="28"
-DoutputTableName="standardize_test_input_output_using_model"
-DinputTableName="standardize_test_input";
运行结果

standardize_test_input_output

+------------+------------+------------+-------------+--------------+--------------------+--------------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime | stdized_col_bigint | stdized_col_double |
+------------+------------+------------+-------------+--------------+--------------------+--------------------+
| 01         | 10         | 10.1       | true        | 2016-07-01 10:00:00 | -1.2649110640673518 | -1.2649110640683832 |
| NULL       | 11         | 10.2       | false       | 2016-07-02 10:00:00 | -0.6324555320336759 | -0.6324555320341972 |
| 02         | NULL       | 10.3       | true        | 2016-07-03 10:00:00 | NULL               | 0.0                |
| 03         | 12         | NULL       | false       | 2016-07-04 10:00:00 | 0.0                | NULL               |
| 04         | 13         | 10.4       | NULL        | 2016-07-05 10:00:00 | 0.6324555320336759 | 0.6324555320341859 |
| 05         | 14         | 10.5       | true        | NULL         | 1.2649110640673518 | 1.2649110640683718 |
+------------+------------+------------+-------------+--------------+--------------------+--------------------+
standardize_test_input_model_output

+------------+------------+
| feature    | json       |
+------------+------------+
| col_bigint | {"name": "standardize", "type":"bigint", "paras":{"mean":12, "std": 1.58113883008419}} |
| col_double | {"name": "standardize", "type":"double", "paras":{"mean":10.3, "std": 0.1581138830082909}} |
+------------+------------+
standardize_test_input_output_using_model

+------------+------------+------------+-------------+--------------+
| col_string | col_bigint | col_double | col_boolean | col_datetime |
+------------+------------+------------+-------------+--------------+
| 01         | -1.2649110640673515 | -1.264911064068383 | true        | 2016-07-01 10:00:00 |
| NULL       | -0.6324555320336758 | -0.6324555320341971 | false       | 2016-07-02 10:00:00 |
| 02         | NULL       | 0.0        | true        | 2016-07-03 10:00:00 |
| 03         | 0.0        | NULL       | false       | 2016-07-04 10:00:00 |
| 04         | 0.6324555320336758 | 0.6324555320341858 | NULL        | 2016-07-05 10:00:00 |
| 05         | 1.2649110640673515 | 1.2649110640683716 | true        | NULL         |
+------------+------------+------------+-------------+--------------+
standardize_test_input_output_using_model_model_output

+------------+------------+
| feature    | json       |
+------------+------------+
| col_bigint | {"name": "standardize", "type":"bigint", "paras":{"mean":12, "std": 1.58113883008419}} |
| col_double | {"name": "standardize", "type":"double", "paras":{"mean":10.3, "std": 0.1581138830082909}} |
+------------+------------+
类型转换
组件功能介绍
将表的字段类型转成另一个类型。

PAI命令
PAI -name type_transform
    -project algo_public
    -DinputTable="pai_dense"
    -DselectedCols="gail,loss,work_year"
    -Dpre_type="double"
    -Dnew_type="bigint"
    -DoutputTable="pai_temp_2250_20272_1"
    -Dlifecycle="28"
算法参数
参数名称	参数描述	参数值可选项	默认值
inputTable	必选，输入表的表名	-	-
inputTablePartitions	可选，输入表中指定哪些分区参与训练，格式为: partition_name=value。如果是多级，格式为name1=value1/name2=value2；如果指定多个分区，中间用‘,’分开	-	输入表的所有partition
outputTable	必选，类型转换的结果表	-	-
selectedCols	必选，需要类型转换特征列，必须是同一个类型	-	-
pre_type	原字段类型，必须与selectedCols勾选的字段类型一致，否则会报字段类型不一致错误	-	-
new_type	新的字段类型，用户设置		100
lifecycle	outputTable结果表生命周期，可选，默认7		7
使用DEMO
拖拽一个读数据表组件，配置数据表pai_dense结构如下。
image

image

拖拽一个类型转换组件, 在右侧参数配置栏中：勾选需要转换的特征，比如下图勾选了3个原数据类型为double的列（主要double类型必须与勾选的字段类型一致），转换成bigint类型。
image

image

右键点击运行后，可以看到输出结果表字段类型变化如下。
image

KV2Table
给定kv（key:value）格式，转成普通表格式，key转换成表的某列名，value转成该列在对应行的值。

kv表格式定义：Key是列名的index，value支持bigint或double，这样能直接作为稀疏格式输入其他包括逻辑回归、线性回归等算法组件，key也支持string类型。在该组件中可以输入用户定义的key_map表，是列名和key的映射，但无论是否输入这张表，该组件都会输出key_map表记录转化后列名和key的映射。

kv
1:10;2:20;3:30
KeyMap表格式定义：包含列名和index的映射以及类型信息的三元组表：col_name，col_index，col_datatype，这三列类型要求是string，其中col_datatype缺失时默认值为“double”。

col_name	col_index	col_datatype
col1	1	bigint
col2	2	double
PAI命令
PAI -name KVToTable
    -project algo_public
    -DinputTableName=test
    -DoutputTableName=test_out
    -DoutputKeyMapTableName=test_keymap_out
    -DkvColName=kv;
参数说明
参数名称	参数描述	取值范围	是否必选，默认值
inputTableName	输入表名	-	必选，不能为空表
kvColName	kv列名	限选一列	必选
outputTableName	输出表名	-	必选
outputKeyMapTableName	输出索引表名	-	必选
inputKeyMapTableName	输入索引表名	-	可选，“”
appendColName	附加列名	可选多列	可选，“”
inputTablePartitions	输入表分区	-	可选，””
kvDelimiter	key和value之间分隔符	-	可选，默认”:”
itemDelimiter	kv对之间分隔符	-	可选，默认”,”
top1200	是否只截取前1200列	-	可选，默认true，该选项为false时，超过最大列数会报错
lifecycle	生命周期	>=-1整数	可选，默认-1，不设生命周期
coreNum	核数目	>0整数	可选，默认-1，会根据输入数据量计算所起instance的数量
memSizePerCore	内存数	(100,64*1024)	可选，默认-1，会根据输入数据量计算所需内存大小
具体示例
数据生成

    drop table if exists test;
        create table test as
          select
            *
          from
          (
            select '1:1,2:2,3:-3.3' as kv from dual
            union all
            select '1:10,2:20,3:-33.3' as kv from dual
          ) tmp;
PAI命令行

 PAI -name KVToTable
    -project algo_public
    -DinputTableName=test
    -DoutputTableName=test_out
    -DoutputKeyMapTableName=test_keymap_out
    -DkvColName=kv;
输出说明

输出表

+------------+------------+------------+
| kv_1       | kv_2       | kv_3       |
+------------+------------+------------+
| 1.0        | 2.0        | -3.3       |
| 10.0       | 20.0       | -33.3      |
+------------+------------+------------+
输出映射表

+------------+------------+------------+
| col_name   | col_index  | col_type   |
+------------+------------+------------+
| kv_1       | 1          | double     |
| kv_2       | 2          | double     |
| kv_3       | 3          | double     |
+------------+------------+------------+
算法规模
转化后的列包含append列和kv所转化的列，先输出kv列再输出append列，当总列数超过odps最大列数限制，输出top1200选项为True，则输出最大列数，否则报错，目前odps的最大列数为1200列。

数据量不超过1亿条记录。

常见问题
若有输入key_map表，转化的列是key_map表中的key和kv表中的key的交集。

转化的列类型只支持数值类型。

若有输入key_map表，转化后key列类型和key_map表中一致，若无，类型为double。

没有输入keymap表，转化后key列名的命名规则为：kv列的列名+“”+key， 若key中包含”%&()*+-./;<>=?”中任一字符，都不符合odps的命名规则，会报错。

列名冲突规则：若指定了append列，且append列名和转化后key列名相同，会报错。

同一行有重复key处理：value值相加。

列名长度超过128个字符将被截断成128个字符。

Table2KV
将普通表（table）转成kv格式的表。原表中的空值不会被表示出来。用户可以指定在生成表中保留某些列，这些列会原样输出。

PAI命令
PAI -name TableToKV
    -project algo_public
    -DinputTableName=maple_tabletokv_basic_input
    -DoutputTableName=maple_tabletokv_basic_output
    -DselectedColNames=col0,col1,col2
    -DappendColNames=rowid;
参数说明
参数名称	参数描述	取值范围	是否必选，默认值
inputTableName	输入表	表名	必选
inputTablePartitions	输入表中指定哪些分区参与训练， 格式为: Partition_name=value，如果是多级格式为name1=value1/name2=value2; 如果是指定多个分区， 中间用‘,’分开		可选， 默认值选择所有分区
selectedColNames	选择列名，只能是bigint或double	列名	可选，选择全表
appendColNames	需要保留的列名，该列会被原样写入到输出表中	列名	可选，无
outputTableName	输出KV表	表名	必选
kvDelimiter	key和value的分割符	字符	可选，‘:’
itemDelimiter	kv间的分割符	字符	可选，‘,’
convertColToIndexId	指定是否将列转换为编号，1为转换，0为不转换	0或1	可选，0
inputKeyMapTableName	输入索引表名，只在convertColToIndexId=1时有效，若未指定该参数，程序自动计算一套编号	表名	可选，””
outputKeyMapTableName	输出索引表名，当且仅当convertColToIndexId=1时必选	表名	由convertColToIndexId决定
lifecycle	可选，指定输出表的生命周期	正整数	没有生命周期
coreNum	节点个数	与参数memSizePerCore配对使用，正整数，范围[1, 9999]	可选， 默认自动计算
memSizePerCore	单个节点内存大小，单位M	正整数，范围[1024, 64*1024]	可选， 默认自动计算
具体示例1
数据生成

rowid	kv
0	col0:1,col1:1.1,col2:2
1	col0:0,col1:1.2,col2:3
2	col0:1,col1:2.3
3	col0:1,col1:0.0,col2:4
PAI命令行

PAI -name TableToKV
    -project algo_public
    -DinputTableName=maple_tabletokv_basic_input
    -DoutputTableName=maple_tabletokv_basic_output
    -DselectedColNames=col0,col1,col2
    -DappendColNames=rowid;
输入说明输出说明

输出表
maple_tabletokv_basic_output

rowid:bigint	kv:string
0	1:1.1,2:2
1	1:1.2,2:3
2	1:2.3
3	1:0.0,2:4
具体示例2
PAI -name TableToKV
    -project projectxlib4 -DinputTableName=maple_tabletokv_basic_input
    -DoutputTableName=maple_tabletokv_basic_output
    -DselectedColNames=col0,col1,col2 -DappendColNames=rowid
    -DconvertColToIndexId=1
    -DinputKeyMapTableName=maple_test_tabletokv_basic_map_input
    -DoutputKeyMapTableName=maple_test_tabletokv_basic_map_output;
输入说明输出说明

输出表
maple_test_tabletokv_basic_map_output

col_name:string	col_index:string	col_datatype:string
col1	1	bigint
col2	2	double
常见问题
若有输入key_map表，转化的列是key_map表中的key和kv表中的key的交集。
若有输入key_map表，且类型与输入表冲突，则新输出的key_map表以用户指定类型为准。
输入表中要转为kv的列只能为bigint和double类型




===========

ETL 34个子系统

子系统1 数据分析

数据内容 一致性 结构描述
select distinct 查询数据库字段

投资采用某种工具
学会使用工具

分析评估 用或不用
可用性

预先开展数据

预先开展数据分析工作


---------------

子系统2 变化数据获取系统

变化数据获取 CDC 最新的数据源分离

仅传输那些山慈加载后发生变化的数据


目标：

分离变化数据 允许可选择加载过程
获取源数据所有的变化 删除 更新 插入 包括非标准接口产生的变化
用变化原因标记变化了得数据 区分对错误更正和真正的更新
利用其他元数据支持合规性跟踪
尽早执行CDC步骤 最好是在大量数据传输到数据仓库前完成

获取数据变化不是一件小事

您必须仔细评估针对每个数据源的获取策略

确定适当策略以区分变化的数据 采用一些侦察性工作

获取源数据变化的方式：

1.审计列

存储一条记录插入或修改的日期和时间
一般是激活触发器自动产生的 或者是由其他任何方式建立的字段被加载时 一定要注意他们的完整性
分析并测试每一列以确保他们是表示变化的可靠来源

如果存在null 一定要找到其他用于检测变化的方法

当这些字段原系统应用建立 DBA小组允许在后端用脚本更新数据
记录被从源中删除 查询和审计列可能无法获得删除事件


2.定时获取

选择那些建立日期或修改日期=SYSDATE-1 （昨天的记录）的行

人工干预对数据清洗
如果晚间加载过程失败 推迟一天 意味着丢失的数据无法进入数据仓库


3.全差异比较

保存所有昨天数据快照 比较

将其与今天的数据逐个比较 找到发生变化的数据
非常严密 保证找出所有的变化
缺点 对资源消耗巨大

尽力利用源机器

研究使用循环冗余校验 Cyclic Redundancy Checksum CRC
算法 快速检测某个复杂的记录是否发生改变 不需要对每个字段检验

4.数据库日志获取
有效日志抓取利用时间计划点（通常是午夜时间）的数据库重做日志快照
并在那些ETL加载用到的受影响的表的事务搜寻

检测包括轮询重做日志
实时获取事务

事务日志经常被装满 阻碍过程中新事务的加入
日志可能被DBA清空

一定要和DBA沟通好

5.消息队列监控

基于消息事务系统中 监视所有针对感兴趣表的事务的队列
开销相对较低

消息队列 没有回放功能
与消息队列的连接消失 将会丢失数据

--------------------

子系统3  获取系统

各种主机环境获取数据
COBOL复制手册
EBCDIC到ASCII转换
压缩十进制
重新定义
OCCURS字段
多个可变记录类型

另外一些组织可能需要从关系DBMS 平面文件 XML源 web日志 复杂的ERP系统

文件 流

数据流 出原系统 通过转换引擎 以单一过程进入过渡数据库

文件->ETL服务器->转换文件内容->加载转换后的数据->过渡服务器

文件 加密 压缩 正确性 对比 ftp

对数据压缩后再传输

压缩 加密

=====================

清洗 整合数据

增加数据价值
子系统建立用于诊断系统何处出现问题的元数据
业务流再造 随着时间不断改进数据质量

----------------------

提高数据质量文化和过程

拆散业务流程

定义一个针对数据质量文化的高级别的承诺
在执行层面上发起过程再造
改进数据录入环境
改进应用集成
改变工作过程
促进端到端团队意识
促进部门间合作
大力褒奖卓越的数据质量
不断度量并改进数据质量

改进数据录入系统

--------------

子系统4 数据清洗系统

提供清洗数据 获取数据质量事件
度量并最终控制数据质量的全面结构

清洗数据 获取数据质量事件 度量并最终控制数据仓库中的数据质量

数据轮廓

有助于理解改善潜在的脏数据 不可靠数据的分析

确定完成工作的复杂程序

尽早诊断 分类数据质量问题
为获得更好的数据 对源系统及集成工作的需求
提供在数错误的专门描述
获取所有数据质量错误以及随时间变化精确度量数据质量矩阵的框架
附加到最终数据上的质量可信度度量

1.质量屏幕

诊断过滤数据流流水线的质量屏幕集合

每个质量屏幕就是一个测试

挂起 标记
屏幕结构类似

三种类型

列屏幕测试

单一列的数据
简单的 比较明显的测试

测试某列是否包含未预期的空值
某个值是否超出了规定的范围
某个值是否未遵守需要的格式

结构屏幕
测试数据列之间的关系

测试两个或更多的列 以验证他们是否满足层次要求
一系列多对一关系
测试两个表存在的外键主键约束
对整个列快的





=========================


列块测试 验证邮政编码是否合法

业务规则屏幕 以上两种测试无法适应的测试

聚集阈值数据质量检查


质量事件响应

错误发生时决定如何操作每种质量屏幕

1.终止过程 需要人工参与

2.将错误记录发送到搁置文件中，以便后续处理 不清楚何时或者是否这些记录将被更正并重新进入流水线
数据库完整行无法保证 丢失了记录 少量数据错误时不要使用搁置文件

3.仅对数据进行标注 并放到流水线下一个步骤（推荐）

不好的维度 可以使用审计维度进行标记 面对丢失或垃圾数据 可以在属性上标记唯一错误值

-------------------------

对数据采集、数据同步、ETL、数据离线实时开发、数据建模、数据分析挖掘、数据服务的规范有系统、深入的研究和总结
元数据 数据质量保证


===================

错误事件模式

集中式的维度模式
记录ETL流水线中所有质量屏幕出现的错误事件
可用于一般的需要在遗留应用之间传输数据的数据集成应用

该模式中 主表错误事件事实表
粒度是每个由ETL系统的质量屏幕抛出的（产生的）错误
事实表的粒度 是对每个事实存在的原因的物理描述
每个质量屏幕错误在表中用一行表示 对应一个观察到的错误

错误事件事实表维度：
错误的日历日期
错误产生的批处理作业
产生错误的屏幕

日历日期：按照日历的通常属性 例如平日或某财务周期的最后一天 提供一种约束并汇总错误事件的方法

错误日期 时间事实是一种完整关联的日期 时间戳 精确定义了错误发生的时间  方便计算错误事件发生的时间间隔
可通过获得两个日期/时间戳的差别获得不同事件发生的间隔时间

批处理维度 可泛化为正对数据流的处理步骤 不仅仅正对批处理
屏幕维度准确识别是何种屏幕约束 驻留在屏幕上的是何种代码 屏幕抛出错误采取了什么措施
例如停止过程 发送记录到挂起文件或标记数据

错误时间事实表
单列的主键 作为错误事件的键
此代理键类似于维度表主键  随着行增加到事实表中而顺序分配的整数
该键列 同时被增加到错误事件事实表中。

错误事件细节事实表
每行确定与错误有关的某个特定记录个体字段。

单一错误时间或业务规则错误多行表示
错误事件键关联

多字段 多记录错误

包含准确的日期 时间戳 完整描述聚集阈值错误事件 涉及某个时间内多条记录

每个质量屏幕具有在错误发生时将错误添加到表中的功能


提供一种约束并

--------------------
审计维度装配器
特殊维度
用于在后端装配ETL系统每个事实表
审计维度包括建立特定事实行 元数据 实际数据

按批处理文件每天更新一次
分类 分数 版本号


audit key
overall quality rating
complete flag
validation flag


------------------------------------------


重复数据删除deduplication
维度多个源

多个面向客户的源系统建立管理不同的客户主表 组织中常见

客户信息可能需要从多个业务项和外部数据源中融合获得
不同字段评估 模糊评判标准 近似匹配

数据保留survivorship 合并匹配记录集合到统一视图的过程 反向引用 最佳保留

一致性处理包含 所有需要调整唯独中的一些或所有列内容相似 必须至少有一个具有相同名称和内容的公共属性
考虑使用单一的一致性属性

扩展列表 参与横向钻取查询过程
如城市 州和国家

合并且集成从多个源输入的数据
相同结构 重复数据删除  过滤无效 标准化

两个需要一致性处理的唯独


================
发布准备展现

子系统对清洗和对数据的一致性处理

基本加载规划简单
对数据转换 并建立维度行
加载到目标展现表

代理减分配
代码查找
划分或合并列

缓慢变化维度

类型1 重写

重写一个或多个熟悉
更新可以警用数据库日志 或使用DBMS批量加载

更新会使聚集操作失效 必须通知受影响的事实提供者删除并重新受影响的聚集



类型2 增加新行

强大的变化数据获取系统 检测发生的变化

复制先前维度行的版本 从头开始建立新行

更新变化和增加列
代理减映射表
巨大作用

使用循环冗余校验CRC比较确定上次更新 原数据变化

按序列建立新的代理减 并更新代理减映射表
时间戳
描述变化熟悉

时间戳定义了从开始到结束有效时间访问

增加5个附加的ETL管理列
找到先前有效的行 对管理列适当更新

改变日期 日期唯独支架表 外键
行有效日期/时间 准确的发生变化时的
行截止日期/时间 准确的下一次变化的日期 时间戳 大多数当前维度行默认值为12 31 9999
列变化原因 可选属性
当前标志 当前 失效

坚持使用系统或截止日期获取类型2有效时间戳
Kimball Design TOp #80 增加行

http://www.kimballgroup.com/wp-content/uploads/2015/12/Kimball-Group-Reader-2nd-Edition-Table-of-Contents.pdf


类型3：增加新属性

允许用户既可以使用属性旧值 也可以使用新值

需要将已经存在的列值添加新增加的列中 并在原列中存储ETL系统提供的值

更新会使更新列聚集操作失效 必须通知受影响的事实提供者删除并重新受影响的聚集

----------------

类型4 增加微型维度

一组属性变化非常快

快速变化超大维度
设计阶段做好

微型维度有自己的唯一主键

主维度的主键
微型维度的主键都必须出现在事实表中

销售事实-客户维度
    -人口统计维度

------------------
类型5 增加微型维度和类型1支架

在主维度-微型维度上嵌入类型1引用
可允许直接通过主维度访问微型维度上的当前值 不需要通过事实表连接

只要微型维度当前状态随时间变化

ETL小组必须在主维度上增加类型1键引用 且必须在所有主维度的副本上重写该键引用


-------------------------

类型6 在类型2维度中增加类型1属性

包含一个嵌入属性  作为通常类型2属性替换值
通常该属性就是类型3的另一种实现
但在此情况下 一旦属性被更新 将被系统性重写

-------------------------------

类型7 双重类型1 及类型2 维度

常见的类型2维度  与特定构建的事实表成对出现
均有一个与维度关联的常态化外键

用于处理类型2历史过程

另外也包含一个持久性外键 参考图19-9 用于替换类型1当前过程 连接到维度表的持久键标记为PDK
还包括当前行标识  表示该行是否用作SCD类型1场景

ETL小组必须增加一个正常构建的包含该常量值持久性外键的事实表

销售事实表
Fk,FDK

客户维度表
pk pdk

---------------

子系统10：代理键产生器

独立为所有维度产生代理减
独立于数据库实例 能够为分布式客户提供服务

目标是产生无语义的键 通常是一个整数 将成为维度行的主键

也可以由DBMS分派代理减
-------------------

子系统11 层次管理器

维度中通常具有多个同时存在的 嵌入的层次结构
以属性形式简单地共存于同一个维度表中

作为维度主键的属性必须具有单一值

固定深度的层次具有一致层次好 可以将其建模 将不同维度熟悉 添加到每个层次

通信地址 这样轻微层次不齐的层次被建模成固定层次

包含组织映射的桥接表


桥接表

简述桥接表是如何将维度表和事实表进行关联的？

答：桥接表（Bridge Table）是维度建模中的一类比较特殊的表。

在数据仓库的建模时，会遇到具有层次结构的维度表，对于这样的表有一种建模方式是建立父子表，即每条记录上包括一个指向其父记录的字段。这种父子表的建立在层级深度可变时尤其有用，是一个紧凑而有效的建模方式。但是这种建模方式也有缺点，就是用标准SQL很难对递归结构进行操作。

与这种递归结构的父子表不同，桥接表采用不同的建模方式也可以表示这种层级结构。桥接表是建立在维度表和事实表中间的一个具有较多冗余信息的表，其中的记录包含层级结构中节点到其下面每个节点的路径。表结构如下所示：

父关键字

子关键字

父层数

层名

底端标识

顶端标识

在桥接表中，节点与其下面的任意一个节点都建立一个关联记录保存在表中，即父子关系不再局限在相邻层，如第一层与第三层同样有父子关系，通过父层数可以区分相隔了几层。这样，可以通过父层数和父子关系来进行层级结构的查询。

当然，桥接表也不是一个完备的解决方案，它只能是在某些情况下是查询变得容易。


ELT系统负责强化业务规则 确保在维度表张加入适当的层次

可用于辅助维护ETL数据流以添加或维护层次属性

-------------------

特定维度管理器

全方位的子系统

1.支持组织特定维度设计特征的ETL结构占位符；另一些可能仅需要其中的一些设计技术

1.日期/时间维度
唯一一种在数据仓库项目开始时就完整定义的维度  没有约定的来源
当处理跨国组织环境时  考虑到多个财务报表周期或多种不同的传统日历 即使这样一个简单的维度也会带来挑战

-----------------------------
2.杂项维度
从事实表中删除所有关键属性遗留下来文本和繁杂表示

1.行的理论上的号码确定已知 可以预先建立杂项维度

需要聚集杂项维度属性 并将它们与已经存在的杂项维度行比较 以确定该行是否已经存在
不存在 组建新的维度行
建立代理减 在处理事实表过程中时时将该行加载到杂项维度中...


3 微型维度

大型维度中
当类型2 技术不可用时
客户维度 跟踪维度熟悉变化的技术

1.预先建立所有合法组合或重组并及时建立新的组合

维护多列代理减查询表确定基本维度号码和适当微型维度行 支持代理流水线过程

非常大 复杂的客户维度通常需要多个微型维度


4.缩减子集维度
一种一致性唯独 行 或者列是基唯独的子集

ETL数据流应当根据基础维度建立一致性缩减维度
以确保一致性

主键必须独立构建

5.小型静态维度

某些维度 可能是ETL系统在没有真实外部来源情况下建立
通常是小型查询维度  在该维度中操作型代码被转换成字词

6.用户维护的维度
全新的 “主”维度表 没有正式的记录系统
由业务报表和分析过程建立的自定义描述 分组和层次

最好由适当的业务用户部门维护 DWBI提供适当的接口

为新行添加默认属性值 由用户更新

在没有发生变化情况下被加载到数据仓库

建立唯一的默认维度属性描述 标识等待人工完成 如Not Yet Assigned

与代理链值连接起来的方式 Not Yet Assigned 157

多个未分配值不会再聚集表或报表中被无意集中在一起 有利于区分哪些后期更正的行

-------------------

事实表建立器

组织度量

事务

周期快照

累积快照

加载事实表时一个主要的需求是维护相关维度表直接的参照完整性

代理减流水线 设计帮助实现该需求

1.事务事实表加载器

特定时刻度量事件

发票的列表项 就是一种事务事件的示例

现金收款台的扫描设备事件

时间戳
日历粒度外键

一堆包含日期 时间戳的日期粒度外键
取决于源系统提供的需求

必须与粒度吻合合并且仅应该描述在那个时刻发生了什么

事务粒度事实表
三类事实表中最大且最详细

从变化数据获取系统接收数据 并以适当的维度外键加载

目标事实表按时间分区 提高表的性能

应当包含审计键 系列化ID或时间戳列 以方便备份 重新开始加载工作

更新：1.插入更正的行 2.删除原始行

顺序分配单一代理减


2.周期快照事实表加载器

一种常规重复的度量或度量集合 类似银行账户每月报表

还包括单一日期列 表示整个周期

描述定义周期的时间范围的度量

通常用于表示账户余下额度 每月财务报表 库存余额
周期：天 周 月

插入和更新过程相同

可以以最近时间分区聚类

适当的时期结束时被集体加载

信用卡公司 可以在每月结束时 按有效余下额度加载每月账户余下额度快照表

组织将添加热轧制
还加载一些前一天的包含最新有效余下额度的特殊行

当前月行不断更新


3.累积快照事实表加载器

有一个明确的开始和结束过程的当前发展状态
持续时间段 无法归类到周期快照
订单在一个报告期内被发出 货运及支付
事务粒度提供了太多的细节 分布在个体事实表行中
包含一系列日期 用于描述典型处理工作流
订单日期 实际发货日期 交货日期 最终付款日期 退货日期
5个不同的日期值代理减外键出现的

事实地被修改 日期外键被重写 各类事实被更新

利用可变长度的行 有时偶尔在更新活动发生后 删除并重新加载行有利于改善性能

最近的视图

--------------------------------------------

子系统14：代理减流水线

将输入事实表行的操作型自然见

再计算一年最多可能产生的分区数是： 12 ×（ 1 +(30+29)/2)=5232 个。

采用极限存储的处理方式，极大地压缩了全量存储的成本，

又可以达到对下游用户透明的效果，是一种比较理想的存储方式。但是其本身也有一定的局限性，

首先，其产出效率很低，大部分极限存储通常需要t-2;

其次，对于变化频率高的数据并不能达到节约成本的效果。

因此，在实际生产中，做极限存储需要进行一些额外的处理。

在做极限存储前有一个全量存储表，全量存储表仅保留最近一段时间的全量分区数据，

历史数据通过映射的方式关联到极限存储表。

即用户只访问全量存储表，所以对用户来说极限存储是不可见的

·对于部分变化频率频繁的宇段需要过滤。例如，用户表中存在用户积分宇段，这种宇段的值每天都在发生变化，

如果不过滤的话，极限存储就相当于每个分区存储一份全量数据，起不到节约存储成本的效果。


--------------------------------------------------------------

类型3 增加新列


采用极限存储的处理方式，极大地压缩了全量存储的成本，
又可以达到对下游用户透明的效果，是一种比较理想的存储方式。
但是其本身也有一定的局限性，

首先，其产出效率很低，大部分极限存储通常需要t-2 ；
其次，对于变化频率高的数据并不能达到节约成本的效果。因此，
在实际生产中，做极限存储需要进行一些额外的处理。

·在做极限存储前有一个全量存储表，全量存储表仅保留最近一段
时间的全量分区数据，历史数据通过映射的方式关联到极限存储
表。即用户只访问全量存储表，所以对用户来说极限存储是不可
见的。

·对于部分变化频率频繁的宇段需要过滤。例如，用户表中存在用
户积分宇段，这种宇段的值每天都在发生变化，如果不过滤的话，
极限存储就相当于每个分区存储一份全量数据，起不到节约存储
成本的效果。


------------------------------------------------


代理键

层次管理

特定唯独管理

事实表建立

代理键流水线

多值维度桥接表建立器

迟到数据处理器

维度管理器

事实提供者

聚集建立器

OLAP多维数据库建立器

数据传播管理器

---------------
任务调度器
备份系统
恢复与重启系统
版本控制系统
版本迁移系统
工作流监视器
排序系统
世系及依赖分析器
问题提升系统
并行 流水线系统
安全系统
合规性管理
元数据存储库

----------------

ETL 系统设计与开发过程和任务

过程概览
开发规划
    设计高层规划
    选择ETL工具
    开发默认策略
    按照目标表钻取数据
    开发ETL规范文档
开发一次性的历史加载过程
    用历史数据填充维度表
    完成事实表历史加载
开发增量式ETL过程
     维度表增量处理
     事实表增量处理
     聚集表与OLAP加载
     ETL系统操作与自动化
实时影响
    实时分类
    实时结构权衡
    展现服务器上的实时分区

实时分类
实时结构权衡
展现服务器上的实时分区

驱动ETL 开发

BPMS流程驱动

通过元数据，指导ETL 工作，提高ETL 的效率。在“数据同步”
章节中，我们提到了通过元数据驱动一键、批量高效数据同步的
OneClick 。OneClick 覆盖的另一个场景是存量数据日常维护，其主要功
能如图12.3 中的“数据运维”部分所示。

流程管理

一键同步

全量同步
增量同步
增全量同步
一次性全量
小时增量
小时全量同步


流程设计



流程实例管理

流程运行管理

数据运维

任务查询定位
加字段
同步类型转换
数据压缩
表备份
任务下线
下线恢复
任务删除
表删除
自动通知



流程变量绑定

接入ACL权限管理
