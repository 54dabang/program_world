15.1 数据质量保障原则 285
15.2 数据质量方法概述 287
15.2.1 消费场景知晓 289
15.2.2 数据加工过程卡点校验 292
15.2.3 风险点监控 295
15.2.4 质量衡量 299


数据质量，主要从四个方面进行评估，即完整性、准确性、一致性和及时性，

数据，最终是要服务于业务价值的，因此，本文不会单纯讲解理论，而是会从数据质量监控这一数据的应用为出发点，为大家分享居士对数据质量的思考。通过本文，你将获得如下几方面的知识点：

数据质量核心关注的要点
从数据计算链条理解，每一个环节会出现哪些数据质量问题
从业务逻辑理解，数据质量监控能带来的帮助
实现数据质量监控系统时要关注的点
数据质量监控面临的一些难点和解决思路

一、完整性
完整性是指数据的记录和信息是否完整，是否存在缺失的情况。数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成统计结果不准确，所以说完整性是数据质量最基础的保障。

简单来讲，如果要做监控，需要考虑两个方面：一是，数据条数是否少了，二是，某些字段的取值是否缺失。完整性的监控，多出现在日志级别的监控上，一般会在数据接入的时候来做数据完整性校验。

二、准确性
准确性是指数据中记录的信息和数据是否准确，是否存在异常或者错误的信息。

直观来讲就是看数据是否上准确的。一般准确性的监控多集中在对业务结果数据的监控，比如每日的活跃、收入等数据是否正常。

三、一致性
一致性是指同一指标在不同地方的结果是否一致。

数据不一致的情况，多出现在数据系统达到一定的复杂度后，同一指标会在多处进行计算，由于计算口径或者开发人员的不同，容易造成同一指标出现的不同的结果。

四、及时性
在确保数据的完整性、准确性和一致性后，接下来就要保障数据能够及时产出，这样才能体现数据的价值。

及时性很容易理解，主要就是数据计算出来的速度是否够快，这点在数据质量监控中可以体现在监控结果数据数据是否在指定时间点前计算完成。

0x02 数据处理各环节的数据质量
数据质量监控之所以难做，是因为在数据的各个环节都会出现数据质量的问题。因此，本节将以一个典型的数据处理链条为例，为大家分享在每个阶段容易出现哪些数据质量问题。

如下图，为了举例说明，我画了一个简单的数据处理流程（在实际中的情况会比该情况复杂很多），我将数据处理分为 3 个阶段：数据接入、中间数据清洗、结果数据计算。



如上图所示，数据接入环节最容易出现的是数据完整性的问题，这里要特别注意的是数据量是否陡增和陡降。

陡增意味着可能会出现大量数据重复上报或者异常数据侵入等情况，陡降意味着可能出现数据丢失的情况。

另一方面，也要检查不同字段的的取值是否有丢失，比如地址和设备字段是否出现大量空值等异常。

数据清洗
在这里，我将数据清洗的范围局限在数据仓库的中间表清洗上，这一部分一般也是我们的数据仓库要建设的核心部分，业务到了一定程度，数据中间层的建设必不可少！

在这一环节，最容易出现的是数据一致性和数据准确性的问题。数据中间层保障来数据是从统一出口而出，让数据一起对或者一起错。但是很难保证数据准确性的问题，因此在数据清洗阶段需要尽量保障数据的准确性。

数据结果
结果数据，主要是强调对外提供数据的过程，一般是从中间表中计算或直接取得的可展示数据。这里是业务方和老板最容易感知的到的地方，因此在这环节，主要关注的是数据准确性和数据及时性。

整体来讲，数据的完整性、准确性、一致性和及时性在数据处理的各个阶段都需要关注，但是可以先抓住的核心的问题来解决。

0x03 业务流程各环节的数据质量
聊完数据处理，我们继续聊一下业务流程。数据最终的价值是要服务于业务的，因此数据质量最好也是能从解决业务问题出发，因此，本节从典型的业务场景来讲解数据质量该怎么做。

首先，居士认为，既然做监控肯定是要考虑使用方的，而我们的数据质量监控平台一个很重要的作用是希望让老板、产品和运营这些使用方对我们的数据放心，那么他们的关注点是什么？居士认为，是业务指标！

那么，这个业务指标可以从两个角度来考虑：

单个指标的数值异常，比如说数据是否达到来某个临界值？是否有陡增和陡降？
整个业务链条的数据是否有异常，比如从曝光到注册的转化是否有异常？
如下图，是一个 App 的用户行为漏斗分析，其实也就是从获取用户到转化的简单链路。

那么针对该链路，我们数据质量监控要做的事，除了告诉使用方某一个节点的值有问题，也需要告诉他们整个链条哪里出了问题，哪里的转化低了。


==================================

0x04 如何实现数据质量监控

一、设计思路
数据质量监控的设计要分为四个模块：数据、规则、告警和反馈。

数据：主要是需要被数据质量监控到的数据，数据可能存放在不同的存储引擎中，比如Hive、PG、ES等。

规则：是指如何设计发现异常的规则，一般而言主要是数值的异常和环比等异常监控方式。也会有一些通过算法来发掘异常数据的方法。

告警：告警是指出发告警的动作，这里可以通过微信消息、电话、短信或者是微信小程序的方式来触发告警内容。

反馈：这里需要特别注意，反馈是指对告警内容的反馈，比如说收到的告警的内容，那么负责人要来回应这个告警消息是否是真的异常，是否需要忽略该异常，是否已经处理了该异常。有了反馈的机制，整个数据质量监控才容易形成闭环。更能体现业务价值。



二、技术方案

最开始可以先关注核心要监控的内容，比如说准确性，那么就对核心的一些指标做监控即可，不用开始就做很大的系统。
监控平台尽量不要做太复杂的规则逻辑，尽量只对结果数据进行监控。比如要监控日志量是否波动过大，那么把该计算流程前置，先计算好结果表，最后监控平台只监控结果表是否异常即可。
多数据源，多数据源的监控有两种方式可以处理：针对每个数据源定制实现一部分计算逻辑，也可以通过额外的任务将多数据源中的数据结果通过任务写入一个数据源中，再该数据源进行监控，这样可以减少数据监控平台的开发逻辑。具体的优缺点可以自行衡量。
实时数据的监控，实时和离线数据监控的主要区别在于扫描周期的不同，因此在设计的时候可以先以离线数据为主，但是尽量预留好实时监控的设计。
在设计之初，尽量预留好算法监控的设计，这是一个很大的加分项，具体的结合方式也可以和第二点建议接近，比如算法异常数据放到一张结果表中，再在上面配置简单的告警规则即可。

问题一：假设你的结果表要经过多层的中间表计算，你怎么保证每个环节都是正确的，且最终结果是正确的？

每一层代码有 Code Review，保证代码逻辑正常。
单独一条计算流，对关键指标从原始数据直接计算结果，和日常的结果表做对比，发现不同则告警。这种方式也可以理解为是结果数据和源数据的对账。
问题二：告警信息太多了，太容易被忽略怎么办？

思路：主要是思路是提高告警的准确率，避免无用的告警，有三个思路：

多使用机器学习算法的方式来发现异常点，比如：异常森林。
加入反馈机制，如果业务负责人认为该告警是正常的，就打上正常的tag，后续告警规则根据反馈进行优化。
加入屏蔽功能，屏蔽不感兴趣的告警。
0x06 补充
草稿发出来后，收到了一些反馈，但是要将这些反馈都融入到文章中需要较多的时间，因此先将内容在展现出来，供大家参考。

数据准确性 是建立在合理的业务口径下，从口径角度去统一才会获得准确的结果。

而不是仅仅认为从某个面去看这个数据是准确的就要做统一，不应从数据去逆推口径。

0xFF 总结
和本系列其它文章相似，本文更侧重的是做数据质量过程的思考，这个思考主要体现的地方是，怎么去定义问题和解决问题，而不是直接给出解决的方案。

比如说从数据流程的各个环节来梳理需要做数据质量的点，以及业务方核心会关注的点，这些才是能决定你的数据质量监控平台能否获得认可的关键因素。当这些东西都理清之后，技术实现只是把你的想法具像化的工具，这并非是不重视技术，而是更看重如何让技术的价值最大化。

===============

实验环境准备
必备条件：

开通大数据计算服务MaxCompute
创建大数据开发套件项目空间
进入大数据开发套件，创建DataWorks项目空间
确保阿里云账号处于登录状态。

step1：点击进入大数据（数加）管理控制台>大数据开发套件tab页面下。
step2：点击右上角创建项目或者直接在项目列表—>创建项目，跳出创建项目对话框。1
选择相应的服务器时如果没有购买是选择不了会提示您去开通购买。数据开发、运维中心、数据管理默认是被选择中。

step3：勾选相应的服务单击 确认，跳转到下面的界面，填写相应的信息单击确认，创建项目完成。2
项目名需要字母或下划线开头，只能包含字母下划线和数字。【注意】项目名称全局唯一，建议大家采用自己容易区分的名称来作为本次workshop的项目空间名称。

step4：单击进入项目跳转到下面的界面：进入大数据开发套件
数据质量
数据质量（DQC），是支持多种异构数据源的质量校验、通知、管理服务的一站式平台。数据质量以数据集（DataSet）为监控对象，目前支持MaxCompute数据表和DataHub实时数据流的监控，当离线MaxCompute数据发生变化时，数据质量会对数据进行校验，并阻塞生产链路，以避免问题数据污染扩散。同时，数据质量提供了历史校验结果的管理，以便您对数据质量分析和定级。在流式数据场景下，数据质量能够基于Datahub数据通道进行断流监控，第一时间告警给订阅用户，并且支持橙色、红色告警等级，以及告警频次设置，以最大限度的减少冗余报警。

数据质量的使用流程是，针对已有的表进行监控规则配置，配置完规则后可以进行试跑，验证此规则是否试用。当试跑成功后，可将此规则和调度任务进行关联。关联成功后，每次调度任务代码运行完毕，都会触发数据质量的校验规则，以提升任务准确性。在关联调度后，可根据业务情况，对重要的表进行订阅。订阅成功后，此表的数据质量一旦出问题，都会有邮件或者报警进行通知。

注：数据质量会产生额外的计算费用，在使用时请注意。

新增表规则配置
若已完成《日志数据上传》、《用户画像》实验，我们会得到表：ods_raw_log_d、ods_user_info_d、ods_log_info_d、dw_user_info_all_d、rpt_user_info_d。

数据质量最重要的就是表规则的配置，那么如何配置表规则才是合理的呢？我们来看一下上面这几张表应该如何配置表规则。

ods_raw_log_d
在数据质量中可以看到该项目下的所有表信息，现在我们来给 ods_raw_log_d 表进行数据质量的监控规则配置。



选择ods_raw_log_d表，点击配置监控规则，将会进入如下页面。



我们可以回顾一下 ods_raw_log_d 这张表的数据来源，ods_raw_log_d 这张表的数据是从ftp中获取到的日志数据，其分区是以${bdp.system.bizdate}格式写入进表中（”dbp.system.bizdate” 是获取到前一天的日期）。



对于这种每日的日志数据，我们可以配置一下表的分区表达式，分区表达式有如下几种，我们选择 dt=$[yyyymmdd-1] 这种表达式，有关调度表达式的详细解读，请参考文档调度参数。



注：若表中无分区列，可以配置无分区，请根据真实的分区值，来配置对应的分区表达式。

确认以后，可以见到如下界面，我们可以选择创建规则。



选择创建规则后，出现如下界面：



点击添加监控规则，会出现一个提示窗，来配置规则。



这张表里的数据来源于FTP上传的日志文件，作为源头表，我们需要尽早判断此表分区中是否有数据。如果这张表中没有数据，那么就需要阻止后面的任务运行，因为来源表没有数据，后面的任务运行是没有意义的。

注：只有强规则下红色报警会导致任务阻塞，阻塞会将任务的实例状态置为失败。

我们在配置规则的时候，选择模板类型为表行数，将规则的强度设置为强，比较方式设置为期望值不等于0，设置完毕后点击批量保存按钮即可。



此配置主要是为了避免分区中没有数据，导致下游任务的数据来源为空的问题。

规则试跑
右上角有一个节点试跑的按钮，可以在规则配置完毕后，进行规则校验，试跑按钮可立即触发数据质量的校验规则。



点击试跑按钮后，会提示一个弹窗，确认试跑日期。点击试跑后，下方会有一个提示信息，点击提示信息，可跳转至试跑结果中。





可根据试跑结果，来确认此次任务产出的数据是否符合预期。建议每个表规则配置完毕后，都进行一次试跑操作，以验证表规则的适用性。

在规则配置完毕，且试跑又都成功的情况下。我们需要将表和其产出任务进行关联，这样每次表的产出任务运行完毕后，都会触发数据质量规则的校验，以保证数据的准确性。

关联调度
数据质量支持任务关联调度，在表规则和调度任务绑定后，每次任务运行完毕，都会触发数据质量的检查。可以在表规则配置界面，点击关联调度，配置规则与任务的绑定关系。



点击关联调度，可以与已提交到调度的节点任务进行绑定，我们会根据血缘关系给出推荐绑定的任务，也支持自定义绑定。



选中搜索结果后，点击添加，添加完毕后即可完成与调度节点任务的绑定。



关联调度后，表名后面的小图标会变成蓝色。



配置任务订阅
关联调度后，每次调度任务运行完毕，都会触发数据质量的校验，但是我们如何去跟进校验结果呢？数据质量支持设置规则订阅，可以针对重要的表及其规则设置订阅，设置订阅后会根据数据质量的校验结果，进行告警。若数据质量校验结果异常，则会根据配置的告警策略进行通知。

点击订阅管理，设置接收人以及订阅方式，目前支持邮件通知及邮件和短信通知。



订阅管理设置完毕后，可以在我的订阅中进行查看及修改。



建议将全部规则订阅，避免校验结果无法及时通知。

ods_user_info_d
ods_user_info_d 表的数据来至于rds的数据库，为用户信息表。我们在配置规则的时候，需要配置表的行数校验；还需要配置主键唯一的校验，避免数据重复。

同样，我们还是需要先配置一个分区字段的监控规则，监控的时间表达式为：dt=$[yyyymmdd-1]，配置成功后，在已添加的分区表达式中可以看到成功的分区配置记录。



分区表达式配置完毕后，点击右侧的创建规则，进行数据质量的校验规则配置。添加表行数的监控规则，规则强度设置为强，比较方式设置为期望值不等于0。



添加列级规则，设置主键列(uid)为监控列，模板类型为：字段重复值个数校验，规则设置为弱，比较方式设置为字段重复值个数小于1，设置完毕后，点击批量保存按钮即可。



此配置主要是为了避免数据重复，导致下游数据被污染的情况。

请不要忘记试跑->关联调度->规则订阅。

ods_log_info_d
ods_log_info_d 这张表的数据，主要是解析ods_raw_log_d 表里的数据，鉴于日志中的数据无法配置过多监控，只需配置表数据不为空的校验规则即可。先配置表的分区表达式为：dt=$[yyyymmdd-1]



配置表数据不为空的校验规则，规则强度设置为强，比较方式设置为期望值不等于0，设置完毕后，点击批量保存按钮即可。



请不要忘记试跑->关联调度->规则订阅。

dw_user_info_all_d
dw_user_info_all_d 这个表是针对ods_user_info_d 和 ods_log_info_d 表的数据汇总，由于此流程较为简单，ods层又都已配置了表行数不为空的规则，所以此表不进行数据质量监控规则的配置，以节省计算资源。

rpt_user_info_d
rpt_user_info_d 表是数据汇总后的结果表，根据此表的数据，我们可以进行表行数波动监测，针对主键进行唯一值校验等。先配置表的分区表达式：dt=$[yyyymmdd-1]



然后配置监控规则，单击右侧创建规则，点击添加监控规则。添加列级规则，设置主键列(uid)为监控列，模板类型为：字段重复值个数校验，规则设置为弱，比较方式设置为字段重复值个数小于1。



继续添加监控规则，添加表级规则，模板类型为：SQL任务表行数，7天波动检测；规则强度设置为弱，橙色阈值设置成0%，红色阈值设置成50%（此处阈值范围根据业务逻辑进行设置），配置完毕后，点击批量保存即可。



注：此处我们监控表行数主要是为了查看每日uv的波动，好及时了解应用动态。

请不要忘记试跑->关联调度->规则订阅。

大家可能注意到了，我们在设置表规则强度的时候，数据仓库中越底层的表，设置强规则的次数越多。那是因为ods层的数据作为数仓中的原始数据，一定要保证其数据的准确性，避免因ods层的数据质量太差而影响其他层的数据，及时止损。

数据质量还提供了一个任务查询的界面，在此界面上，我们可以查看已配置规则的校验结果。



