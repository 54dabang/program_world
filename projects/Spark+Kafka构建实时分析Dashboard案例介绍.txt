Spark+Kafka构建实时分析Dashboard案例介绍

 罗道文 2017年4月21日 (updated: 2018年2月26日) 2631
大数据技术原理与应用
返回本案例首页
《Spark+Kafka构建实时分析Dashboard案例介绍》

开发团队：厦门大学数据库实验室 联系人：林子雨老师ziyulin@xmu.edu.cn

版权声明：版权归厦门大学数据库实验室所有，请勿用于商业用途；未经授权，其他网站请勿转载

本教程介绍大数据课程实验案例“Spark+Kafka构建实时分析Dashboard”。在本篇博客中，将要介绍本案例的总体架构，包括案例整体的运行流程以及每个过程具体执行内容。


案例概述
本案例利用Spark+Kafka实时分析男女生每秒购物人数，利用Spark Streaming实时处理用户购物日志，然后利用websocket将数据实时推送给浏览器，最后浏览器将接收到的数据实时展现，案例的整体框架图如下：

Spark+Kafka构建实时分析Dashboard案例框架图
Spark+Kafka构建实时分析Dashboard案例框架图

下面分析详细分析下上述步骤：
1. 应用程序将购物日志发送给Kafka，topic为”sex”，因为这里只是统计购物男女生人数，所以只需要发送购物日志中性别属性即可。这里采用模拟的方式发送购物日志，即读取购物日志数据，每间隔相同的时间发送给Kafka。
2. 接着利用Spark Streaming从Kafka主题”sex”读取并处理消息。这里按滑动窗口的大小按顺序读取数据，例如可以按每5秒作为窗口大小读取一次数据，然后再处理数据。
3. Spark将处理后的数据发送给Kafka，topic为”result”。
4. 然后利用Flask搭建一个web应用程序，接收Kafka主题为”result”的消息。
5. 利用Flask-SocketIO将数据实时推送给客户端。
6. 客户端浏览器利用js框架socketio实时接收数据，然后利用js可视化库hightlights.js库动态展示。

至此，本案例的整体架构已介绍完毕，下篇文章将开始介绍具体操作细节。

《Spark+Kafka构建实时分析Dashboard案例——步骤一：实验环境准备》
开发团队：厦门大学数据库实验室 联系人：林子雨老师 ziyulin@xmu.edu.cn

版权声明：版权归厦门大学数据库实验室所有，请勿用于商业用途；未经授权，其他网站请勿转载

本教程介绍大数据课程实验案例“Spark+Kafka构建实时分析Dashboard案例”的第一个步骤，实验环境准备工作，有些软件的安装在相应的章节还会介绍。


预备知识
Linux系统命令使用、了解如何安装Python库。

训练技能
熟悉Linux基本操作、Pycharm的安装、Spark安装，Kafka安装，PyCharm安装。

任务清单
Spark安装
Kafka安装
Python安装
Python依赖库
PyCharm安装
实验系统和软件要求
Ubuntu: 16.04
Spark: 2.1.0
Scala: 2.11.8
kafka: 0.8.2.2
Python: 3.x(3.0以上版本)
Flask: 0.12.1
Flask-SocketIO: 2.8.6
kafka-python： 1.3.3
系统和软件的安装
Spark安装
Spark的安装可以参考Spark系列教程，地址为Spark2.1.0入门：Spark的安装和使用；

Kafka安装
kafka的安装可以参考博客Kafka的安装和简单实例测试；

Python安装
Ubuntu16.04系统自带Python2.7和Python3.5，本案例直接使用Ubuntu16.04自带Python3.5；

Python依赖库
本案例主要使用了两个Python库，Flask和Flask-SocketIO，这两个库的安装非常简单，请启动进入Ubuntu系统，打开一个命令行终端（可以使用快捷键Ctrl+Alt+T）。
Python之所以强大，其中一个原因是其丰富的第三方库。pip则是python第三方库的包管理工具。Python3对应的包管理工具是pip3。因此，需要首先在Ubuntu系统中安装pip3，命令如下：

sudo apt-get install python3-pip
Shell 命令
安装完pip3以后，可以使用如下Shell命令完成Flask和Flask-SocketIO这两个Python第三方库的安装以及与Kafka相关的Python库的安装：

pip3 install flask
pip3 install flask-socketio
pip3 install kafka-python
Shell 命令
这些安装好的库在我们的程序文件的开头可以直接用来引用。比如下面的例子。

from flask import Flask
from flask_socketio import SocketIO
from kafka import KafkaConsumer
Python
from import 跟直接import的区别举个例子来说明。
import socket的话,要用socket.AF_INET,因为AF_INET这个值在socket的名称空间下。
from socket import* 是把socket下的所有名字引入当前名称空间。

PyCharm安装
PyCharm是一款Python开发IDE，可以极大方便工程管理以及程序开发。请在Ubuntu系统中打开自带的火狐浏览器，前往PyCharm官网下载免费的Community版本，下载后默认会被保存到当前登录用户的主目录下的“下载”目录中。比如，如果当前使用hadoop用户名登录了Ubuntu系统，那么，就会被保存在“/home/hadoop/下载”这个目录下。然后执行如下命令对安装文件进行解压缩：

cd ~  #进入当前hadoop用户的主目录
sudo tar -zxvf ~/下载/pycharm-community-2016.3.2.tar.gz -C /usr/local  #把pycharm解压缩到/usr/local目录下
cd /usr/local
sudo mv pycharm-community-2016.3.2 pycharm  #重命名
sudo chown -R hadoop ./pycharm  #把pycharm目录权限赋予给当前登录Ubuntu系统的hadoop用户
Shell 命令
然后，执行如下命令启动PyCharm：

cd /usr/local/pycharm
./bin/pycharm.sh  #启动PyCharm
Shell 命令
执行上述命令之后，即可开启PyCharm。

Python工程目录结构
这里先给出本案例Python工程的目录结构，后续的操作可以根据这个目录进行操作。

Python工程目录结构
Python工程目录结构

data目录存放的是用户日志数据；
scripts目录存放的是Kafka生产者和消费者；
static/js目录存放的是前端所需要的js框架；
templates目录存放的是html页面；
app.py为web服务器，接收Spark Streaming处理后的结果，并推送实时数据给浏览器；
External Libraries是本项目所依赖的Python库，是PyCharm自动生成。
至此，本案例需要的开发环境就介绍完毕，顺带说一句，Spark自带Scala，因此如果是开发Spark应用程序，则没必要单独安装Scala。

===========

Spark+Kafka构建实时分析Dashboard案例——步骤二：案例介绍》

开发团队：厦门大学数据库实验室 联系人：林子雨老师ziyulin@xmu.edu.cn

版权声明：版权归厦门大学数据库实验室所有，请勿用于商业用途；未经授权，其他网站请勿转载

本教程介绍大数据课程实验案例“Spark+Kafka构建实时分析Dashboard”的第二个步骤，数据处理和Python操作Kafka。在本篇博客中，首先介绍如何预处理数据，以及如何使用Python操作Kafka。

Python基本使用，Python操作Kafka代码库kafka-python使用

利用Python预处理数据
Python操作Kafka
数据预处理
数据集介绍
本案例采用的数据集压缩包为data_format.zip点击这里下载data_format.zip数据集，该数据集压缩包是淘宝2015年双11前6个月(包含双11)的交易数据(交易数据有偏移，但是不影响实验的结果)，里面包含3个文件，分别是用户行为日志文件user_log.csv 、回头客训练集train.csv 、回头客测试集test.csv. 在这个案例中只是用user_log.csv这个文件，下面列出文件user_log.csv的数据格式定义：

用户行为日志user_log.csv，日志中的字段定义如下：
1. user_id | 买家id
2. item_id | 商品id
3. cat_id | 商品类别id
4. merchant_id | 卖家id
5. brand_id | 品牌id
6. month | 交易时间:月
7. day | 交易事件:日
8. action | 行为,取值范围{0,1,2,3},0表示点击，1表示加入购物车，2表示购买，3表示关注商品
9. age_range | 买家年龄分段：1表示年龄<18,2表示年龄在[18,24]，3表示年龄在[25,29]，4表示年龄在[30,34]，5表示年龄在[35,39]，6表示年龄在[40,49]，7和8表示年龄>=50,0和NULL则表示未知
10. gender | 性别:0表示女性，1表示男性，2和NULL表示未知
11. province| 收获地址省份

数据具体格式如下：

user_id,item_id,cat_id,merchant_id,brand_id,month,day,action,age_range,gender,province
328862,323294,833,2882,2661,08,29,0,0,1,内蒙古
328862,844400,1271,2882,2661,08,29,0,1,1,山西
328862,575153,1271,2882,2661,08,29,0,2,1,山西
328862,996875,1271,2882,2661,08,29,0,1,1,内蒙古
328862,1086186,1271,1253,1049,08,29,0,0,2,浙江
328862,623866,1271,2882,2661,08,29,0,0,2,黑龙江
328862,542871,1467,2882,2661,08,29,0,5,2,四川
328862,536347,1095,883,1647,08,29,0,7,1,吉林
这个案例实时统计每秒中男女生购物人数，因此针对每条购物日志，我们只需要获取gender即可，然后发送给Kafka，接下来Spark Streaming再接收gender进行处理。

数据预处理
本案例使用Python对数据进行预处理，并将处理后的数据直接通过Kafka生产者发送给Kafka，这里需要先安装Python操作Kafka的代码库，请在Ubuntu中打开一个命令行终端，执行如下Shell命令来安装Python操作Kafka的代码库（备注：如果之前已经安装过，则这里不需要安装）：

pip3 install kafka-python
Shell 命令
接着可以写如下Python代码，文件名为producer.py：(具体的工程文件结构参照步骤一)

# coding: utf-8
import csv
import time
from kafka import KafkaProducer

# 实例化一个KafkaProducer示例，用于向Kafka投递消息
producer = KafkaProducer(bootstrap_servers='localhost:9092')
# 打开数据文件
csvfile = open("../data/user_log.csv","r")
# 生成一个可用于读取csv文件的reader
reader = csv.reader(csvfile)

for line in reader:
    gender = line[9] # 性别在每行日志代码的第9个元素
    if gender == 'gender':
        continue # 去除第一行表头
    time.sleep(0.1) # 每隔0.1秒发送一行数据
    # 发送数据，topic为'sex'
    producer.send('sex',line[9].encode('utf8'))
Python
上述代码很简单，首先是先实例化一个Kafka生产者。然后读取用户日志文件，每次读取一行，接着每隔0.1秒发送给Kafka，这样1秒发送10条购物日志。这里发送给Kafka的topic为’sex’。

Python操作Kafka
我们可以写一个KafkaConsumer测试数据是否投递成功，代码如下，文件名为consumer.py

from kafka import KafkaConsumer

consumer = KafkaConsumer('sex')
for msg in consumer:
    print((msg.value).decode('utf8'))
Python
在开启上述KafkaProducer和KafkaConsumer之前，需要先开启Kafka，命令如下：

cd /usr/local/kafka
bin/zookeeper-server-start.sh config/zookeeper.properties &
bin/kafka-server-start.sh config/server.properties
Shell
在Kafka开启之后，即可开启KafkaProducer和KafkaConsumer。开启方法如下：
请在Ubuntu中，打开一个命令行 终端窗口，执行如下命令：

cd /home/hadoop/mydir/labproject/scripts  #进入到代码目录
python3 producer.py #启动生产者发送消息给Kafaka
Shell 命令
然后，请在Ubuntu中，打开另外一个命令行 终端窗口，执行如下命令：

cd /home/hadoop/mydir/labproject/scripts  #进入到代码目录
python3 consumer.py #启动消费者从Kafaka接收消息
Shell 命令
运行上面这条命令以后，这时，你会看到屏幕上会输出一行又一行的数字，类似下面的样子：

2
1
1
1
2
0
2
1
……
当然，也可以不用上面这种命令行方式来启动生产者和消费者，如果你目前使用的是开发工具PyCharm，则也可以直接在代码区域右键，点击Run ‘producer’以及Run ‘consumer’，来运行生产者和消费者。如果生产者和消费者运行成功，则在consumer窗口会输出如下信息:

consumer输出
consumer输出

如果有上述的输出，恭喜你，Python操作Kafka运行成功。接下来，下篇文章将分析Spark Streaming如何处理Kafka的实时数据。

==========================================

《Spark+Kafka构建实时分析Dashboard案例——步骤三：Spark Streaming实时处理数据》

开发团队：厦门大学数据库实验室 联系人：林子雨老师ziyulin@xmu.edu.cn

版权声明：版权归厦门大学数据库实验室所有，请勿用于商业用途；未经授权，其他网站请勿转载

本教程介绍大数据课程实验案例“Spark+Kafka构建实时分析Dashboard”的第三个步骤，Spark Streaming实时处理数据。在本篇博客中，将介绍如何利用Spark Streaming实时接收处理Kafka数据以及将处理后的结果发给的Kafka。


所需知识储备
会使用Scala编写Spark Streaming程序，Kafka原理。

训练技能
编写Spark Streaming程序，熟悉Spark操作Kafka。

任务清单
Spark Streaming实时处理Kafka数据；
将处理后的结果发送给Kafka；
编程思想
本案例在于实时统计每秒中男女生购物人数，而Spark Streaming接收的数据为1,1,0,2…，其中0代表女性，1代表男性，所以对于2或者null值，则不考虑。其实通过分析，可以发现这个就是典型的wordcount问题，而且是基于Spark流计算。女生的数量，即为0的个数，男生的数量，即为1的个数。

因此利用Spark Streaming接口reduceByKeyAndWindow，设置窗口大小为1，滑动步长为1，这样统计出的0和1的个数即为每秒男生女生的人数。

编程实现
配置Spark开发Kafka环境
kafka的安装可以参考Kafka的安装和简单实例测试。如果之前没有学习过Spark和Kafka的组合使用方法，建议先阅读厦门大学数据库实验室博客文章《Spark2.1.0入门：Apache Kafka作为DStream数据源》。下面主要介绍配置Spark开发Kafka环境。首先点击下载spark-streaming-kafka，下载Spark连接Kafka的代码库。然后把下载的代码库放到目录/usr/local/spark/jars目录下，命令如下：

sudo mv ~/下载/spark-streaming-kafka-0-8_2.11-2.1.0.jar /usr/local/spark/jars
Shell
然后在/usr/local/spark/jars目录下新建kafka目录，把/usr/local/kafka/libs下所有函数库复制到/usr/local/spark/jars/kafka目录下，命令如下

cd /usr/local/spark/jars
mkdir kafka
cd kafka
cp /usr/local/kafka/libs/* .
Shell
执行上述步骤之后，Spark开发Kafka环境即已配置好，下面介绍如何编码实现。

建立Spark项目
之前有很多教程都有说明如何创建Spark项目，这里再次说明。首先在/usr/local/spark/mycode新建项目主目录

cd /usr/local/spark/mycode
mkdir kafka
Shell
然后在kafka目录下新建scala文件存放目录以及scala工程文件

cd kafka
mkdir -p src/main/scala
Shell
接着在src/main/scala文件下创建两个文件，一个是用于设置日志，一个是项目工程主文件，设置日志文件为StreamingExamples.scala

package org.apache.spark.examples.streaming
import org.apache.spark.internal.Logging
import org.apache.log4j.{Level, Logger}
/** Utility functions for Spark Streaming examples. */
object StreamingExamples extends Logging {
  /** Set reasonable logging levels for streaming if the user has not configured log4j. */
  def setStreamingLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
      // We first log something to initialize Spark's default logging, then we override the
      // logging level.
      logInfo("Setting log level to [WARN] for streaming example." +
        " To override add a custom log4j.properties to the classpath.")
      Logger.getRootLogger.setLevel(Level.WARN)
    }
  }
}
scala
这个文件不做过多解释，因为这只是一个辅助文件，下面着重介绍工程主文件，文件名为KafkaTest.scala

package org.apache.spark.examples.streaming

import java.util.HashMap
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
import org.json4s._
import org.json4s.jackson.Serialization
import org.json4s.jackson.Serialization.write
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.Interval
import org.apache.spark.streaming.kafka._

object KafkaWordCount {
  implicit val formats = DefaultFormats//数据格式化时需要
  def main(args: Array[String]): Unit={
    if (args.length < 4) {
      System.err.println("Usage: KafkaWordCount <zkQuorum> <group> <topics> <numThreads>")
      System.exit(1)
    }
    StreamingExamples.setStreamingLogLevels()
    /* 输入的四个参数分别代表着
    * 1. zkQuorum 为zookeeper地址
    * 2. group为消费者所在的组
    * 3. topics该消费者所消费的topics
    * 4. numThreads开启消费topic线程的个数
    */
    val Array(zkQuorum, group, topics, numThreads) = args
    val sparkConf = new SparkConf().setAppName("KafkaWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint(".")  //这里表示把检查点文件写入分布式文件系统HDFS，所以要启动Hadoop
    // 将topics转换成topic-->numThreads的哈稀表
    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    // 创建连接Kafka的消费者链接
    val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2)
    val words = lines.flatMap(_.split(" "))//将输入的每行用空格分割成一个个word
    // 对每一秒的输入数据进行reduce，然后将reduce后的数据发送给Kafka
    val wordCounts = words.map(x => (x, 1L))
      .reduceByKeyAndWindow(_+_,_-_, Seconds(1), Seconds(1), 1).foreachRDD(rdd => {
          if(rdd.count !=0 ){
               val props = new HashMap[String, Object]()
               props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092")
               props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
               "org.apache.kafka.common.serialization.StringSerializer")
               props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
               "org.apache.kafka.common.serialization.StringSerializer")
               // 实例化一个Kafka生产者
               val producer = new KafkaProducer[String, String](props)
               // rdd.colect即将rdd中数据转化为数组，然后write函数将rdd内容转化为json格式
               val str = write(rdd.collect)
               // 封装成Kafka消息，topic为"result"
               val message = new ProducerRecord[String, String]("result", null, str)
               // 给Kafka发送消息
               producer.send(message)
          }
      })
    ssc.start()
    ssc.awaitTermination()
  }
}
scala
上述代码注释已经也很清楚了，下面在简要说明下：
1. 首先按每秒的频率读取Kafka消息；
2. 然后对每秒的数据执行wordcount算法，统计出0的个数，1的个数，2的个数；
3. 最后将上述结果封装成json发送给Kafka。

另外，需要注意，上面代码中有一行如下代码：

ssc.checkpoint(".")
这行代码表示把检查点文件写入分布式文件系统HDFS，所以一定要事先启动Hadoop。如果没有启动Hadoop，则后面运行时会出现“拒绝连接”的错误提示。如果你还没有启动Hadoop，则可以现在在Ubuntu终端中，使用如下Shell命令启动Hadoop：

cd /usr/local/hadoop  #这是hadoop的安装目录
./sbin/start-dfs.sh
Shell 命令
另外，如果不想把检查点写入HDFS，而是直接把检查点写入本地磁盘文件（这样就不用启动Hadoop），则可以对ssc.checkpoint()方法中的文件路径进行指定,比如下面这个例子：

ssc.checkpoint("file:///usr/local/spark/mycode/kafka/checkpoint")
运行项目
编写好程序之后，下面介绍下如何打包运行程序。在/usr/local/spark/mycode/kafka目录下新建文件simple.sbt，输入如下内容：

name := "Simple Project"
version := "1.0"
scalaVersion := "2.11.8"
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.1.0"
libraryDependencies += "org.apache.spark" % "spark-streaming_2.11" % "2.1.0"
libraryDependencies += "org.apache.spark" % "spark-streaming-kafka-0-8_2.11" % "2.1.0"
libraryDependencies += "org.json4s" %% "json4s-jackson" % "3.2.11"
然后，即可编译打包程序，输入如下命令

/usr/local/sbt/sbt package
Shell
打包成功之后，接下来编写运行脚本，在/usr/local/spark/mycode/kafka目录下新建startup.sh文件，输入如下内容：

 /usr/local/spark/bin/spark-submit --driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/kafka/* --class "org.apache.spark.examples.streaming.KafkaWordCount" /usr/local/spark/mycode/kafka/target/scala-2.11/simple-project_2.11-1.0.jar 127.0.0.1:2181 1 sex 1
Shell
其中最后四个为输入参数，含义如下
1. 127.0.0.1:2181为Zookeeper地址
2. 1 为consumer group标签
3. sex为消费者接收的topic
4. 1 为消费者线程数

最后在/usr/local/spark/mycode/kafka目录下，运行如下命令即可执行刚编写好的Spark Streaming程序

sh startup.sh
Shell
程序运行成功之后，下面通过之前的KafkaProducer和KafkaConsumer来检测程序。

测试程序
下面开启之前编写的KafkaProducer投递消息，然后将KafkaConsumer中接收的topic改为result，验证是否能接收topic为result的消息，更改之后的KafkaConsumer为

from kafka import KafkaConsumer

consumer = KafkaConsumer('result')
for msg in consumer:
    print((msg.value).decode('utf8'))
Python
在同时开启Spark Streaming项目，KafkaProducer以及KafkaConsumer之后，可以在KafkaConsumer运行窗口看到如下输出：
KafkaConsumer输出

到此为止，Spark Streaming程序编写完成，下篇文章将分析如何处理得到的最终结果。

========

本教程介绍大数据课程实验案例“Spark+Kafka构建实时分析Dashboard”的第四个步骤，结果展示。在本篇博客中，将介绍如何利用Flask-SocketIO向客户端发送消息以及客户端如何利用highcharts.js展示数据。


所需知识储备
了解Flask创建web程序，了解如何在html编写js代码

训练技能
利用Flask创建web程序，利用Flask-SocketIO实现实时推送数据，利用socket.io.js实现实时接收数据，hightlights.js展现数据

任务清单
利用Flask-SocketIO实时推送数据
socket.io.js实时获取数据
highlights.js展示数据
Flask-SocketIO实时推送数据
上篇文章说道Spark Streaming实时接收Kafka中topic为’sex’发送的日志数据，然后Spark Streaming进行实时处理，统计好每秒中男女生购物人数之后，将结果发送至Kafka，topic为’result’。在本章节，将介绍如何利用Flask-SocketIO将结果实时推送到浏览器。

下面讲述的内容需要使用Flask-SocketIO，相关文档可以查看Flask-SocketIO文档。
这里我们为了方便查看,再一次贴出项目工程结构图。

备注：为了方便大家完成上述实验，这里提供源代码文件的下载，请点击这里从百度云盘下载。大家下载完源代码以后，可以导入到自己的IntelliJIDEA开发工具中，查看源代码。备注：点击这里查看IntelliJIDEA工具的安装方法。

首先我们创建如图中的app.py文件,app.py的功能就是作为一个简易的服务器，处理连接请求，以及处理从kafka接收的数据，并实时推送到浏览器。app.py的代码如下:

import json
from flask import Flask, render_template
from flask_socketio import SocketIO
from kafka import KafkaConsumer
#因为第一步骤安装好了flask，所以这里可以引用

app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret!'
socketio = SocketIO(app)
thread = None
# 实例化一个consumer，接收topic为result的消息
consumer = KafkaConsumer('result')

# 一个后台线程，持续接收Kafka消息，并发送给客户端浏览器
def background_thread():
    girl = 0
    boy = 0
    for msg in consumer:
        data_json = msg.value.decode('utf8')
        data_list = json.loads(data_json)
        for data in data_list:
            if '0' in data.keys():
                girl = data['0']
            elif '1' in data.keys():
                boy = data['1']
            else:
                continue
        result = str(girl) + ',' + str(boy)
        print(result)
        socketio.emit('test_message',{'data':result})

# 客户端发送connect事件时的处理函数
@socketio.on('test_connect')
def connect(message):
    print(message)
    global thread
    if thread is None:
        # 单独开启一个线程给客户端发送数据
        thread = socketio.start_background_task(target=background_thread)
    socketio.emit('connected', {'data': 'Connected'})

# 通过访问http://127.0.0.1:5000/访问index.html
@app.route("/")
def handle_mes():
    return render_template("index.html")

# main函数
if __name__ == '__main__':
    socketio.run(app,debug=True)
Python
这段代码实现比较简单，最重要就是background_thread函数，该函数从Kafka接收消息，并进行处理，获得男女生每秒钟人数，然后将结果通过函数socketio.emit实时推送至浏览器。

浏览器获取数据并展示
index.html文件负责获取数据并展示效果，该文件中的代码内容如下（也可以点击这里下载index.html文件）：

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>DashBoard</title>
    <script src="static/js/socket.io.js"></script>
    <script src="static/js/jquery-3.1.1.min.js"></script>
    <script src="static/js/highcharts.js"></script>
    <script src="static/js/exporting.js"></script>
    <script type="text/javascript" charset="utf-8">
    var socket = io.connect('http://' + document.domain + ':' + location.port);
    socket.on('connect', function() {
        socket.emit('test_connect', {data: 'I\'m connected!'});
    });

    socket.on('test_message',function(message){
        console.log(message);
        var obj = eval(message);
        var result = obj["data"].split(",");
        $('#girl').html(result[0]);
        $('#boy').html(result[1]);
    });

    socket.on('connected',function(){
        console.log('connected');
    });

    socket.on('disconnect', function () {
        console.log('disconnect');
    });
    </script>
</head>
<body>
<div>
    <b>Girl: </b><b id="girl"></b>
    <b>Boy: </b><b id="boy"></b>
</div>
<div id="container" style="width: 600px;height:400px;"></div>

<script type="text/javascript">
    $(document).ready(function () {
    Highcharts.setOptions({
        global: {
            useUTC: false
        }
    });

    Highcharts.chart('container', {
        chart: {
            type: 'spline',
            animation: Highcharts.svg, // don't animate in old IE
            marginRight: 10,
            events: {
                load: function () {

                    // set up the updating of the chart each second
                    var series1 = this.series[0];
                    var series2 = this.series[1];
                    setInterval(function () {
                        var x = (new Date()).getTime(), // current time
                        count1 = $('#girl').text();
                        y = parseInt(count1);
                        series1.addPoint([x, y], true, true);

                        count2 = $('#boy').text();
                        z = parseInt(count2);
                        series2.addPoint([x, z], true, true);
                    }, 1000);
                }
            }
        },
        title: {
            text: '男女生购物人数实时分析'
        },
        xAxis: {
            type: 'datetime',
            tickPixelInterval: 50
        },
        yAxis: {
            title: {
                text: '数量'
            },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {
            formatter: function () {
                return '<b>' + this.series.name + '</b><br/>' +
                    Highcharts.dateFormat('%Y-%m-%d %H:%M:%S', this.x) + '<br/>' +
                    Highcharts.numberFormat(this.y, 2);
            }
        },
        legend: {
            enabled: true
        },
        exporting: {
            enabled: true
        },
        series: [{
            name: '女生购物人数',
            data: (function () {
                // generate an array of random data
                var data = [],
                    time = (new Date()).getTime(),
                    i;

                for (i = -19; i <= 0; i += 1) {
                    data.push({
                        x: time + i * 1000,
                        y: Math.random()
                    });
                }
                return data;
            }())
        },
        {
            name: '男生购物人数',
            data: (function () {
                // generate an array of random data
                var data = [],
                    time = (new Date()).getTime(),
                    i;

                for (i = -19; i <= 0; i += 1) {
                    data.push({
                        x: time + i * 1000,
                        y: Math.random()
                    });
                }
                return data;
            }())
        }]
    });
});
</script>
</body>
</html>
HTML
可以看到，在上面给出的index.html文件的开头部分，包含如下几行代码：

 <script src="static/js/socket.io.js"></script>
    <script src="static/js/jquery-3.1.1.min.js"></script>
    <script src="static/js/highcharts.js"></script>
    <script src="static/js/exporting.js"></script>
HTML
也就是说，在index.html中，需要用到几个js库，包括socket.io.js、jquery-3.1.1.min.js、highcharts.js和exporting.js。

下面分别介绍这几个js库文件的用途和下载方法（备注：如果上面已经下载了我们数据库实验室提供的源码文件，则里面已经包含了这些js库文件，不需要再去网络上下载）。

socket.io.js
客户端浏览器需要使用js框架socket.io.js来实时接收服务端的消息，该js框架用法和Flask-SocketIO使用类似。首先我们可以看到在工程目录中有两个js库文件跟socket.io.js相关，一个就是socket.io.js，下载方式很简单，点击给出的链接，然后在打开的页面右键—>另存为..就可以下载该文件;还有一个文件就是socket.io.js.map，点击链接就可以下载。（备注：如果上面已经下载了我们数据库实验室提供的源码文件，则里面已经包含了这些js库文件，不需要再去网络上下载）
下载好socket.io.js和socket.io.js.map这两个js库文件后，按照上面给出的工程文件目录结构，把这两个文件复制到js文件夹下（这两个文件之间没有嵌套关系，不要被我们上面给出的工程结构图迷惑）。

然后，再来看一下，在index.html中是如何调用socket.io.js和socket.io.js.map这两个js库文件，可以看到，在index.html中包含了如下一段代码，就是用来调用这两个库文件的：

<script type="text/javascript" charset="utf-8">
    // 创建连接服务器的链接
    var socket = io.connect('http://' + document.domain + ':' + location.port);
    socket.on('connect', function() {// 连上服务器后的回调函数
        socket.emit('connect', {data: 'I\'m connected!'});
    });
    // 接收服务器实时发送的数据
    socket.on('test_message',function(message){
        console.log(message);
        var obj = eval(message);
        var result = obj["data"].split(",");
        // 将男生和女生人数展示在html标签内
        $('#girl').html(result[0]);
        $('#boy').html(result[1]);
    });

    socket.on('connected',function(){
        console.log('connected');
    });
    // 链接断开时的回调函数
    socket.on('disconnect', function () {
        console.log('disconnect');
    });
    </script>
JavaScript
上面这段代码就是使用socket.io.js库来实时地接收服务端发送过来的消息，并将消息数据实时地设置在html标签内，交给highcharts.js进行实时获取和展示。

highchart.js
highcharts.js是一个用纯JavaScript编写的一个图表库。能够很简单便捷地在Web网站或是Web应用程序中添加有交互性的图表。可以到官网下载最新版本的highchart.js库文件(点击这里下载)。注意，到官网下载highchart.js库文件时，下载到本地的是一个类似Highcharts-6.0.7.zip这样的压缩文件，对这个文件进行解压缩，可以看到里面有个code子目录，在code子目录下面就可以找到highchart.js库文件，按照上面给出的工程文件目录结构，把这个highchart.js库文件复制到IntelliJIDEA工程目录中的js文件夹下。（备注：如果上面已经下载了我们数据库实验室提供的源码文件，则里面已经包含了这些js库文件，不需要再去网络上下载）

在index.html中包含如下一段代码，就是调用highcharts.js库，来实时地从html标签内获取数据并展示在网页中。

<script type="text/javascript">
    $(document).ready(function () {
    Highcharts.setOptions({
        global: {
            useUTC: false
        }
    });

    Highcharts.chart('container', {
        chart: {
            type: 'spline',
            animation: Highcharts.svg, // 这个在ie浏览器可能不支持
            marginRight: 10,
            events: {
                load: function () {

                    //设置图表每秒更新一次
                    var series1 = this.series[0];
                    var series2 = this.series[1];
                    setInterval(function () {
                        var x = (new Date()).getTime();// 获取当前时间
                        count1 = $('#girl').text();
                        y = parseInt(count1);
                        series1.addPoint([x, y], true, true);

                        count2 = $('#boy').text();
                        z = parseInt(count2);
                        series2.addPoint([x, z], true, true);
                    }, 1000);
                }
            }
        },
        title: { //设置图表名
            text: '男女生购物人数实时分析'
        },
        xAxis: { //x轴设置为实时时间
            type: 'datetime',
            tickPixelInterval: 50
        },
        yAxis: {
            title: {
                text: '数量'
            },
            plotLines: [{ //设置坐标线颜色粗细
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {
            //规范显示时间的格式
            formatter: function () {
                return '<b>' + this.series.name + '</b><br/>' +
                    Highcharts.dateFormat('%Y-%m-%d %H:%M:%S', this.x) + '<br/>' +
                    Highcharts.numberFormat(this.y, 2);
            }
        },
        legend: {
            enabled: true
        },
        exporting: {
            enabled: true
        },
        series: [{
            name: '女生购物人数',
            data: (function () {
                // 随机方式生成初始值填充图表
                var data = [],
                    time = (new Date()).getTime(),
                    i;

                for (i = -19; i <= 0; i += 1) {
                    data.push({
                        x: time + i * 1000,
                        y: Math.random()
                    });
                }
                return data;
            }())
        },
        {
            name: '男生购物人数',
            data: (function () {
                // 随机方式生成初始值填充图表
                var data = [],
                    time = (new Date()).getTime(),
                    i;

                for (i = -19; i <= 0; i += 1) {
                    data.push({
                        x: time + i * 1000,
                        y: Math.random()
                    });
                }
                return data;
            }())
        }]
    });
});
JavaScript
这段代码其实很简单，就是在设置表格的格式，因为highchart.js这个框架，让我们在网页中显示数据图表的工作变得很轻松。

exporting.js
在上面，到官网下载highchart.js库文件时，下载到本地的是一个类似Highcharts-6.0.7.zip这样的压缩文件，对这个文件进行解压缩，可以看到里面有个code子目录，在code子目录下面就可以找到一个js子目录，在js子目录下可以找到一个modules子目录，在modules子目录中就可以找到库文件exporting.js。然后，按照上面给出的工程文件目录结构，把这个exporting.js库文件复制到IntelliJIDEA工程目录中的js文件夹下。（备注：如果上面已经下载了我们数据库实验室提供的源码文件，则里面已经包含了这些js库文件，不需要再去网络上下载）

exporting.js这个库文件的功能是实现导出功能。也就是下图中的红色圈内的按钮的功能。


jquery.js
除了上述三个js库文件外，还有一个jquery.js库文件。jQuery是一个快速、简洁的JavaScript框架，它封装JavaScript常用的功能代码，提供一种简便的JavaScript设计模式，优化HTML文档操作、事件处理、动画设计和Ajax交互。点击这里下载jquery.js，然后，按照上面给出的工程文件目录结构，把这个jquery.js库文件复制到IntelliJIDEA工程目录中的js文件夹下。（备注：如果上面已经下载了我们数据库实验室提供的源码文件，则里面已经包含了这些js库文件，不需要再去网络上下载）

效果展示
经过以上步骤，一切准备就绪，我们就可以启动程序来看看最后的效果。启动步骤如下：

1.确保kafka开启，参考教程步骤二。
2.开启producer.py模拟数据流，参考步骤二。
3.启动Spark Streaming实时处理数据，参考步骤三。提示你可以在实时处理数据启动之后，把comsumer.py的topic改成result，运行comsumer.py就可以看到数据处理后的输出结果。
4.启动app.py。如果你是使用pycharm客户端，那右键就可以了。当然也可以使用终端命令：

python app.py
Shell
启动后的效果如下图：


这时候你可以用浏览器访问上图中给出的网址 http://127.0.0.1:5000/ ，就可以看到最终效果图了。

下面截图展示了本案例的最终效果，本案例最终效果是一个动态效果，这里截取两张截图为例：

本案例样例截图一
本案例样例截图一

本案例样例截图二
本案例样例截图二

到此为止，本案例就顺利完成了！
为了方便大家完成上述实验，这里提供源代码文件的下载，请点击这里从百度云盘下载。

