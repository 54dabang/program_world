第1章　开发环境配置　　1

1.1　Python 3的安装　　1

1.1.1　Windows下的安装　　1

1.1.2　Linux下的安装　　6

1.1.3　Mac下的安装　　8

1.2　请求库的安装　　10

1.2.1　requests的安装　　10

1.2.2　Selenium的安装　　11

1.2.3　ChromeDriver的安装　　12

1.2.4　GeckoDriver的安装　　15

1.2.5　PhantomJS的安装　　17

1.2.6　aiohttp的安装　　18

1.3　解析库的安装　　19

1.3.1　lxml的安装　　19

1.3.2　Beautiful Soup的安装　　21

1.3.3　pyquery的安装　　22

1.3.4　tesserocr的安装　　22

1.4　数据库的安装　　26

1.4.1　MySQL的安装　　27

1.4.2　MongoDB的安装　　29

1.4.3　Redis的安装　　36

1.5　存储库的安装　　39

1.5.1　PyMySQL的安装　　39

1.5.2　PyMongo的安装　　39

1.5.3　redis-py的安装　　40

1.5.4　RedisDump的安装　　40

1.6　Web库的安装　　41

1.6.1　Flask的安装　　41

1.6.2　Tornado的安装　　42

1.7　App爬取相关库的安装　　43

1.7.1　Charles的安装　　44

1.7.2　mitmproxy的安装　　50

1.7.3　Appium的安装　　55

1.8　爬虫框架的安装　　59

1.8.1　pyspider的安装　　59

1.8.2　Scrapy的安装　　61

1.8.3　Scrapy-Splash的安装　　65

1.8.4　Scrapy-Redis的安装　　66

1.9　部署相关库的安装　　67

1.9.1　Docker的安装　　67

1.9.2　Scrapyd的安装　　71

1.9.3　Scrapyd-Client的安装　　74

1.9.4　Scrapyd API的安装　　75

1.9.5　Scrapyrt的安装　　75

1.9.6　Gerapy的安装　　76

第2章　爬虫基础　　77

2.1　HTTP基本原理　　77

2.1.1　URI和URL　　77

2.1.2　超文本　　78

2.1.3　HTTP和HTTPS　　78

2.1.4　HTTP请求过程　　80

2.1.5　请求　　82

2.1.6　响应　　84

2.2　网页基础　　87

2.2.1　网页的组成　　87

2.2.2　网页的结构　　88

2.2.3　节点树及节点间的关系　　90

2.2.4　选择器　　91

2.3　爬虫的基本原理　　93

2.3.1　爬虫概述　　93

2.3.2　能抓怎样的数据　　94

2.3.3　JavaScript渲染页面　　94

2.4　会话和Cookies　　95

2.4.1　静态网页和动态网页　　95

2.4.2　无状态HTTP　　96

2.4.3　常见误区　　98

2.5　代理的基本原理　　99

2.5.1　基本原理　　99

2.5.2　代理的作用　　99

2.5.3　爬虫代理　　100

2.5.4　代理分类　　100

2.5.5　常见代理设置　　101

第3章　基本库的使用　　102

3.1　使用urllib　　102

3.1.1　发送请求　　102

3.1.2　处理异常　　112

3.1.3　解析链接　　114

3.1.4　分析Robots协议　　119

3.2　使用requests　　122

3.2.1　基本用法　　122

3.2.2　高级用法　　130

3.3　正则表达式　　139

3.4　抓取猫眼电影排行　　150

第4章　解析库的使用　　158

4.1　使用XPath　　158

4.2　使用Beautiful Soup　　168

4.3　使用pyquery　　184

第5章　数据存储　　197

5.1　文件存储　　197

5.1.1　TXT文本存储　　197

5.1.2　JSON文件存储　　199

5.1.3　CSV文件存储　　203

5.2　关系型数据库存储　　207

5.2.1　MySQL的存储　　207

5.3　非关系型数据库存储　　213

5.3.1　MongoDB存储　　214

5.3.2　Redis存储　　221

第6章　Ajax数据爬取　　232

6.1　什么是Ajax　　232

6.2　Ajax分析方法　　234

6.3　Ajax结果提取　　238

6.4　分析Ajax爬取今日头条街拍美图　　242

第7章　动态渲染页面爬取　　249

7.1　Selenium的使用　　249

7.2　Splash的使用　　262

7.3　Splash负载均衡配置　　286

7.4　使用Selenium爬取淘宝商品　　289

第8章　验证码的识别　　298

8.1　图形验证码的识别　　298

8.2　极验滑动验证码的识别　　301

8.3　点触验证码的识别　　311

8.4　微博宫格验证码的识别　　318

第9章　代理的使用　　326

9.1　代理的设置　　326

9.2　代理池的维护　　333

9.3　付费代理的使用　　347

9.4　ADSL拨号代理　　351

9.5　使用代理爬取微信公众号文章　　364

第10章　模拟登录　　379

10.1　模拟登录并爬取GitHub　　379

10.2　Cookies池的搭建　　385

第11章　App的爬取　　398

11.1　Charles的使用　　398

11.2　mitmproxy的使用　　405

11.3　mitmdump爬取“得到”App电子书

信息　　417

11.4　Appium的基本使用　　423

11.5　Appium爬取微信朋友圈　　433

11.6　Appium+mitmdump爬取京东商品　　437

第12章　pyspider框架的使用　　443

12.1　pyspider框架介绍　　443

12.2　pyspider的基本使用　　445

12.3　pyspider用法详解　　459

第13章　Scrapy框架的使用　　468

13.1　Scrapy框架介绍　　468

13.2　Scrapy入门　　470

13.3　Selector的用法　　480

13.4　Spider的用法　　486

13.5　Downloader Middleware的用法　　487

13.6　Spider Middleware的用法　　494

13.7　Item Pipeline的用法　　496

13.8　Scrapy对接Selenium　　506

13.9　Scrapy对接Splash　　511

13.10　Scrapy通用爬虫　　516

13.11　Scrapyrt的使用　　533

13.12　Scrapy对接Docker　　536

13.13　Scrapy爬取新浪微博　　541

第14章　分布式爬虫　　555

14.1　分布式爬虫原理　　555

14.2　Scrapy-Redis源码解析　　558

14.3　Scrapy分布式实现　　564

14.4　Bloom Filter的对接　　569

第15章　分布式爬虫的部署　　577

15.1　Scrapyd分布式部署　　577

15.2　Scrapyd-Client的使用　　582

15.3　Scrapyd对接Docker　　583

15.4　Scrapyd批量部署　　586

15.5　Gerapy分布式管理　　590

