 mkdir taxidata
$ cd taxidata
$ curl -O https://nyctaxitrips.blob.core.windows.net/data/trip_data_1.csv.zip
$ unzip trip_data_1.csv.zip
$ head -n 10 trip_data_1.csv


import com.esri.core.geometry.Geometry
import com.esri.core.geometry.GeometryEngine
import com.esri.core.geometry.SpatialReference
class RichGeometry(val geometry: Geometry,
val spatialReference: SpatialReference =
SpatialReference.create(4326)) {
def area2D() = geometry.calculateArea2D()
def contains(other: Geometry): Boolean = {
GeometryEngine.contains(geometry, other, spatialReference)
}
def distance(other: Geometry): Double = {
GeometryEngine.distance(geometry, other, spatialReference)
}
}


object RichGeometry {
implicit def wrapRichGeo(g: Geometry) = {
new RichGeometry(g)
}
}


import RichGeometry._


curl -O https://nycdatastables.s3.amazonaws.com/2013-08-19T18:15:35.172Z/
nyc-borough-boundaries-polygon.geojson
$ mv nyc-borough-boundaries-polygon.geojson nyc-boroughs.geojson

import spray.json.JsValue
case class Feature(
val id: Option[JsValue],
val properties: Map[String, JsValue],
val geometry: RichGeometry) {
def apply(property: String) = properties(property)
def get(property: String) = properties.get(property)
}

case class FeatureCollection(features: Array[Feature])
extends IndexedSeq[Feature] {
def apply(index: Int) = features(index)
def length = features.length
}

implicit object FeatureJsonFormat extends
RootJsonFormat[Feature] {
def write(f: Feature) = {

val buf = scala.collection.mutable.ArrayBuffer(
"type" -> JsString("Feature"),
"properties" -> JsObject(f.properties),
"geometry" -> f.geometry.toJson)
f.id.foreach(v => { buf += "id" -> v})
JsObject(buf.toMap)
}
def read(value: JsValue) = {
    val jso = value.asJsObject
    val id = jso.fields.get("id")
    val properties = jso.fields("properties").asJsObject.fields
    val geometry = jso.fields("geometry").convertTo[RichGeometry]
    Feature(id, properties, geometry)
    }
    }
}

 hadoop fs -mkdir taxidata
$ hadoop fs -put trip_data_1.csv taxidata/


 mvn package
$ spark-shell --jars ch08-geotime-2.0.0-jar-with-dependencies.jar

val taxiRaw = spark.read.option("header", "true").csv("taxidata")
taxiRaw.show()


case class Trip(
license: String,
pickupTime: Long,
dropoffTime: Long,
pickupX: Double,
pickupY: Double,
dropoffX: Double,
dropoffY: Double)

class RichRow(row: org.apache.spark.sql.Row) {
def getAs[T](field: String): Option[T] = {
if (row.isNullAt(row.fieldIndex(field))) {
None
} else {
Some(row.getAs[T](field))
}
}
}

def parseTaxiTime(rr: RichRow, timeField: String): Long = {
val formatter = new SimpleDateFormat(
"yyyy-MM-dd HH:mm:ss", Locale.ENGLISH)
val optDt = rr.getAs[String](timeField)
optDt.map(dt => formatter.parse(dt).getTime).getOrElse(0L)
}





def parseTaxiLoc(rr: RichRow, locField: String): Double = {
rr.getAs[String](locField).map(_.toDouble).getOrElse(0.0)
}

def parse(row: org.apache.spark.sql.Row): Trip = {
val rr = new RichRow(row)
Trip(
    license = rr.getAs[String]("hack_license").orNull,
    pickupTime = parseTaxiTime(rr, "pickup_datetime"),
    dropoffTime = parseTaxiTime(rr, "dropoff_datetime"),
    pickupX = parseTaxiLoc(rr, "pickup_longitude"),
    pickupY = parseTaxiLoc(rr, "pickup_latitude"),
    dropoffX = parseTaxiLoc(rr, "dropoff_longitude"),
    dropoffY = parseTaxiLoc(rr, "dropoff_latitude")
)
}

def safe[S, T](f: S => T): S => Either[T, (S, Exception)] = {
new Function[S, Either[T, (S, Exception)]] with Serializable {
def apply(s: S): Either[T, (S, Exception)] = {
try {
Left(f(s))
} catch {
case e: Exception => Right((s, e))
}
}
}
}

val safeParse = safe(parse)
val taxiParsed = taxiRaw.rdd.map(safeParse)

taxiParsed.map(_.isLeft).
countByValue().
foreach(println)

val taxiGood = taxiParsed.map(_.left.get).toDS
taxiGood.cache()

val hours = (pickup: Long, dropoff: Long) => {
TimeUnit.HOURS.convert(dropoff - pickup, TimeUnit.MILLISECONDS)
}

import org.apache.spark.sql.functions.udf
val hoursUDF = udf(hours)
taxiGood.
groupBy(hoursUDF($"pickupTime", $"dropoffTime").as("h")).
count().
sort("h").
show()


taxiGood.
where(hoursUDF($"pickupTime", $"dropoffTime") < 0).
collect().
foreach(println)


spark.udf.register("hours", hours)
val taxiClean = taxiGood.where(
"hours(pickupTime, dropoffTime) BETWEEN 0 AND 3"
)

val geojson = scala.io.Source.
fromFile("nyc-boroughs.geojson").
mkString