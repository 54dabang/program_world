 mkdir taxidata
$ cd taxidata
$ curl -O https://nyctaxitrips.blob.core.windows.net/data/trip_data_1.csv.zip
$ unzip trip_data_1.csv.zip
$ head -n 10 trip_data_1.csv


import com.esri.core.geometry.Geometry
import com.esri.core.geometry.GeometryEngine
import com.esri.core.geometry.SpatialReference
class RichGeometry(val geometry: Geometry,
val spatialReference: SpatialReference =
SpatialReference.create(4326)) {
def area2D() = geometry.calculateArea2D()
def contains(other: Geometry): Boolean = {
GeometryEngine.contains(geometry, other, spatialReference)
}
def distance(other: Geometry): Double = {
GeometryEngine.distance(geometry, other, spatialReference)
}
}


object RichGeometry {
implicit def wrapRichGeo(g: Geometry) = {
new RichGeometry(g)
}
}


import RichGeometry._


curl -O https://nycdatastables.s3.amazonaws.com/2013-08-19T18:15:35.172Z/
nyc-borough-boundaries-polygon.geojson
$ mv nyc-borough-boundaries-polygon.geojson nyc-boroughs.geojson

import spray.json.JsValue
case class Feature(
val id: Option[JsValue],
val properties: Map[String, JsValue],
val geometry: RichGeometry) {
def apply(property: String) = properties(property)
def get(property: String) = properties.get(property)
}

case class FeatureCollection(features: Array[Feature])
extends IndexedSeq[Feature] {
def apply(index: Int) = features(index)
def length = features.length
}

implicit object FeatureJsonFormat extends
RootJsonFormat[Feature] {
def write(f: Feature) = {

val buf = scala.collection.mutable.ArrayBuffer(
"type" -> JsString("Feature"),
"properties" -> JsObject(f.properties),
"geometry" -> f.geometry.toJson)
f.id.foreach(v => { buf += "id" -> v})
JsObject(buf.toMap)
}
def read(value: JsValue) = {
    val jso = value.asJsObject
    val id = jso.fields.get("id")
    val properties = jso.fields("properties").asJsObject.fields
    val geometry = jso.fields("geometry").convertTo[RichGeometry]
    Feature(id, properties, geometry)
    }
    }
}

 hadoop fs -mkdir taxidata
$ hadoop fs -put trip_data_1.csv taxidata/


 mvn package
$ spark-shell --jars ch08-geotime-2.0.0-jar-with-dependencies.jar

val taxiRaw = spark.read.option("header", "true").csv("taxidata")
taxiRaw.show()


case class Trip(
license: String,
pickupTime: Long,
dropoffTime: Long,
pickupX: Double,
pickupY: Double,
dropoffX: Double,
dropoffY: Double)

class RichRow(row: org.apache.spark.sql.Row) {
def getAs[T](field: String): Option[T] = {
    if (row.isNullAt(row.fieldIndex(field))) {
    None
    } else {
    Some(row.getAs[T](field))
    }
    }
}

def parseTaxiTime(rr: RichRow, timeField: String): Long = {
val formatter = new SimpleDateFormat(
"yyyy-MM-dd HH:mm:ss", Locale.ENGLISH)
val optDt = rr.getAs[String](timeField)
optDt.map(dt => formatter.parse(dt).getTime).getOrElse(0L)
}





def parseTaxiLoc(rr: RichRow, locField: String): Double = {
rr.getAs[String](locField).map(_.toDouble).getOrElse(0.0)
}

def parse(row: org.apache.spark.sql.Row): Trip = {
val rr = new RichRow(row)
Trip(
    license = rr.getAs[String]("hack_license").orNull,
    pickupTime = parseTaxiTime(rr, "pickup_datetime"),
    dropoffTime = parseTaxiTime(rr, "dropoff_datetime"),
    pickupX = parseTaxiLoc(rr, "pickup_longitude"),
    pickupY = parseTaxiLoc(rr, "pickup_latitude"),
    dropoffX = parseTaxiLoc(rr, "dropoff_longitude"),
    dropoffY = parseTaxiLoc(rr, "dropoff_latitude")
)
}

def safe[S, T](f: S => T): S => Either[T, (S, Exception)] = {
new Function[S, Either[T, (S, Exception)]] with Serializable {
def apply(s: S): Either[T, (S, Exception)] = {
try {
Left(f(s))
} catch {
case e: Exception => Right((s, e))
}
}
}
}

val safeParse = safe(parse)
val taxiParsed = taxiRaw.rdd.map(safeParse)

taxiParsed.map(_.isLeft).
countByValue().
foreach(println)

val taxiGood = taxiParsed.map(_.left.get).toDS
taxiGood.cache()

val hours = (pickup: Long, dropoff: Long) => {
TimeUnit.HOURS.convert(dropoff - pickup, TimeUnit.MILLISECONDS)
}

import org.apache.spark.sql.functions.udf
val hoursUDF = udf(hours)
taxiGood.
groupBy(hoursUDF($"pickupTime", $"dropoffTime").as("h")).
count().
sort("h").
show()


taxiGood.
where(hoursUDF($"pickupTime", $"dropoffTime") < 0).
collect().
foreach(println)


spark.udf.register("hours", hours)
val taxiClean = taxiGood.where(
"hours(pickupTime, dropoffTime) BETWEEN 0 AND 3"
)

val geojson = scala.io.Source.
fromFile("nyc-boroughs.geojson").
mkString


import com.cloudera.datascience.geotime._
import GeoJsonProtocol._
import spray.json._

val features = geojson.parseJson.convertTo[FeatureCollection]

import com.esri.core.geometry.Point
val p = new Point(-73.994499, 40.75066)
val borough = features.find(f => f.geometry.contains(p))

val areaSortedFeatures = features.sortBy(f => {
val borough = f("boroughCode").convertTo[Int]
(borough, -f.geometry.area2D())
})


val bFeatures = sc.broadcast(areaSortedFeatures)
val bLookup = (x: Double, y: Double) => {
val feature: Option[Feature] = bFeatures.value.find(f => {
f.geometry.contains(new Point(x, y))
})
feature.map(f => {
f("borough").convertTo[String]
}).getOrElse("NA")
}
val boroughUDF = udf(bLookup)

taxiClean.
groupBy(boroughUDF($"dropoffX", $"dropoffY")).
count().
show()


taxiClean.
where(boroughUDF($"dropoffX", $"dropoffY") === "NA").
show()

val taxiDone = taxiClean.where(
"dropoffX != 0 and dropoffY != 0 and pickupX != 0 and pickupY != 0"
).cache()

taxiDone.
groupBy(boroughUDF($"dropoffX", $"dropoffY")).
count().
show()

val sessions = taxiDone.
repartition($"license").
sortWithinPartitions($"license", $"pickupTime")


sessions.cache()



def boroughDuration(t1: Trip, t2: Trip): (String, Long) = {
val b = bLookup(t1.dropoffX, t1.dropoffY)
val d = (t2.pickupTime - t1.dropoffTime) / 1000
(b, d)
}


val boroughDurations: DataFrame =
sessions.mapPartitions(trips => {
val iter: Iterator[Seq[Trip]] = trips.sliding(2)
val viter = iter.
filter(_.size == 2).
filter(p => p(0).license == p(1).license)
viter.map(p => boroughDuration(p(0), p(1)))
}).toDF("borough", "seconds")



boroughDurations.
selectExpr("floor(seconds / 3600) as hours").
groupBy("hours").
count().
sort("hours").
show()

boroughDurations.
where("seconds > 0 AND seconds < 60*60*4").
groupBy("borough").
agg(avg("seconds"), stddev("seconds")).
show()


























