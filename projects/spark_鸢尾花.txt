Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species
5.1	3.5	1.4	0.2	Iris-setosa
4.9	3	1.4	0.2	Iris-setosa
4.7	3.2	1.3	0.2	Iris-setosa
4.6	3.1	1.5	0.2	Iris-setosa
5	3.6	1.4	0.2	Iris-setosa
5.4	3.9	1.7	0.4	Iris-setosa
4.6	3.4	1.4	0.3	Iris-setosa
5	3.4	1.5	0.2	Iris-setosa



import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.{SparkConf, SparkContext}

object irisALL{
  def main(args: Array[String]) {
val conf = new SparkConf()                                       //创建环境变量
.setMaster("local")                                               //设置本地化处理
.setAppName("irisAll ")                                          //设定名称
    val sc = new SparkContext(conf)                                  //创建环境变量实例
    val data = sc.textFile("c:// Sepal.Length.txt")                        //创建RDD文件路径
.map(_.toDouble))                                             //转成Double类型
      .map(line => Vectors.dense(line))                                //转成Vector格式
val summary = Statistics.colStats(data)						  //计算统计量
println("全部Sepal.Length的均值为：" + summary.mean)		      //打印均值
println("全部Sepal.Length的方差为：" + summary.variance)	      //打印方差
  }
}
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

object irisBayes {
  def main(args: Array[String]) {

val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisBayes")                              		//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = MLUtils.loadLabeledPoints(sc,"c://a.txt")				//读取数据集
    val model = NaiveBayes.train(data, 1.0)						//训练贝叶斯模型
val test = Vectors.dense(7.3,2.9,6.3,1.8)						//创建待测定数据
val result = model.predict(“测试数据归属在类别:” + test)			//打印结果
}
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.{SparkConf, SparkContext}

object irisCorrect {
  def main(args: Array[String]) {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisCorrect ")                                    //设定名称
    val sc = new SparkContext(conf)                                //创建环境变量实例
    val dataX = sc.textFile("c://x.txt")                                //读取数据
        .flatMap(_.split(' ')                                         //进行分割
        .map(_.toDouble))                                         //转化为Double类型
    val dataY = sc.textFile("c://y.txt")                                 //读取数据
      .flatMap(_.split(' ')                                            //进行分割
      .map(_.toDouble))                                           //转化为Double类型
    val correlation: Double = Statistics.corr(dataX, dataY)         //计算不同数据之间的相关系数
    println(correlation)                                              //打印结果
  }
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.{SparkConf, SparkContext}

object irisCorrect2 {
  def main(args: Array[String]) {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisCorrect2")                                    //设定名称
    val sc = new SparkContext(conf)                                //创建环境变量实例
    val dataX = sc.textFile("c://x.txt")                                //读取数据
        .flatMap(_.split(' ')                                         //进行分割
        .map(_.toDouble))                                         //转化为Double类型
    val dataY = sc.textFile("c://y.txt")                                 //读取数据
      .flatMap(_.split(' ')                                            //进行分割
      .map(_.toDouble))                                           //转化为Double类型
    val correlation: Double = Statistics.corr(dataX, dataY)         //计算不同数据之间的相关系数
    println("setosa和versicolor中Sepal.Length的相关系数为：" + correlation) //打印相关系数
  }
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.{SparkConf, SparkContext}

object irisCorrect {
  def main(args: Array[String]) {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisCorrect3 ")                                    //设定名称
    val sc = new SparkContext(conf)                                //创建环境变量实例
    val dataX = sc.textFile("c://x.txt")                                //读取数据
        .flatMap(_.split(' ')                                         //进行分割
        .map(_.toDouble))                                         //转化为Double类型
    val dataY = sc.textFile("c://y.txt")                                 //读取数据
      .flatMap(_.split(' ')                                            //进行分割
      .map(_.toDouble))                                           //转化为Double类型
    val correlation: Double = Statistics.corr(dataX, dataY)         //计算不同数据之间的相关系数
    println("setosa和versicolor中Sepal.Length的相关系数为：" + correlation) //打印相关系数
  }
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.util.MLUtils

object irisDecisionTree {
  def main(args: Array[String]) {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisDecisionTree ")                              	//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = MLUtils.loadLibSVMFile(sc, "c://a.txt")				//输入数据集
    val numClasses = 3 										//设定分类数量
    val categoricalFeaturesInfo = Map[Int, Int]()					//设定输入格式
    val impurity = "entropy"									//设定信息增益计算方式
    val maxDepth = 5										//设定树高度
    val maxBins = 3											//设定分裂数据集
    val model = DecisionTree.trainClassifier(data, numClasses, categoricalFeaturesInfo,
      impurity, maxDepth, maxBins)								//建立模型
    val test = Vectors.dense(Array(7.2,3.6,6.1,2.5))
    println(model.predict(“预测结果是:” + test))
   }
}
import org.apache.spark.mllib.clustering.GaussianMixture
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkConf, SparkContext}

object irisGMG {
  def main(args: Array[String]) {

val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisGMG")                              			//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = sc.textFile("c://a.txt")							//输入数个
val parsedData = data.map(s => Vectors.dense(s.trim.split(' ')		//转化数据格式
.map(_.toDouble))).cache()
    val model = new GaussianMixture().setK(2).run(parsedData)		//训练模型

    for (i <- 0 until model.k) {
      println("weight=%f\nmu=%s\nsigma=\n%s\n" format			//逐个打印单个模型
        (model.weights(i), model.gaussians(i).mu, model.gaussians(i).sigma))	//打印结果
    }
  }
}
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkConf, SparkContext}

object irisKmeans{
  def main(args: Array[String]) {

val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisKmeans ")                              		//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = MLUtils.loadLibSVMFile(sc, "c://a.txt")			//输入数据集
val parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble)))
.cache()												//数据处理

    val numClusters = 3										//最大分类数
    val numIterations = 20									//迭代次数
    val model = KMeans.train(parsedData, numClusters, numIterations)	//训练模型
    model.clusterCenters.foreach(println)							//打印中心点坐标

  }
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
import org.apache.spark.{SparkConf, SparkContext}


object irisLinearRegression {
 val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisLinearRegression ")                            //设定名称
    val sc = new SparkContext(conf)                                //创建环境变量实例
    val data = sc.textFile("c:/a.txt")								//读取数据
    val parsedData = data.map { line =>							//处理数据
        val parts = line.split('	')								//按空格分割
        LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).toDouble)) //固定格式
      }.cache()												//加载数据
    val model = LinearRegressionWithSGD.train(parsedData, 10,0.1)	//创建模型
    println("回归公式为: y = " + model.weights + " * x + " + model.intercept) //打印回归公式
  }
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
import org.apache.spark.{SparkConf, SparkContext}


object irisLinearRegression2 {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisLinearRegression2")                            //设定名称
    val sc = new SparkContext(conf)                                //创建环境变量实例
    val data = sc.textFile("c:/a.txt")								//读取数据
    val parsedData = data.map { line =>							//处理数据
        val parts = line.split('	')								//按空格分割
        LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).toDouble)) //固定格式
      }.cache()												//加载数据
    val model = LinearRegressionWithSGD.train(parsedData, 10,0.1)	//创建模型
val valuesAndPreds = parsedData.map { point => {				//创建均方误差训练数据
    val prediction = model.predict(point.features)					//创建数据
      (point.label, prediction)									//创建预测数据
}
    val MSE = valuesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.mean() //计算均方误差
println(“均方误差结果为:” + MSE)  							//打印结果
}
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
import org.apache.spark.{SparkConf, SparkContext}


object irisLogicRegression{
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisLogicRegression ")                            //设定名称
    val sc = new SparkContext(conf)                                //创建环境变量实例
    val data = sc.textFile("c:/a.txt")								//读取数据
    val parsedData = data.map { line =>							//处理数据
        val parts = line.split('	')								//按空格分割
        LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).toDouble)) //固定格式
      }.cache()												//加载数据
    val model = irisLogicRegression.train(parsedData, 20)	         //创建模型
val valuesAndPreds = parsedData.map { point => {				//创建均方误差训练数据
    val prediction = model.predict(point.features)					//创建数据
      (point.label, prediction)									//创建预测数据
}
    val MSE = valuesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.mean() //计算均方误差
println(“均方误差结果为:” + MSE)  							//打印结果
}
}

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.{SparkConf, SparkContext}

object irisMean{
  def main(args: Array[String]) {
val conf = new SparkConf()                                       //创建环境变量
.setMaster("local")                                               //设置本地化处理
.setAppName("irisMean ")                                        //设定名称
    val sc = new SparkContext(conf)                                  //创建环境变量实例
    val data = sc.textFile("c:// Sepal.Length_setosa.txt")                  //创建RDD文件路径
.map(_.toDouble))                                             //转成Double类型
      .map(line => Vectors.dense(line))                                //转成Vector格式
val summary = Statistics.colStats(data)						  //计算统计量
println("setosa中Sepal.Length的均值为：" + summary.mean)		  //打印均值
println("setosa中Sepal.Length的方差为：" + summary.variance)	  //打印方差
  }
}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.{SparkConf, SparkContext}

object irisNorm{
  def main(args: Array[String]) {
val conf = new SparkConf()                                       //创建环境变量
.setMaster("local")                                               //设置本地化处理
.setAppName("irisNorm")                                         //设定名称
    val sc = new SparkContext(conf)                                   //创建环境变量实例
    val data = sc.textFile("c:// Sepal.Length_setosa.txt")                   //创建RDD文件路径
.map(_.toDouble))                                              //转成Double类型
      .map(line => Vectors.dense(line))                                 //转成Vector格式
val summary = Statistics.colStats(data)						   //计算统计量
    println("setosa中Sepal的曼哈顿距离的值为：" + summary.normL1)	   //计算曼哈顿距离
    println("setosa中Sepal的欧几里得距离的值为：" + summary.normL2)  //计算欧几里得距离
  }
}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.util.MLUtils

object irisRFDTree {
  def main(args: Array[String]) {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("irisRFDTree")                              		//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = MLUtils.loadLibSVMFile(sc, "c://a.txt")				//输入数据集

val numClasses = 3										//设定分类的数量
    val categoricalFeaturesInfo = Map[Int, Int]()					//设置输入数据格式
    val numTrees = 3 								   //设置随机雨林中决策树的数目
val featureSubsetStrategy = "auto"							//设置属性在节点计算数
val impurity = "entropy"									//设定信息增益计算方式
    val maxDepth = 5										//设定树高度
    val maxBins = 3											//设定分裂数据集

    val model = RandomForest.trainClassifier(data, numClasses, categoricalFeaturesInfo,
      numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)	//建立模型

    model.trees.foreach(println)								//打印每棵树的相信信息
  }
}
