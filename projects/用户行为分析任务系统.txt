
离线数据分析平台，一般借助于HDFS、 MapReduce、 Hive、 HBase、 Flume、Sqoop等框架进行搭建，适用于对于分析结果反馈时间不敏感的应用场景，各个框架/组件的作用如下：

HDFS：数据存储。日志原始数据、分析中间结果、 HBase数据等存储

MapReduce：离线批量数据处理框架

HBase：基于列存储的实时NoSQL数据库，不支持SQL查询

Hive：统计分析

Flume：日志收集

Sqoop：传统关系型数据库与HDFS平台之间的数据传输

--------------------------------------------------------------------------------------------

目标：为领导层提供数据来了解网站，为产品经理提供修改网站的数据修改，为运营人员提供潜
在的促销商机

两大类：离线和实时

数据来源：服务器数据、业务数据、用户行为数据、爬虫数据、购买数据

数据处理流程：数据收集、数据处理&分析、数据可视化、结果数据分析处理(数据应用)

适合行业：电商、旅游、游戏、金融产品等依赖用户行为的新型互联网行业

市场产品：IBM(Watson Analytics)、 GA(Google Analytics)、 百度统计、 umeng等

优势：
数据是自己的
容易扩展后期服务：用户画像、用户推荐、精准营销等
可进行定制化开发
数据分析平台

数据来源：服务器数据、业务数据、用户行为数据、爬虫数据、购买数据

==============================================================

数据采集

采用原生JavaScript编写，以JS的方式嵌入到需要收集数据的页面中，
当事件触发的时候调用对应的JS方法，JS方法访问Nginx的服务器地
址，并将用户行为数据发送给服务器
当访问服务器产生异常的时候，可以将数据缓存到local storage中，
待下次网络有效的时候，再将数据发送给服务器
JS SDK设计思路
JS SDK执行流程
JS SDK – analytics.js
JS SDK – 页面集成1
JS SDK – 页面集成2

核心要求：
成功发送数据到Nginx服务器
不能对业务系统产生太大的压力
发送数据的方式必须是异步操作，业务系统不需要知道数据的发送结果
使SDK尽可能的小，直接提供API供使用方调用，在正常业务逻辑之
前或者业务逻辑之后，调用SDK提供的API即可

SDK将数据发送到Nginx服务后，服务器将访问数据以Nginx日志的
方式保存到磁盘文件中
日志分割符：^A
日志格式为：客户端ip地址^A服务器时间^A用户行为信息/访问信息

将日志数据上传到HDFS中进行处理，可以分为以下几种情况：

如果日志服务器数据较小、压力较小，直接shell脚本上传

如果日志服务器非常多、压力比较大，可以使用Flume的多路复用机制将数据集中到Flume专门服务器，最终将数据上传到HDFS

为了防止日志文件过大，可以直接使用shell脚本将日志文件进行分割操作，这种方式可以降低Nginx服务器的磁盘压力

读取切割后保存的日志文件，直接通过hdfs命令将文件上传到HDFS上容错-shell脚本直接上传日志文件到HDFS

=======================================================================================================

日志数据上传HDFS

如何实现数据收集的容错性？减少丢失数据的问题！
客户端：如果网络不好或者发送数据失败，可以直接将数据进行缓存操作，
PC端可以缓存到local storage，
移动端可以直接缓存到客户端

Nginx：可以直接使用Nginx的容错机制

Flume：
使用flume的sinkgroups机制，进行sink容错
使用flume的扇入扇出机制，解决大数据量日志对单台flume机器的压力
人工干预，手动上传文件到hdfs上数据收集问题及解决方案


Flume启动命令
原理：Nginx记录的是文件句柄，所以将文件移动后，Nginx仍然会将日志写入到文件中。
Flume的exec source使用tail –F命令当文件移动后，会重试命令。所以将文件移动，然后重启
Nginx的方式是可行的。
缺点：部分数据会丢失，存在于移动后的文件中，Flume不会传送该部分数据，所以选择凌晨执
行文件切割命令。




==============================================================
数据清洗
ETL：Extract-Transform-Load，数据抽取-转换-加载过程。
目标：过滤无效数据，补全数据(解析补全数据)
无效数据：缺少访客id、会话id、订单id等关键属性的，针对不同事件有不同的属性要求
补全数据：浏览器信息、操作系统信息、地域信息等
数据来源：存储在HDFS之上的用户行为日志数据
数据存储位置：使用HBase作为最终存储解析成功数据的位置
作用：保存ETL处理好的数据，为后续任务提供数据。

日志数据ETL操作代码编写
Windows平台下MapReduce运行介绍
自定义OutputFormat
自定义基于Hadoop RPC的服务
用户分析代码编写

ETL：Extract-Transform-Load，数据抽取-转换-加载过程。
目标：过滤无效数据，补全数据(解析补全数据)
无效数据：缺少访客id、会话id、订单id等关键属性的，针对不同事件有不同
的属性要求
补全数据：浏览器信息、操作系统信息、地域信息等
数据来源：存储在HDFS之上的用户行为日志数据
数据存储位置：使用HBase作为最终存储解析成功数据的位置
ETL

目标：解析出地域信息，至少解析出国家、省份、城市三级地域信
息
方式：
使用纯真IP数据库来进行IP解析
调用淘宝提供的IP解析服务。访问限制：访问频率10qps一下
使用公司内部的IP数据库。公司内部的IP数据库可以采用解析纯真IP数据来进
行初始填充，使用淘宝IP解析服务补全IP数据库的内容
IP解析



==============================================================
数据仓库

通过两种不同的方式计算数据
MapReduce
数据输入源是HBase，通过MR任务后，直接将数据保存到MySQL关系型数据库中
Hive
在Hive创建HBase对应的外部表(external table)
通过编写HQL语句将结果计算出来并保存到hdfs上
通过Sqoop将HDFS上的数据移动到MySQL关系型数据库中

数据处理任务
数据仓库：单个数据存储，出于分析性报告和决策支持目的而创建。
能为有需要的企业提供业务流程改进、成本控制、质量控制等方面的
建议和指导
数据仓库模型：星型模型、雪花模型
数据仓库基本知识概念



==============================================================
数据存储

rowKey：
访客id+会员id+服务器时间+事件类型经过crc32编码/哈希编码后作为rowKey。

优点：不会出现

热点问题，rowKey长度不算特别长；缺点：无法利用rowKey来明确获取某条数据
服务器时间戳+访客id直接作为rowKey。优点：方便取值；缺点：热点问题
热点问题：由于我们统计数据的最小单位是以天为单位的，所以我们可以直接将数据按
天分表，这样可以直接使用第一种rowKey设计方式，可以有效的减少HBase split操作。
减少HBase split操作的另外一种方式是：在创建表的时候通过预分区来解决该问题。
为了贴近现实，在项目实战中采用第一种rowKey设计方式
列簇：采用单列簇的设计方式，列簇名为info，通过具体的列名称来区分数据


HBase表结果设计
表创建：在代码中进行表创建
rowKey: 采用crc32(所有数据)方式产生随机rowkey
采用单列簇表结果设计
ETL-HBase表




==============================================================
数据分析

数据格式

用户(访客)
由于访客的唯一标识ID不能够由业务系统来保证完成，所以我们需要通过其他方
式来进行访客的区分操作：

PC端/移动WEB端：
采用IP地址来区分用户，可能会由于代理、 NET技术等问题导致多个用户使用一个IP地址
采用客户端种植cookie的方式。当用户第一次访问网站的时候，产生一个唯一uuid，存放到cookie中，
这种方式更加细化，对于数据的准确性和精准性会有更有效的提高，确定是增加了数据收集的复杂性

移动端：
采用手机固定的机器码识别，比如IMEI码、 MEID码、 MAC地址等，可能由于刷机、 wifi未使用等情况
导致值为空或者多个用户对应一个值的情况
同PC端，采用uuid种植的方式
用户(访客)区分方式



分析项/指标

购买率
复购率
订单数量&金额&类别情况
成功订单数量&金额&类别情况
退款订单数量&金额&类别情况
访客/会员数量
访客转会员率(新访客和老访客分布的转会员比率)
广告推广效果
网站内容(跳出率等)

用户(访客)
会员
会话
跳出率
外链

Pv(page view)


Uv(unique visitor)


Dv(depth view)
DV：depth view值，指用户访问深度，一般统计各个不同深度的用户数量、会话数量、会话长度等指标
DV反映了一个网站内容是否对用户有吸引力，能否有用户关注的内容，提高DV的值能够提高用户对网站的粘性，结合页面跳出率能够更好的提升系统的用户友好性


访问网站的非登陆客户
涉及分析项/指标：

新增用户数量：给定时间段内第一次访问系统的用户数量
活跃用户数量：给定时间段内访问过系统的用户数量
总用户数量：计算迄今为止，所有总的访问系统的用户数量
流失用户数量：给定时间段内没有访问过系统的用户数量
回流用户数量：很久没有访问过，但最近访问过的用户数量


时间维度：年、月、季度、周、日、小时
平台维度：PC端、 android、 ios、后台系统等
渠道维度：PC端可以理解为访问来源(解析比较复杂)、移动端可以理解为网络运营商
浏览器维度：浏览器类型&浏览器版本
操作系统维度：操作系统类型&操作系统版本
地域维度：国家、 省份、市(城市)
系统版本：比如v1、 v2等等，一般用于版本之间的效果比较(AB测试)


跳出的意思是指离开系统的行为。一般分为两大类：会话跳出率和页面跳
出率。
会话跳出率：在一个会话中只进行一个用户动作的会话数量占总会话的百
分比，一般作为一个系统的整体跳出率展示，用来判断系统整体的用户友
好性。
页面跳出率：给定时间段内，从该页面离开的用户数量占访问该页面总数
量的百分比。有时会将多个页面作为一组页面来进行分析计算，一般用来
判断系统具体页面的用户友好性。


会员其实就是指业务系统中的注册用户，也就是已经登陆的访客，此时我
们可以使用业务系统生成的唯一umid来标识用户(一定是唯一的)
涉及分析项/指标：(基本同访客)
新增会员
活跃会员
总会员
流失会员
回流会员
访客转会员率：给定时间段内，新/老用户转换为会员的比率
会员

会话是指用户进去系统到用户完全离开系统这段时间被称为一次会话，这个过程所花的
时间长度就是会话时长。
如何定义会话范围
PC端/移动WEB端：通过定义访问的最大间隔时间来定义会话的范围，比如定义访问的最大间隔
时间为6分钟，那么超过六分钟的就可以算是一个新会话
移动端：通过android、 ios等移动端APP的session来定义会话。
涉及的分析项/指标：
会话数量：给定时间段内会话的数量
会话时长：给定时间段内所有会话的时长总和
会话跳出率：一次会话过程中只反问一次的会话数量占总会话数量的比率
会话


当用户通过其他第三方的外部系统进入到我们系统的时候，我们称此时的
外部系统就是外链，一般外链是针对PC端而言的。
涉及分析项/指标：
外链带来的访客数量
外链带来的会话数量
外链带来的新访客转会员比率
外链跳出率
作用：更好的广告投入，节省推广成本


外链
PV：page view值，网页浏览数
用户每次对网站的每个页面访问均被记录1次
用户对同一个页面的多次访问，访问量累计
UV：unique visitor值，指通过互联网访问、浏览系统的自然人
UV的统计需要进行数据去重操作，一个用户的多次访问、操作记录只记录
一次
独立IP：访问系统的IP数量，一般辅助UV来进行数据展示
为什么选择UV，而不选择独立IP数量？
UV更能够真实反映网站的访问情况
IP地址在某些情况下，会出现多个用户共享一个IP的情况，比如NAT、代理访问、拨
号上网等，一般而言IP数量小于UV值


launch事件
launch事件主要
就是表示用户
(访客)第一次到
网站的事件类型，
主要应用于计算
新用户等相关任
务的计算。
参数名 说明
en 事件名称， launch事件为： e_l
ver 版本号
pl 平台名称， launch事件中为： website
sdk sdk版本号， website平台中为js
u_ud 用户id，唯一标示访客(用户)
u_mid 会员id，业务系统的用户id
u_sd 会话id，标示会话id
c_time 客户端时间
l 平台语言， window.navigator.language
b_iev 浏览器信息， window.navigator.userAgent
b_rst 浏览器屏幕大小， screen.width + "*" + screen.height
pageView事件
pageView事件
是pc端的基本事
件类型，主要是
描述用户访问网
站信息，应用于
基本的各个不同
计算任务。
参数名 说明
其他参数 launch事件中除了en外的所有参数
en 事件名称， pageView事件为： e_pv
p_url 当前页面的url
p_ref 前一个页面的url，如果没有前一个页面，那么值为空
tt 当前页面的标题



MapReduce任务
输入：存储在HBase上的ETL操作完的日志数据，要求事件类型为launch、
pageview
Mapper：数据过滤操作
Reduce：分别计算各个指标
输出：直接存储到MySQL中
用户行为分析-MR任务

只需要Mapper阶段，不需要Reduce阶段
多次运行ETL数据结果一致
输入数据：存储在HDFS上的日志数据
Mapper：对每条数据进行清洗、过滤、补全等操作，对于符合要求的数据进
行输出操作
数据输出：数据最终输出到HBase中
ETL-MapReduce程序


事件分析MR任务实现
Hourly分析Hive实现
Highcharts介绍
数据展示代码讲解
目标：统计事件流中各个事件的触发次数，不涉及到去重，一个会话
中一个用户触发多次，对于事件值计算多次。
注意：
计算事件触发次数的时候，除了数据流的第一个事件外，其他事件必须发生
之前必须有前一个事件的发生
所有事件必须位于同一个会话中，如果位于多个会话，分为两个事件流来进
行计算
事件分析业务讲解

MapReduce任务
输入：存储在HBase上的ETL操作完的日志数据，要求事件类型为event
Mapper：数据数据过滤操作
Reduce：事件流分析，过滤非正常的事件流，计算事件流中各个事件的触发
次数
输出：直接存储到MySQL中
事件分析-MR任务


目标：分析一天24个时间段的新增用户、活跃用户、会话个数和会话
长度四个指标的数据
注意：
如果一个用户/会话跨小时，那么分别在两个小时指标中计算
如果没有访问数据，默认值为0
Hourly分析业务讲解

1、创建UDF，用于解析维度id
2、 Hive中创建HBase对于表的external表
3、编写HQL语句进行数据分析
4、编写Sqoop脚步将数据移动到MySQL中(作业)

以JSON格式返回结果
支持同时获取多个指标的值
数据展示代码讲解

Hadoop RPC
Hadoop服务间数据调用的一种架构，使用Server和RPC两个类分别创建服
务和调用服务，具有两种不同的服务实现方式：
基于Writable的RPC调用
• 必须实现VersionedProtocol接口，该接口用于控制版本
• 具体实现查看源码WritableRpcEngine
基于ProtoBuf的RPC调用
• 具体实现查看源码ProtobufRpcEngine源码对于自定义服务的要求
自定义RPC服务

自定义OutputFormat
自定义输出器，将MR任务的数据输出到自定义的目的地，比如：redis等
实现抽象类Outputformat，必须实现方法getRecordWriter和
getOutputCommitter
RecordWriter
具体数据写出对象，明确数据如何进行写出操作
必须实现方法write方法，定义具体的数据写出操作



本地运行
通过eclipse等开发工具直接在开发环境运行MR任务
注意点：
设置HADOOP_HOME环境变量，并将winutils.exe移动到HADOOP的bin目录中
由于平台环境的不一致，需要修改HADOOP的源码，根据运行异常来进行修改
本地提交集群运行
通过eclipse等开发工具将代码直接提交到HADOOP集群中进行运行
注意点：
提交Job需要将代码一起进行提交，使用工具类:Ejob.java(缺陷：依赖包无法一起提交)
提交Job需要额外设置yarn服务的相关IP地址、端口号等配置信息
跨平台提交Job必须设置参数mapreduce.app-submission.cross-platform为false
类YarnClientProtocolProvider必须在classpath中


集群运行
直接打包成为jar，然后直接通过hadoop jar命令进行job提交
Windows平台MR任务运行




目标：分析各个不同维度的各种不同指标
维度：
时间维度：月、周、日
平台维度：全部、 website、 android、 ios等
分析指标：
新增用户
活跃用户
总用户
流失用户：选择五天的数据，在前四天访问过网站但是当天没有访问网站的用户数量
新增用户流失数量：前天新增用户，昨天继续访问的用户数量
用户行为分析业务讲解

目标：解析出来浏览器信息和操作系统信息
使用第三方jar包： UASparser
User Agent解析







=====================

结果存储





维度信息表 dimension_xxx
定位具体维度信息，确定数据的维度关系，比如时间、平台、渠道、地域等

事实数据表/统计分析结果表 stats_xxx
保存最终的分析数据，外键关联维度表，所有维度字段作为联合主键(可选)

分析辅助表
保存一些有助于分析的辅助信息，这类信息既可以保存到MySQL这样的关系型数据库中，
也可以保存到HBase中，比如订单信息、会员信息等等


MR任务如何将数据直接写入到MySQL中？
Hadoop自带的DBOuputFormat可能无法满足我们的要求，所以可以自定义
OutputFormat输出器，实现较复杂

将数据写入到HDFS，然后通过Sqoop将数据移动到MySQL中，比较繁琐
多reduce task的任务或者hive任务如何将维度信息数据写入到MySQL中？
严格控制任务只能有一个reduce task，不适用Hive，而且严重影响性能
数据库加锁，对影响性能

远程服务，使用一个单独的服务专门提供维度信息管理功能，实现较复杂
数据分析开发难点及解决方案


其实数据展示不是我们项目的重点，对应最终数据的展示，我们可以让BI
这类的项目组（具体需求的项目组）来做。但是作为一个完整的项目，自
己内部人员还是需要了解数据的，所以一个简单的web页面也是需要的。
数据展示模块采用前后台分离的结构。后台采用SpringMVC + MyBatis的
框架读取MySQL中的分析结果数据，并以json的格式返回给调用方；前端
选择JavaScript+Highcharts+CSS的方式进行结果展示。




========================
数据可视化


Nginx是一个高性能的HTTP和反向代理服务器，单台Nginx服务器最
多能够支持高达50000的并发请求，所以一般情况下，会将Nginx作
为静态资源的访问服务器或者作为访问流量分流的服务器。

主要特点：占用内存少、并发能力强、扩展容易

操作：修改conf文件夹中的nginx.conf文件即可完成nginx服务器的
配置。




============================================
结果数据分析处理

前端

用户基本信息分析

用户基本数据分析

用户分析

会员分析

会话分析

Hourly分析

浏览器信息分析

地域信息分析

用户访问深度分析

外链数据分析

订单分析

事件分析

event事件
event事件是专
门记录用户对于
某些特定事件/活
动的触发行为，
主要是用于计算
各活动的活跃用
户以及各个不同
访问链路的转化
率情况等任务。
参数名 说明
其他参数 launch事件中除了en外的所有参数
en 事件名称， event事件为： e_e
ca 事件的category值，即事件的种类名称，不为空
ac 事件的action值，即事件的活动名称，不为空
du 事件持续时间，可以为空
kv_ 事件自定义属性键值对。比如kv_keyname=value，这里的
keyname和value就是用户自定义，支持在事件上定义多个属性键
值对

chargeRequest事件
该事件的主要作
用是记录用户产
生订单的行为/
数据，为统计计
算订单相关的统
计结果提供基础
数据。
参数名 说明
其他参数 launch事件中除了en外的所有参数
en 事件名称， event事件为： e_crt
oid 订单id(order id)
on 订单名称(order name)
cua 订单金额(currency amount)
cut 订单支付货币类型(currency type)
pt 订单支付方式(payment type)


chargeSuccess事件
该事件的主要作
用是记录用户在
产生订单后，支
付订单的行为。
为统计订单转化
率提供基础数据
的支持。
参数名 说明
u_mid 会员id，业务系统的用户id
en 事件名称， chargeSuccess事件为： e_cs
oid 订单id(order id)
c_time 客户端时间
ver 版本信息
pl 平台名称，后台平台名称为： java_server
sdk sdk名称，后台平台名称为： jdk

chargeRefund事件
该事件的主要
作用是记录用
户对应订单退
款的相关行为。
为统计订单退
款率提供基础
数据的支持。
参数名 说明
u_mid 会员id，业务系统的用户id
en 事件名称， chargeSuccess事件为： e_cr
oid 订单id(order id)
c_time 客户端时间
ver 版本信息
pl 平台名称，后台平台名称为： java_server
sdk sdk名称，后台平台名称为： jdk
============================





