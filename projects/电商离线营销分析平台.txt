2016/08 - 2016/09	电商离线营销分析平台
	软件环境：Nginx、Flume、Hdfs、MapReduce、Hbase、Hive、Zookeeper、Oozie、Hue、 Spring、Redis、Mybatis、Mysql、echarts
	开发工具：intellij IDEA、Maven、SVN
	责任描述：
1、参与使用Flume数据收集和数据清洗工作
2、使用hive，按照活跃度、会话时长与步长和时间维度对用户基本数据进行分析
3、解决hive数据倾斜问题，并进行HiveSQL优化，取得了一定成果。
4、利用Oozie、Hue等协作框架进行任务调度、监控及可视化操作
	项目简介：通过分析电商公司的服务器日志、部分业务数据、用户行为日志，为该公司的未来营销策略提供数据支持。
整个项目分为数据采集、数据分析、数据展示三个子系统，数据采集系统分为Web端和业务系统后端采集，数据分析系统共分为七个模块，分别为：用户基本数据分析、浏览器维度分析、地域维度分析、外链维度分析、用户浏览深度分析、事件分析、订单分析，数据展示系统用Echarts图表的形式展示各分析模块的结果。


项目过程：
初期系统建设：
数据采集系统：
1、PC端和移动Web端采用js埋点技术，后端编写SDK,采用异步接收,多线程消息队列的方式发送数据给Nginx入口服务
2、采用Flume多agent串行传输的方式将数据从web服务器集群收集到HDFS上
数据分析系统：
1、使用mapreduce对日志文件进行数据清洗,存入Hbase表
2、读取Hbase中的日志数据，用HiveSQL语句分析各个模块(编写udf、创建外部表，创建临时表，将结果保存到hive表中)，部分操作使用mapreduce
3、使用sqoop将结果保存到Mysql
数据展示系统：使用spring+springMVC+mybatis框架，读取Mysql中的分析结果数据,并使用ECharts进行报表展示

中期对工具类集成，优化Hive

后期向Spark转移，利用SparkSQL进行数据分析


===========================================================================

通过分析电商公司的服务器日志和用户行为日志，根据会员数，ip,Session的步长和时长为该公司的未来营销策略提供数据支持整个项目分为收集、分析、展示三个层面，共包含基本数据分析，地域维度分析，浏览器维度分析等8个子模块

项目描述：
通过分析电商公司的服务器日志、部分业务数据、用户行为日志，为该公司的未来营销策略提供数据支持。
整个项目分为数据采集、数据分析、数据展示三个子系统，数据采集系统分为Web端和业务系统后端采集，数据分析系统共分为七个模块，分别为：用户基本数据分析、浏览器维度分析、地域维度分析、外链维度分析、用户浏览深度分析、事件分析、订单分析，数据展示系统用Echarts图表的形式展示各分析模块的结果。

项目过程：
初期系统建设：
    数据采集系统：
        1.PC端和移动Web端采用js埋点技术，后端编写SDK,采用异步接收，多线程消息队列的方式发送数据给Nginx入口服务器
        2.采用Flume agent多agent串行传输的方式将数据从web服务器集群收集到HDFS上

    数据分析系统：
        1、使用mapreduce对日志文件进行数据清洗,存入Hbase表
        2、读取Hbase中的日志数据，用HiveSQL语句分析各个模块(编写udf、创建外部表，创建临时表，将结果保存到hive表中)，部分操作使用mapreduce
        3、使用sqoop将中间结果集导入Mysql

    数据展示系统：
        使用spring+springMVC+mybatis框架，读取Mysql中的分析结果数据,并使用ECharts进行报表展示

中期对工具类集成，优化Hive
后期向Spark转移，利用SparkSQL进行数据分析

职责：
    1、参与使用Flume数据收集和数据清洗工作

    2、使用hive，按照活跃度、会话时长与步长和时间维度对用户基本数据进行分析

    3、解决hive数据倾斜问题，并进行HiveSQL优化，取得了一定成果。
    比如在做两张GB级别大表join的时候，发现数据倾斜，经过三次优化，使作业的执行时间从1.5小时缩短到10分钟

    4、利用Oozie、Hue等协作框架进行任务调度、监控及可视化操作


----------------------------------------------------------------------

    from trackinfo a
    left outer join pm_info b
    on (a.ext_field7 = b.id)


    第一次优化：
    from trackinfo a
    left outer join pm_info b
    on (cast(a.ext_field7 as bigint) = b.id)
    关联时数据类型不一致，默认的hash操作会按bigint型的id进行分配，这样会导致所有string类型的ext_field7集中到一个reduce里面

    第二次优化：
    from trackinfo a
    left outer join pm_info b
    on (a.ext_field7 is not null
    and length(a.ext_field7) > 0
    and a.ext_field7 rlike'^[0-9]+$'
    and a.ext_field7 = b.id)

    发现trackinfo表的ext_field7字段缺失率很高（为空、字段长度为零、字段填充了非整数）
    如果左表关联字段ext_field7为无效字段时（为空、字段长度为零、字段填充了非整数），不去关联右表，
    由于空字段左关联以后取到的右表字段仍然为null，所以不会影响结果

    第三次优化：
    from trackinfo a
    left outer join pm_info b
    on (
        casewhen (a.ext_field7 is not null
            andlength(a.ext_field7) > 0
            anda.ext_field7 rlike '^[0-9]+$')
        then
            cast(a.ext_field7as bigint)
        else
            cast(ceiling(rand() * -65535)as bigint)
        end= b.id
    )

    左表中未关联的记录（ext_field7为空）将会全部聚集在一个reduce中进行处理
    若左表关联字段无效（为空、字段长度为零、字段填充了非整数），则在关联前将左表未关联字段设置为一个随机数，再去关联右表，
    使得左表的未关联记录key分布均匀


-----------------------------------------------------------------------------------------




=================参考======================================

用户基本数据分析、浏览器维度分析、地域维度分析、外链维度分析、用户浏览深度分析、事件分析、订单分析

1.初期使用mapreduce 清洗数据后，存入Hbase表中，通过Sqoop导入hive表
利用HiveSQL语句进行分析，并将中间结果集存储在Mysql中，汇总后交给前端人员使用SSM框架及HIghCharts进行报表展示
2.中期对工具类集成，优化HiveSQL语句进行分析
3.后期向Spark转移，利用SparkSQL进行数据的读写分析

项目成果：
通过Nginx及Flume收集的日志信息利用MapReduce和Hive完成数据清洗，并向SparkSQL转移，同时利用Zookeeper、Oozie、Hue等协作框架进行任务调度、监控及可视化操作







基本数据分析，地域维度分析，浏览器维度分析等8个子模块


目标:为领导层提供数据来了解网站，为产品经理提供修改网站的数据修改，为运营人员提供潜在的促销商机

数据来源:服务器数据 业务数据 用户行为数据 爬虫数据 购买数据

数据处理流程：数据收集  数据处理&分析 数据可视化  结果数据分析处理(数据应用）

适合行业：电商 旅游 游戏 金融产品

根据会员数，ip,Session的步长和时长

日活跃量1000万
200M
36byte*1000 =330M *30

业务需求 二次排序 需要排序
内存不够
hourly hive分析
存在哪些问题：
数据输出到Mysql
    自定义outputformat 导入数据   多个项目组之间的合作 redis mongodb oracle 自定义
    维度ID产生 多个JVM同时运行 可能导致mysql中数据出现重复
    使用Hadoop RPC 解决
数据倾斜：










==========

用户行为分析系统
用户基本数据分析
 用户/访客分析
   新增用户
   活跃用户
    新增用户
    老用户
   总用户
 会员分析:
昨日 新增会员 活跃会员 总会员
选择时间段
新会员和活跃会员时间段变化图
选择时间段
活跃会员
新会员
老会员
   新增会员
   活跃会员
    新会员
    老会员
   总会员
 会话分析
昨日 会话个数
昨日 会话长度
昨日 平均会话长度
选择时间段
会话长度
会话个数
选择时间段
用户平均会话长度
会员平均会话长度
   会话长度
   会话个数
   平均会话长度
 Hourly分析
每天每小时
选择时间段
用户数
一天的数据 柱状图 else 折线图
选择时间段
会话长度
选择时间段
会话个数
  用户数
  会话长度
  会话个数
浏览器维度分析
 用户分析
 会员分析
 会话分析
 PV分析
用户访问深度分析
用户访问记录的深度
选择时间段
访客访问深度变化
昨日访问深度
 访客访问深度变化
 昨日访问深度
地域维度分析
不同省份 用户和会员情况
 活跃访客地域分析
 不同地域的跳出率分析
外链维度分析
不同外链端带来的用户访问量
单天外链偏好
一个时间段外链偏好
搜索引擎使用偏好
访客搜索引擎偏好
 外链偏好分析:外链带来的活跃访客数量
 外链会话（跳出率）分析
事件分析
=================
事件分析
选择事件category名称
订单产生事件
选择时间：
订单产生事件事件转换
订单开始 订单产生事件
-->
添加购物车
确认订单信息
订单支付
订单分析
昨日：
订单数
支付成功订单数
退款订单个数
到昨天为止，订单总金额
当天订单金额
当天支付金额
当天退款金额
到昨天为止，订单退款总金额
选择时间段
订单数量
支付成功订单数量
退款订单数量
订单金额
退款金额
 订单数
 成功支付订单
 退款订单




1、extr中的代码是在windows平台上运行mr任务出现异常，修改后的源码，该部分需要根据实际需要进行操作。
	如果你不需要再windows上运行该代码，那么可以不考虑这部分内容
	如果你运行的异常，和课堂中的异常时一样的，那么你可以直接将该文件夹中的内容copy到项目中即可；
	但是如果你的异常和课堂中不一样，那么需要自行修改源码
2、java中的代码是在课堂中使用到的常量类、工具类；在编写代码之前，可以直接将代码copy进入项目中
3、resources中是配置文件
4、pom文件的内容可以直接copy， 也可以自己写
5、IP文件夹中是保存的ip解析数据的



===========================
事件流：是
目标：以事件流为单位 分析事件流中各个事件的触发次数
事件流特征:
每个事件存在来源/父事件 除了第一个事件
每个事件均会存在子事件 可以是一个也可以是多个
后一个事件的触发次数一定不会超过前一个事件的触发次数
数据流中各个事件的前后顺序
时间维度 平台维度

MapReduce：
input:从hbase中读取event事件的数据
Mapper：
时间顺序 及规则
category action

需要从mysql中读取事件维度数据 补全数据（补全数据流id、补全数据的序列号）


数据产生：
搜索点击-->搜索页面浏览-->课程详情点击-->购买按钮点击

===============







