kafka线上真实环境实战及调优进阶系列
1. kafka生产者吞吐量测试指标
kafka-producer-perf-test :是kafka提供的测试Producer性能脚本，通过脚本，可以计算出Producer在一段时间内的平均延时和吞吐量。

1.1 kafka-producer-perf-test
在kafka安装目录下面执行如下命令,生产环境中尽量让脚本运行较长的时间，才会有意义：

bin/kafka-producer-perf-test.sh --topic test --num-records 500000 --record-size 200 --througthput -1 --producer-props bootstrap.servers=bd-master:9092,bd-slave1=9092,bd-slave3=9092 acks=1

1.2 测试结果分析如下：
500000 records sent ,41963 records/sec (8.00 MB/sec),2362.85 ms/avg latency ,3513.00 ms max latency ,2792ms 50h ,3144ms 95th ,3364 ms 99h,3503ms 99.9th

看到上面的结果肯定蒙了，看我细细讲来：
kafka 的平均吞吐量是8.00 MB/sec ，即占用64Mb/s左右的带宽，平均每一秒发送41963条消息。平均延时为2362.85 ms，最大延时为3513.00 ms，95%的消息发送需要3144ms，99%的消息发送需要3364ms，99.9%的消息发送需要3503ms。

2. kafka消费者吞吐量指标说明：
2.1 kafka-consumer-perfs
我们总共测试500万条数据量
bin/kafka-consumer-perfs-test.sh --broker-list bd-master:9092,bd-slave1=9092,bd-slave3=9092 --message-size 200 --messages 500000 --topic test

2.2 得到如下结果：
2018-10-28 9:39:02 95.4188 92.2313 500271 484289
看到上面的结果肯定蒙了，看我细细讲来：
该环境下，1s内总共消费了95.4188MB消息，吞吐量为92.2313MB/s,也即736Mb/s。

3 结语
秦凯新 于深圳 2018-10-28


===================================================================================

kafka生产环境规划-kafka 商业环境实战

本套系列博客从真实商业环境抽取案例进行总结和分享，并给出kafka商业应用的调优建议和集群环境容量规划等内容，请持续关注本套博客。版权声明：本套kafka调优系列版权归作者(秦凯新)所有，禁止转载，欢迎学习。

kafka真实环境部署规划
1. 操作系统选型
因为kafka服务端代码是Scala语言开发的，因此属于JVM系的大数据框架，目前部署最多的3类操作系统主要由Linux ，OS X 和Windows,但是部署在Linux数量最多，为什么呢？因为I/O模型的使用和数据网络传输效率两点。

第一：Kafka新版本的Clients在设计底层网络库时采用了Java的Select模型，而在Linux实现机制是epoll,感兴趣的读者可以查询一下epoll和select的区别，明确一点就是：kafka跑在Linux上效率更高，因为epoll取消了轮询机制，换成了回调机制，当底层连接socket数较多时，可以避免CPU的时间浪费。
第二：网络传输效率上。kafka需要通过网络和磁盘进行数据传输，而大部分操作系统都是通过Java的FileChannel.transferTo方法实现，而Linux操作系统则会调用sendFile系统调用，也即零拷贝（Zero Copy 技术），避免了数据在内核地址空间和用户程序空间进行重复拷贝。
2. 磁盘类型规划
机械磁盘（HDD） 一般机械磁盘寻道时间是毫秒级的，若有大量随机I/O，则将会出现指数级的延迟，但是kafka是顺序读写的，因此对于机械磁盘的性能也是不弱的，所以，基于成本问题可以考虑。
固态硬盘（SSD） 读写速度可观，没有成本问题可以考虑。
JBOD (Just Bunch Of Disks ) 经济实惠的方案，对数据安全级别不是非常非常高的情况下可以采用，建议用户在Broker服务器上设置多个日志路径，每个路径挂载在不同磁盘上，可以极大提升并发的日志写入速度。
RAID 磁盘阵列 常见的RAID是RAID10，或者称为（RAID 1+0） 这种磁盘阵列结合了磁盘镜像和磁盘带化技术来保护数据，因为使用了磁盘镜像技术，使用率只有50%，注意，LinkedIn公司采用的就是RAID作为存储来提供服务的。那么弊端在什么地方呢？如果Kafka副本数量设置为3，那么实际上数据将存在6倍的冗余数据，利用率实在太低。因此，LinkedIn正在计划更改方案为JBOD.
3. 磁盘容量规划
我们公司物联网平台每天大约能够产生一亿条消息，假设副本replica设置为2 （其实我们设置为3），数据留存时间为1周，平均每条上报事件消息为1K左右，那么每天产生的消息总量为：1亿 乘 2 乘 1K 除以 1000 除以 1000 =200G磁盘。预留10%的磁盘空间，为210G。一周大约为1.5T。采用压缩，平均压缩比为0.5，整体磁盘容量为0.75T。 关联因素主要有：

新增消息数
副本数
是否启用压缩
消息大小
消息保留时间
4. 内存容量规划
kafka对于内存的使用，并不过多依赖JVM 内存,而是更多的依赖操作系统的页缓存，consumer若命中页缓存，则不用消耗物理I/O操作。一般情况下，java堆内存的使用属于朝生夕灭的，很快会被GC,一般情况下，不会超过6G，对于16G内存的机器，文件系统page cache 可以达到10-14GB。

怎么设计page cache，可以设置为单个日志段文件大小，若日志段为10G,那么页缓存应该至少设计为10G以上。
堆内存最好不要超过6G。
5. CPU选择规划
kafka不属于计算密集型系统，因此CPU核数够多就可以，而不必追求时钟频率，因此核数选择最好大于8。

6. 网络带宽决定Broker数量
带宽主要有1Gb/s 和10 Gb/s 。我们可以称为千兆位网络和万兆位网络。举例如下： 我们的物联网系统一天每小时都要处理1Tb的数据，我们选择1Gb/b带宽，那么需要选择多少机器呢？

假设网络带宽kafka专用，且分配给kafka服务器70%带宽，那么单台Borker带宽就是710Mb/s，但是万一出现突发流量问题，很容易把网卡打满，因此在降低1/3,也即240Mb/s。因为1小时处理1TTB数据，每秒需要处理292MB,1MB=8Mb，也就是2336Mb数据，那么一小时处理1TB数据至少需要2336/240=10台Broker数据。冗余设计，最终可以定为20台机器。

典型推荐
cpu 核数 32
内存 32GB
磁盘 3TB 7200转 SAS盘三块
带宽 1Gb/s

================================================

　　Kafka提供了非常多有用的工具，如Kafka设计解析（三）- Kafka High Availability （下）中提到的运维类工具——Partition Reassign Tool，Preferred Replica Leader Election Tool，Replica Verification Tool，State Change Log Merge Tool。本文将介绍Kafka提供的性能测试工具，Metrics报告工具及Yahoo开源的Kafka Manager。

Kafka性能测试脚本
$KAFKA_HOME/bin/kafka-producer-perf-test.sh 该脚本被设计用于测试Kafka Producer的性能，主要输出4项指标，总共发送消息量（以MB为单位），每秒发送消息量（MB/second），发送消息总数，每秒发送消息数（records/second）。除了将测试结果输出到标准输出外，该脚本还提供CSV Reporter，即将结果以CSV文件的形式存储，便于在其它分析工具中使用该测试结果
$KAFKA_HOME/bin/kafka-consumer-perf-test.sh 该脚本用于测试Kafka Consumer的性能，测试指标与Producer性能测试脚本一样
Kafka Metrics
　　Kafka使用Yammer Metrics来报告服务端和客户端的Metric信息。Yammer Metrics 3.1.0提供6种形式的Metrics收集——Meters，Gauges，Counters，Histograms，Timers，Health Checks。与此同时，Yammer Metrics将Metric的收集与报告（或者说发布）分离，可以根据需要自由组合。目前它支持的Reporter有Console Reporter，JMX Reporter，HTTP Reporter，CSV Reporter，SLF4J Reporter，Ganglia Reporter，Graphite Reporter。因此，Kafka也支持通过以上几种Reporter输出其Metrics信息。

使用JConsole查看单服务器Metrics
　　使用JConsole通过JMX，是在不安装其它工具（既然已经安装了Kafka，就肯定安装了Java，而JConsole是Java自带的工具）的情况下查看Kafka服务器Metrics的最简单最方便的方法之一。
　　首先必须通过为环境变量JMX_PORT设置有效值来启用Kafka的JMX Reporter。如export JMX_PORT=19797。然后即可使用JConsole通过上面设置的端口来访问某一台Kafka服务器来查看其Metrics信息，如下图所示。

JConsole Kafka JMX
　　使用JConsole的一个好处是不用安装额外的工具，缺点很明显，数据展示不够直观，数据组织形式不友好，更重要的是不能同时监控整个集群的Metrics。在上图中，在kafka.cluster->Partition->UnderReplicated->topic4下，只有2和5两个节点，这并非因为topic4只有这两个Partition的数据是处于复制状态的。事实上，topic4在该Broker上只有这2个Partition，其它Partition在其它Broker上，所以通过该服务器的JMX Reporter只看到了这两个Partition。

通过Kafka Manager查看整个集群的Metrics
　　Kafka Manager是Yahoo开源的Kafka管理工具。它支持如下功能

管理多个集群
方便查看集群状态
执行preferred replica election
批量为多个Topic生成并执行Partition分配方案
创建Topic
删除Topic（只支持0.8.2及以上版本，同时要求在Broker中将delete.topic.enable设置为true）
为已有Topic添加Partition
更新Topic配置
在Broker JMX Reporter开启的前提下，轮询Broker级别和Topic级别的Metrics
监控Consumer Group及其消费状态
支持添加和查看LogKafka
　　安装好Kafka Manager后，添加Cluster非常方便，只需指明该Cluster所使用的Zookeeper列表并指明Kafka版本即可，如下图所示。

Add Cluster

　　这里要注意，此处添加Cluster是指添加一个已有的Kafka集群进入监控列表，而非通过Kafka Manager部署一个新的Kafka Cluster，这一点与Cloudera Manager不同。

Kafka Benchmark
　　Kafka的一个核心特性是高吞吐率，因此本文的测试重点是Kafka的吞吐率。
　　本文的测试共使用6台安装Red Hat 6.6的虚拟机，3台作为Broker，另外3台作为Producer或者Consumer。每台虚拟机配置如下

CPU：8 vCPU， Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz，2 Sockets，4 Cores per socket，1 Thread per core
内存：16 GB
磁盘：500 GB
　　开启Kafka JMX Reporter并使用19797端口，利用Kafka-Manager的JMX polling功能监控性能测试过程中的吞吐率。

　　本文主要测试如下四种场景，测试的指标主要是每秒多少兆字节数据，每秒多少条消息。

Producer Only
　　这组测试不使用任何Consumer，只启动Broker和Producer。

Producer Number VS. Throughput
　　实验条件：3个Broker，1个Topic，6个Partition，无Replication，异步模式，消息Payload为100字节
　　测试项目：分别测试1，2，3个Producer时的吞吐量
　　测试目标：如Kafka设计解析（一）- Kafka背景及架构介绍所介绍，多个Producer可同时向同一个Topic发送数据，在Broker负载饱和前，理论上Producer数量越多，集群每秒收到的消息量越大，并且呈线性增涨。本实验主要验证该特性。同时作为性能测试，本实验还将监控测试过程中单个Broker的CPU和内存使用情况
　　测试结果：使用不同个数Producer时的总吞吐率如下图所示
Producer Number VS. Throughput

　　由上图可看出，单个Producer每秒可成功发送约128万条Payload为100字节的消息，并且随着Producer个数的提升，每秒总共发送的消息量线性提升，符合之前的分析。

　　性能测试过程中，Broker的CPU和内存使用情况如下图所示。
Broker CPU Usage

　　由上图可知，在每秒接收约117万条消息（3个Producer总共每秒发送350万条消息，平均每个Broker每秒接收约117万条）的情况下，一个Broker的CPU使用量约为248%，内存使用量为601 MB。

Message Size VS. Throughput
　　实验条件：3个Broker，1个Topic，6个Partition，无Replication，异步模式，3个Producer
　　测试项目：分别测试消息长度为10，20，40，60，80，100，150，200，400，800，1000，2000，5000，10000字节时的集群总吞吐量
　　测试结果：不同消息长度时的集群总吞吐率如下图所示
Message Size VS. Throughput

　　由上图可知，消息越长，每秒所能发送的消息数越少，而每秒所能发送的消息的量（MB）越大。另外，每条消息除了Payload外，还包含其它Metadata，所以每秒所发送的消息量比每秒发送的消息数乘以100字节大，而Payload越大，这些Metadata占比越小，同时发送时的批量发送的消息体积越大，越容易得到更高的每秒消息量（MB/s）。其它测试中使用的Payload为100字节，之所以使用这种短消息（相对短）只是为了测试相对比较差的情况下的Kafka吞吐率。

Partition Number VS. Throughput
　　实验条件：3个Broker，1个Topic，无Replication，异步模式，3个Producer，消息Payload为100字节
　　测试项目：分别测试1到9个Partition时的吞吐量
　　测试结果：不同Partition数量时的集群总吞吐率如下图所示
Partition Number VS. Throughput

　　由上图可知，当Partition数量小于Broker个数（3个）时，Partition数量越大，吞吐率越高，且呈线性提升。本文所有实验中，只启动3个Broker，而一个Partition只能存在于1个Broker上（不考虑Replication。即使有Replication，也只有其Leader接受读写请求），故当某个Topic只包含1个Partition时，实际只有1个Broker在为该Topic工作。如之前文章所讲，Kafka会将所有Partition均匀分布到所有Broker上，所以当只有2个Partition时，会有2个Broker为该Topic服务。3个Partition时同理会有3个Broker为该Topic服务。换言之，Partition数量小于等于3个时，越多的Partition代表越多的Broker为该Topic服务。如前几篇文章所述，不同Broker上的数据并行插入，这就解释了当Partition数量小于等于3个时，吞吐率随Partition数量的增加线性提升。
　　当Partition数量多于Broker个数时，总吞吐量并未有所提升，甚至还有所下降。可能的原因是，当Partition数量为4和5时，不同Broker上的Partition数量不同，而Producer会将数据均匀发送到各Partition上，这就造成各Broker的负载不同，不能最大化集群吞吐量。而上图中当Partition数量为Broker数量整数倍时吞吐量明显比其它情况高，也证实了这一点。

Replica Number VS. Throughput
　　实验条件：3个Broker，1个Topic，6个Partition，异步模式，3个Producer，消息Payload为100字节
　　测试项目：分别测试1到3个Replica时的吞吐率
　　测试结果：如下图所示
Replica Number VS. Throughput

　　由上图可知，随着Replica数量的增加，吞吐率随之下降。但吞吐率的下降并非线性下降，因为多个Follower的数据复制是并行进行的，而非串行进行。


Consumer Only
　　实验条件：3个Broker，1个Topic，6个Partition，无Replication，异步模式，消息Payload为100字节
　　测试项目：分别测试1到3个Consumer时的集群总吞吐率
　　测试结果：在集群中已有大量消息的情况下，使用1到3个Consumer时的集群总吞吐量如下图所示

Consumer Number VS. Throughput

　　由上图可知，单个Consumer每秒可消费306万条消息，该数量远大于单个Producer每秒可消费的消息数量，这保证了在合理的配置下，消息可被及时处理。并且随着Consumer数量的增加，集群总吞吐量线性增加。
　　根据Kafka设计解析（四）- Kafka Consumer设计解析所述，多Consumer消费消息时以Partition为分配单位，当只有1个Consumer时，该Consumer需要同时从6个Partition拉取消息，该Consumer所在机器的I/O成为整个消费过程的瓶颈，而当Consumer个数增加至2个至3个时，多个Consumer同时从集群拉取消息，充分利用了集群的吞吐率。

Producer Consumer pair
　　实验条件：3个Broker，1个Topic，6个Partition，无Replication，异步模式，消息Payload为100字节
　　测试项目：测试1个Producer和1个Consumer同时工作时Consumer所能消费到的消息量
　　测试结果：1,215,613 records/second


